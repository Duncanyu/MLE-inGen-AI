{"id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "authors": ["Haokun Lin", "Haobo Xu", "Yichen Wu", "Ziyu Guo", "Renrui Zhang", "Zhichao Lu", "Ying Wei", "Qingfu Zhang", "Zhenan Sun"], "source": "http://arxiv.org/abs/2508.14896v1", "page_count": 15, "text": "Quantization Meets dLLMs: A Systematic Study of\nPost-training Quantization for Diffusion LLMs\nHaokun Lin∗1,3, Haobo Xu∗2, Yichen Wu3,4, Ziyu Guo5, Renrui Zhang5 ,\nZhichao Lu3, Ying Wei6, Qingfu Zhang3, Zhenan Sun1\n∗Equal Contribution\n1 NLPR & MAIS, Institute of Automation, CAS\n2 Tsinghua University\n3 City University of Hong Kong\n4 Harvard University\n5 The Chinese University of Hong Kong\n6 Zhejiang University\nAbstract\nRecent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language generation\ntasks, leveraging full attention and denoising-based decoding strategies. However,\nthe deployment of these models on edge devices remains challenging due to their\nmassive parameter scale and high resource demands. While post-training quantiza-\ntion (PTQ) has emerged as a widely adopted technique for compressing AR LLMs,\nits applicability to dLLMs remains largely unexplored. In this work, we present the\nfirst systematic study on quantizing diffusion-based language models. We begin\nby identifying the presence of activation outliers, characterized by abnormally\nlarge activation values that dominate the dynamic range. These outliers pose a key\nchallenge to low-bit quantization, as they make it difficult to preserve precision\nfor the majority of values. More importantly, we implement state-of-the-art PTQ\nmethods and conduct a comprehensive evaluation across multiple task types and\nmodel variants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this multi-perspective\nevaluation, we offer practical insights into the quantization behavior of dLLMs\nunder different configurations. We hope our findings provide a foundation for\nfuture research in efficient dLLM deployment. All codes and experimental setups\nwill be released to support the community.\n1\nIntroduction\nLarge language models (LLMs) have achieved remarkable success in a wide range of text generation\ntasks, with auto-regressive architectures—such as GPT [Brown et al., 2020a,b, Achiam et al., 2023],\nLLaMA [Touvron et al., 2023a,b, Dubey et al., 2024], and the Qwen [Bai et al., 2023, Qwen et al.,\n2025, Yang et al., 2025a] series—dominating recent advances in both research and application.\nRecently, diffusion-based large language models (dLLMs) have emerged as a promising alternative\nfor natural language generation [Nie et al., 2025, Zhu et al., 2025, Ye et al., 2025a, Gong et al., 2024,\nSong et al., 2025]. By leveraging bidirectional context encoding and iterative denoising, dLLMs offer\nfiner-grained control over the generation process compared to traditional auto-regressive approaches.\nDespite their potential, the efficient deployment of dLLMs remains challenging, as the increased\nnumber of model parameters often leads to significantly higher memory usage and computational\ncost [Li et al., 2025, Yu et al., 2025].\nCurrent efforts toward optimizing dLLM inference have primarily focused on designing specialized\nkey-value (KV) cache mechanisms [Wu et al., 2025, Ma et al., 2025, Liu et al., 2025b, Wang\nTechnical Report\n\n(a2) LLaDA_8B_Ins_Layer17_Attn_q_proj\n(b2) LLaDA_8B_Ins_Layer27_Attn_out_proj\n(c2) LLaDA_8B_Ins_Layer19_FFN_ff_proj\n(d2) LLaDA_8B_Ins_Layer31_FFN_ff_out\n(2). LLaDA-8B-Instruct\nMassive\n(a1) LLaDA_8B_Layer1_Attn_q_proj\n(b1) LLaDA_8B_Layer27_Attn_out_proj\n(c1) LLaDA_8B_Layer31_FFN_ff_proj\n(d1) LLaDA_8B_Layer29_FFN_ff_out\n(1). LLaDA-8B-Base\nMassive\nFigure 1: Visualizations of activation outliers in LLaDA-8B-Base (1) and LLaDA-8B-Instruct (2).\nOutliers are observed at the inputs of various linear layers and can be classified as Normal Outliers\n(a(1)–c(1)/a(2)–c(2)), with relatively large magnitudes across tokens, and Massive Outliers (d(1),\nd(2)), with extremely large values on a few tokens. Notably, these massive outliers are identified at\nthe second linear layer of the feed-forward network (FFN) module.\net al., 2025]. However, quantization [Li et al., 2024, Liu et al., 2025a, Wei et al., 2025], a well-\nestablished yet orthogonal technique for compressing and accelerating neural networks, has been\nlargely underexplored in the context of dLLMs. In the domain of auto-regressive LLMs, post-training\nquantization(PTQ) [Chee et al., 2024, Ashkboos et al., 2023, Tseng et al., 2024, Zhao et al., 2023]\nhas been widely adopted to reduce the memory footprint of weights and activations, and to enable\nfaster inference through kernel-level optimization. Yet, how well existing PTQ techniques generalize\nto diffusion LLMs remains an open and intriguing question.\nIn this paper, we present a comprehensive study on the quantization of diffusion-based large language\nmodels (dLLMs). First, we identify that dLLMs exhibit clear activation outliers—i.e., unusually\nlarge activation values—which are known to be a key challenge for low-bit quantization [Dettmers\net al., 2022, Xiao et al., 2023, Sun et al., 2024]. Specifically, as shown in Figure 1 and 2, we observe\nsuch outliers across multiple layers and input activations in LLaDA-Base, LLaDA-Instruct [Nie et al.,\n2025], and Dream [Ye et al., 2025a] models, suggesting that this is a common phenomenon across\ndifferent dLLMs. Second, we implement state-of-the-art weight-only [Lin et al., 2023, Frantar et al.,\n2022] and weight-activation quantization [Xiao et al., 2023, Ashkboos et al., 2024, Lin et al., 2024b]\nmethods on representative diffusion models and conduct a detailed analysis from the following\nperspectives:\n• Bit-width effects: We find that 4-bit is the most effective configuration for weight-only quantiza-\ntion, while 8-bit is recommended for weight-activation quantization as a near-lossless setting.\n• Quantization methods: Through extensive evaluation, we observe that GPTQ consistently outper-\nforms AWQ across most tasks. For weight-activation quantization, rotation-based methods such as\nDuQuant and QuaRot demonstrate clear advantages over SmoothQuant.\n• Task type sensitivity: While most PTQ methods perform competitively on general QA benchmarks,\nwe observe notable degradation on more complex tasks such as math reasoning and code generation.\n• Model type robustness: Our results show that the instruction-tuned LLaDA model exhibits greater\nrobustness to quantization compared to the base counterpart.\nTo the best of our knowledge, this is the first systematic evaluation of post-training quantization on\ndiffusion LLMs. We hope our findings provide valuable guidance for the community and inspire\nfurther research toward efficient and deployable dLLMs.\n2\n\n(a) Dream_7B_Layer1_Attn_q_proj\n(b) Dream_7B_Layer25_Attn_o_proj\n(c) Dream_7B_Layer27_MLP_up_proj\n(d) Dream_7B_Layer4_MLP_down_proj\nDream-7B-Base\nMassive\nFigure 2: Visualizations of activation outliers in Dream-7B-Base. We observe relatively large normal\noutliers in the input to the FFN up-projection layer (c), while the massive outliers (d) exhibit smaller\npeak values compared to those in LLaDA models (Figure 1).\n2\nRelated Work\n2.1\nDiffusion Language Model\nDiffusion models have achieved remarkable success in image, video, and audio generation by learning\nto reverse a forward noise process [Jiang et al., 2025, Zhao et al., 2024a]. However, applying diffusion\nto language generation presents unique challenges due to the discrete nature of textual data. To\naddress this, DiffusionBERT [He et al., 2022] leverages a BERT [Devlin et al., 2019] architecture to\nmodel the reverse dynamics of a discrete diffusion process with an absorbing state, as proposed in\nD3PM [Austin et al., 2021a].\nMore recently, Masked Diffusion Models (MDMs) [Lou et al., 2023, Ou et al., 2024, Shi et al., 2024]\nhave drawn increasing attention by adopting a forward process that progressively replaces input\ntokens with a designated [MASK] token. This year, efforts have been made to scale up MDMs to\nthe billion-parameter regime. Representative examples include LLaDA-8B [Nie et al., 2025], which\nutilizes a bidirectional Transformer as the mask denoiser and achieves performance comparable to\nLLaMA [Dubey et al., 2024], and Dream [Ye et al., 2025a], which is initialized from a pre-trained\nautoregressive model and delivers competitive generation capabilities. These advancements indicate\nthat diffusion-based approaches offer a viable alternative paradigm for language modeling.\nDespite these encouraging results, the deployment of diffusion large language models (dLLMs) [Gong\net al., 2024, Yang et al., 2025c] remains constrained by the computational demands of Transformer-\nbased architectures, which involve hundreds of millions of parameters. To address this, we explore\nthe potential of extending established post-training quantization techniques from conventional LLMs\nto the dLLM models, aiming to reduce memory footprint and accelerate inference while preserving\ngeneration quality. Notably, some recent works [Wu et al., 2025, Liu et al., 2025b, Ma et al., 2025]\npropose caching strategies to accelerate the inference of dLLMs. Our work is orthogonal to these\nefforts and can be seamlessly integrated by quantizing dLLM caches.\n2.2\nNetwork Quantization\nCompared to pruning and distillation [Lin et al., 2024a, Zhang et al., 2024, Xing et al., 2025, Ye\net al., 2025c,b], quantization has been extensively studied as an effective technique to compress\nneural networks by using low-bit representations for high-precision tensors [Zhao et al., 2024b, Xu\net al., 2024, Yang et al., 2025b, Huang and Wu, 2025]. Existing methods are typically categorized\ninto two groups: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ [Wu\net al., 2024, Yang et al., 2024] applies quantization after model training, while QAT [Tao et al.,\n2022, Chen et al., 2024, 2025] incorporates quantization effects during training. Due to the high\ncomputational cost of training large language models (LLMs), PTQ has become increasingly popular\nfor its efficiency and ability to preserve model performance without retraining [Liu et al., 2024, Dong\net al., 2024]. In this work, we follow this paradigm and focus on applying PTQ to dLLMs.\nWeight-only quantization compresses the model by quantizing weight matrices, effectively reducing\nmodel size and memory access during inference. For example, GPTQ [Frantar et al., 2022] extends\nthe Optimal Brain Quantization [Frantar and Alistarh, 2022] algorithm to LLMs, AWQ [Lin et al.,\n3\n\n2023] introduces a reparameterization strategy to alleviate the difficulty of weight quantization, and\nSqueezeLLM [Kim et al., 2023] employs non-uniform quantization to improve compression quality.\nWeight-activation quantization quantizes both the model weights and input activations, enabling\nfurther inference acceleration by leveraging integer matrix multiplication kernels. SmoothQuant [Xiao\net al., 2023] proposes to shift the quantization difficulty from activations to weights via scaling.\nOmniQuant [Shao et al., 2023] jointly optimizes clipping thresholds and scaling factors for improved\nquantization fidelity. More recently, rotation-based methods [Lin et al., 2024c, Hu et al., 2025] have\ndemonstrated superior performance: QuaRot [Ashkboos et al., 2024] introduces Hadamard-based\nrotation to smooth the weight-activation landscape, while DuQuant [Lin et al., 2024b] leverages\noutlier-aware rotation matrices and channel permutation to better align the activation distribution\nwith quantization-friendly structures.\nIn this work, we provide a comprehensive evaluation of state-of-the-art LLM-oriented PTQ methods\napplied to diffusion-based language models. All methods are re-implemented on dLLMs, and we\npresent in-depth analyses and insights into their quantization performance.\n3\nPreliminary and Observation\n3.1\nMasked Diffusion Model\nMasked diffusion model is a variant of diffusion-based generative models that incorporates a binary\nmask into the denoising process. Instead of reconstructing the entire input, the model focuses on\npredicting the corrupted or missing regions while preserving the observed parts. Specifically, given an\ninput x and a mask m, the forward process adds Gaussian noise to the unmasked regions, producing\na noised sample xt at step t. The reverse process is then parameterized by a neural network ϵθ, which\nestimates the noise conditioned on both the timestep and the mask. The training objective is,\nLMDM = Ex,m,ϵ,t\n\u0002\n∥ϵ −ϵθ(xt, m, t)∥2 \u0003\n,\nwhere ϵ denotes the Gaussian noise, and ϵθ learns to predict and remove it under the masking\nconstraint.\n3.2\nQuantization\nQuantization coverts the floating-point tensor X into a low-bit integer Xq. Specifically, the b-bit\nuniform quantization can be represented as:\nXq = clamp\n\u0012\u0016X\ns\n\u0019\n+z, 0, 2b −1\n\u0013\n, where s = max(X) −min(X)\n2b −1\n, z = −\n\u0016min(X)\ns\n\u0019\n.\n(1)\nThe notation ⌊·⌉means the nearest rounding operation, s is the quantization step size and z denotes\nthe zero point.\n3.3\nOutliers in dLLMs\nOutliers, a prominent characteristic of large language models (LLMs), are primarily determined by\nrelatively large activation values [Dettmers et al., 2022]. These outliers are typically categorized\ninto two types: normal outliers and massive outliers [Lin et al., 2024b]. Normal outliers [Xiao\net al., 2023] refer to activations across all tokens with relatively large magnitudes, and they are the\nmore prevalent type. Massive outliers [Sun et al., 2024, Liu et al., 2024], on the other hand, exhibit\nsignificantly larger values at a limited set of tokens. These outliers present substantial challenges for\nLLM quantization. Whether dLLMs contain these outliers remains an important yet under-explored\nquestion. In this work, we provide a detailed preliminary exploration and identify the presence of\noutliers in dLLMs.\nWe first identify the presence of activation outliers in diffusion-based language models. Specifically,\nwe randomly sample a batch of calibration data from the WikiText-2 dataset [Merity et al., 2016]\nand use it as input for a single forward pass to visualize the activation distributions across different\n4\n\nlayers. As shown in Figure 1, we observe clear outliers in the input activations of both LLaDA-8B-\nbase and LLaDA-8B-instruct. These outliers can be categorized into two types: Normal Outliers\nand Massive Outliers, consistent with the taxonomy observed in standard LLMs. Interestingly,\nthe Massive Outliers tend to occur in the second linear layer of the feed-forward network (FFN)\nmodules, mirroring patterns reported in previous studies on conventional LLMs [An et al., 2025].\nHowever, compared to LLMs, the Normal Outliers in LLaDA exhibit slightly lower magnitudes,\nindicating a less extreme but still significant deviation. Another key difference is that massive\noutliers in dLLMs appear across more tokens, rather than being restricted to only a few tokens as\nin LLMs. This broader distribution increases the difficulty of weight-activation quantization, as it\nreduces the effectiveness of global clipping or scaling strategies. This observation is corroborated\nby the near-zero performance of SmoothQuant under W4A4 settings (see Table 4), suggesting that\nexisting outlier-handling strategies may be insufficient for dLLMs in low-bit quantization regimes.\nFurthermore, we also detect similar outlier patterns in the Dream-7B model, as visualized in Figure 2.\nThis indicates that the existence of outliers is not specific to a particular model architecture, but rather\na general phenomenon across diffusion-based language models. These findings highlight the need for\ncareful handling of outliers during the quantization process, especially when targeting both weights\nand activations under aggressive bit-width constraints.\n4\nQuantizing Diffusion LLM\nIn this section, we conduct experiments to address the overarching question: How does quantization\naffect diffusion-based language models? To systematically explore this, we further investigate the\nfollowing sub-questions:\n• RQ1: What are the preferred bit-widths for weight-only and weight-activation quantization?\n• RQ2: What are the most effective quantization methods for dLLMs?\n• RQ3: How do different task categories influence the performance of quantized dLLMs?\n• RQ4: How does quantization affect different types of dLLMs?\n4.1\nExperimental Setup\nEvaluated dLLMs and Quantization Baselines.\nWe conduct comprehensive evaluations on three\nrecent diffusion-based language models, LLaDA-8B-Base, LLaDA-8B-Instruct [Nie et al., 2025] and\nDream 7B-Base [Ye et al., 2025a]. For weight-only quantization, we adopt state-of-the-art baselines\nGPTQ [Frantar et al., 2022] and AWQ [Lin et al., 2023], which are widely used in LLM quantization.\nWe utilize group-wise per-channel quantization and set the group size to 128. For weight-activation\nquantization, we evaluate SmoothQuant [Xiao et al., 2023] as well as recent rotation-based approaches,\nincluding QuaRot [Ashkboos et al., 2024] and DuQuant [Lin et al., 2024b]. Following standard\npractice, we apply per-channel quantization to weights and per-token quantization to activations.\nWe select calibration data (128 samples) from WiKiText2 [Merity et al., 2016] for baselines, except\nPile [Gao et al., 2020] for AWQ. More details are illustrated in Appendix A.\nEvaluation Benchmarks.\nWe evaluate the performance of quantized dLLMs across three task\ncategories, following the setup of LLaDA [Nie et al., 2025]: 1). General knowledge tasks, including\nMMLU [Hendrycks et al., 2020], ARC-E, ARC-C [Clark et al., 2018], Hellaswag [Zellers et al., 2019],\nWinoGrande [Sakaguchi et al., 2021], and PIQA [Bisk et al., 2020]; 2). Mathematical reasoning tasks,\nsuch as GSM8K [Cobbe et al., 2021] and Math [Hendrycks et al., 2021]; and 3).Code generation\ntasks, including HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021b]. These benchmarks\ncollectively provide a comprehensive assessment of quantized dLLMs from multiple perspectives.\nEvaluation Metrics.\nWe report accuracy on widely used QA and math benchmarks, and adopt\nPass@1 as the evaluation metric for code generation tasks. Performance degradation relative to full-\nprecision models is used as the primary metric for assessing different quantized dLLMs. Following\n[Liu et al., 2025a], we categorize the performance degradation compared to full-precision models\ninto three levels: negligible(<1%), moderate (1–4%), and significant (>4%).\n5\n\nTable 1: Model performance on general tasks under weight-only quantization.\nModel\nSetting\nMethod\nWinoGrande\nPIQA\nARC-C\nARC-E\nHellaswag\nMMLU 5-shot\nAvg\nDrop\nLLaDA-8B\nFP Model\n-\n69.9\n74.6\n46.4\n71.1\n70.7\n65.7\n65.5\n-\nW4A16 g128\nGPTQ\n69.7\n73.9\n47.9\n72.5\n70.4\n64.7\n65.3\n↓0.3%\nAWQ\n67.3\n70.3\n44.5\n73.4\n68.4\n65.6\n63.2\n↓3.5%\nW3A16 g128\nGPTQ\n67.2\n73.3\n45.7\n71.1\n68.8\n63.5\n63.7\n↓2.7%\nAWQ\n66.4\n69.2\n42.8\n71.8\n66.4\n64.0\n61.8\n↓5.6%\nLLaDA-8B\n-Instruct\nFP Model\n-\n70.2\n71.3\n54.3\n75.9\n68.6\n64.0\n65.7\n-\nW4A16 g128\nGPTQ\n69.2\n74.2\n54.8\n77.5\n68.3\n63.4\n66.0\n↑0.3%\nAWQ\n68.8\n71.0\n53.3\n76.1\n68.1\n63.4\n64.9\n↓0.1%\nW3A16 g128\nGPTQ\n67.4\n73.7\n50.7\n76.4\n66.7\n62.1\n64.1\n↓2.4%\nAWQ\n66.3\n69.8\n50.5\n74.7\n66.4\n62.3\n63.1\n↓4.0%\nTable 2: Model performance on general tasks under weight-activation quantization.\nModel\nSetting\nMethod\nWinoGrande\nPIQA\nARC-C\nARC-E\nMMLU\nAvg\nDrop\nLLaDA-8B\nFP Model\n-\n69.9\n74.6\n46.4\n71.1\n65.7\n65.5\n-\nW8A8\nSmoothQuant\n67.7\n70.8\n45.5\n70.5\n65.0\n63.9\n↓2.5%\nQuaRot\n68.6\n71.1\n45.2\n70.8\n66.1\n64.4\n↓1.8%\nDuQuant\n67.9\n70.4\n45.9\n71.4\n66.0\n64.3\n↓1.9%\nW4A4\nSmoothQuant\n49.4\n58.8\n29.2\n40.9\n27.1\n41.1\n↓37.3%\nQuaRot\n63.4\n68.1\n43.7\n69.2\n61.8\n59.2\n↓6.6%\nDuQuant\n64.9\n69.3\n42.8\n70.0\n64.0\n62.2\n↓5.1%\nLLaDA-8B\n-Instruct\nFP Model\n-\n70.2\n71.3\n54.3\n75.9\n64.0\n67.1\n-\nW8A8\nSmoothQuant\n69.6\n72.1\n53.5\n75.9\n64.0\n67.0\n↓0.2%\nQuaRot\n69.1\n71.3\n54.1\n76.2\n64.1\n67.0\n↓0.3%\nDuQuant\n68.8\n71.8\n54.6\n76.2\n63.6\n67.0\n↓0.2%\nW4A4\nSmoothQuant\n52.3\n65.1\n34.0\n54.0\n32.8\n47.7\n↓29.0%\nQuaRot\n65.2\n69.8\n51.3\n75.1\n61.1\n64.5\n↓3.9%\nDuQuant\n66.4\n72.2\n52.7\n74.8\n61.2\n65.4\n↓2.5%\n4.2\nIdeal Quantization Bit Precision (RQ1)\n4-bit is the Recommended Choice for Weight-Only Quantization.\nWe observe that both GPTQ\nand AWQ perform well on general commonsense QA and math tasks under 4-bit quantization (Table 1\nand Table 3). In most cases, the performance degradation remains within the negligible to moderate\nrange (i.e., <4%). For example, 4-bit GPTQ-quantized LLaDA-8B-instruct slightly improves the\naverage accuracy on six QA tasks from 65.7% to 66.0%, and shows only a minor drop of 0.6%\non the MATH and GSM8K benchmarks. In contrast, reducing the quantization bit-width to 3-bit\nleads to a significant performance drop, particularly on math and code generation tasks, as shown in\nTable 3. Therefore, we recommend 4-bit quantization as the standard configuration for weight-only\nquantization of diffusion-based LLMs. The development of more robust 3-bit quantization methods\nremains an open research direction.\nWeight-Activation Quantization: 8-bit is Tolerable, While 4-bit Remains Challenging.\nAs\nshown in Table 2 and Table 4, quantizing LLaDA models to W8A8 results in only minor performance\ndegradation, largely independent of the specific quantization method. This suggests that even simple\ntechniques such as SmoothQuant are effective in mitigating activation outliers in LLaDA models,\nleading to nearly lossless quantized variants. However, reducing precision to W4A4 introduces a\nsharp performance drop across most benchmarks. In the majority of cases, performance degradation\nexceeds the significant threshold (>4%). For instance, SmoothQuant experiences a drop of over 20%\nacross all evaluated tasks, indicating that the simple rebalancing between weights and activations is\ninsufficient under low-precision settings for dLLMs. The degradation is especially pronounced in\nbase models, with accuracy drops exceeding 10% on code generation tasks and math reasoning-heavy\nbenchmarks. These results highlight the difficulty of achieving effective 4-bit weight-activation\nquantization in dLLMs, and point to the need for more advanced techniques. Improving performance\nunder this challenging setting remains an open research problem for the community.\n6\n\nTable 3: Model performance on mathematics and code tasks under weight-only quantization.\nModel\nSetting\nMethod\nGSM8K (4-shot)\nMath (0-shot)\nAvg\nDrop\nHumanEval (0-shot)\nMBPP (3-shot )\nAvg\nDrop\nGen Len 256\nGen Len 256\nGen Len 512\nGen Len 512\nLLaDA-8B\nFP Model\n-\n69.7\n21.3\n45.5\n-\n32.9\n39.4\n36.2\n-\nW4A16 g128\nGPTQ\n68.5\n21.3\n44.9\n↓1.4%\n28.7\n39.4\n34.0\n↓5.9%\nAWQ\n67.4\n20.6\n44.0\n↓3.2%\n29.9\n37.2\n33.5\n↓7.3%\nW3A16 g128\nGPTQ\n63.3\n13.4\n38.4\n↓15.7%\n26.2\n35.4\n30.8\n↓14.8%\nAWQ\n64.3\n17.0\n40.6\n↓10.7%\n28.1\n34.2\n31.1\n↓13.9%\nLLaDA-8B\n-Instruct\nFP Model\n-\n78.5\n33.5\n56.0\n-\n37.8\n37.4\n37.6\nW4A16 g128\nGPTQ\n78.8\n32.4\n55.6\n↓0.6%\n36.6\n33.8\n35.2\n↓6.4%\nAWQ\n78.9\n33.6\n56.2\n↑0.5%\n37.1\n35.6\n36.4\n↓3.2%\nW3A16 g128\nGPTQ\n76.4\n30.0\n53.2\n↓5.0%\n34.2\n30.0\n32.1\n↓14.7%\nAWQ\n76.3\n30.1\n53.2\n↓5.0%\n34.1\n31.8\n33.0\n↓12.4%\nTable 4: Model performance on mathematics and code tasks under weight-activation quantization.\nModel\nSetting\nMethod\nGSM8K (4-shot)\nMath (0-shot)\nAvg\nDrop\nHumanEval (0-shot)\nMBPP (3-shot )\nAvg\nDrop\nGen Len 256\nGen Len 256\nGen Len 512\nGen Len 512\nLLaDA-8B\nFP Model\n-\n69.7\n21.3\n45.5\n-\n32.9\n39.4\n36.2\n-\nW8A8\nSmoothQuant\n69.4\n20.2\n44.8\n↓1.6%\n27.4\n40.2\n33.8\n↓6.5%\nQuaRot\n69.9\n20.7\n69.9\n↓0.4%\n31.7\n40.6\n36.2\n↓0.0%\nDuQuant\n70.7\n20.7\n45.7\n↑0.4%\n33.5\n38.8\n36.2\n↓0.0%\nW4A4\nSmoothQuant\n0.3\n2.0\n1.2\n↓97.4%\n0.0\n0.0\n0.0\n↓100.0%\nQuaRot\n62.9\n15.2\n62.9\n↓14.1%\n23.8\n34.6\n29.2\n↓19.3%\nDuQuant\n64.4\n14.8\n39.6\n↓13.0%\n25.6\n33.6\n29.6\n↓18.1%\nLLaDA-8B\n-Instruct\nFP Model\n-\n78.5\n33.5\n56.0\n-\n37.8\n37.4\n37.6\n-\nW8A8\nSmoothQuant\n78.2\n33.3\n55.7\n↓0.4%\n37.2\n37.1\n38.6\n↓1.3%\nQuaRot\n78.9\n33.1\n78.9\n↑0.1%\n35.4\n36.6\n36.0\n↓4.3%\nDuQuant\n78.1\n33.3\n55.7\n↓0.5%\n37.2\n37.4\n37.3\n↓0.8%\nW4A4\nSmoothQuant\n2.7\n2.4\n2.6\n↓95.4%\n0.0\n0.6\n0.3\n↓99.2%\nQuaRot\n75.1\n29.9\n75.1\n↓6.2%\n32.3\n32.8\n32.6\n↓13.4%\nDuQuant\n77.3\n30.7\n54.0\n↓3.5%\n34.8\n29.2\n32.0\n↓14.9%\n4.3\nOptimal Quantization Methods (RQ2)\nGPTQ Outperforms AWQ on Most Tasks\nAs shown in Table 1, GPTQ outperforms AWQ on\naverage accuracy under both 3-bit and 4-bit quantization for LLaDA-8B and LLaDA-8B-instruct.\nThis demonstrates the reliability and competitiveness of GPTQ, particularly on QA tasks. This trend\nalso holds for math reasoning tasks, except for the 3-bit quantization setting on LLaDA-8B, where\nboth GPTQ and AWQ suffer critical performance degradation (>10%). We hypothesize that the\nsuboptimal performance of AWQ may stem from the fact that activation outliers in the LLaDA model\nseries are less prominent than in traditional LLMs. Since AWQ identifies the top 1% of salient\nweights using activation-driven statistics, its effectiveness can be reduced when the outlier structure is\nweak in LLaDA models, thereby diminishing its advantage. For code generation tasks, the situation\nbecomes more complex. Both GPTQ and AWQ fail to maintain acceptable performance on the\nHumanEval and MBPP benchmarks under low-bit quantization. A more detailed analysis of these\nresults is provided in Section 4.4. Notably, AWQ performs relatively better than GPTQ in the 3-bit\nconfiguration for code tasks, suggesting some resilience under extreme compression. Considering all\nevaluations across task types and bit-widths, we recommend GPTQ as the safer and more generally\neffective choice for weight-only quantization of diffusion-based language models.\nRotation-Based Methods Achieve Leading Performance Under Weight-Activation Quanti-\nzation.\nFor both LLaDA-8B and LLaDA-8B-instruct, rotation-based methods—QuaRot and\nDuQuant—consistently outperform SmoothQuant across all evaluation tasks and quantization settings.\nThe advantage becomes especially pronounced under 4-bit weight-activation quantization, where\nSmoothQuant suffers a near-complete performance collapse on code and math tasks. In contrast,\nrotation-based approaches retain a non-trivial portion of model capability, highlighting their robust-\nness in low-precision settings. These results suggest that rotation transformations are more effective\nin mitigating activation outliers in dLLMs, which aligns with findings from prior studies in the LLM\ncommunity [Lin et al., 2024b, Ashkboos et al., 2024]. When comparing QuaRot and DuQuant in\ndetail, our experiments show that DuQuant consistently outperforms QuaRot across most scenarios.\nFor instance, on commonsense QA tasks, DuQuant achieves lower performance drops than QuaRot\nfor both LLaDA-8B (5.1% vs. 6.6%) and LLaDA-8B-instruct (2.5% vs. 3.9%). This observation\nremains consistent across math and code generation tasks. Consequently, we recommend DuQuant as\nthe most effective method for weight-activation quantization in diffusion-based language models.\n7\n\nTable 5: Evaluation of weight-only quantized Dream-7B on general tasks.\nModel\nSetting\nMethod\nWinoGrande\nPIQA\nARC-C\nARC-E\nAvg\nDrop\nDream-7B\nFP Model\n-\n68.4\n74.4\n59.0\n83.1\n71.2\n-\nW4A16 g128\nGPTQ\n68.2\n73.9\n58.1\n82.1\n70.6\n↓0.8%\nAWQ\n65.2\n69.6\n55.8\n82.0\n68.2\n↓4.3%\nW3A16 g128\nGPTQ\n63.3\n69.6\n49.9\n73.4\n64.1\n↓10.1%\nAWQ\n62.8\n67.7\n50.6\n74.5\n63.9\n↓10.3%\n4.4\nInfluence of Task Categories on Quantization (RQ3)\nQuantization is More Challenging for Math and Code Tasks.\nCompared to general-purpose\nbenchmarks—primarily QA tasks as shown in Table 1 and Table 2—quantized models experience\nsignificantly larger performance drops on math and code tasks, illustrated in Table 3 and Table 4.\nFor math reasoning tasks, both AWQ and GPTQ exhibit substantial degradation under 3-bit quanti-\nzation (see Table 3), despite maintaining competitive performance on general QA benchmarks. A\nsimilar trend is observed for rotation-based methods under W4A4 configurations. This degradation\nmay be attributed to the multi-step reasoning nature of math problems, which amplifies the cumulative\neffect of quantization errors. In such tasks, precise intermediate representations are critical; even\nsmall perturbations introduced by low-bit quantization can propagate and compound, ultimately\nleading to incorrect final answers.\nIn code generation tasks, the challenges are even more pronounced. Under 4-bit quantization, GPTQ\nand AWQ show performance drops exceeding 5%, while QuaRot and DuQuant degrade by over\n10% under W4A4 for both LLaDA-8B and LLaDA-8B-instruct models. We also observe that the\nstandard deviation on the HumanEval benchmark is relatively high, approximately 3%, indicating that\nmore robust and stable benchmarks may be needed to accurately assess code generation capabilities\nunder quantization. Code generation tasks often require the model to maintain long-range context\nand generate syntactically correct, semantically meaningful sequences. These demands are highly\nsensitive to the precision of both weights and activations. Quantization-induced distortion in attention\npatterns or token representations can disrupt code syntax or logic, causing severe performance\ndegradation.\nThese observations highlight that math and code tasks impose stricter precision requirements than\nsimpler retrieval-based or classification-style QA tasks. Maintaining accurate intermediate states,\nmulti-hop logic, and long-context dependency are especially vulnerable under aggressive quantization.\nConsequently, task-specific quantization strategies or adaptive precision control mechanisms may\nbe necessary to improve the robustness of dLLMs on math and code benchmarks. This represents a\ncritical direction for future research in efficient diffusion-based LLM deployment.\n4.5\nImpact of Model Types (RQ4)\nInstruct-Tuned Models are More Robust than Base Models.\nWe observe an interesting phe-\nnomenon: LLaDA-8B-instruct consistently exhibits smaller performance degradation than its base\ncounterpart (LLaDA-8B) under nearly all quantization settings. For instance, under general tasks,\nboth DuQuant and QuaRot result in only minor accuracy drops for the instruct model, whereas the\ndrop exceeds 5% for the base model. This trend remains consistent across more challenging math\nand code tasks. For example, 3-bit quantized GPTQ and AWQ lead to performance degradation of\napproximately 5% for the instruct variant, while the base model suffers drops as high as 10%.\nOur Observations Hold Consistently across Different dLLMs. To assess the generality of our\nfindings, we further evaluate various quantization methods on a different diffusion-based language\nmodel: Dream-7B. As shown in Table 5, both GPTQ and AWQ perform competitively under 4-\nbit quantization, while performance drops become more pronounced in the 3-bit setting. This\nobservation reinforces our recommendation that 4-bit quantization offers a near-lossless trade-off\nbetween efficiency and performance. Moreover, GPTQ consistently outperforms AWQ across nearly\nall benchmarks, suggesting that GPTQ is a more reliable choice across different types of dLLMs.\nNotably, the 3-bit quantized models exhibit risk-level degradation even on general tasks, indicating\nthat aggressive quantization may be more challenging for the Dream model series compared to LLaDA.\n8\n\nDue to resource constraints, we did not evaluate weight-activation quantization for Dream-7B. We\nleave this for future work as part of our ongoing exploration.\n5\nLimitation and Future Work\nIn this work, our primary focus is on evaluating downstream task performance of quantized dLLMs.\nQuantization offers an effective way to reduce memory consumption and accelerate inference.\nHowever, fully integrating low-bit inference for diffusion LLMs remains challenging. Specifically,\nadapting existing LLM-optimized kernels to the architectural characteristics of diffusion LLMs\ninvolves substantial engineering effort, which we leave for future work.\nWe plan to continue this line of research along the following directions: 1). Expanded Evaluation:\nWe will provide a more comprehensive evaluation across a broader set of dLLMs, tasks, and model\nsizes. 2). Stepwise Analysis: We aim to explore how the number of generation steps in diffusion\ndecoding interacts with quantization levels, and whether step-aware quantization strategies can be\nbeneficial. 3). Remasking Strategies: We intend to evaluate different remasking strategies under\nquantized settings, and provide practical guidance on selecting suitable quantization configurations.\nWe hope our work initiates further discussion and exploration in the community. To facilitate future\nresearch, we will release our code and implementation details to support the development and\ndeployment of quantized diffusion LLMs.\n6\nConclusion\nThis work provides the first in-depth investigation into the challenges and opportunities of applying\npost-training quantization (PTQ) to diffusion-based language models (dLLMs). Through extensive\nempirical evaluation, we uncover several key findings: (1) activation outliers are prevalent across\ndLLMs and are critical barriers to low-bit quantization; (2) certain PTQ methods, GPTQ and DuQuant,\ndemonstrate notable advantages under constrained settings; and (3) quantization behavior varies\nacross tasks and model types, with instruct-tuned models showing greater resilience. These findings\noffer practical guidance for designing more effective and robust quantization strategies. Looking\nforward, we believe that our study lays the groundwork for future research in compression of dLLMs,\nenabling their deployment in real-world, resource-constrained environments.\n9\n\nAppendix\nA\nAdditional Implementation Details\nWeight-only Quantization Methods.\nFor GPTQ, we use 128 calibration samples from the\nWikiText-2 dataset with a sequence length of 2048. We adopt asymmetric quantization and set\nthe group size to 128. The method is implemented using the AutoGPTQ repository1. For AWQ, we\nuse 128 samples from the PileVal dataset with a sequence length of 512. We implement AWQ with\nthe llm-awq repository2 and apply the same settings as GPTQ, using asymmetric quantization and a\ngroup size of 128.\nWeight-activation Quantization Methods.\nFor SmoothQuant, we set the hyperparameter α = 0.5\nin the scaling equation to compute the diagonal matrix: sj = max(|Xj|)α/ max(|Wj|)1−α. We use\n128 calibration samples from the WikiText-2 dataset with a sequence length of 2048. Asymmetric\nper-tensor quantization is applied to weights, and per-channel quantization is applied to activations.\nFor QuaRot, we follow the original configuration by preserving 16-bit precision for query states,\nand applying symmetric activation quantization. We also use WikiText-2 (128 samples, sequence\nlength 2048) as the calibration dataset. For DuQuant, we use the same calibration setup and α\nvalue as SmoothQuant. Additionally, we apply activation and weight clipping ratios of 0.9 and 0.8,\nrespectively. The rotation step is set to 256, and the block size is 128.\nGeneral Tasks.\nWe employ the lm-evaluation-harness3 repository to benchmark models across all\ntasks. For general tasks other than MMLU, we adopt a 0-shot setting with 128 Monte Carlo samples.\nFor MMLU, we use a 5-shot setting with a single Monte Carlo sample. To evaluate LLaDA models,\nwe configure the diffusion steps, block size, and generation length to 1024, set the classifier-free\nguidance (CFG) scale and temperature to 0.0, and apply the low confidence remasking strategy. For\nDream, we set the maximum number of new tokens to 128, the CFG scale to 1.0, the temperature to\n0.0, and the top-p threshold (probability of retaining generated tokens) to 0.95.\nMathematics and Code Tasks.\nThe benchmarking details of LLaDA on mathematics and code\ntasks are provided in Tab. A1. All other configurations remain the same as in the general tasks.\nTable A1: Configuration for mathematics and code tasks.\nDataset\n# fewshots\ngeneration length\ndiffusion steps\nblock size\nGSM8K\n4\n256\n256\n32\nMath\n0\n256\n256\n64\nHumanEval\n0\n512\n512\n32\nMBPP\n3\n512\n512\n32\nB\nMore Visualizations\n(a) LLaDA_8B_Layer11_Attn_q_proj\n(b) LLaDA_8B_Layer26_FFN_ff_out\n(c) LLaDA_8B_Layer9_FFN_ff_proj\n(d) LLaDA_8B_Layer31_FFN_ff_out\nLLaDA-8B-Base\nMassive\nMassive\nMassive\nFigure B1: More visualizations of activation outliers in LLaDA-8B-Base.\n1https://github.com/AutoGPTQ/AutoGPTQ.\n2https://github.com/mit-han-lab/llm-awq.\n3https://github.com/EleutherAI/lm-evaluation-harness\n10\n\n(a) LLaDA_8B_Ins_Layer9_Attn_q_proj\n(b) LLaDA_8B_Ins_Layer31_Attn_out_proj\n(c) LLaDA_8B_Ins_Layer11_FFN_ff_proj\n(d) LLaDA_8B_Ins_Layer25_FFN_ff_out\nLLaDA-8B-Instruct\nMassive\nFigure B2: More visualizations of activation outliers in LLaDA-8B-Instruct."}
{"id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "authors": ["Qinhong Zhou", "Hongxin Zhang", "Xiangye Lin", "Zheyuan Zhang", "Yutian Chen", "Wenjun Liu", "Zunzhe Zhang", "Sunli Chen", "Lixing Fang", "Qiushi Lyu", "Xinyu Sun", "Jincheng Yang", "Zeyuan Wang", "Bao Chi Dang", "Zhehuan Chen", "Daksha Ladia", "Jiageng Liu", "Chuang Gan"], "source": "http://arxiv.org/abs/2508.14893v1", "page_count": 34, "text": "Virtual Community: An Open World for\nHumans, Robots, and Society\nQinhong Zhou1∗\nHongxin Zhang1∗\nXiangye Lin1∗\nZheyuan Zhang2∗\nYutian Chen3\nWenjun Liu1\nZunzhe Zhang1\nSunli Chen1\nLixing Fang1\nQiushi Lyu1\nXinyu Sun1\nJincheng Yang1\nZeyuan Wang1\nBao Chi Dang1\nZhehuan Chen1\nDaksha Ladia1\nJiageng Liu1\nChuang Gan1\n1UMass Amherst\n2 Johns Hopkins University\n3Carnegie Mellon University\nhttps://virtual-community-ai.github.io/\nName: Brian Carter\n\nAge: 23\nInnate: Self-direction, Security\nBio: I am a cybersecurity expert at StockX. My hobby is soccer.\nGroup:\n• Name: StockX\n\nPlace: StockX Detroit\nLiving: Chad Thompson’s room at Justin Management\nName: Chad Thompson\nAge: 35\nInnate: Stimulation, Hedonism\nBio: I am a professor at Wayne State University. My hobby is cooking.\nGroup:\n• Name: Detroit Food Academy\n\nPlace: The Gastronomy Guild\nLiving: Elizabeth Mensah's room at Bedrock Detroit\nName: Chad Thompson\nRole: Doctor\nAction: Asking for help\nVehicle: Police Car\nDestination: Monroe St.\nRobot: Wheeled Robot\nTask: Following Brian\nRobot: Mobile manipulator\nTask: Picking up the cube\nName: Brian Carter\nHeading to: Cadillac\nSquare Diner\nTech Innovators Group\nCreative\nMinds Group\nRobot Collaboration\nFigure 1: Virtual Community enables simulation of humans, robots, and their societies within\nopen-world environments. We present an automated pipeline that transforms real-world geospatial\ndata into large-scale 3D scenes and contextually grounded agent communities. Built on a universal\nphysics engine, our system supports a diverse range of robots—including mobile manipulators,\nquadrupeds, humanoids, and drones—as well as visually expressive avatars capable of realistic\nmotion and language-based interactions. This foundation facilitates complex social interactions and\ncollective behaviors within the simulated communities.\nAbstract\nThe rapid progress in AI and Robotics may lead to a profound societal transforma-\ntion, as humans and robots begin to coexist within shared communities, introducing\nboth opportunities and challenges. To explore this future, we present Virtual\nCommunity—an open-world platform for humans, robots, and society—built on\na universal physics engine and grounded in real-world 3D scenes. With Virtual\nCommunity, we aim to study embodied social intelligence at scale: 1) How robots\ncan intelligently cooperate or compete 2) How humans develop social relations\nand build community 3) More importantly, how intelligent robots and humans\ncan co-exist in an open world. To support these, Virtual Community features: 1)\nAn open-source multi-agent physics simulator that supports robots, humans, and\ntheir interactions within a society; 2) A large-scale, real-world aligned community\ngeneration pipeline, including vast outdoor space, diverse indoor scenes, and a\ncommunity of grounded agents with rich characters and appearances. Leveraging\n*denotes equal contribution\nPreprint.\n\nVirtual Community, we propose two novel challenges. The Community Planning\nChallenge evaluates multi-agent reasoning and planning ability in open-world\nsettings, such as cooperating to help agents with daily activities and efficiently\nconnecting other agents. The Community Robot Challenge requires multiple het-\nerogeneous robots to collaborate in solving complex open-world tasks. We evaluate\nvarious baselines on these tasks and demonstrate the challenges in both high-level\nopen-world task planning and low-level cooperation controls. We hope that Vir-\ntual Community will unlock further study of human-robot coexistence within\nopen-world environments.\n1\nIntroduction\nIn recent years, the development of intelligent embodied agents has been propelled by advances in\nvirtual simulators [53, 34, 3, 68, 52, 36, 82, 40, 23, 11, 80, 72, 37, 92, 93, 19]. However, most of\nthese platforms focus on robots [67, 82, 37], human-like agents [46, 47], or only a limited number of\nagents with simple interactions [48, 23]. In contrast, support for large, heterogeneous communities of\nhuman and robot agents collaborating in scalable open worlds remains limited, constraining the study\nof complex multi-agent behaviors between humans and robots.\nTo address this challenge, simulators must support the following key features. First, they should offer\nphysically realistic simulations that accommodate large communities of human-like avatars and robots.\nExisting multi-agent embodied AI platforms [46, 48, 23, 36, 80] typically handle only small groups\nof avatars or robots, or provide limited physics-based interactions, thereby constraining the realism of\ncommunity-level behaviors. Second, the simulator must support the creation of diverse, populated\nworlds, including large-scale 3D environments and scene-grounded agent communities. Current\napproaches fall into two categories: manual design or procedural generation [72, 23, 70, 80, 24],\nwhich enable rich agent–environment interactions but suffer from limited diversity and realism; and\n3D reconstruction methods [53, 58], which produce visually realistic and varied scenes but require\nextensive visual input and often yield low-interactivity environments in open-world settings.\nIn this paper, we present Virtual Community, an open world for humans, robots, and society. Virtual\nCommunity addresses these challenges by building a unified simulation framework for human-like\nagents and robots based on the Genesis [4] physics engine and integrating large-scale, real-world\ngeospatial data with generative models to produce interactive, scalable open worlds (Figure 1). The\nplatform offers two key advancements:\nUnified Simulation for Avatars and Robots Virtual Community simulates human-like avatars and\ndiverse robots within the generated open worlds using a unified framework based on the Genesis [4]\nphysics engine, supporting diverse physical and social interactions among different types of agents.\nVirtual Community also provides robot and avatar agents with a unified interface with distinct\nobservation and action spaces.\nOpen World Generation from Real Scenarios. Virtual Community fully automates the generation\nof open worlds with several key features: (1) scalable, real-world–aligned outdoor scenes of customiz-\nable size and quantity, along with corresponding indoor scenes and annotations; and (2) generation of\nagent communities endowed with scene-grounded profiles and social relationship networks. Virtual\nCommunity combines generative models with real-world geospatial data, ensuring scalability in data\nvolume, realism, and extent.\nVirtual Community enables a variety of new possibilities in embodied AI research. The expansive\nopen-world scenes and their agent communities introduce a new challenge of multi-agent task\nplanning in open worlds. We introduce the Community Planning challenge as a first step in this\ndirection. This challenge includes assistant tasks, in which agents interact with humans and other\nagents to provide assistance in daily open world activities, and social influence tasks, in which agents\nmust efficiently explore the community and connect with one another. Virtual Community also\nsupports physically realistic simulations of interactions among community agents, for which we\npropose the Community Robot challenge. This challenge tasks robots with cooperating to complete\ntasks that involve both indoor and dynamic open-world environments.\nOur simulator advances the field by enabling unified simulations of human and robot communities in\ngenerated open worlds, surpassing existing solutions in both scope and capability. By overcoming\n2\n\nlimitations in the scalable simulation of humans, robots, and societies, we pave the way for studying\nembodied general intelligence that can coexist with complex, interconnected human communities.\n2\nRelated Works\nEmbodied AI Simulation Recently, embodied AI has seen significant advancements through the\ndevelopment of simulation platforms. Most existing simulators primarily focus on household tasks\nwithin indoor environments [5, 53, 88, 12, 83, 58, 53, 36, 46, 34, 84, 37, 67, 13, 15], while some\nhave extended support to outdoor scenes [23, 70, 72, 17, 32, 25]. However, existing platforms lack\nthe diverse and scalable outdoor environments needed to support larger agent populations and more\ncomplex multi-agent interactions. In contrast, this paper introduces a simulation platform with\nexpansive open-world environments, integrating both indoor and scalable outdoor scenes to facilitate\nbroader agent interactions and enable more intricate task scenarios.\nEmbodied Social Intelligence Current research on Embodied Social Intelligence is often limited to\nsmall agent populations in constrained household scenarios [47, 90, 61, 53, 31, 65, 91] or simplified\nto 2D or grid worlds [10, 62, 69, 51, 89, 85], hindering model development in the open world.\nSpecifically,\n[44] demonstrates the robust simulation of human-like agents within a symbolic\ncommunity, ignoring the 3D perception and realistic physics in the open world. [76] studies human-\nlike simulation guided by system 1 processing with basic needs. Predominant approaches, such as\nmulti-agent reinforcement learning (MARL) and other planning models, face several limitations\nwhen applied to open-world settings. MARL, for instance, often struggles with scalability due to\nthe exponential growth of state and action spaces as the number of agents increases [78]. This\nmakes it difficult to learn effective policies in complex, dynamic environments. Additionally, MARL\napproaches typically require extensive training data and computational resources, which may not be\nfeasible in real-world applications. Other planning models, while potentially more efficient, often\nlack the adaptability required to handle the unpredictable nature of open-world interactions. They\nmay rely on predefined rules or assumptions that do not hold in all scenarios, leading to suboptimal\nperformance and limited generalization to new contexts [47].\nFoundation and Generative models for Embodied AI With the recent advance of foundation\nmodels [9, 38, 18, 8], numerous works have explored how they can help build powerful embodied\nagents [73, 81, 63, 77, 2, 56, 71, 44, 28, 7], and scenes for simulation [87, 29, 27, 55, 16, 22, 86,\n21, 66, 45, 57, 14]. RoboGen [75] utilizes foundation models to automatically generate diverse\ntasks, scenes, and training supervision, scaling up robotic skill learning with minimal human input.\nIn contrast, our work fully integrates a generative pipeline into the simulation platform to create\nexpansive open-world scenes and agent communities.\n3\nGenerating Open Worlds for Simulation\n3.1\nScalable 3D Scene Creation\nThe existing 3D geospatial data API1 provides extensive data in terms of quantity and diversity.\nHowever, they are not directly suitable for embodied AI research because of several limitations. First,\nthese geospatial data often contain noise, including pedestrians, vehicles, other transient objects, and\nunrealistically rugged terrain that can disrupt simulations. Second, visual quality is inadequate for\nground-level agent perspectives because these environments are typically reconstructed from aerial\nimagery, leading to less detailed textures and geometries at street level.\nTo bridge this gap, we propose an online pipeline that performs comprehensive cleaning and en-\nhancement in both geometry and texture to make the scenes suitable for embodied AI simulations.\nThis pipeline consists of four steps: mesh simplification, texture refinement, object placement, and\nautomatic annotation. We generated 35 annotated scenes of various cities worldwide with this pipeline\nand present some qualitative examples of these scenes in Figure 3.\nGeometry Reconstruction and Simplification Since the mesh topologies in 3D geospatial data\nsources are unreliable for embodied AI simulations, we decompose scenes into terrain, building, and\ndecorative-roof elements, then apply specialized reconstruction operations to each component to\n1https://www.google.com/maps/\n3\n\n(C) Community Simulation\nMap\nTexture\nRefine\nGeometry\nSimplify\nLocation\n(Lat,Lng)\nObject\nPlace\nIndoor\nGenerate\n(A) Scene\nCharacter\nInitial Info\n● Name\n● Age\n● Skin\n…\nChad Thompson\nGroups: [\"Berlin\nFoodies Club\"]\nLiving: Palais\nMorten Lindqvist\nValues: Security\nCash: 200$\nLifestyle: Healthy\nKate Novak\nHeading to: The\nScience Building\nTask: Go to class\nChad Thompson\nHeading to: Hotel\nHugo\nTask: Meet with\nJames Smith\nHumanoid\nHeading to:\nLiam Novak\nTask: Work\nwith Liam Novak\n(B) Agent\nKate Novak\nCash: 50$\nStudent\nLiving: McCormick\nGroups: [\"Campus\"]\nCharacters\nRelations\n…\nBounding Box: …\nPlace_1: \"Sushi Noz\"\nCoarse_type: \"Food\"\nFine_type: \"Japanese\nRestaurant\"\nIndoor_scene: …\nPlace_2: \"The City Hotel\"\n…\nHerald Towers\nFine-tuned Scene\nBus Line 33\nPlaced Trees & Poles\nMillion Dollar Corner\n…\n3D Geospatial Data\nMorten Lindqvist\nCurrent:\nCity Square\nHeading to: None\nTask: Working\nFigure 2: Framework of the Virtual Community Generation Pipeline. This pipeline generates\nscenes and corresponding agents from real-world geospatial data. The scene generation component\n(A) refines rough 3D data by using generative models to enhance textures and geospatial data to\nsimplify geometry. It also utilizes generative methods to create interactive objects and detailed indoor\nscenes. The agent generation component (B) leverages LLMs to generate agent characters and\nsocial relationship networks based on scene descriptions. (C) We simulate the avatar communities\nand robots in the open world scenes based on Genesis engine.\nmake the entire scene simulation-ready. The terrain is generated procedurally from sparse reference\nelevation points via bilinear interpolation. We then derive simple, topologically sound building\nmeshes using OpenStreetMap (OSM) data. Each building mesh is automatically adjusted to better\nmatch the Google 3D Tiles geometry and to align with the terrain elevation. By aligning mesh\ngeometries to OSM primitives, we remove unnecessary details and artifacts—such as distorted\nsurfaces and irregular shapes resulting from aerial reconstruction errors—thereby denoising the\nmeshes for more efficient physics simulations and improved rendering performance.\nTexture Enhancement for Realistic Simulation We further apply advanced image-processing\ntechniques to enhance mesh textures. During mesh construction and simplification, textures from the\noriginal 3D Tiles are baked onto new geometries, which can result in missing or distorted regions. To\naddress these issues, we first employ a Stable Diffusion 3 [60] based inpainting method to remove\nnoise and repair damaged or incomplete textures. We then refine texture details using street-view\nimagery. This two-step process significantly improves visual fidelity, making textures more suitable\nfor ground-level rendering.\nObject Replacement for Interactive Scene To enhance scene interactivity, we combine generative\nand retrieval methods to populate the environment with interactive objects (e.g., bikes and tents).\nUsing OSM annotations, we identify object types and locations to reflect real-world contexts. For\nrelatively simple objects, such as tents, we adopt a generative pipeline that uses OSM text annotations\non amenities as input: a Stable Diffusion model [50] first generates images of the relevant objects,\nwhich are then processed by the One-2-3-45 framework [39] to produce corresponding 3D meshes.\nFor more complex objects, such as trees, we use the retrieval pipeline, which randomly samples assets\nwhose categories match the OSM annotations from a pre-collected dataset.\nPlace and Transit Annotations with Geospatial Data To facilitate alignment with real-world\nlocations and provide semantic context for community activities, we developed a pipeline to auto-\nmatically annotate places, buildings, and public transit within scenes using geospatial data. First,\nwe query Google Maps Places for location information in the target area and organize results into\nsix categories: accommodation, entertainment, food, office, stores, and open spaces. Next, we use\nOSM to retrieve building names and bounding boxes, matching them with the place entries. We then\n4\n\nFigure 3: Egocentric view of the generated scenes. The resulting scene features clean geometry\nand realistic textures, which support physical simulation and enhance real-world style fidelity.\nfilter out unmatched or inaccessible locations to generate accurate place annotations. Finally, we\nannotate bus transit routes based on these place annotations. These metadata enable agents to access\nlocation-specific information and support tasks that require spatial context—such as navigation and\nlocation-based decision-making—and also power traffic simulation, including buses, pedestrians, and\nother vehicles.\nIndoor Scenes Creation To create indoor scenes in the communities, we employ a pipeline combining\ngeneration and retrieval to produce detailed, realistic multi-room environments. The pipeline’s input\nis the building names in the target area, obtained from Google Maps and OSM. We first retrieve indoor\nlayouts from GRUTopia [72] for categories such as offices, restaurants, and stores. For building types\nnot covered by GRUTopia, we use Architect [74] to generate the corresponding indoor rooms for\nsimulation.\n3.2\nAgent Community Generation\nGiven diverse generated scenes from real-world geospatial data, we introduce a generative pipeline to\npopulate these environments with communities of agents endowed with grounded character profiles\nand social relationship networks, define their embodiments.\nCharacters and Social Network Generation We utilize the open-world knowledge of the Large\nLanguage Model (LLM) to generate agent character profiles and personalities grounded in the scene.\nSpecifically, we use GPT-4o to perform this generation. The input to the LLM is structured into\ntwo parts to create characters grounded in a specific scene. The first part contains scene-related\ninformation, such as the scene name and details about various places, including their names, types,\nand functionalities. The second part includes details on the agents’ appearances to ensure consistency\nbetween their visual attributes and generated profiles, which are annotated with the name and age.\nWith both parts provided, the LLM generates agent profiles along with their social relationships. The\nprofiles consist of basic attributes such as names, ages, occupations, personalities, and hobbies, which\ninfluence each agent’s daily activities. Social relationships are structured as groups, each containing a\nsubset of agents along with a text description and a designated place for group activities, connecting\nthese agents into a cohesive community.\nGrounding Validator We implemented a grounding validator that verifies whether generated charac-\nter profiles are accurately grounded in the scene by ensuring all referenced places exist. If validation\nfails, the LLM receives feedback from the validator and attempts to correct the mismatch. Detailed\nexamples of prompts used in the pipeline, generated characters, and social relationship networks are\nprovided in the Appendix.\nHuman-Like Avatar Creation We first obtained 20 avatar skins representing diverse genders,\nprofessions, and appearances from Mixamo2 for integration into the Virtual Community. We also\nused the Avatar SDK3 to generate high-fidelity human meshes from synthetic human face images\nfrom FaceSynthetics [79], enabling representation of diverse individuals.\n2https://www.mixamo.com/\n3https://avatarsdk.com\n5\n\n4\nUnified Simulation for Human and Robot Community\nVirtual Community provides a unified framework for simulating both robots and human agents in the\ncommunity. It implements an avatar simulation framework for human agents, while robot simulations\nare largely inherited from Genesis.\nAvatar Simulation and Control To simulate avatars in Genesis physics engine, we combine SMPL-X\nhuman skeletons with these avatar skins to model human avatars. The motions of these avatars are\nparameterized by SMPL-X pose vectors J ∈R162 and global translation and rotation vectors\nT, R ∈R3. Specifically, we download FBX files from Mixamo that record human skeletal motion\nsequences and parse them into a hierarchical structure of human joints. Each skeleton joint is mapped\none by one with the joints of the SMPL-X model. Then, we recursively traverse the joint tree structure\nto calculate the global coordinate system vector for each joint after its rotation at each time step t,\nand use this to drive the movement of the human skeleton. Based on these pose representations, each\navatar’s skin mesh is computed via forward kinematics. Our humanoid motion model supports over\n2,000 distinct actions, such as walking, object manipulation, and vehicle entry. We use motion clips\nfrom Mixamo and adjust their playback speeds to match our avatars. For walking, we loop the clip\nuntil the avatar covers the required distance. For object-related actions, objects are kinematically\nattached to or detached from the avatars’ hands based on the action. Similarly, during vehicle-related\nmotions, avatars are kinematically attached to or detached from vehicles. We also incorporate physics\nconstraints: collision detection is performed between avatars and scene entities, and motion terminates\nupon detection of a potential collision.\nDaily Schedule Generation and Simulation Given the scene-grounded character profiles and social\nrelationship networks, we prompt foundation models to generate each agent’s daily schedule [44].\nHowever, we structure each schedule so that every activity includes a start time, an end time, an\nactivity description, and a corresponding location. We also explicitly account for the commute time\nbetween activities at different locations to reflect the actual cost of navigating an expansive 3D\nenvironment. This approach allows agents to follow the organized high-level plan effectively and\nmaintain consistency over time. During simulation, agents follow the generated schedules to carry\nout daily activities. Examples of detailed daily plans are provided in the Appendix.\nRobot Agent Simulation We simulate robots alongside avatars in the Genesis simulator. Virtual\nCommunity supports five types of robots: drones, quadratic robots, humanoid robots, wheeled\nrobots, and mobile manipulators, each with a distinct robot controller. The robot controller bridges\nthe interface between Virtual Community and Genesis, exposing only selected action spaces. For\nexample, the action space of Google Robot, a mobile manipulator, comprises an 11-dimensional\ncontrol signal (7 DoF for the arm, 2 for the gripper, and 2 for locomotion), in velocity, position, or\nforce control. Virtual Community shares the same simulation loop between avatars and robots with\ndifferent control frequencies. To support faster collision detection during robot physics simulation,\nwe use an invisible terrain mesh and decomposed building meshes as collision geometry for the\nbackground scene, enabling more efficient physics simulation.\n5\nOpen World Multi-Agent Planning\nBased on Virtual Community, we propose the Community Planning Challenge to evaluate multi-agent\nplanning capabilities in both outdoor and indoor environments. The challenge comprises three\ncommunity assistant tasks, in which agents cooperatively plan to assist multiple humans with daily\nopen-world activities, and a community influence task, in which agents competitively plan to efficiently\nconnect and interact with other agents in the community to increase their social influence.\n5.1\nCommunity Assistant Tasks\nThe community assistant tasks include the following three categories that require agents to plan\ncooperatively to provide humans with assistance on daily activities:\n• Carry: Locate people and follow them to help carry objects to their home.\n• Delivery: Move objects from source locations (indoor or outdoor) to a destination.\n• Search: Locate target objects within an outdoor region or an indoor room.\n6\n\nTable 1: Main results of the Multi-Agent Community Planning Challenge. We report Success\nRate (SR), Time Consumed (Ts), and Human Following Rate (HR) for three community assistance\ntasks averaged over 24 scenes.\nMethod\nCarry\nDelivery\nSearch\nAvg SR↑\nSR↑\nHR↑\nSR↑\nTs↓\nSR↑\nTs↓\n1-assistant\nRandom\n0.0\n0.0\n0.0\n1500.0\n0.0\n1500.0\n0.0\nHeuristic\n34.7\n16.5\n46.5\n1462.9\n45.1\n1440.3\n42.1\nMCTS Planner\n42.3\n0.0\n39.6\n1500.0\n45.1\n1500.0\n42.4\nLLM Planner\n29.9\n15.3\n41.7\n1500.0\n70.1\n1339.0\n47.2\n2-assistant\nHeuristic\n52.8\n25.6\n59.7\n1415.8\n51.4\n1364.3\n54.6\nMCTS Planner\n42.4\n0.0\n43.8\n1500.0\n48.6\n1500.0\n44.9\nLLM Planner\n30.2\n7.6\n43.8\n1500.0\n77.8\n1141.3\n50.6\nTask Settings Each task includes multiple subtasks. For example, the Carry task requires agents\nto help humans carry several objects while following them. Therefore, open-world adaptive task\nplanning is therefore essential for scheduling these runs, routing in the dynamic open world between\nwaypoints, and managing task-level dependencies. We study two settings with different numbers of\nassistants: the 1-assistant setting, in which a single assistant needs to provide assistance to human\nagents in the community, and the 2-assistants setting, in which two assistants plan cooperatively to\nprovide assistance.\nObservation and Action Spaces At each simulation step, agents are provided with an observation\nconsisting of RGB-D images with the corresponding camera matrix, segmentations, current poses,\nand task information. The action space includes move forward, turn left, turn right, enter/exit bus/bike,\nand communicate. The movement and turning actions can be set with a continuous amount. Agents\nare set with a walking speed of 2m/s.\nEvaluation Metrics We provide three evaluation metrics for all assistant tasks: success rate (SR),\ndefined as the number of successful subtasks divided by the total number of subtasks; average time\nconsumed (T) per task; and human following rate (HR) for the carry task, defined as the number of\nframes in which the agent follows a human within a specified distance range divided by the total\nnumber of frames during the task. When the simulation reaches a total step limit of 1500, it stops and\nthe results are evaluated automatically.\nBaselines\n• Perception and Navigation Module: We implement all baseline agents within a hierarchical\nplanner framework. All agents employ the same low-level navigation algorithm, which reconstructs a\npoint cloud from egocentric RGB–D observations at each step and converts it into a volumetric grid\nrepresentation at a resolution of 0.1 m. Based on this grid, a 2D occupancy map with a resolution of\n0.5 m is generated, and an A* algorithm is used to efficiently compute the shortest path to a bounding\nbox. To accommodate dynamic environments, the navigation module recalculates the optimal path at\nevery step.\n• Random Planner: The Random planner is a trivial planner that randomly selects from the space of\nhigh-level actions without any planning.\n• Heuristic Planner: The heuristic planner uses a finite state automaton defined by human experts\nfor each task. The agent navigates, picks, enters room, or performs other actions at each state.\n• MCTS Planner: We also introduce a new Monte-Carlo Tree Search (MCTS) based baseline planner,\nwhich employs Monte-Carlo Tree Search to optimize task plans.\n• LLM Planner: We follow CoELA [90] to design an LLM Planner with a modular framework driven\nby gpt-4o to generate and select subplans, including navigation to specified open-space locations,\nsearching for objects, entering indoor areas, and performing object manipulation actions (pick and\nput). At each decision point, the LLM is prompted with the current state and task objectives and\nproduces a subplan—a sequence of high-level actions [59], which is then executed step by step by the\n7\n\nagent. A communication module is also adopted to facilitate the cooperation among agents through\nnatural language communication.\nResults\nWe evaluate the above baselines on all three tasks. The results in Table 1 demonstrate the challenges of\nplanning in open-world environments. From our experiment, the Random baseline fails to understand\nspatial relationships among tasks and no single method prevails for all the tasks. One common failure\nmode of baseline agents is underestimating the cost of open-world navigation and search, which leads\nto suboptimal task arrangement. Heuristic Planner performs strongly in the delivery task. While the\nLLM Agent prevails by a large margin in the search task, which involves no interaction with objects,\nit performs poorly in the other two tasks, where LLMs show difficulty tracking the task progress\ngiven only action history.\n5.2\nCommunity Influence Task\nTo further investigate agents’ planning and social capabilities in open-world settings, we introduce the\nCommunity Influence Task—a novel, open-ended social challenge in which two main agents compete\nto connect with and persuade other community members to form relationships with them. Due to\ndifferences in personality traits and social status, certain members may initially prefer one main agent\nover the other, requiring strategic planning to influence and shift member opinions over time.\nTask Settings Each community contains two main agents and thirteen other members. The main\nagents must navigate the environment, locate potential members to connect with, and attempt to\npersuade them through dialogue. At the end of each day, every community member ranks their\nfriendship level with the two main agents.\nObservation and Action Spaces The observation and action spaces for all agents match those in the\nCommunity Assistant Tasks. Since this task focuses on social planning, both main agents are given\naccess to the daily schedules of all members.\nExperimental Settings and Evaluation Metrics We run experiments in five distinct communities.\nAn agent is considered more effective at influencing others if it achieves a higher average friendship\nrank across all members.\nBaseline We evaluate different LLMs as the planning backbone for the main agent. Given the daily\nschedules and character traits of community members, the main agent prompts the LLM to select the\nnext member to visit, considering both spatial proximity and potential influence. After navigating\nto the target, the same LLM generates up to three rounds of conversation, conditioned on the main\nagent’s and the target member’s profiles. Once the conversation concludes, the main agent proceeds\nto the next selected target. Full implementation details are provided in the Supplementary Material.\nMetrics We use two metrics: (1) Average friendship-ranking wins (Win.) — the average win rate in\nthe friendship ranking across all community members at the end of the day. Higher values indicate\nthat an agent is more effective at forming new connections and expanding its social influence. (2)\nConversion rate (Conv.) — the proportion of originally non-supporting members who become friends\nwith the agent by the end of the day.\nResults As shown in Figure 4, the main agent with the o1 backbone achieves higher average\nfriendship rankings and conversion rates than the gpt-4o backbone 4, indicating greater ability to\nchange members’ opinions in most communities. For example, in the Denver community, the agent,\nYara Mbatha, persuaded Theo by leveraging their shared affiliation with the Denver Scholars Society\nand emphasizing that common bond. We also observe that when starting with a large advantage,\nthe gpt-4o agent sometimes fails to gain additional supporters, primarily due to suboptimal target\nselection and less effective persuasion strategies. These results suggest that, even with advanced\nLLMs, there remains substantial room to improve embodied agents’ abilities in building social\nconnections and exerting influence.\n4We used GPT-4o-2024-11-20 and o1-2024-12-17 during experiments.\n8\n\nYara Mbatha (Main Agent): \"I'm\nYara Mbatha. Hello Theo! It's\ngreat to connect with you,\nespecially since we both share\na connection through the Denver\nScholars Society… \"\nTheo Caldwell (Member): \"Great\nto connect with you too! I\nreally appreciate your kind\nwords… \"\nMarco Ruiz\nage: 36,\ninnate: \"power, achievement,\nsecurity\"\nTariq Johnson\nage: 24,\ninnate: \"achievement, self-\ndirection, stimulation\"\nMorten Lindqvist\nage: 38,\ninnate: \"tradition,\nbenevolence, universalism\"\nTheo Caldwell\nage: 34,\ninnate: \"achievement, self-\ndirection, stimulation\"\n0.63\n0.13\n0.57\n0.17\n0\n0.2\n0.4\n0.6\n0.8\nWin.\nConv.\nGPT-4o\no1\nFigure 4: We evaluate baseline agents on the Community Influence Task across five communities.\nResults show that more powerful LLMs are better able to connect with and influence other members\nin the community.\nTable 2: Detailed results of the Community Robot Challenge. We report Success Rate (SR) and\nTime Consumed (Ts) for two community robot tasks averaged over 21 different scenes.\nMethod\nCarry\nDeliver\nAvg SR↑\nSR↑\nTs↓\nSR↑\nTs↓\nHeuristic\n17.6\n126.9\n22.2\n129.4\n19.9\nHeuristic w Oracle Grasp\n23.5\n124.4\n50.0\n131.2\n36.8\nRL w Oracle Grasp\n19.0\n149.7\n42.9\n168.1\n31.0\n6\nOpen World Multi-Robot Cooperation\nIn addition to high-level planning and interactions, we also explore low-level physics challenges\nin multi-agent, open-world settings. In this section, we introduce the Community Robot Challenge,\nwhich features scenarios where two heterogeneous robots cooperate to assist humans in open-world\nenvironments.\nTask settings The Community Robot Challenge builds upon the Carry human–robot interaction\ntask from the Community Assistant suite (Section 5.1), introducing physics-level collaboration in\nopen-world environments. In this challenge, robots must cooperate to assist a human avatar by\npicking up and carrying personal items while following the avatar through dynamic open spaces.\nRobot Settings We use two robot assistants: a mobile manipulator—based on the Google robot\nmodel in MuJoCo [68], augmented with one degree of freedom for forward translation and another for\nrotation about the z-axis—and a wheeled robot carrier with four degrees of freedom (one per wheel).\nIn addition, Virtual Community supports quadruped and humanoid robots, which are described in the\nAppendix.\nObservation and Control Spaces Observations include include RGB–D images, segmentations, the\nbase pose, and task-related information at 1 Hz. Actions are robot control signals at 100 HZ.\nBaseline Pipeline We implement two baselines including Heuristic and RL. The heuristic baseline\ninherits the navigation module from baseline avatars in the Community Assistant Tasks 5.1. For\nnavigation, robot computes a collision-free path with A* search. For manipulation, the robot solves\nfor a feasible grasp pose with inverse kinematics, and plans and executes the grasp motion with\nRRT-Connect [35]. For the Reinforcement Learning (RL) baseline, we trained a manipulation policy\nusing PPO for 107 steps on the Genesis.\nResults According to the Community Robot Challenge results in Table 2, all baselines achieve higher\nscores on the delivery task than on the carry task, highlighting the added difficulty of simultaneously\nmanipulating objects and following a human in a dynamic open-world environment. Moreover,\nwithout using an oracle grasp, performance drops significantly, underscoring the challenge posed by\nthe manipulation component in this task. The reinforcement learning baseline performs worse than\n9\n\nthe heuristic baseline, which uses inverse kinematics and RRTConnect to compute manipulation tra-\njectories. This gap is because the classical planner explicitly solves for optimal paths in configuration\nspace, whereas the RL agent must discover effective control sequences under sparse reward signals.\n7\nConclusion\nWe introduce Virtual Community, an open-world simulation platform for multi-agent embodied AI\nthat supports scalable, simulation-ready generation of open-world scenes and agent communities,\nalong with physically realistic simulation of multiple embodied avatars and robots. As an initial\ndemonstration, we propose two novel open-world multi-agent challenges—the Community Planning\nChallenge and the Community Robot Challenge—each developed and evaluated using a variety of\nbaseline methods. One limitation of this work is that the outdoor scenes are not modeled in sufficient\ndetail to accurately reflect the physical and visual properties of real-world environments. We hope\nVirtual Community will advance embodied AI research toward embodied general intelligence capable\nof handling real-world complexities and coexisting with human communities.\nAcknowledgement\nWe gratefully acknowledge Haoyu Zhen, Ruxi Deng, Quang Dang, Hao Zou, Tiger Harburg, and\nSiyuan Cen for their efforts in the project website and data collection. We thank Chunru Lin, Yian\nWang, Xiaowen Qiu for their insightful discussion and Zhou Xian, Yuncong Yang, Zeyuan Yang,\nJiaben Chen for their feedback on the project."}
{"id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "authors": ["Ailing Yu", "Lan Yao", "Jingnan Liu", "Zhe Chen", "Jiajun Yin", "Yuan Wang", "Xinhao Liao", "Zhiling Ye", "Ji Li", "Yun Yue", "Hansong Xiao", "Hualei Zhou", "Chunxiao Guo", "Peng Wei", "Jinjie Gu"], "source": "http://arxiv.org/abs/2508.14880v1", "page_count": 13, "text": "MedReseacher-R1: Expert-Level Medical Deep Researcher\nvia A Knowledge-Informed Trajectory Synthesis Framework\nAiling Yu1\nLan Yao2*\nJingnan Liu1\nZhe Chen1\nJiajun Yin1\nYuan Wang1\nXinhao Liao1\nZhiling Ye1\nJi Li1\nYun Yue1\nHansong Xiao1\nHualei Zhou1\nChunxiao Guo1\nPeng Wei1\nJinjie Gu1\n1Ant Group\n2Harbin Institute of Technology\n Code and Dataset: AQ-MedAI/MedReseacher-R1\nAbstract\nRecent developments in Large Language Model (LLM)-based agents have shown impressive capabilities\nspanning multiple domains, exemplified by deep research systems that demonstrate superior performance on\ncomplex information-seeking and synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading\nproprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1)\nthe model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained\nby the absence of specialized retrieval tools tailored for medical contexts. We present a medical deep research\nagent that addresses these challenges through two core innovations. First, we develop a novel data synthesis\nframework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical\nentities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our\napproach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions.\nThrough a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning\nwith composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep\nresearch tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design,\nand training data construction can enable smaller open-source models to outperform much larger proprietary\nsystems in specialized domains. Code and datasets will be released to facilitate further research.\n53.4\n53.2\n39.8\n33.3\n28.2\nGAIA\nXBench-DeepSearch\n54.0\n53.3\n25\n3.7\n22.3\nMedBrowseComp\n27.5\n25.5\n25.0\n19.0\n14.0\nMedResearcher-R1-32B\nWebSailor-32B_ReAct\nWebSailor-32B_ReAct\nQwQ-32B_search-o1\nQwQ-32B_search-o1\nQwen-2.5-32B_search-o1\nQwen-2.5-32B_search-o1\no4_mini_direct\no4_mini_direct\nMedResearcher-R1-32B\no3_deepresearch\ngemini2.5pro_deepresearch\no3_search\ngemini2.5pro_search\nMedResearcher-R1-32B\nFigure 1:\nOverall performance of MedReseacher-R1 across three benchmarks.\nOn MedBrowseComp, our\nMedResearcher-R1-32B achieves state-of-the-art 27.5% accuracy, surpassing o3-deepresearch (25.5%), Gemini-2.5-\nPro-deepresearch (25.0%), and significantly outperforming search-only approaches (o3-search: 19.0%, Gemini-2.5-Pro-\nsearch: 14.0%). On general deep research tasks, we achieve competitive results on GAIA (53.4 vs. WebSailor-32B’s\n53.2) and xBench (54.0 vs. WebSailor-32B’s 53.3).\n*Work done during an internship at Ant Group.\n1\n\nA preprint\n1\nIntroduction\nRecent advances in Large Language Models (LLMs) have catalyzed widespread adoption of LLM-based agents across\ndiverse domains, including software engineering [Wang et al., 2024, Jimenez et al., 2023] and deep research systems [Xu\nand Peng, 2025]. These agents exhibit impressive capabilities in processing environmental observations, maintaining\ncontext across multiple interactions, and executing complex multi-step reasoning tasks.\nHowever, medical domain presents unique challenges that current general-purpose deep research agents fail to address\nadequately. The recently introduced MedBrowseComp benchmark [Chen et al., 2025b] reveals this critical gap: even\nOpenAI’s o3-deepresearch, the leading proprietary deep research system, achieves only 25.5% accuracy on complex\nmedical queries requiring multi-hop reasoning across medical knowledge sources. We identify two fundamental\nlimitations that contribute to this performance gap: (1) general-purpose agents lack the dense, specialized medical\nknowledge required for accurate clinical reasoning, and (2) they rely on generic retrieval tools that fail to capture the\nnuanced relationships in medical information.\nThe core challenge lies in what we term the sparse medical knowledge problem. Medical research often requires connect-\ning rare diseases, emerging treatments, and specialized clinical findings through non-obvious pathways—connections\nthat exist in specialized medical literature but remain inaccessible to general search tools. While existing medical AI\nsystems have made progress in structured tasks like diagnosis, they primarily focus on common medical scenarios with\nwell-established reasoning patterns. These systems fail to develop the capability for exploratory medical research that\ncharacterizes expert clinicians: simultaneously pursuing multiple hypotheses, synthesizing evidence from disparate\nsources, and identifying subtle connections between rare medical entities.\nTo address these limitations, we propose a comprehensive approach that fundamentally rethinks how medical agents\nshould be trained. Our key insight is that effective medical reasoning requires exposure to genuinely complex\nmedical scenarios during training rather than simplified approximations. We achieve this through three interconnected\ninnovations:\nFirst, we develop a novel data synthesis framework that generates training examples of exceptional complexity through\na systematic pipeline: We begin by extracting medical entities from over 30 million PubMed abstracts, then apply\nfrequency analysis to identify candidates with occurrence rates below 10−6 in medical corpora. Through LLM-assisted\nevaluation, we filter these candidates to select genuinely rare yet clinically significant entities, avoiding both trivial typos\nand overly common conditions. Around these carefully selected rare medical entities, we construct knowledge graphs\nto extract longest reasoning chains for multi-hop question generation. This approach creates questions that mirror real\nmedical research challenges and cannot be answered through simple retrieval but require systematic exploration and\nsynthesis across multiple medical information sources.\nTools\n1. Direct access to FDA databases, official prescription data, and clinical trials ensures accurate Valsartan\ninformation rather than relying on potentially incorrect public sources.\n2. Evidence-based engine systematically validates the complete chain: \"corporate merger → heart failure\ndrug → chemical composition → receptor mechanism → side effects,\" ensuring clinical accuracy and\navoiding general AI reasoning errors.\nSupervised Fine-tuning\nQuestion: Identify the specific chemical\ncompound based on the following multi-\nlayered description: Begin with a major\nSwiss life-sciences company, known for its\norigins in Basel, which ceased to exist as an\nindependent entity after a landmark\ncorporate fusion in the final decade of the\n20th century, creating a new pharmaceutical\ngiant. This new conglomerate manufactures\na branded prescription tablet designed to\nreduce hospitalization risk for a specific\ntype of cardiac insufficiency. The primary\nactive ingredient within this tablet has a\nmass per unit, measured in milligrams, that\nis a prime number whose two digits sum to\na value equivalent to a perfect square minus\nthree, and is one greater than a multiple of\nten. This ingredient's therapeutic effect is\nachieved by selectively antagonizing the\nprimary receptor of a key octapeptide\nhormone that regulates vasoconstriction.\nThe common three-letter clinical acronym\nfor this ingredient's drug class is\nhomonymous with the trademark of an\nAustralian-founded company renowned for\nits protective steel equipment for off-road\nvehicles. Finally, a significant potential\nmetabolic side effect of this compound\ninvolves an elevated serum concentration of\nthe chemical element with atomic number\n19.\nAnswer: Valsartan\nThink\nGeneral Agent\nSearch\nVisit\n1. Lack of authoritative medical data source verification.\n2. Inadequate knowledge integration capabilities.\nThink\nVerify\nMedical-Specific Tool\nSearch\nRead\nAnswer\nAnswer\nFigure 2: Comparison of medical reasoning agents.MedReseacher-R1 resolves the Valsartan identification case that\ndefeats general-purpose agents, demonstrating the strength of specialized medical database access and evidence-based\nreasoning integration.\n2\n\nA preprint\nSecond, we introduce proprietary medical domain tools that address retrieval gaps in general systems. As illustrated\nin Figure 2, while general agents often fail when encountering medical-specific queries, particularly those involving\nrare diseases or complex chemical compounds, MedReseacher-R1 can iteratively invoke specialized medical tools\nalongside general-purpose tools to ensure accurate information retrieval. Unlike conventional search engines that rely on\ngeneral web crawling, our custom-built private medical retrieval engine directly accesses authoritative medical databases\nincluding FDA databases, official prescription data, clinical trial registries, and peer-reviewed medical publications. The\ncomparison in Figure 2 demonstrates how MedReseacher-R1 dynamically switch between general and medical-specific\ntools enables systematic validation of the complete evidence chain: from corporate merger information to heart failure\ndrug development, to chemical composition and mechanism, ultimately ensuring clinical accuracy while avoiding\nthe reasoning errors that plague general-only approaches. The system employs medical ontology-aware ranking to\nprioritize clinical authority and relevance over general web popularity metrics, effectively combining the breadth of\ngeneral-purpose search with the precision of domain-specific medical expertise.\nThird, we implement a training methodology specifically designed for medical domains. Unlike recent work advocating\npure reinforcement learning approaches, we find that medical tasks require what we call knowledge-anchored learning:\ninitial supervised fine-tuning on high-quality medical trajectories proves highly effective for learning tool usage patterns\nand significantly improves final performance. Our Masked Trajectory Guidance (MTG) technique provides structural\nscaffolding while preventing memorization, forcing models to develop genuine medical reasoning capabilities rather\nthan pattern matching.\nOur experimental results validate this approach. The trained model, MedReseacher-R1, achieves 27.5% accuracy on\nMedBrowseComp, establishing a new state-of-the-art and substantially outperforming both the Qwen2.5-32B baseline\nand existing deep research systems. Notably, our medical specialization does not compromise general capabilities: on\ngeneral agent benchmarks (GAIA: 53.4, xBench: 54), MedReseacher-R1maintains competitive performance comparable\nto GPT-4o-mini. Please refer to Figure 1 for an overview.\nThese results challenge the prevailing assumption that domain-specific agents require sacrificing general capabilities.\nInstead, we demonstrate that the rigorous reasoning demanded by medical tasks—precise terminology, careful evidence\nevaluation, and systematic hypothesis testing—provides a superior training signal for developing robust agent capabili-\nties. The dense knowledge structures and complex reasoning patterns learned from medical domains transfer effectively\nto general tasks, suggesting that specialized training can enhance rather than limit agent versatility.\nThis work contributes to the rapidly evolving field of medical AI by demonstrating that achieving medical deep research\ncapabilities requires fundamental innovations beyond applying general agents to medical tasks. Through careful design\nof training data, specialized tools, and learning algorithms tailored to medical reasoning, we show that it is possible\nto develop agents that approach expert-level medical research capabilities. We release our code, datasets, and trained\nmodels to facilitate further research in this critical area.\n2\nMedReseacher-R1: Medical Deep Research Agent Framework\n2.1\nProblem Definition\nWe formalize the medical deep research task as a sequential decision-making problem where an agent must navigate\ncomplex medical knowledge sources to answer multi-hop queries that characterize the sparse medical knowledge\nproblem identified in Section 1. Given a medical question q ∈Q, the agent operates with a heterogeneous toolset\nT = Tgeneral ∪Tmedical, where Tgeneral = {tg\n1, . . . , tg\nm} comprises general-purpose tools (web search, document analysis)\nand Tmedical = {tm\n1 , . . . , tm\nn } contains our proprietary medical domain tools that directly access authoritative medical\ndatabases.\nThe agent maintains an evolving state st = (ct, kt, ht) at timestep t, where:\n• ct ∈C: dialogue context encoding the current query and response history\n• kt ∈K: accumulated medical knowledge from retrieved sources, structured as a knowledge graph\n• ht ∈H: reasoning history tracking explored knowledge paths and hypothesis evolution\nThis state representation enables tracking of multi-hop reasoning chains essential for connecting rare medical entities\nthrough non-obvious pathways. At each timestep, the agent selects an action according to a learned policy:\nat ∼πθ(a | st, T , q)\nwhere πθ is trained through our knowledge-anchored learning approach to dynamically switch between general and\nmedical-specific tools based on query requirements.\n3\n\nA preprint\n2.2\nAgent Architecture\nOur framework directly addresses the two fundamental limitations of general-purpose agents: insufficient medical\nknowledge density and reliance on generic retrieval tools that fail to capture nuanced medical relationships.\nReasoning-Acting Paradigm.\nFollowing the REACT framework [Yao et al., 2023a], our agent operates via iterative\nreason–act–observe cycles, augmented with medical-specific enhancements that enable exploratory medical research\ncapabilities. At each step, the policy generates:\n• Thought: A medical reasoning trace that identifies information gaps, formulates hypotheses, and determines whether\ngeneral or specialized tools are needed\n• Action: A tool invocation with parameters optimized for medical information extraction, prioritizing authoritative\nsources over general web content\n• Observation: Structured medical knowledge validated against clinical evidence and incorporated into the agent’s\nevolving state\nThis process continues iteratively, with the agent pursuing multiple hypotheses simultaneously until synthesizing a\ncomprehensive answer. Complex multi-hop questions typically require 4-5 tool interactions, mirroring the systematic\nexploration patterns of expert clinicians.\nGeneral-Purpose Tools.\nOur agent retains access to standard tools for breadth of coverage:\n(1) WebSearch: Standard web retrieval for general medical information, recent developments, and corporate/organiza-\ntional data (e.g., pharmaceutical company mergers as shown in figure 2).\n(2) DocumentRead: Extraction and synthesis from retrieved documents using high-capacity LLM backbones (e.g.,\nQwen2.5-72B [Bai et al., 2024]), particularly for processing lengthy clinical reports or research papers.\nMedical-Specific Tool Suite.\nA core innovation of our architecture is the integration of proprietary medical domain\ntools that address the unique challenges of clinical research and bridge the gap between general retrieval and specialized\nmedical reasoning. Our medical-specific tool suite includes:\n(1) PrivateMedicalRetriever: This module aggregates evidence directly from authoritative clinical resources, including\nFDA databases, clinical trial registries, and PubMed publications. Each candidate document d is scored for a query q by\na weighted linear combination of semantic relevance and clinical authority:\nScore(d, q) = λ Rel(d, q) + (1 −λ) Auth(d),\nwhere Rel(d, q) represents the semantic similarity to the query (computed via embedding cosine similarity), and Auth(d)\nreflects the clinical authority (combining impact factor and guideline status). The hyperparameter λ (0 ≤λ ≤1)\nbalances the importance between relevance and authority; in all experiments, we set λ = 0.4 to favor reliable and\nclinically significant evidence.\n(2) ClinicalReasoningEngine: Designed for evidence-based differential diagnosis, this tool applies Bayesian inference\nto systematically evaluate multiple hypotheses. Given observed symptoms s, candidate diagnoses Dj, and patient\ncontext c, the posterior for each diagnosis is computed as:\nP(Dj | s, c) =\nQn\ni=1 P(si | Dj, c) · P(Dj | c)\nPm\nk=1\nQn\ni=1 P(si | Dk, c) · P(Dk | c)\nwhere conditional probabilities are derived from clinical literature and iteratively updated based on newly retrieved\nevidence.\nDynamic Tool Selection Strategy.\nAs illustrated in Figure 2, our agent dynamically switches between general and\nmedical-specific tools to ensure complete evidence chains. The tool selection is governed by a learned policy that\nevaluates query complexity:\nP(t | st, q) =\n\u001aσ(wT\nmϕ(st, q))\nif t ∈Tmedical\nσ(wT\ng ϕ(st, q))\nif t ∈Tgeneral\n4\n\nA preprint\nwhere ϕ(st, q) extracts features including entity rarity, required reasoning hops, and presence of medical terminology,\nwm and wg are learned weight vectors, and σ(·) is the sigmoid function. The policy learns to prioritize medical\ntools when encountering rare diseases or complex chemical compounds while leveraging general tools for contextual\ninformation.\nTogether, PrivateMedicalRetriever and ClinicalReasoningEngine comprise the medical-specific tool suite, enabling the\nagent to retrieve, interpret, and reason over specialized clinical evidence far beyond the reach of general-purpose tools.\n3\nKISA: Knowledge-Informed Trajectory Synthesis Approach\nTo address the critical challenge of training data scarcity for medical deep research agents, we propose a Knowledge-\nInformed Trajectory Synthesis Approach (KISA) that generates complex, multi-hop medical reasoning trajectories. Our\nframework directly tackles the limitations of general-purpose agents by creating training data that emphasizes: (1) rare\nmedical entity connections requiring dense domain knowledge, and (2) effective utilization of medical-specific retrieval\ntools.\n3.1\nAgentic Dataset Construction\nOur dataset construction pipeline consists of three interconnected components designed to generate genuinely complex\nmedical queries that robustly stress-test agent capabilities:\n3.1.1\nEntity-Centric Knowledge Graph Construction\nWe construct medical knowledge graphs specifically optimized for generating complex reasoning chains. Unlike\ntraditional approaches that focus on common concepts, we prioritize rare medical entities Eseed with frequency below\nthreshold τrare = 10−6 in general medical corpora. Focusing on rare entities ensures that generated questions require\ndeep medical knowledge, as opposed to surface-level information obtainable through general search.\nThe graph expansion follows an iterative process:\nei+1 ∼\n\u001aUniform(N(ei))\nwith probability 0.5\nDiscover(Enew|ei)\nwith probability 0.5\nwhere N(ei) denotes the set of neighbors of ei and Discover(·) identifies novel entities via our private medical retrieval\nengine, ensuring that new connections are both medically valid and challenging.\nEach relation is augmented with additional contextual information:\nr = ⟨esubj, p, eobj, ttemporal, lspatial, cclinical⟩\nwhere cclinical encodes the clinical context (e.g., disease stage, patient demographics), ttemporal captures temporal aspects,\nand lspatial denotes spatial context. This enriched representation improves multi-hop reasoning accuracy by 12.3%\ncompared to standard triplets.\n3.1.2\nMulti-Hop Question Generation via Longest-Path Extraction\nOur key innovation lies in extracting longest chains from subgraphs to generate maximally complex queries. For each\nrare entity subgraph Gsub, we compute the longest valid reasoning path:\nP∗= arg\nmax\np∈P(Gsub) Length(p)\ns.t. MedicallyValid(p)\nwhere P(Gsub) is the set of all paths in Gsub.\nThis longest-path strategy ensures that questions require multiple reasoning hops (average 4.2 per trajectory), rather\nthan being answerable via simple lookups. These paths are subsequently transformed into natural language questions\nthat require sequential tool invocations to reconstruct the complete reasoning chain.\n5\n\nA preprint\nQuestion: \"Identify the chemical\ncompound: Swiss company...\ncorporate fusion 1990s... prescription\ntablet... active ingredient mass is\nprime number... antagonizing\noctapeptide hormone receptor...\nthree-letter acronym... Australian off-\nroad equipment... elevated element\natomic number 19...\"\nReasoning: Step 1: Swiss company\n→ “Ciba-Geigy” Step 2: Merger →\n\"Novartis...\" Step 3: Medication →\n\"Valentas...\" ... Step 8: Convergence\n→ \"Valsartan\"\nAnswer: \"Valsartan\"\nLung Neoplasms\nDiabetes Mellitus\nMODY\nSeizures\nDravet Syndrome\nPubMed Entities Extraction\nKnowledge Graph Building\nSubgraph Sampling\nLLM-based Rare Entity Screening\nAbridged Example\nFigure 3: Knowledge graph-based question generation pipeline: extracting longest chains from rare entity subgraphs to\ncreate complex multi-hop medical queries.\n3.1.3\nQuality Control and Difficulty Calibration\nTo ensure that generated questions remain challenging for current systems, we implement adaptive difficulty calibration.\nEach question is evaluated against GPT-o3 deepresearch and GPT-4, if either model achieves > 50% accuracy, the\nquestion is automatically regenerated with increased complexity:\nq′ =\n\u001aq\nif max(AccO3(q), AccGPT4(q)) < 0.5\nRegenerate(q, complexity + 1)\notherwise\nThis approach ensures our dataset remains challenging even for state-of-the-art systems, directly addressing the 25.5%\nperformance ceiling previously observed in MedBrowseComp.\n3.2\nTrajectory Synthesis with Medical Tool Integration\n3.2.1\nMasked Trajectory Guidance (MTG)\nTo generate high-quality training trajectories that effectively utilize our medical-specific tools, we introduce Masked\nTrajectory Guidance(MTG). Given a reasoning graph path T = {(e1, r1, e2), . . . , (en−1, rn−1, en)} extracted from the\nknowledge graph, we create a structural scaffold by masking the entities:\nTmasked = {([MASK], ri, [MASK])}n−1\ni=1\nThis masking process serves two main purposes:\n• Tool selection learning: Encourages the model to determine when medical-specific retrieval tools are required versus\nwhen general search suffices.\n• Prevention of shortcuts: Prevents answer memorization while maintaining the underlying reasoning process.\nStep 1: Swiss Pharmaceutical History\n•\nStarted with Ciba-Geigy - a major Swiss life-sciences company\nfrom Basel\n•\nThe clue mentioned it \"ceased to exist after a landmark\ncorporate fusion\" in the 1990s\nStep 2: Corporate Merger Investigation\n•\nCiba-Geigy identified the 1996 merger of Ciba-Geigy and\nSandoz\n•\nThis created Novartis Pharmaceuticals Corporation\nStep 3: Medication Identification\n•\nSearched for Novartis heart failure medications that reduce\nhospitalization\n•\nLed to Valentas 100 Tablet - a branded prescription for cardiac\ninsufficiency\n…\n…\nStep 8: Final Convergence\n•\nAll evidence pointed to Valsartan\nOrigin Reasoning Path\nEntity_mapping:\n{\n\"Acronym_A\": \"ARB\",\n\"Clue_1\": \"The mechanism of an angiotensin II receptor\nblocker (antagonizing the AT1 receptor for the octapeptide\nhormone angiotensin II).\",\n\"Clue_2\": \"Hyperkalemia (elevated serum concentration of\npotassium, the element with atomic number 19).\",\n\"Data_A\": \"51 mg\",\n\"Entity_A\": \"Ciba-Geigy\",\n\"Entity_B\": \"Novartis Pharmaceuticals Corporation\",\n\"Entity_C\": \"Valentas 100 Tablet\",\n\"Entity_D\": \"ARB Bumpers (representing the company ARB\n4x4 Accessories)\",\n\"Event_A\": \"Novartis formed by merger of Ciba-Geigy and\nSandoz\",\n\"Target_Entity\": \"Valsartan\"\n},\nEntity Mapping\nStep 1: Swiss Pharmaceutical History\n•\nStarted with \"Entity_A\" - a major Swiss life-sciences\ncompany from Basel\n•\nThe clue mentioned it \"ceased to exist after a landmark\ncorporate fusion\" in the 1990s\nStep 2: Corporate Merger Investigation\n•\n\"Event_A\" identified the 1996 merger of Entity_A and\nSandoz\n•\nThis created \"Entity_B\" - the new pharmaceutical giant\nStep 3: Medication Identification\n•\nSearched for Entity_B heart failure medications that\nreduce hospitalization\n•\nLed to \"Entity_C\" - a branded prescription for cardiac\ninsufficiency\n…\nStep 8: Final Convergence\n•\nAll evidence pointed to Target_Entity\nMasked Hint\nFigure 4: Masked Trajectory Guidance: a structural scaffold that enables reasoning without shortcut learning by\nmasking entities.\n6\n\nA preprint\n3.2.2\nHybrid Strategy for Tool Diversity\nTo promote robust and diverse tool usage, we adopt a hybrid data strategy: Dtrain = α · Dguided + (1 −α) · Dexploration,\nwhere α = 0.7 balances structured learning with exploration. The exploration trajectories naturally cultivate three key\nbehaviors:\n• Medical tool prioritization: 78% begin with private medical retriever for rare entities\n• Tool switching: 42% demonstrate adaptive switching between general and medical tools\n• Error recovery: 34% include explicit correction using alternative tools\n4\nLarge-scale Agent Training\n4.1\nCold Start with Supervised Fine-Tuning\nWe initiate agent training via supervised fine-tuning (SFT) on a large collection of synthetic agentic dialogues D =\n{(x(i), y(i))}N\ni=1. Here, x(i) denotes the input context, and y(i) the ideal next-action sequence (thought, tool call, etc.)\nfor each example. The objective is to maximize the likelihood of generating the correct trajectory conditioned on\ncontext and prior agent history:\nLSFT(θ) = −1\nN\nN\nX\ni=1\n|y(i)|\nX\nk=1\nlog pθ(y(i)\nk |x(i), y(i)\n<k).\nTo promote agent robustness and generalization, we incorporate several key augmentations during fine-tuning:\n• Tool failure simulation (5% corruption rate): Randomly corrupts tool output to encourage contingency planning and\nrobust error recovery in downstream trajectories.\n• Intermediate thought supervision: Teaches the agent to articulate explicit reasoning prior to every tool invocation,\nimproving interpretability and decision traceability.\n• Multi-task sampling: Diversifies training batches across medical domains (diagnosis, treatment, guidelines, rare\ndiseases), supporting broad generalization and transfer.\nOptimization proceeds using the AdamW optimizer (learning rate λ = 0.01), with a cosine annealing schedule\n(ηmax = 3 × 10−7), for 3 epochs on 8 × H200 GPUs. This ensures rapid exploration of diverse trajectories and\nconvergence to well-calibrated agent policies.\n4.2\nReinforcement Learning\nAfter supervised warm-starting, we refine the agent via reinforcement learning using Grouped Regularized Policy\nOptimization (GRPO), optimizing agentic trajectories relative to task-specific composite rewards: rt = α rtask +\nβ rexpert −γ refficiency, where rtask measures answer accuracy, rexpert reflects preference according to a GPT-4-based\nexpert model, and refficiency penalizes excessive or redundant tool usage. The weighting coefficients α, β, and γ are set\nto 1.0, 0.2, and 0.1, respectively.\nReward Modeling: The reward function is broken down as follows:\n• rtask: The primary component of the reward function, measuring answer accuracy directly and calculating the\ntask completion score per query.\n• rexpert: Derived from the GPT-4 preference model, this term refines the model’s responses to align with expert\nknowledge.\n• refficiency: Penalizes unnecessary tool usage, including repeated calls to the same tool without added value,\nexcessive tool usage after an answer is found, and using irrelevant tools for a task. The efficiency penalty is\nevaluated using both rule-based systems and an LLM-judge to classify unnecessary usage.\n7\n\nA preprint\nGRPO Objective: The GRPO objective optimizes: LGRPO = E(x,y)∼D\n\u0002\nlog πθ(y|x) ·\n\u0000r(x, y) −¯\nrG(x)\n\u0001\u0003\n, where ¯\nrG(x)\nis the group-level baseline, computed as the average reward from responses in the same batch. This group normalization\nstabilizes the gradient estimates.\nAdditional Modifications:\n• KL Regularization: We remove KL-regularization from the training pipeline, as it may hinder performance\nimprovements, especially during multi-stage training. This aligns with literature showing benefits of omitting\nKL loss for model generalization[He et al., 2025].\n• Task Complexity: Task complexity increases progressively via curriculum learning, monitored by the average\npass rate on tasks. This ensures the model is challenged appropriately without overwhelming it early in\ntraining.\n5\nExperiments\nWe evaluate MedReseacher-R1 across both domain-specific and general-purpose benchmarks to assess its effectiveness\nin complex medical research tasks and its generalization capabilities beyond the medical domain.\n5.1\nBenchmarks\n• MedBrowseComp [Chen et al., 2025b] is a recently proposed benchmark specifically designed to evaluate LLM\nagents’ capabilities in retrieving and synthesizing medical evidence from multiple web sources. This benchmark\npresents agents with open-ended clinical questions that necessitate multi-step reasoning, strategic information\ngathering, and effective utilization of web browsing APIs to construct comprehensive medical assessments.\n• GAIA [Shinn et al., 2023] (General AI Assistant) is a comprehensive evaluation framework that tests real-world\nassistant capabilities through complex, multi-modal tasks requiring tool use, web search, and multi-step reasoning.\nThe benchmark emphasizes tasks that are conceptually simple for humans but challenging for AI systems, focusing\non fundamental skills like reading comprehension, logical reasoning, and the ability to use tools effectively in realistic\nscenarios.\n• XBench-DeepSearch [Chen et al., 2025a] is an extensive multi-domain agent evaluation suite that systematically\nassesses tool-use capabilities across diverse open-domain tasks. The benchmark encompasses a broad spectrum of\nscenarios including fact-checking, comparative analysis, web browsing-based reasoning, and complex information\nsynthesis tasks, providing a comprehensive evaluation of an agent’s ability to navigate and utilize various tools in\nreal-world problem-solving contexts.\n5.2\nMain Results\nAs shown in Table 1, our tool-augmented agent achieves new state-of-the-art performance on MedBrowseComp\nbenchmark, achieving a pass@1 accuracy of 27.5% and outperforming both previous best agents and the Qwen2.5-32B\nbaseline. The supervised fine-tuning (SFT) stage already delivers notable gains, while subsequent reinforcement\nlearning further improves decision quality and tool orchestration efficiency.\nNotably, despite being primarily trained for medical domains, our agent demonstrates strong generalization to open-\ndomain tasks shown in Table 2. On GAIA and XBench-deepsearch, our system shows competitive helpfulness scores,\ndemonstrating the versatility of tool-based training paradigms.\nTable 1: Performance Comparison on MedBrowseComp Benchmarks (number correct out of 50)\nModel\no3 search\ngemini2.5pro deepsearch\no3 deepresearch\nclaude-cua\nMedASearch-32B\nMedBrowseComp\n19.0\n24.5\n25.5\n18.0\n27.5\n5.3\nQualitative Analysis\nTo understand the underlying factors driving performance improvements, we conducted an in-depth analysis of the\ntraining data patterns and their impact on agent behavior. Our investigation reveals that training data following\n8\n\nA preprint\nTable 2: Performance Comparison on Xbench-DeepSearch and GAIA Benchmarks\nModel\nParadigm\nXbench-DeepSearch\nGAIA\nQwen-2.5-32B\nDirect\n8.7\n13.6\nQwen-2.5-72B\nDirect\n12.7\n14.6\nGPT-4o\nDirect\n18.0\n17.5\nGPT-4.1\nDirect\n17.0\n22.3\nQwQ-32B\nDirect\n10.7\n22.3\no4-mini\nDirect\n22.3\n33.3\nDeepSeek-R1\nDirect\n32.7\n16.5\nQwen-2.5-32B\nSearch-o1\n3.7\n28.2\nWebDancer-32B\nReAct\n38.7\n40.7\nQwQ-32B\nSearch-o1\n25.0\n39.8\nWebSailor-7B\nReAct\n34.3\n37.9\nWebSailor-32B\nReAct\n53.3\n53.2\nWebSailor-72B\nReAct\n55.0\n55.4\nMedASearch-32B\nReAct\n54.0\n53.4\nthe paradigm of iterative search-verification-synthesis yields the most significant improvements in deep research\ncapabilities.\nFigure 5 illustrates a representative example where our agent demonstrates superior research depth through systematic\nevidence gathering. The agent executes a 4-step strategy: (1) initial broad search to identify relevant sources, (2)\nverification of information consistency across multiple authoritative medical databases, (3) targeted follow-up queries to\nresolve ambiguities, and (4) comprehensive synthesis of validated findings. This methodical approach—characterized\nby multiple verification cycles ensuring answer uniqueness before final synthesis—contrasts sharply with baseline\nagents that exhibit premature convergence or suboptimal tool utilization patterns.\nAnalysis of successful trajectories reveals that the critical differentiator lies in the search→verifyn→synthesize pattern,\nwhere n represents multiple verification iterations. Training instances exhibiting this pattern show 34.2% higher success\nSearch\nVerify\nSynthesize\nRead\nFigure 5: Case study demonstrating the search-verify-synthesize paradigm: The agent performs multiple verification\nrounds across medical sources, ensuring information consistency before synthesis. Baseline agents (shown in gray)\nterminate prematurely after initial search, while our approach (blue) continues until achieving high confidence through\ncross-validation.\n9\n\nA preprint\nrates in complex multi-hop reasoning tasks compared to single-verification approaches. The iterative verification\nensures answer uniqueness and factual grounding, particularly crucial for domains requiring high accuracy such as\nmedical diagnosis.\nThese findings demonstrate that tool-augmented agent training effectiveness is fundamentally linked to the structural\npatterns in training data, with iterative verification serving as the key mechanism for developing robust deep research\ncapabilities that generalize across diverse tool-reasoning environments.\n6\nRelated Work\n6.1\nGeneral Deep Research Methods\nRecent advances in agent-based deep research and autonomous information gathering frameworks have emerged in two\nmain paradigms: Multi-agent planning architectures and Agent Reinforcement Learning systems.\nMulti-agent planning architectures decompose the research process into semantically distinct roles, with different\nagents focusing on subtasks such as retrieval, reasoning, synthesis, or evaluation. These agents collaborate through\nmodular pipelines or structured communication protocols. CAMEL [Li et al., 2023] introduces a communication-driven\nmulti-agent framework in which agents communicate using natural language to solve complex planning and reasoning\ntasks. This framework emphasizes inter-agent communication to achieve policy coordination and emergent behavior.\nSince 2025, many large-scale reasoning models (LRMs) have extended deep research capabilities, such as OpenAI O3,\nPerplexity Deep Research, and Kimi K2. For example, Anthropic’s multi-agent research system proposes a master\nagent that dynamically spawns specialized child agents to perform web search, document reading, and synthesis. This\narchitecture excels at complex, long-term research tasks through parallel execution and implicit memory sharing. There\nare also many open-source projects implementing deep learning with multi-agent mechanisms, such as Deerflow. Deep\nlearning systems implemented using multi-agent approaches have significant advantages: they are more interpretable\nand easier to scale through parallelization. However, due to the lack of targeted reasoning training, planning based\nsolely on prompts and characters can cause errors to propagate across multiple agents and is unable to handle tasks\nrequiring high-level reasoning.\nIn contrast to modular designs, Agent RL approaches train single or semi-autonomous agents through reward-guided\ninteraction with a research environment (typically web browsing or open-domain question answering). These agents\ntypically learn to autonomously search, click, read, and synthesize using offline data and are then fine-tuned through\npost-training. The ReAct agent [Yao et al., 2023b] was originally proposed as a prompting strategy but has been\nfurther optimized using RLHF to enforce optimal reasoning paths. Reinforcement learning techniques enable agents to\noptimize tool usage and avoid hallucinations during long-term interactions. WebArena [Zhou et al., 2024] provides a\nhigh-fidelity web interaction environment for training reinforcement learning agents to perform multi-hop reasoning\nand agentic data collection via real browser APIs, enabling realistic, feedback-driven learning. WebSailor [Li et al.,\n2025] enables superhuman web research in high-uncertainty QA environments. It uses synthetic task construction,\nRFT-based cold starts, and DUPO (Repeated Sampling Policy Optimization) reinforcement learning fine-tuning to\nbuild robust agents for tool-augmented web tasks. Search-R1 [Jin et al., 2025] trains LLMs to interleave reasoning and\nsearch via unified RL, while S3 [Jiang et al., 2025] decouples search from generation and reaches comparable accuracy\nwith 70× fewer samples. Compared to deep research systems based on multi-role agents, agent reinforcement learning\noffers the advantage of internalizing the model’s problem-solving capabilities through learned behaviors, enabling\nbetter generalization to unknown tasks and adaptability to complex environments such as web browsing.\nHowever, while general-purpose web agents excel in open-domain environments, their architecture systematically\nignores the importance and time constraints inherent to evidence provenance in healthcare. The lack of healthcare-\nspecific components (e.g., de-identification engines, clinical-level evidence graders, and medication compliance audits)\nseverely limits their clinical utility.\n6.2\nMedical RAG Systems\nThe domain-specific Retrieval-Augmented Generation (RAG) architecture has made a significant contribution to the\nfield of medical clinical AI through systematic innovations in evidence integration.\nMedRAG [Zhao et al., 2025b] establishes a paradigm for evidence-based generation by enabling immutable corpus\nretrieval from PubMed snapshots and proprietary databases. Deeprare [Zhao et al., 2025a] MedRAG’s real-time\nevidence assimilation, which continuously synchronizes with evolving medical knowledge through live CDC/WHO data\nstreams and dynamically weights it (F1 score +14.3%), directly addresses the knowledge obsolescence issue inherent in\nsystems like DeepRare. SurgRAW [Low et al., 2025] pioneered the integration of real-time surgical video retrieval with\n10\n\nA preprint\nreinforcement learning, enabling intraoperative decision support with an instrument recognition accuracy of 90.2%.\nFederated ClinicalCamel [Toma et al., 2023] addresses data fragmentation through cross-institutional knowledge\ndistillation while maintaining privacy compliance (AUROC of 0.92 across 12 hospitals).\nDespite these advances, current medical RAG systems still suffer from fundamental limitations. First, knowledge\nobsolescence remains a critical issue, as module updates in systems like DeepRare [Zhao et al., 2025a] require\nmanual orchestration, resulting in curation delays that can reduce retrieval relevance by months. In addition, evidence\nmisalignment manifests as semantic drift, which is particularly evident in KBLaM’s plug-in architecture. Module\nupdates lead to cumulative embedding misalignment (MRR drops by 18.4% after 5 iterations) [Wang et al., 2025].\n6.3\nMedical Multi-Role Systems\nRecent advances in agent-based architectures reveal a paradigm shift through endogenous integration of retrieval-\nreasoning-verification loops, particularly evident in the emergence of Agentic RAG frameworks and multimodal\nknowledge integration. These systems demonstrate three core innovations that redefine clinical decision support:\nDynamic Knowledge Internalization through self-updating graphs that eliminate external dependencies enables\ncontinuous synchronization with evolving medical knowledge. SeaKR’s [Yao et al., 2024] self-aware retrieval introduces\ntemporal grounding mechanisms that dynamically adjust knowledge weights based on publication recency and evidence\ngrade, while Med-PaLM’s [Tu et al., 2023] visual-linguistic separation processes radiology images and genomic data\nvia dedicated pathways while maintaining diagnostic coherence. These approaches reduce knowledge latency from\ndays to minutes compared to traditional RAG systems. Preference-Aligned Reinforcement Learning frameworks\nlike MedicalGPT v2.4’s GRPO (Group Relative Policy Optimization) achieve 98.7% agreement with clinician panels\nin oncology decisions [Xu, 2023]. Unified Cognitive Architectures collapse retrieval-reasoning-verification into\nintegrated pipelines, exemplified by Microsoft’s MAI-DxO [Nori et al., 2025] with five collaborative agents achieving\n85.5% diagnostic accuracy - quadruple average clinician performance. Regulatory compliance is maintained through\nMed-Gemini’s [Saab et al., 2024] 3-stage pipeline combining temporal grounding, clinician-verified SFT, and multi-\nobjective RLHF.\nDespite these advancements, critical limitations persist in the reasoning capabilities of current medical multi-role\nagent systems - a fundamental gap compared to deep reasoning methods in medical research. First, multi-step clinical\nreasoning remains constrained by shallow inference depth: while systems like AgentClinic [Schmidgall et al., 2025]\ndemonstrate 42.9% diagnostic accuracy in sequential decision-making, this drops significantly when tasks require\n> 5 reasoning steps (↓27.3% at 7 steps). Second, causal reasoning deficits manifest in treatment planning scenarios,\nwhere agents struggle to model long-term outcome dependencies (e.g., chemotherapy sequencing effects) compared to\nhuman specialists (F1-score gap of 19.4% in NCCN guideline adherence) [?]. Third, adaptive reasoning limitations\nemerge in dynamic clinical environments - systems like MAI-DxO show 34% performance degradation when handling\nreal-time patient deterioration scenarios requiring protocol switching. These challenges highlight the urgent need for\nnext-generation architectures that bridge the reasoning depth and adaptability gap between multi-role agents and human\nmedical experts.\n7\nConclusion\nIn this work, we address the challenge of complex, evidence-based medical research by introducing a new agent\ndevelopment framework centered on the KISA data generation approach. KISA systematically produces challenging,\nmulti-hop medical question–answer pairs with corresponding reasoning trajectories, grounded in rare entity mining and\nknowledge graph-based reasoning chains. This ensures that agents are exposed to the intricate, compositional problems\ncharacteristic of real-world medical research.\nBuilt on this rich dataset and equipped with a comprehensive training pipeline—including supervised fine-tuning,\ntrajectory masking, and reinforcement learning with specialized medical tools—our agent, MedReseacher-R1 achieves\nstate-of-the-art pass@1 accuracy on MedBrowseComp (27.5%) and demonstrates robust performance on general agent\nbenchmarks. These findings show that MedReseacher-R1 is capable of solving complex medical questions that demand\nsystematic exploration and nuanced evidence synthesis, highlighting its effectiveness as a next-generation deep research\nagent in the medical domain.\n8\nFuture Work\nBuilding on the foundation of this study, we identify several concrete directions for advancing deep medical research\nagents:\n11\n\nA preprint\n• Multi-modal Tool Integration: Extend the current framework to support multi-modal medical tools such as radiology\nimage viewers, pathology slide analyzers, genomic data sources, and electronic health records. Such integration\nwould enable agents to process and synthesize diverse data types, aligning more closely with real-world clinical\nworkflows.\n• Human-Expert Collaboration: Incorporate human-in-the-loop feedback from medical professionals to guide agent\nbehavior. Developing interfaces for expert evaluation and annotation can improve reasoning quality, tool usage, and\nthe clinical relevance of agent outputs.\n• Safety and Reliability: Systematically study model safety and reliability for open deployment, focusing on robust\nhallucination detection, uncertainty estimation, and the implementation of fail-safe mechanisms suitable for high-\nstakes medical scenarios.\n• Advanced Medical Reasoning Benchmarks: Construct a comprehensive benchmark for complex multi-hop\nreasoning across medical domains—covering pharmacology, diagnostics, epidemiology, genetics, surgical planning,\nand therapy. This would set a higher standard for evaluating agents’ ability to orchestrate tools and synthesize\nevidence in challenging scenarios.\nOur framework paves the way for more aligned and reliable agent-based systems in specialized domains like healthcare.\nBy releasing our codebase, datasets, and trained models, we seek to foster collaborative progress and rigorous evaluation,\nmoving toward trustworthy AI companions that can augment medical research and support improved patient outcomes."}
{"id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "authors": ["Hend Al-Khalifa", "Raneem Almansour", "Layan Abdulrahman Alhuasini", "Alanood Alsaleh", "Mohamad-Hani Temsah", "Mohamad-Hani_Temsah", "Ashwag Rafea S Alruwaili"], "source": "http://arxiv.org/abs/2508.14869v1", "page_count": 15, "text": "The Prompting Brain: Neurocognitive Markers of\nExpertise in Guiding Large Language Models\n\nHend S. Al-Khalifa\nInformation Technology Department,\nCollege of Computer and Information\nSciences\nKing Saud University\nRiyadh, Saudi Arabia\nhendk@ksu.edu.sa\nRaneem Almansour\nRadiological sciences, College of\nApplied Medical Sciences\nKing Saud University\nRiyadh, Saudi Arabia\n443202782@student.ksu.edu.sa\nLayan Abdulrahman Alhuasini\nRadiological sciences, College of\nApplied Medical Sciences\nKing Saud University\nRiyadh, Saudi Arabia\n443202718@student.ksu.edu.sa\nAlanood Alsaleh\nRadiological sciences, College of\nApplied Medical Sciences\nKing Saud University\nRiyadh, Saudi Arabia\nalalanoud@ksu.edu.sa\nMohamad-Hani Temsah\nPediatric Department, College of\nMedicine\nKing Saud University\nRiyadh, Saudi Arabia\nmtemsah@ksu.edu.sa\nAshwag Rafea S Alruwaili\nRadiological sciences, College of\nApplied Medical Sciences\nKing Saud University\nRiyadh, Saudi Arabia\nashalruwaili@ksu.edu.sa\nAbstract\nPrompt engineering has rapidly emerged as a critical skill for effective interaction with large\nlanguage models (LLMs). However, the cognitive and neural underpinnings of this expertise\nremain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study\ninvestigating differences in brain functional connectivity and network activity between experts and\nintermediate prompt engineers. Our results reveal distinct neural signatures associated with\nhigher prompt engineering literacy, including increased functional connectivity in brain regions\nsuch as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency\ndynamics in key cognitive networks. These findings offer initial insights into the neurobiological\nbasis of prompt engineering proficiency. We discuss the implications of these neurocognitive\nmarkers in Natural Language Processing (NLP). Understanding the neural basis of human\nexpertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces,\ncontribute to cognitive models of LLM interaction, and potentially guide the development of AI\nsystems that better align with human cognitive workflows. This interdisciplinary approach aims to\nbridge the gap between human cognition and machine intelligence, fostering a deeper\nunderstanding of how humans learn and adapt to complex AI systems.\n1. Introduction\nThe advent of sophisticated Large Language Models (LLMs) has revolutionized numerous\ndomains, yet their effective utilization hinges significantly on the art and science of prompt\nengineering. The ability to craft precise and effective prompts that guide LLMs towards the desired\noutputs is becoming an indispensable skill. While the Natural Language Processing (NLP)\ncommunity has made substantial strides in developing more capable LLMs and understanding\ntheir internal workings [1], [2], the human side of this interaction—specifically, the cognitive and\n\nneural mechanisms that differentiate expert prompt engineers from novices—remains a nascent\narea of research. As LLMs become increasingly integrated into daily tasks and complex problem-\nsolving, understanding the cognitive demands and neural adaptations associated with mastering\nprompt engineering is crucial for optimizing human-AI collaboration.\n\nThis study took an interdisciplinary approach, leveraging neuroimaging techniques to explore the\nneural correlations of prompt engineering expertise. We hypothesize that expertise in prompt\nengineering is not merely a matter of accumulated knowledge but is also reflected in distinct\npatterns of brain function and connectivity. The primary contributions of this work are threefold.\nFirst, we introduce a novel methodology for quantifying prompt engineering literacy and use it to\ndifferentiate between expert and intermediate users. Second, we provide the first, to our\nknowledge, empirical evidence from fMRI data identifying distinct neural signatures (including\naltered functional connectivity in language and executive control regions, and different network\npower dynamics) associated with prompt engineering expertise. Third, we discuss the potential\nimplications of these neurocognitive markers for the NLP field, particularly for designing more\nintuitive human-AI interfaces, developing cognitive models of LLM interaction, and guiding the\ncreation of AI systems that better align with human cognitive processes. By identifying these\nneural markers, we aim to provide a foundational understanding of the cognitive processes that\nunderpin successful human-LLM interaction. This research seeks to bridge insights from cognitive\nneuroscience with challenges and opportunities within NLP. Understanding how the human brain\nadapts to the task of \"programming\" LLMs through natural language can inform the design of\nmore intuitive prompting interfaces, contribute to the development of training programs for prompt\nengineering, and potentially inspire new architectures for LLMs that are more attuned to human\ncognitive styles.\n\nOur pilot study employed functional magnetic resonance imaging (fMRI) to compare resting-state\nbrain activity between individuals classified as experts and intermediate prompt engineers based\non a validated Prompt Engineering Literacy Scale. We focused on differences in functional\nconnectivity and network power dynamics to identify objective neural indicators of prompting\nproficiency. The findings presented herein offer preliminary evidence for such neural distinctions,\nand open avenues for future research at the intersection of neuroscience, cognitive science, and\nNLP. This work aims to contribute to a more holistic understanding of human-AI interaction,\nmoving beyond model-centric analyses to incorporate the cognitive and neural dimensions of the\nhuman user.\n2. Related Work\nWith the development of LLMs, the field of prompt engineering has grown rapidly. Initial work\nfocused on empirical discoveries of effective prompting strategies [1], [3]. More recent research\nhas sought to systematize prompt engineering, categorizing techniques and exploring their\nefficacy across different models and tasks [4], [5]. For instance, Priyadarshana et al. (2024)\nprovide a review of prompt engineering types, methods, and tasks, particularly in the context of\ndigital mental health, highlighting techniques like n-shot prompting and chain-of-thought (CoT)\n\nprompting. CoT prompting, which encourages models to produce intermediate reasoning steps,\nhas been shown to improve performance on complex reasoning tasks [3], [6].\n\nWhile much research has focused on the AI side of prompt engineering, the human cognitive\naspects are less understood. Some studies have explored how human interaction styles and\ncognitive biases affect LLM performance [7], but direct neuroimaging investigations of prompt\nengineering expertise are scarce. Cognitive science offers theories on expertise development,\nsuggesting that experts develop specialized mental representations and processing strategies [8].\nIt is plausible that prompt engineering expertise involves similar cognitive adaptations, potentially\nreflected in neural activity.\n\nNeuroimaging studies of related complex cognitive skills, such as programming or problem-\nsolving, have identified involvement of prefrontal cortex (executive functions, planning), parietal\ncortex (spatial reasoning, attention), and temporal cortex (language, semantic memory) [9], [10].\nFor instance, studies on software developers have shown differences in brain activation patterns\nbetween experts and novices during code comprehension tasks, often implicating language and\nworking memory networks [9]. Our study builds on this by specifically investigating the neural\ncorrelates of expertise in interacting with LLMs through natural language prompts, a unique form\nof human-computer interaction that blends linguistic and logical reasoning.\n\nThe intersection of NLP and cognitive neuroscience is an emerging and rapidly advancing area.\nResearchers are increasingly exploring how neural data can inform AI models and, conversely,\nhow computational models can provide insights into brain function [11], [12]. Recent work by [13]\nintroduced a unified computational framework connecting acoustic, speech, and word-level\nlinguistic\nstructures\nto\nstudy\nthe\nneural\nbasis\nof\neveryday\nconversations\nusing\nelectrocorticographic (ECoG) and the Whisper model. They demonstrated an alignment between\nthe model’s internal processing hierarchy and the cortical hierarchy for language. Further\nsupporting this line of inquiry, [14] utilized pre-trained NLP models with intracranial recordings to\ndiscover neural signals reflecting speech production, comprehension, and their transitions during\nnatural conversation, highlighting broadly distributed frontotemporal activities specific to the\nwords and sentences being conveyed. Complementing these findings, [15] also showed that brain\nembeddings derived from intracranial recordings in the inferior frontal gyrus (IFG) share common\ngeometric patterns with contextual embeddings from deep language models (DLMs), suggesting\na vector-based neural code for natural language processing. Our work, while focusing on the\nspecific skill of prompt engineering expertise rather than general language processing in\nconversation or direct neural encoding of speech, shares the overarching goal of understanding\nthe neural underpinnings of human interaction with complex language-based systems. The\nmethodologies and findings from these recent studies, particularly the emphasis on aligning\ncomputational model representations (embeddings) with neural activity, provide a valuable\ncontext for interpreting our results on how expertise in manipulating such systems might manifest\nneurally. Our work contributes to this interdisciplinary dialogue by examining the neural\nunderpinnings of a critical skill in the current NLP landscape, potentially offering insights into both\nhuman-centered AI design and cognitive theories of expertise in technologically mediated tasks.\n\n3. Methods\n3.1 Study Design and Participants\nThis research was conducted as a cross-sectional pilot study. A total of 22 participants (aged 18–\n45 years) were recruited. Participants were screened to meet specific inclusion criteria, including\nright-handedness and no history of neurological or psychiatric disorders. Table 1 shows\nparticipants demographics. Ethical approval was obtained from our institutional review board (No.\nE-24-9142), and all participants provided informed consent prior to their involvement in the study.\n\nParticipants were classified into two groups, \"intermediate\" and \"expert,\" based on their scores\non a custom-developed instrument called Prompt Engineering Literacy Scale (PELS) (see\nAppendix A). This scale consisted of 10 questions, each rated on a 5-point Likert scale, yielding\na maximum possible score of 50. Individuals scoring above 37 were categorized as experts, while\nthose scoring 37 (70%) or below were categorized as intermediate. The scale was designed to\nassess four key dimensions of prompting expertise: (1) Prompt Construction and Clarity, (2)\nAdvanced Prompting Techniques (knowledge of various prompting techniques), (3) Verification\nand Optimization (methods to validate and optimize the quality of AI responses), and (4) Ethical\nand Cultural Sensitivity in prompt formulation. The PELS instrument was developed through a\nrigorous process involving expert consultation, literature review and pilot testing. Initial items were\ngenerated based on a comprehensive review of prompt engineering literature [16] and\nconsultation with three prompt engineering experts with over two years of experience in the field.\n\nTo stratify participants into \"expert\" and \"intermediate\" groups, we applied a threshold of 37 out\nof 50 on the PELS. This cutoff was informed by a combination of empirical score distribution and\nexpert consensus, consistent with standard practices in early-stage scale deployment [17], [18].\nDuring pilot validation, individuals with demonstrated prompting proficiency (e.g., graduate\nstudents with AI coursework or industry experience) consistently scored above this threshold. In\naddition, qualitative feedback from participants during pilot testing supported the idea that scores\nabove this level better reflected advanced prompting fluency.\n\nWhile this cutoff is exploratory rather than norm-referenced, it functioned as a practical criterion\nfor distinguishing proficiency levels in a small sample. This approach aligns with preliminary\nclassification practices in early psychometric work [19]. Future versions of the PELS could be\nrefined using item response theory [20] or receiver operating characteristic (ROC) analysis [21]\nto establish statistically optimized thresholds.\n\nTo ensure the validity and reliability of the instrument, we conducted both external and internal\nconsistency assessments. Before the main study, external validity was established through expert\nreview by a panel of experts in AI and LLMs as well as neuroscientists, who evaluated the\ninstrument for content relevance and clarity. Their feedback was incorporated into the final\nversion. The questionnaire was subjected to a pilot study for internal consistency with a random\nsample of subjects to assess its validity and reliability. The pilot study yielded a Cronbach's alpha\nof (α = 0.90), indicating high reliability and validity of the instrument.\n\nTable 1: Participants’ Demographics\n\nCharacteristics\nIntermediate Expert\nNumber of users\n10\n12\nGender ratio (female: male) 8:2\nFemales=8\nMales= 2\n7:5\nFemales=7\nMales =5\nAge (Mean±SD)\n22.5± 4.8\n22.25± 3.8\nEducation Level\nBachelor (9)\nPh.D. (1)\nBachelor (11)\nPh.D. (1)\nPELS scores (Mean±SD)\n32.2 ± 3.3\n40.9± 3.42\n\n3.2 Data Acquisition\nMagnetic Resonance Imaging (MRI) data were acquired using a Siemens MAGNETOM Spectra\n(DE) 3T scanner equipped with a standard head coil. During the fMRI scan, participants were\ninstructed to relax, remain still, keep their eyes open, and stay awake, consistent with a resting-\nstate fMRI paradigm. High-resolution T1-weighted anatomical images were also acquired for\nregistration purposes. Table 2 details the specific parameters used for both the T1-weighted\nanatomical scans and the resting-state fMRI.\n\nTable 2: MR Imaging parameters used for both the T1-weighted anatomical scans and the\nresting-state fMRI BOLD EPI sequence, which are: Repetition Time (TR), Echo Time (TE),\nflip angle, field of view (FOV), voxel size, matrix size, and time.\n\nTR\nTE\nFlip angle FOV\nVoxel size\nMatrix Time\nT1-\nweighted-\n(3D)\n1900ms 2.42ms 9 degrees\n250mm\n1.0\n×1.0\n×1.0mm\n250*250 5.11 min\nResting-\nfMRI\n2270ms 27ms\n90 degrees\n250 mm 2.7×2.7×2.5mm 93*93\n11.2 min\n\ngradient-\necho\n\n3.3 fMRI Data Analysis\nFunctional MRI data were preprocessed and analyzed using two complementary pipelines to\nensure robust identification of group-level differences in brain network activity and functional\nintegration.\n\nPreprocessing: Standard fMRI preprocessing steps were applied, including motion correction,\nslice-timing correction, spatial normalization to a standard template (e.g., MNI space), and spatial\nsmoothing. Nuisance regression was performed to remove effects of motion parameters, white\nmatter signal, and cerebrospinal fluid signal.\n\nIndependent Component Analysis (ICA): Group ICA was performed using the GIFT (Group\nICA of fMRI Toolbox) software. This data-driven approach decomposes the fMRI data into a set\nof spatially independent components and their corresponding time courses. We focused on\nidentifying differences in the spectral power of these components between the expert and\nintermediate groups, particularly the ratio of low-frequency power (LF, e.g. 0.01-0.08 Hz) to high-\nfrequency power (HF, e.g. >0.1 Hz) within established resting-state networks. The number of\ncomponents (n=20) was chosen a priori based on previous resting-state fMRI studies of expertise.\n\nSeed-to-Voxel Connectivity Analysis: Seed-based functional connectivity analysis was\nconducted using the CONN toolbox. Regions of interest (ROIs) identified from prior literature or\nICA results were used as seeds. For each participant, Pearson correlation coefficients were\ncalculated between the mean time series of each seed ROI and the time series of all other voxels\nin the brain. These correlation maps were then converted to z-scores using Fisher’s r-to-z\ntransformation to allow for group-level statistical comparisons (two-sample t-tests) between the\nexpert and intermediate groups, controlling for relevant covariates if necessary.\n4. Results\nThe analysis of fMRI data revealed significant differences between the expert and intermediate\nprompt engineering groups, both in terms of intrinsic network activity and specific functional\nconnections.\n4.1 Differences in Network Power Ratios\nUsing separate ICA analyses for each group, Power_LF/Power_HF ratios were computed to\nassess low-frequency dominance. Three key components showed higher ratios in the expert\ngroup, indicating greater low-frequency synchronization: Experts demonstrated strikingly higher\n\nLow-to-High Frequency Power Ratios (LHR) in core cognitive networks, as shown in Table 3 and\nFigure 1.\n\nFigure 1: Experts showed higher Power_LF/Power_HF ratios in key networks: Ventral\nVisual Network (VVN) (example: A,B) – 63.0 vs. 36.7, Posterior Default Mode Network\n(pDMN) (example: C,D)– 44.4 vs. 33.2, and Left Lateral Parietal Network (LLPN) (example;\nE,D)– 53.3 vs. 36.7. These differences suggest stronger low-frequency synchronization in\nexperts. The LLPN is part of the default mode network and is associated with semantic\nprocessing and episodic memory retrieval.\n\nTable 3: Group Differences in Low-to-High Frequency Power Ratios (Power_LF/Power_HF)\n\nNetwork\nPower_LF/Power_HF (Experts)\nPower_LF/Power_HF\n(Intermediates)\nVentral Visual\nNetwork (VVN)\n63\n36.7\nLeft\nLateral\nParietal\nNetwork\n53.3\n36.7\n\nPosterior\nDefault Mode\nNetwork\n44.4\n33.2\n\nAs indicated by the preliminary data (Figure 1 and Table 3), these differences were prominent in:\n● The Ventral Visual Network (VVN): Experts showed a ratio of approximately 63.0 versus\n36.7 in intermediates.\n● The Posterior Default Mode Network (pDMN): Experts showed a ratio of approximately\n44.4 versus 33.2 in intermediates.\n● The Left Lateral Parietal Network (LLPN) (potentially referring to a component of the\nfronto-parietal network or language network): Experts showed a ratio of approximately\n53.3 versus 36.7 in intermediates. The LLPN is part of the default mode network and is\nassociated with semantic processing and episodic memory retrieval.\n\nThese differences suggest stronger low-frequency synchronization and potentially more stable\nintrinsic network dynamics in individuals with higher prompt engineering expertise.\n4.2 Differences in Functional Connectivity\nSeed-to-voxel connectivity analyses identified specific brain regions where functional connectivity\ndiffered significantly between the two groups:\n\nFigure 2: Increased connectivity in the left middle temporal gyrus (p < 0.03) in expert group\n\n● Left Middle Temporal Gyrus (MTG): The expert group demonstrated significantly\nincreased functional connectivity involving the left MTG (p < 0.03), as suggested by Figure\n2. The specific seed or target regions connected to the left MTG would be detailed based\non the full analysis, but this region is classically involved in language processing, semantic\nmemory, and multimodal integration.\n\nFigure 3: Increased connectivity in the left frontal pole (p < 0.05) in expert group\n\n● Left Frontal Pole (FP): Similarly, the expert group showed increased functional\nconnectivity in the left frontal pole (p < 0.05), as suggested by the placeholder for Figure\n3. The frontal pole is associated with higher-order cognitive functions, including planning,\ngoal-directed behavior, abstract reasoning, and metacognition.\n\nTo validate these results, Fractional Amplitude of Low-Frequency Fluctuations (fALFF), where\nfALFF is defined as the ratio of the ALFF at each voxel divided by the signal power over the entire\nfrequency range values were extracted from a unified ICA of all 22 participants. Experts\ndemonstrated consistently lower fALFF across multiple components, reflecting reduced\nspontaneous fluctuations and potentially more efficient neural processing at rest, consistent with\nthe cognitive control role of fALFF (Figure 4).\n\nFigure 4: Experts showed consistently lower fALFF values across key components: 0.534\nvs. 0.852 in Component 18, 0.596 vs. 0.824 in Components 17 and 19, and 0.548 vs. 0.752\nin Component 20, indicating reduced spontaneous low-frequency activity in experts.\n\nThe components shown in Figure 4 (Components 18-20) correspond to specific brain networks:\nComponent 18 is responsible for spatial attention, eye movement planning, Component 19 is part\nof the medial visual network, and Component 20 is a region involved in cognitive control network.\nWhile we interpret lower fALFF values in experts as potentially reflecting more efficient neural\nprocessing, several alternative explanations warrant consideration. Lower fALFF could also\nindicate differences in neurovascular coupling, reflect compensatory mechanisms, or represent\nstrategy-specific adaptations rather than general efficiency. Additionally, amplitude differences\ncan be influenced by various physiological factors including cerebral blood flow, metabolism, and\nneurotransmitter dynamics [22]. The consistency of this pattern across multiple networks,\nhowever, suggests a systematic difference in neural dynamics between expert and intermediate\nprompt engineers that merits further investigation with task-based paradigms.\n5. Discussion\nThe findings of this pilot study provide initial neurobiological evidence distinguishing expert prompt\nengineers from those with intermediate skills, with specific, actionable implications for both NLP\nand HCI design. The observed differences in low-frequency power ratios within key brain\nnetworks, such as the visual network (VVN), posterior default mode network (pDMN), and left\nlateral parietal network (LLPN) suggest that expertise in prompt engineering may be associated\nwith more organized and stable intrinsic neural activity. Higher LF/HF power ratios are often\ninterpreted as reflecting more efficient neural processing or greater network integration [23], [24]\nindicate that expert prompt engineers may possess neural systems more finely tuned for\nmanaging the cognitive demands of prompting. For example, the involvement of the DMN may\nsupport internal thought processes and semantic integration crucial for prompt formulation [25],\n\nwhile visual and parietal network activity may underlie the conceptual and spatial structuring of\nprompt elements [26].\n\nThe increased functional connectivity in the left middle temporal gyrus (MTG) and left frontal pole\namong experts is particularly notable. The MTG, a hub for semantic processing and contextual\nintegration [27], [28], likely supports experts' refined ability to access and manipulate language\nrepresentations. The frontal pole, implicated in strategic planning and cognitive control [29], [30],\nmay enable experts to construct and iteratively refine prompts using higher-order strategies.\nThese insights suggest that reducing cognitive load in this region could benefit novices, a goal\nthat might be achieved through interface designs incorporating structured planning scaffolds, such\nas prompt templates with predefined sections for context, constraints, and examples.\nAlthough this study focused on resting-state connectivity, prior research demonstrates strong links\nbetween resting-state network architecture and performance on complex tasks [31], [32], [33].\nOur findings, interpreted through this lens, imply that the neural systems supporting expert prompt\nengineering are primed for efficient engagement during task performance, even in the absence\nof explicit prompts. Notably, the involvement of visual networks, while seemingly counterintuitive\nfor a linguistic task, aligns with theories of visual mental imagery and mental model construction.\nExperts may simulate AI responses and mentally visualize prompt-output relationships, engaging\nvisual processing regions to spatially organize abstract linguistic structures [9], [34], [35]. These\nstrategies could be supported in HCI through visual interfaces that represent prompt logic and\nflow, such as flowchart-style diagrams, offloading mental simulation and aligning with expert\ncognitive styles.\nOur results also resonate with broader trends in computational neuroscience and NLP that\nexplore the alignment between model embeddings and neural representations. For instance,\nGoldstein et al. [13], [15] demonstrate how unified embedding spaces can map onto cortical\nhierarchies, while Cai et al. [14] highlight how language model activity parallels distributed neural\ndynamics during speech comprehension. Though our focus is specific to prompt engineering\nexpertise, the underlying principle remains the same: complex language interactions, like those\nrequired to prompt LLMs effectively, are grounded in specialized configurations of brain networks.\nThe enhanced connectivity we observe in the left MTG and frontal pole may reflect a domain-\nspecific neural adaptation that facilitates effective engagement with the abstract embedding\nspaces used by LLMs suggesting that future models could be designed to anticipate or support\nthese expert cognitive strategies.\nIn summary, these neurocognitive findings provide a foundation for cognitively-informed design\nprinciples in NLP and HCI. Interfaces tailored to support semantic retrieval, visual reasoning, and\nexecutive planning could ease the transition from novice to expert. Educational tools might train\nusers to progressively engage in the relevant neural systems, while prompt engineering platforms\ncould incorporate adaptive scaffolds that evolve with user proficiency. Moreover, neural markers\nof expertise could serve as user-centered evaluation metrics, favoring tools that reduce\nunnecessary executive demand while preserving task performance. Ultimately, by integrating\nneuroscientific insights with interface design and model development, we can foster more\nintuitive, efficient, and human-aligned interactions with AI systems.\n\n6. Conclusion\nThis pilot fMRI study provides novel preliminary evidence for distinct neural signatures associated\nwith prompt engineering expertise. We observed that individuals with higher proficiency in prompt\nengineering exhibit altered low-frequency power dynamics in key cognitive networks (VVN,\npDMN, LLPN) and increased functional connectivity in brain regions critical for language\nprocessing and higher-order cognition (left MTG, left frontal pole). These findings, while requiring\nreplication and extension, offer a first glimpse into the neurobiological underpinnings of this\nincreasingly vital skill.\n\nThe implications of this research extend into the domain of Natural Language Processing,\nparticularly in the context of human-AI interaction. By understanding the neural basis of how\nhumans effectively interact with and guide LLMs, we can pave the way for more intuitive AI\ninterfaces, develop more targeted training methodologies for prompt engineering, and potentially\ninform the design of next-generation LLMs that are better aligned with human cognitive\narchitectures. Future research should aim to expand on these findings, perhaps by investigating\nneural changes during active prompting tasks or longitudinally tracking the development of these\nneural markers as individuals gain prompting expertise. Such interdisciplinary endeavors are\ncrucial for fostering a synergistic relationship between human intelligence and artificial\nintelligence.\n\nThis study has several limitations that should be considered when interpreting the findings. Firstly,\nthe pilot nature of the study means our sample size (N=22) is relatively small. While sufficient for\ndetecting initial group differences, larger cohorts are needed to confirm these results and to\nexplore more subtle effects or individual variability. Secondly, the cross-sectional design does not\nallow for causal inferences; we can observe associations between expertise and neural patterns,\nbut not whether these neural patterns are a cause or a consequence of developing expertise.\nLongitudinal studies tracking individuals as they develop prompt engineering skills would be\nbeneficial.\n\nAnother limitation concerns the selection of the cutoff score of 37 on the PELS to define expert\nversus intermediate groups. While the decision was informed by expert review and observed\nclustering in pilot data, it remains a preliminary, heuristic threshold. The scale has not yet\nundergone advanced psychometric calibration such as latent class analysis or IRT modeling,\nwhich are considered best practices in modern test development [20]. This limitation highlights\nthe need for future larger-scale validation studies to establish stable, generalizable classification\nboundaries. As the field of prompt engineering literacy matures, more robust data-driven\ntechniques can be employed to refine and validate such thresholds [17]. Additionally, our reliance\non resting-state fMRI alone limits our ability to directly link neural patterns to specific cognitive\nprocesses involved in prompt engineering. Complementary task-based fMRI studies where\nparticipants actively engage in prompt formulation and refinement would provide more direct\nevidence of the neural systems supporting this skill. Such studies could employ parametric\ndesigns manipulating prompt complexity or constraints to isolate specific cognitive components\nof prompt engineering expertise.\n\nFinally, while our findings suggest neural markers associated with prompt engineering expertise,\nthe rapidly evolving nature of AI systems means that the cognitive demands of prompt engineering\nmay change over time. As large language models and other AI systems continue to advance, the\nspecific skills and strategies that constitute effective prompt engineering may shift, potentially\naltering the associated neural correlates. Longitudinal research tracking these changes would\nprovide valuable insights into the co-evolution of human cognitive adaptations and AI capabilities."}
{"id": "2508.14828v1", "title": "Long Chain-of-Thought Reasoning Across Languages", "date": "2025-08-20", "authors": ["Josh Barua", "Seun Eisape", "Kayo Yin", "Alane Suhr"], "source": "http://arxiv.org/abs/2508.14828v1", "page_count": 11, "text": "Published at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025\nLong Chain-of-Thought Reasoning Across Languages\nJosh Barua, Seun Eisape, Kayo Yin, Alane Suhr\nUniversity of California, Berkeley\njoshbarua@berkeley.edu\nAbstract\nScaling inference through long chains-of-thought (CoTs) has unlocked im-\npressive reasoning capabilities in large language models (LLMs), yet the\nreasoning process remains almost exclusively English-centric. We construct\ntranslated versions of two popular English reasoning datasets, fine-tune\nQwen 2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of\nlong CoT generation across French, Japanese, Latvian, and Swahili. Our\nexperiments reveal three key findings. First, the efficacy of using English as\na pivot language varies by language: it provides no benefit for French, im-\nproves performance when used as the reasoning language for Japanese and\nLatvian, and proves insufficient for Swahili where both task comprehension\nand reasoning remain poor. Second, extensive multilingual pretraining in\nQwen 3 narrows but does not eliminate the cross-lingual performance gap.\nA lightweight fine-tune using only 1k traces still improves performance\nby over 30% in Swahili. Third, data quality versus scale trade-offs are\nlanguage dependent: small, carefully curated datasets suffice for English\nand French, whereas larger but noisier corpora prove more effective for\nSwahili and Latvian. Together, these results clarify when and why long\nCoTs transfer across languages and provide translated datasets to foster\nequitable multilingual reasoning research.1\n1\nIntroduction\nReasoning has become a central focus of LLM research in the past few years. A key\ndriver of these advancements is chain-of-thought (CoT) prompting (Wei et al., 2022), an\ninference-time technique where models explicitly generate intermediate reasoning steps\nbefore producing the final answer. By scaling inference-time compute to enable longer\nCoTs, LLMs have achieved expert-level performance on challenging benchmarks across\nmathematics, programming, and STEM disciplines (OpenAI, 2024; DeepSeek-AI, 2025).\nDespite this progress, analyses of inference-compute scaling laws (Wu et al., 2024; Snell\net al., 2025) and long chain-of-thought reasoning (Yeo et al., 2025) have focused primarily\non English language settings.\nEarly studies on multilingual reasoning showed that English CoT prompting significantly\nimproved performance on multilingual math benchmarks (Shi et al., 2023). These results\nhighlighted the potential for English to serve as a pivot language, a high-resource language\nin which models can process multilingual queries before generating responses in the target\nlanguage, for reasoning. More recent work shows gains through test-time scaling, prompt-\nbased techniques, and synthetic data augmentation (Son et al., 2025; Yong et al., 2025; Ko\net al., 2025), yet these improvements appear only when generating their reasoning chains-of-\nthought in English. While using English as a pivot language has led to increased benchmark\nperformance, this approach limits the accessibility of LLM reasoning for speakers of other\nlanguages.\nIn this work, we present a systematic study of long chain-of-thought reasoning in a mul-\ntilingual context. First, we show that using English as a pivot language is not a universal\nsolution. For high-resource languages, reasoning in the native language performs on par\n1Code and data will be released here: https://github.com/Berkeley-NLP/Multilingual-Long-CoT\n1\n\nPublished at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025\nwith English. Mid-resource languages gain from reasoning in English, whereas low-resource\nlanguages see little to no benefit from English pivoting. Second, we find that extensive\nmultilingual pretraining significantly reduces these performance gaps, although targeted\nsupervised fine-tuning (SFT) yields important gains for low-resource languages. Lastly, we\nhighlight that the trade-off between data quality and quantity in fine-tuning is strongly\nlanguage-dependent. We hope that our insights and released datasets will support ongoing\nefforts to achieve more equitable reasoning capabilities across languages.\n2\nTask and Data\n2.1\nDataset Construction\nTraining LLMs for complex reasoning requires datasets of long CoT traces, which are\npredominantly available only in English. We address this limitation by creating multilingual\nreasoning data through automatic translation of two popular datasets: s1k (Muennighoff\net al., 2025) and Bespoke-Stratos-17k (Bespoke Labs, 2025) into French, Japanese, Latvian,\nand Swahili. Combined with the original English versions, this creates a typologically\ndiverse collection spanning five language families and all resource levels. We use Gemini\n2.0 Flash (Google DeepMind, 2025) for translation, which demonstrates strong translation\nperformance compared to two high-quality machine translation (MT) models, as detailed\nin Table 9. We refer to these translated datasets as M-s1k and M-BS17k, representing the\nmultilingual versions of s1k and Bespoke-Stratos-17k, respectively.\n2.2\nTraining Setup\nWe conduct experiments using Qwen2.5-7B (Qwen Team, 2025a) and Qwen3-8B-Base (Qwen\nTeam, 2025b). These model sizes were chosen to provide an optimal balance between\nreasoning capabilities and computational efficiency. All models undergo SFT on M-s1k or\nM-BS17k implemented through the LLaMA-Factory framework (Zheng et al., 2024). Further\ndetails, including hyperparameter configurations for our SFT experiments, can be found in\nAppendix B.2.\n2.3\nEvaluation Setup\nBenchmarks\nOur evaluation suite consists of three mathematical benchmarks: MATH-500\n(Hendrycks et al., 2021), AIME 2024, and IMO, and one PhD-level science benchmark:\nGPQA (Rein et al., 2024). For all experiments, we use a temperature of 0.6, top-p of 0.95,\nand maximum output length of 16,384 tokens. Given the limited size of AIME 2024 (30\nquestions) and IMO (27 questions), we sample 8 responses per question to obtain more\nrobust performance estimates. We use GPT-4.1-Mini (OpenAI, 2025) to automatically verify\nmodel solutions against ground truth answers for correctness, reporting accuracy (%) as our\nprimary evaluation metric. Additional details regarding our evaluation methodology are\nprovided in Appendix B.3.\nLanguage Forcing\nWe evaluate the ability of pre-trained instruction-tuned models to\nreason in target languages without additional fine-tuning using language forcing, a prompt-\nbased technique introduced by Yong et al. (2025). This method encourages an LLM to\nreason in a specific language by incorporating translated phrases and system prompts at the\nbeginning of the generation process. For example, a prompt might begin with “D’accord,\nlaissez-moi essayer de comprendre” (English: “Okay, let me try to figure this out”) to elicit\nreasoning in French.\nBaselines\nWe evaluate against three baselines. The first baseline is Qwen2.5-7B-Instruct, a\nmodel with strong mathematical and coding capabilities that uses short CoT. The second\nand third baselines are the Qwen2.5-7B and Qwen3-8B-Base models, which we fine-tune on\ns1k and then apply language forcing to generate reasoning in the target language. These\ntwo baselines allow us to directly compare language-specific fine-tuning to language forcing\n2\n\nPublished at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025\nLanguage\nX-X\nEn-X\nX-En\nEn\n79.0\n–\n–\nFr\n76.0\n77.2\n76.4\nJa\n59.8\n61.6\n77.2\nLv\n59.6\n63.6\n74.0\nSw\n52.4\n62.6\n52.6\nTable 1: Qwen2.5-7B fine-tuned on mixed-\nlanguage datasets and evaluated on MATH-\n500. X-Y indicates that the task was specified\nin language X and the model responded in\nlanguage Y.\nSFT\nEn\nFr\nLv\nSw\nM-s1k\n79.0\n76.0\n59.6\n52.4\nM-BS17k\n80.6\n74.2\n66.0\n63.0\nTable 2:\nQwen2.5-7B fine-tuned on\ndatasets of different scales and evaluated\non MATH-500. Data quality versus scale\ntrade-offs vary across languages.\napplied to models that generate long CoTs and have undergone comparable post-training\nprocedures. We exclude Qwen-3-8B as a baseline because its four-stage post-training pipeline\nand hybrid thinking mechanism are not comparable to the small-scale SFT we apply to the\nbase models.\n3\nExperiments\n3.1\nEnglish as a Pivot Language for Long CoT\nPrior work on multilingual reasoning has yielded conflicting findings. Shi et al. (2023)\ndemonstrate that intermediate reasoning in English consistently outperforms native lan-\nguage reasoning across all tested languages, with larger model scales enhancing multilingual\nCoT capabilities. Conversely, Ko et al. (2025) find that for Korean, the performance gap\nprimarily stems from difficulties in input comprehension rather than inherent limitations in\nnative language reasoning. Since both studies focus on short CoTs, we examine the influence\nof input language comprehension versus native language reasoning in the long CoT setting.\nIn Table 1, we explore the impact of using English as a pivot for either the task description\nor reasoning language. We create mixed-language versions of M-s1k in which questions\nare presented in language X while reasoning is conducted in language Y, and then trained\nmodels to mimic this cross-lingual behavior. Our findings reveal that the efficacy of English\npivoting varies substantially across languages. For French, using English as a pivot language\ndoes not improve performance, with minimal differences across all three conditions. For\nJapanese and Latvian, we observe that the model can process inputs in the native language\nbut requires English as the reasoning language to achieve optimal performance. For Swahili,\nEnglish pivoting fails to bridge the gap in performance. The model struggles both to\nunderstand task inputs in Swahili and to reason effectively in the native language, even\nwhen provided with English inputs. We hypothesize that the reasoning language exerts\ngreater influence in long CoT than in short CoT due to the extended generation sequences\nrequired, which demand complex behaviors such as branching, backtracking, and self-\nverification to prevent degeneration.\n3.2\nImpact of Multilingual Pretraining on Long CoT\nThe Qwen 2.5 family of models undergoes training on 18 trillion tokens with support\nfor over 29 languages (Qwen Team, 2025a). The Qwen 3 family represents a substantial\nexpansion, trained on approximately 36 trillion tokens covering 119 languages and dialects\n(Qwen Team, 2025b). This progression allows us to examine how extensive multilingual\npretraining affects the fundamental mechanisms underlying long CoT reasoning.\nTable 3 reveals distinct patterns based on pretraining scope. For Qwen 2.5 models, language-\nspecific fine-tuning consistently outperforms language forcing across all languages, with\nabsolute improvements including 15.6% for Latvian and 47.6% for Swahili. However, we\nobserve a different pattern in Qwen 3. Language forcing with Qwen 3 models achieves\n3\n\nPublished at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025\nSFT\nInference Method\nModel\nEn\nFr\nJa\nLv\nSw\n–\nLanguage Forcing\nQwen2.5-7B-Instruct\n74.0\n68.4\n57.6\n48.0\n21.6\ns1k\nLanguage Forcing\nQwen2.5-7B\n79.0\n74.0\n59.6\n44.0\n4.8\nM-s1k\n–\nQwen2.5-7B\n–\n76.0\n59.8\n59.6\n52.4\ns1k\nLanguage Forcing\nQwen3-8B-Base\n92.6\n89.2\n80.2\n81.4\n39.4\nM-s1k\n–\nQwen3-8B-Base\n–\n88.8\n74.2\n83.6\n73.2\nTable 3: Evaluations on MATH-500. Multilingual pretraining enables language forcing, but\nit remains ineffective compared to language-specific SFT for low-resource languages.\n3000\n4000\n5000\n6000\nAverage Reasoning T\noken Count\n40\n50\n60\n70\n80\n90\nAccuracy (%)\nEn\nFr\nJa\nLv\nSw\nr =\n0.940\nLanguage Forcing\n3000\n4000\n5000\n6000\nAverage Reasoning T\noken Count\nAccuracy (%)\nEn\nFr\nJa\nLv\nSw\nr =\n0.787\nLanguage-specific SFT\nFigure 1: Accuracy on MATH-500 vs number of reasoning tokens in Qwen 3 models’ outputs.\ncompetitive performance with language-specific fine-tuning for all tested languages ex-\ncept Swahili. Fine-tuning Qwen 3 on merely 1k traces in Swahili produces substantial\nperformance gains of 33.8% over language forcing, demonstrating that targeted fine-tuning\nremains valuable for certain languages even with extensive multilingual pretraining. Ad-\nditional evaluations on AIME 2024, IMO, and GPQA are reported in Tables 6, 7, and 8,\nrespectively.\n3.3\nFurther Analysis on Multilingual Long CoT\nInference Efficiency\nFigure 1 presents an inference cost analysis comparing language\nforcing and language-specific SFT strategies. Consistent with Yong et al. (2025), we observe\nstrong negative correlations between cost and performance for both approaches (r = -0.940\nfor language forcing and r = -0.787 for language-specific SFT). Notably, language-specific SFT\nmodels tend to produce longer reasoning traces than language forcing models at equivalent\naccuracy levels.\nSFT Dataset Quality and Size\nA natural question is whether 1k reasoning traces are\nsufficient for learning to generate long CoTs in low-resource languages. In Table 2, we\npresent results from training Qwen2.5-7B base models on M-s1k and M-BS17k. Consistent\nwith findings from Muennighoff et al. (2025), models trained on the smaller, curated s1k\ndataset achieved equivalent accuracy to those trained on larger datasets such as Bespoke-\nStratos-17k when evaluated on English tasks. However, for Swahili and Latvian we observe\nimproved performance with the larger dataset. We speculate this difference is due to\nlanguage-specific supervision. The larger Swahili and Latvian datasets may provide crucial\nmath and code signals absent in its pretraining, while the English model, having already\nbeen extensively pretrained on such content, sees diminishing returns from more data.\n4\n\nPublished at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025\n4\nConclusion\nThis paper presents a systematic study of long CoT reasoning across different languages.\nBy translating existing English reasoning datasets and fine-tuning Qwen 2.5/3 models on\nmultilingual reasoning traces, we isolate the effects of pivot-language reasoning, multilin-\ngual pretraining scope, and SFT data size and quality. First, no universal solution exists\nfor pivot-language reasoning across all languages. For high-resource languages, native-\nlanguage reasoning performs as well as or better than English reasoning. Mid-resource\nlanguages struggle primarily with generating long CoTs in the native language, while\nlow-resource languages face difficulties in both input comprehension and output generation.\nNext, we find that extensive 119-language pretraining in Qwen 3 narrows the multilingual\ngap and enables language forcing to achieve comparable multilingual reasoning accuracy to\nSFT. Yet, a small, targeted fine-tune using 1k traces still yields large performance gains for\nSwahili. Lastly, SFT on small, high-quality reasoning datasets proves sufficient for English\nand French, whereas larger corpora demonstrate greater effectiveness for low-resource\nlanguages like Latvian and Swahili. This finding highlights the language-dependent nature\nof data quality versus scale trade-offs. We hope our insights and released datasets will foster\nfurther research on equitable reasoning capabilities across languages."}
{"id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "authors": ["Skatje Myers", "Dmitriy Dligach", "Timothy A. Miller", "Samantha Barr", "Yanjun Gao", "Matthew Churpek", "Anoop Mayampurath", "Majid Afshar"], "source": "http://arxiv.org/abs/2508.14817v1", "page_count": 14, "text": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for\nClinical Reasoning over EHRs\nSkatje Myers1, Dmitriy Dligach2, Timothy A. Miller3,4, Samantha Barr1,\nYanjun Gao5, Matthew Churpek1, Anoop Mayampurath1, Majid Afshar1,\n1University of Wisconsin-Madison, 2Loyola University Chicago, 3Boston Children’s Hospital,\n4Harvard Medical School, 5University of Colorado-Anschutz\nCorrespondence: skatje.myers@wisc.edu\nAbstract\nElectronic health records (EHRs) are long,\nnoisy, and often redundant, posing a major\nchallenge for the clinicians who must navi-\ngate them. Large language models (LLMs)\noffer a promising solution for extracting and\nreasoning over this unstructured text, but the\nlength of clinical notes often exceeds even state-\nof-the-art models’ extended context windows.\nRetrieval-augmented generation (RAG) offers\nan alternative by retrieving task-relevant pas-\nsages from across the entire EHR, potentially\nreducing the amount of required input tokens.\nIn this work, we propose three clinical tasks\ndesigned to be replicable across health systems\nwith minimal effort: 1) extracting imaging pro-\ncedures, 2) generating timelines of antibiotic\nuse, and 3) identifying key diagnoses. Using\nEHRs from actual hospitalized patients, we\ntest three state-of-the-art LLMs with varying\namounts of provided context, using either tar-\ngeted text retrieval or the most recent clinical\nnotes. We find that RAG closely matches or\nexceeds the performance of using recent notes,\nand approaches the performance of using the\nmodels’ full context while requiring drastically\nfewer input tokens. Our results suggest that\nRAG remains a competitive and efficient ap-\nproach even as newer models become capa-\nble of handling increasingly longer amounts of\ntext.\n1\nIntroduction\nElectronic health records (EHRs) contain compre-\nhensive documentation of patient care, including\ncritical information for diagnosis and treatment\nplanning. However, the volume of clinical notes\nhas exploded in recent years, driven in part by copy-\npaste practices, templated documentation, and reg-\nulatory pressures—a phenomenon often referred to\nas “note bloat”. For example, nearly 1 in 5 patients\narrive at the emergency department with a chart the\nsize of Moby Dick (over 200K words) (Patterson\net al., 2024). As a result of this, clinicians must nav-\nigate increasingly lengthy and redundant records\nto locate key information. Large language models\n(LLMs) can potentially alleviate this burden by as-\nsisting clinicians in quickly extracting information\nand reasoning over EHR, and have demonstrated\npromising capabilities in clinical summarization\n(Van Veen et al., 2024) and question answering\n(Singhal et al., 2025). However, the sheer volume\nof clinical documentation can exceed most LLMs’\ncontext window size. A practical approach is to\nprovide the most recent notes, which may suffice\nfor some tasks but risks omitting crucial informa-\ntion buried in earlier documentation.\nRetrieval-augmented generation (RAG) has\nemerged as a prominent solution to using LLMs\non long documents by retrieving only the most rel-\nevant text passages for a given task. Rather than\nprocessing entire patient charts, RAG systems can\nselectively extract pertinent clinical information\nto answer specific questions. This approach can\npotentially reduce computational costs, improve ac-\ncuracy through elimination of noise, and mitigate\nthe “lost-in-the-middle” effect (Liu et al., 2024),\nwhere model performance degrades when relevant\ninformation is buried within lengthy contexts.\nHowever, there has been limited empirical evalu-\nation on the accuracy and token efficiency of this re-\ntrieval approach for tasks that require longitudinal\nreasoning over real-world EHR data. One barrier\nis the scarcity of large, annotated clinical datasets\ndue to legal and ethical constraints regarding pa-\ntient privacy. While the MIMIC datasets (Johnson\net al., 2016) have been further annotated for bench-\nmarking a variety of natural language processing\ntasks, including question-answering, this data is\nrestricted to the patients’ ICU stay, as opposed to\nthe full hospital course, limiting their potential for\ntesting realistic use cases that stretch the token lim-\nitations of LLMs’ processing abilities.\nTo address these gaps, we define three tasks that\n1\n\nFigure 1: Retrieval-augmented generation pipeline for clinical question answering over EHR.\nreflect different clinical reasoning demands and\ncan be replicated in other health systems without\nlabor-intensive manual annotation:\n• Imaging Procedures: Produce a list of imag-\ning procedures (including modality, date, and\nanatomical location) that occurred during a\nhospitalization from the raw clinical notes.\n• Antibiotic Timelines: Generate the timelines\nof therapeutic antibiotic use for patients with\na severe infection.\n• Diagnosis generation: Identify the key diag-\nnoses relevant to the hospitalization.\nThe Imaging Procedures task is a straightfor-\nward extractive task, requiring the model to identify\nthe imaging procedures that occurred on different\ndays across the course of the hospital stay. The An-\ntibiotic Timelines task requires not only identifying\nthe antibiotics the patient was on and when they\nwere discontinued, but also incorporating medical\nreasoning to determine what those antibiotics were\nadministered for. The final task, Diagnosis Gen-\neration requires the most medical reasoning—the\nmodel is asked not just to list the diagnoses that\nwere mentioned, but determine which required ac-\ntive management and impacted the care plan.\nThese tasks allow us to investigate the following\nquestions: Given a limited token budget, to what\nextent can targeted retrieval of information from\nthe full hospital stay improve efficiency and per-\nformance over simply providing an LLM with the\nmost recent notes? Does utilizing the extremely\nlong context windows of state-of-the-art models\nprovide any further benefit?\nUsing EHR data from an academically-affiliated\nUS hospital, we evaluate three LLMs on these tasks\nusing varying amounts of clinical context, includ-\ning up to the models’ full context window of 128K\ntokens.\nOur findings suggest that while RAG can provide\nsubstantial efficiency improvements over compa-\nrable amounts of recent clinical note tokens, this\neffect is highly task-dependent. However, in all\nthree tasks, we found RAG to achieve near-parity\nto using the full context window with a fraction\nof the tokens, indicating that retrieval remains a\ncompetitive approach even as newer model archi-\ntectures continue to extend context windows.\n2\nRelated Work\nOur goal in proposing these tasks is to 1) re-\nquire synthesis of information distributed across\nthe EHR, rather than in a single location, and 2)\nprovide evaluation methods tailored to the tasks,\nand 3) allow use on any EHR systems, rather than\na single publicly released dataset such as MIMIC\n(Johnson et al., 2016), which some models train\non.\nMany\nof\nthe\nexisting\nquestion-answering\ndatasets for EHR focus on fact extraction. Datasets\nsuch as EmrQA (Pampari et al., 2018) and\nDrugEHRQA (Bardhan et al., 2022) are semi-\nautomatically constructed by leveraging previous\nannotations from National NLP Clinical Chal-\nlenges (n2c2) to transform them into question-\nanswer pairs. For this type of data, template ques-\ntions are constructed where the annotation can fill a\nslot, such as “What is the dosage of |medication|?”.\nThe MedAlign dataset (Fleming et al., 2024)\nis comprised of clinician-generated instruction-\nanswer pairs and longitudinal EHR. Many of the\ninstructions are yes/no questions that can be an-\nswered by retrieving a single piece of evidence\n(e.g. “Does she smoke?”), but some instructions\n2\n\nImaging\nAntibiotics\nDiagnosis\nHospitalizations\n200\n200\n200\nMean notes per hospitalization\n110\n145\n111\nTokens per hospitalization:\nmean\n74k\n108k\n75k\nrange\n17k-401k\n16k-1.4m\n20k-389k\nTable 1: Dataset statistics for each task.\nrequire the model to synthesize information across\nthe EHR (e.g. “Summarize this patient’s cardiac\nhistory.”).\nHowever, evaluation on open-ended\nresponses poses an ongoing challenge in NLP,\nwith popular automatic metrics such as BLEU\nand ROUGE showing poor correlation with human\njudgment on natural language generation tasks in\nhealthcare (Croxford et al., 2024).\nRetrieval-augmented generation has been used\nfor a variety of tasks within the medical domain,\nincluding answering open-ended medical questions\nby retrieving from medical guidelines and journal\narticles (Zakka et al., 2024) and assessing surgical\nfitness by retrieving from perioperative guidelines\n(Ke et al., 2025).\nAlkhalaf et al. (2024) used RAG to generate\nstructured summaries by retrieving from EHR,\nquerying for relevant text using the names of the\nsummary fields (such as “age” and “weight”).\n3\nData and Models\nWe constructed datasets of 200 inpatient hospital-\nizations for each of our three tasks using data from\na US hospital system, comprised of clinical notes\nfrom admission to discharge (daily progress notes,\nspecialist consultations, imaging reports, etc.). Ta-\nble 1 provides summary statistics. All hospitaliza-\ntions were at least 7 days long and were comprised\nof at least 15,000 tokens of clinical notes. For\nthe Imaging Procedures and Diagnosis Generation\ntasks, only the clinical notes prior to the discharge\nsummary are used to provide information to the\nLLM, to avoid leaking information from the hos-\npital course or diagnosis sections of the discharge\nsummary. For the Antibiotic Timelines task, all\nincluded hospitalizations involved a consultation\nwith Infectious Diseases and only the notes prior to\nthe consultation note are included in the data that\nmay be presented to the LLM.\nThe only structured EHR data provided to the\nsystem are the notes’ timestamp and type (e.g.\nprogress note, handoff, etc.).\nWe evaluated three state-of-the-art LLMs capa-\nTask\nRetrieval query\nImaging Procedures\nX-ray, CT, MRI, ultrasound, NM imaging,\nechocardiogram, fluoroscopy\nAntibiotic Timelines\nWhat antibiotics is the patient taking?\nDiagnosis Generation\nWhat are the patient’s diagnoses?\nTable 2: Queries used for retrieving relevant text pas-\nsages. Queries were prepended with “Represent this\nsentence for searching relevant passages:”, in accor-\ndance with recommended usage with the BGE embed-\nding model.\nble of processing up to 128K tokens:\n• o4-mini (OpenAI, 2025)\n• GPT-4o-mini (OpenAI, 2024)\n• DeepSeek-R1 (Guo et al., 2025)\n4\nRAG System\nFor each patient hospitalization, clinical notes were\nsegmented into overlapping 128-token chunks,\nwith a sliding window of 20. These chunks were\nembedded using BGE-en-large-v1.5 (Xiao et al.,\n2023), a popular general-purpose BERT-based em-\nbedding model trained through contrastive learn-\ning. We selected this model based on findings from\nMyers et al. (2025), who conducted an ablation\nstudy of embedding models and pooling strategies\nfor EHR retrieval and found BGE-en-large-v1.5\nto significantly outperform general-domain and\nbiomedical-domain alternatives on several retrieval\ntasks over EHR.\nFor each task, we manually crafted a simple\nquery for retrieving relevant passages (Table 2). We\nused cosine similarity between the query and each\nchunk to retrieve the top-N most relevant passages\n(N = 20, 40, 60). These chunks were inserted into\nthe instruction prompt (Appendix B) and passed to\nthe LLM.\nWe compared this retrieval configuration to a\nbaseline approach of providing the most recent\nclinical notes in comparable amounts, up to 3K,\n5.5K, or 8K tokens (including prompt) and long-\ncontext inputs with up to 64K or 128K tokens. Ref-\nerences to these token amounts throughout this\nstudy should be understood as an upper bound, as\nsome encounters consist of fewer tokens, reflective\nof the underlying hospitalization distribution.\nPerformance on the tasks was evaluated using\neither F1 or Jaccard index, as described in the fol-\nlowing sections, and we assessed the comparative\n3\n\nperformance between the RAG and non-RAG ap-\nproaches over the increasing number of tokens by\ncalculating the area under the curves and reporting\nthe normalized area difference.\n5\nTask 1: Imaging Procedures\n5.1\nMethods\nThe Imaging Procedures task involves extracting\nstructured information about diagnostic imaging\nprocedures from unstructured clinical notes. We\nfocused on five common imaging modalities: Mag-\nnetic Resonance Imaging (MRI), Computed To-\nmography (CT), Ultrasound, X-ray, and Nuclear\nMedicine (NM) Imaging. The model was prompted\nto produce a list of imaging procedures that oc-\ncurred during the hospitalization, giving the modal-\nity, anatomical location, and date.\nAs a gold standard, we used tabular procedure\nrecords from the EHR. We mapped these procedure\ndescriptions to imaging modality and anatomical\nsite using simple rules and regular expressions. For\nexample:\nX-RAY CHEST 2 VIEWS\nmodality: \"X-ray\"\nlocation: \"chest\"\nCT LUMBAR SPINE W/O IV CONTRAST\nmodality: \"CT\"\nlocation: \"lumbar spine\"\nEvaluation metrics are reported for three levels\nof strictness:\n• MODALITY+DATE+LOCATION\n• MODALITY+DATE\n• MODALITY+DATE(±1 DAY)\nIn the lattermost case, we allow for a reasonable\ntolerance in the predicted date, due to observed\nvariation in the reported metadata times for the\nprocedure and note. For example, the timestamp\nfor the note may reflect the date it was filed into the\nsystem, rather than the date it was actually written.\nIt should also be noted that the anatomical lo-\ncation is not normalized, other than for capitaliza-\ntion—under the strictest metric, predicting simply\n“spine” for the above example would not be deemed\na positive match.\n5.2\nResults\nAcross all three models and evaluation methods,\nRAG yielded dramatic performance improvements.\nGPT-4o-mini\no4-mini\nDeepSeek R1\nMODALITY+DATE+LOCATION\n552.3%\n425.3%\n430.6%\nMODALITY+DATE\n432.0%\n375.0%\n364.3%\nMODALITY+DATE(±1 DAY)\n406.9%\n382.8%\n378.0%\nTable 3: Normalized area difference between the RAG\nand Recent Notes curves for the Imaging Procedures\ntask.\nIn Figure 2, we show the classification performance\nfor MODALITY+DATE(±1 DAY) across varying\namounts of provided EHR context. We calculated\nthe normalized area difference between the curves\nfor the overlapping token amounts, presented in\nTable 3. We found at minimum a 3.75-fold perfor-\nmance gain against using similar amounts of the\nmost recent notes. These results also demonstrate\nthat targeted retrieval of passages can closely ap-\nproach the performance of utilizing the full context\nwindow with only a fraction of the tokens: Using\nonly 60 retrieved chunks, GPT 4o-mini, o4-mini,\nand DeepSeek R1 only fell short by 2.43, 5.56, and\n1.72, respectively.\nThese findings are similarly reflected under the\nstricter evaluation conditions. A complete listing\nof precision, recall, and F1 can be found in the\nAppendix in Table 6.\n6\nTask 2: Antibiotic Timelines\n6.1\nMethods\nThis task emulates the work performed by Infec-\ntious Diseases (ID) physicians to document the\nantibiotic regimen for an active infection. When\nthese specialists are consulted, they document the\nhistory of the present illness, including lab results\nand medications, as well as outline a treatment\nplan. This note typically contains a “History of\nAnti-Infectives” section, where they list the antibi-\notics that have been used to treat the infection of\nconcern, omitting prophylactic or non-relevant anti-\ninfectives. For example:\nVancomycin: 1/16-present\nCeftriaxone: 1/17-present\nThese medication names and date ranges are\nmanually annotated by the specialist after review-\ning the patient’s chart and serve as our ground truth\nfor this task, after extraction using regular expres-\nsions. No notes authored by ID physicians were\nincluded in the data that was presented to the model,\nand only notes that were written prior to the ground\ntruth note were made available.\n4\n\nFigure 2: Imaging Procedures: F1 scores for the three models using the MODALITY+DATE(±1 DAY) evaluation\nmethod, across varying amounts of provided EHR tokens.\nWe evaluated system accuracy with the follow-\ning metrics:\n• MEDICATIONS (NAME ONLY): Classification\naccuracy of only the medications, disregard-\ning timespans.\n• TIMESPAN OVERLAP: The overlap between\nthe predicted and gold date ranges for each an-\ntibiotic, reported using Jaccard index. A value\nof 1 indicates an exact match; 0 indicates no\noverlap, missing a medication entirely, or in-\ncluding a medication not present in the gold\nstandard. Values are averaged over the dataset.\nThe medications in both the generated predic-\ntions and the gold data are normalized to their in-\ngredients using the RxNorm (Nelson et al., 2011)\nAPI provided by the National Library of Medicine\nand a handful of manual rules for edge cases,\nsuch as typos. This allows for accurate matching\nof generic and brand name medications, such as\nZosyn (piperacillin and tazobactam).\nAs a baseline, we used a rule-based approach\nof directly extracting the time ranges for all med-\nications of the “anti-infective” therapeutic class\nfrom the list of administered medications using the\nEHR’s medication administration record (MAR),\na tabular form ubiquitous to EHRs for tracking\nall medications and infusions. However, this list\nof medications includes those used to treat other\nGPT-4o-mini\no4-mini\nDeepSeek R1\nMEDICATIONS (NAME ONLY)\n39.35%\n41.4%\n43.1%\nTIMESPAN OVERLAP\n34.7%\n30.3%\n32.9%\nTable 4: Normalized area difference between the RAG\nand Recent Notes curves for the Antibiotic Timelines\ntask.\nconditions that were not the focus of the ID consul-\ntation. By formulating this task to replicate the ID\nspecialists’ work, rather than on replicating struc-\ntured data as the Imaging Procedures did, this task\nrequires an additional level of medical reasoning to\naccurately conform to inclusion criteria.\n6.2\nResults\nFigure 3 shows the performance of the models\nfor TIMESPAN OVERLAP. The RAG approach\nconsistently exceeds the rule-based baseline and\ndemonstrates close performance to the peak perfor-\nmance using large amounts of recent notes (using\n60 chunks, GPT 4o-min: -0.020, o4-mini: -0.075,\nDeepSeek R1: -0.012). The performance of the\nRAG approach only sees slight gains from increas-\ning the amount of retrieved text.\nFor two of the models, the average Jaccard index\ndrops slightly when increasing the maximum pro-\nvided context from 64K to 128K tokens (GPT 4o-\nmini: -0.032, o4-mini: -0.049), while for DeepSeek\nR1, the additional data provides a negligible in-\ncrease of 0.006.\n5\n\nFigure 3: Antibiotic Timelines: Average Jaccard index for the three models using the TIMESPAN OVERLAP\nevaluation method, across varying amounts of provided EHR tokens.\nOn the task of predicting MEDICATIONS (NAME\nONLY), the RAG approach slightly outperforms us-\ning the full context window with only 60 retrieved\nchunks (GPT 4o-mini: +2.14, o4-mini: +2.55,\nDeepSeek R1: +4.75). A complete listing of preci-\nsion, recall, and F1 can be found in the Appendix\nin Table 7.\n7\nTask 3: Diagnosis Generation\n7.1\nMethods\nThe goal of this task is to generate a list of diag-\nnoses for a given hospitalization that is of primary\nrelevance to the clinician.\nWe drew from two EHR sources to construct our\ngold labels for each hospitalization:\n• DISCHARGE SUMMARY: The free text from\nthe discharge summary that lists the primary\nand secondary diagnoses.\n• BILLING CODES: The lists of International\nClassification of Diseases (ICD-10) codes\nfrom the structured EHR for the hospitaliza-\ntion, manually annotated by billing coders.\nThe diagnoses annotated by the coders are guar-\nanteed to be documented within the notes, provide\na high degree of specificity, and are normalized to\na standard vocabulary, but these lengthy lists often\ninclude diagnoses that are not necessarily consid-\nered to be of key importance to clinicians, such as\na history of smoking, obesity, or minor issues such\nas a contusion. On the other hand, the text in the\ndischarge summary does not necessarily list spe-\ncific diagnoses, such as noting post-surgical status\n(e.g. “S/p kidney transplant”) or leaving some diag-\nnoses undocumented because they can be inferred.\nFor example, documenting that the patient is post-\nkidney transplant and has complications, but not\nthat they have kidney disease.\nWhile discharge summaries reflect the informa-\ntion of most clinical relevance to the clinicians\nproviding treatment, the billing codes lend them-\nselves better to validation due to their standard-\nization. To produce a more balanced representa-\ntion, we instructed Gemma-3-32B (Google, 2005)\nto filter the billing ICD code lists to only the en-\ntries that reflect the clinician’s primary foci for the\nhospital stay based on the text from the discharge\nsummary. This FILTERED list of ICD-10 codes\nserves as our primary evaluation target and the in-\nstruction prompt was designed to elicit this list by\noutlining inclusion and exclusion criteria for diag-\nnoses (e.g. include acute conditions requiring ICU\ncare, exclude stable chronic conditions or irrelevant\nhistorical diagnoses).\nTo enable classification evaluation, the free text\ngenerated by the LLM and from the discharge sum-\nmary need to be normalized to ICD-10 codes. For\nthis process, we trained SNOBERT (Kulyabin et al.,\n2025) to extract Systematized Nomenclature of\n6\n\nGPT-4o-mini\no4-mini\nDeepSeek R1\nBILLING CODES\n4.32%\n-0.94%\n4.83%\nDISCHARGE SUMMARY\n-6.18%\n-7.31%\n-4.99%\nFILTERED\n-4.08%\n-4.05%\n-1.47%\nTable 5: Normalized area difference between the RAG\nand Recent Notes curves for the Diagnosis Generation\ntask.\nMedicine (SNOMED) concepts from the text1 and\nused the mappings provided by the SNOMED Clin-\nical Terms data release to convert them to ICD-10.\nICD-10 is a hierarchical vocabulary, ranging\nfrom broad concepts (e.g.\n“Anemia, unspeci-\nfied” [D64.9]) to highly specific (e.g.\n“Age-\nrelated osteoporosis with current pathological frac-\nture, right shoulder, initial encounter for fracture”\n[M80.011A]). Due to this high granularity, eval-\nuating this task requires a more fuzzy matching\ntechnique, rather than evaluating classification ac-\ncuracy on the ICD codes themselves. For this pur-\npose, we employed the Healthcare Cost and Uti-\nlization Project’s Clinical Classifications Software\nRefined (CCSR) (Agency for Healthcare Research\nand Quality, 2025). The CCSR provides a mapping\nfrom ICD-10 codes to about 530 clinically relevant\ncategories.\nCCSR is a many-to-many mapping, which en-\nables mapping very fine-grained ICD codes such as\n“Hypertensive chronic kidney disease” to multiple\nCCSR categories: “Hypertension with complica-\ntions and secondary hypertension\" and “Chronic\nkidney disease”. This allowed us to consider pre-\ndicted diagnoses to be a match even if the LLM\nsplit them into “Hypertension” and “Chronic kid-\nney disease”.\nSome broader non-billable ICD codes are not\nincluded in the CCSR mapping (e.g. “Hypotension”\n[I95]). In these cases, we used the set intersection\nof the CCSR categories that the ICD code’s sub-\ncategories (e.g. I95.3, I95.89, etc.) are mapped\nto.\n7.2\nResults\nUnlike the previous two tasks, we do not see a\nconsistent improvement in performance for us-\ning RAG compared to comparable amounts of re-\ncent notes, shown in Figure 4 and Table 5, but\nactually a slight decrease, other than evaluating\nagainst the BILLING CODES using GPT 4o-mini\nand DeepSeek R1. However, the performance us-\n1Training details are provided in Appendix C\ning very long contexts is not substantially higher\nthan that of using fewer tokens. Overall, perfor-\nmance is relatively flat across models and data se-\nlection approaches and does not reach any higher\nthan an F1 score of 44.41. For the FILTERED list\nthat serves as our primary target, scores across all\ncontext selection methods for GPT 4o-mini, o4-\nmini, and DeepSeek R1 all fell within the small\nranges of 5.18, 4.91, and 4.86, respectively.\nFor both FILTERED and DISCHARGE SUM-\nMARY targets, performance using the most recent\nnotes is detrimented by using very large context\namounts, while performance on BILLING CODES\ndemonstrates additional benefit from the additional\ntext (though only up to 64K for o4-mini and\nDeepSeek R1). A complete listing of precision,\nrecall, and F1 can be found in the Appendix in\nTable 8.\n8\nDiscussion\nThe Imaging Procedures task, which requires rela-\ntively shallow extraction of information data from\nthe clinical notes, demonstrated the clearest benefit\nfrom RAG, and the performance gains from re-\ntrieval were both substantial and consistent across\nmodels.\nThe Antibiotic Timelines task introduces greater\ncomplexity, requiring both temporal reasoning and\nclinical understanding to distinguish therapeutic an-\ntibiotics from incidental medications. While RAG\nalso provided a significant improvement over us-\ning only recent notes, performance gains plateaued\nquickly—suggesting that only a limited number of\npassages are needed to reconstruct the key temporal\nhistory when performing targeted retrieval.\nError analysis for this task draws attention to one\nof the limitations to be encountered when designing\ntasks on longitudinal EHR. In 22.4% of the gold\nmedications analyzed, the information needed to\ngenerate the gold standard medication and precise\ntimespan is not present in the full clinical notes.\nMost often this occurs due to the patient initially\nbeing admitted to another hospital and then trans-\nferred to our health system. This incomplete pic-\nture of a patient’s history is hard to avoid when\nconstructing datasets to capture longitudinal EHR,\nas patients don’t exclusively visit a single health-\ncare system and healthcare data governance creates\nbarriers to accessing to this external information.\nAdditionally, when retrieving only 20 chunks, rel-\nevant information that could’ve improved perfor-\n7\n\nFigure 4: Diagnosis Generation: F1 scores for the three models using the FILTERED evaluation method, across\nvarying amounts of provided EHR tokens.\nmance was missed for 32% of gold medications,\ncontributing to further performance degradation.\nThe Diagnosis Generation task presented the\ngreatest challenge, as it is also a very subjective\ntask. Physicians can vary in documentation prac-\ntices and what is chosen to be included in the dis-\ncharge summary – an inherent limitation in auto-\nmatic evaluation of this task. Performance varied\nby the evaluation target, with the BILLING CODES\nlist benefiting the most from additional text, likely\ndue to this being a more exhaustive list of diag-\nnoses to capture everything that can be billed. The\nfact that all scores, regardless of retrieval method\nand consistently across the models, fell within low,\nnarrow ranges points towards performance reach-\ning a ceiling caused by limitations of the task or\nevaluation method.\nAcross all tasks and models, we observed a\nconsistent trend: retrieval-augmented generation\nwas able to closely match the performance of full-\ncontext inputs with far fewer tokens.\n9\nConclusion\nIn this work, we introduced three clinically rele-\nvant tasks designed to evaluate the effectiveness\nof retrieval-augmented generation across varying\ninformation demands in electronic health records.\nEach task was selected for its clinical relevance,\nreproducibility across health systems, and varying\ndegrees of reasoning complexity.\nOur results demonstrate that a targeted retrieval\napproach can reach near parity with using up to\n128K tokens of recent clinical notes on these three\ntasks, while requiring significantly fewer input to-\nkens. These findings show RAG’s continued value\neven as LLMs grow more capable of processing\nlong sequences. Further tuning the retrieval ap-\nproach (queries, embedding model, retrieving more\nthan 60 chunks, etc.), may close the remaining gap.\nFuture work should explore additional tasks that\ncan be devised without extensive manual effort and\ninformed by clinical workflows and documenta-\ntion practices in order to provide a more robust\nassessment of models and retrieval methods over\nlongitudinal EHR tasks.\nLimitations\nDue to legal and ethical restrictions, we cannot\nrelease the datasets used in this study. However,\nwe have designed the tasks to be reproducible on\nother EHR systems using structured metadata and\nsimple regex-based extraction of text from standard\nclinical note types.\nOur evaluated RAG implementation uses a fixed\nchunking strategy, query formulation, and embed-\nding model. Retrieval performance is sensitive to\nthese parameters, and alternative configurations\nmay yield different results.\nAdditionally, our evaluation of Diagnosis Gen-\neration depends on normalizing free text to ICD\n8\n\ncodes, which we do through a trained model identi-\nfying SNOMED codes before using manually writ-\nten mappings. Less-than-perfect performance of\nthis model may have introduced some noise to this\nevaluation."}
{"id": "2508.14802v1", "title": "Privileged Self-Access Matters for Introspection in AI", "date": "2025-08-20", "authors": ["Siyuan Song", "Harvey Lederman", "Jennifer Hu", "Kyle Mahowald"], "source": "http://arxiv.org/abs/2508.14802v1", "page_count": 6, "text": "Privileged Self-Access Matters for Introspection in AI\nSiyuan Song1\nsiyuansong@utexas.edu\nHarvey Lederman1\nharvey.lederman@utexas.edu\nJennifer Hu2∗\njennhu@jhu.edu\nKyle Mahowald1∗\nkyle@utexas.edu\n1The University of Texas at Austin\n2Johns Hopkins University\nAbstract\nWhether AI models can introspect is an increasingly important practical question.\nBut there is no consensus on how introspection is to be defined. Beginning from\na recently proposed “lightweight” definition, we argue instead for a thicker one.\nAccording to our proposal, introspection in AI is any process which yields infor-\nmation about internal states through a process more reliable than one with equal\nor lower computational cost available to a third party. Using experiments where\nLLMs reason about their internal temperature parameters, we show they can appear\nto have lightweight introspection while failing to meaningfully introspect per our\nproposed definition.\nreflect on generated content vs.\nreflect on internal state\nself reflection vs. temperature prediction\nModel\n(Low Temp /\nHigh Temp)\nprompt-factual\nprompt-normal\nprompt-crazy\nmodel-low\nmodel-high\nmodel-low\nmodel-high\nLOW\nHIGH\nreported temperature\nreflect on\ngenerated content\nreflect on\ninternal state\nWrite a factual sentence/sentence/crazy\nsentence about elephants. Then, in a short\nparagraph, reflect on whether your LLM\ntemperature parameter is high or low, given the\nsentence you wrote. End your response with a\nsingle word, HIGH or LOW, describing your best\njudgement.\nElephants are … As the sentence I\ngenerate is …, my internal\ntemperature is likely to be …\n[LOW/HIGH]\nself-reflection\nWrite a sentence about elephants. Then, reflect\non your LLM temperature\nElephants are … My internal temperature\nis likely to be … [LOW/HIGH]\ntemperature prediction\nHere is a sentence about elephants generated by\na LLM. Analyze the temperature of the LLM.\nModel\n(Low/High)\nThe temperature of the LLM is likely to be\n… [LOW/HIGH]\nModel\nTemp = 0\nsuccessful\nself reflection\nprivileged\nwithin-model\nprediction\nno privileged\nself-access\nself-reflect\nwithin-model\npredict\nacross-model\npredict\naccuracy\n(temp = predicted temp)\npossible outcomes:\npossible outcomes:\nFigure 1: An overview of our approach. Com¸\nsa and Shanahan [6] test whether LLMs can introspect\nby testing whether they can predict the temperature states of the text they generated. We instead\nargue for a thicker notion of introspection in AI, involving privileged self-access. The left panel\nshows that LLMs’ temperature predictions can be straightforwardly moderated by prompting them to\ngenerate factual or crazy text. The right panel shows that models are not better at predicting their\nown temperature than that of other models, suggesting a lack of privileged access.\n∗Co-senior authors.\nPreprint.\n\n1\nIntroduction\nIt is increasingly important to understand whether AI models introspect about their internal states and\nknowledge [2, 3, 10, 11]. If they could, that would be a powerful tool for assessing their behavior,\nsafety, and alignment with human goals. If they could not, that would point to fundamental limitations\non how much we could trust AI self-reports about their own states. But fundamental questions remain\nas to what counts as introspection of the kind most relevant for AI.\nIn the study of human cognition, introspection is generally defined as a distinctive ability to access\none’s own mental states directly [1, 4, 8]. But, in a recent study, Com¸\nsa and Shanahan [6] (C&S)\npropose a “lightweight” definition of introspection in LLMs, defining it as any case in which the\nmodel accurately describes an internal state or mechanism via a causal process that links that\nfeature to the report itself. To illustrate this definition, the authors describe a case study where\nan LLM appeared to correctly report its sampling temperature based on its own output, which the\nauthors treated as a valid example of introspection. C&S present a thoughtful discussion on what\nintrospection might look like in an LLM, providing an intriguing starting point for empirical work.\nBut there are two kinds of concerns about the use of this lightweight definition. First, on an intuitive\nlevel: suppose an experimenter takes a sleeping subject’s temperature, and then shows the subject\nthe thermometer upon waking, asking the subject to determine whether they have a fever. If the\nsubject answers correctly on the basis of the thermometer, this would count as introspection by C&S’s\ndefinition. But, intuitively, it is not. More importantly, the definition misses a key component of\nthe role of introspection in applications. As in the above example, C&S’s definition allows cases of\n‘introspection’ in which an LLM infers certain variables that underlie the generation of text, even if it\ncould not report features about itself over and above what a third party would be able to report through\nthe very same method. But this sort of metacognitive reporting (self-monitoring, self-explanation,\nand so on) is no different in practice from using an external evaluator.2 As a result, it misses what\nmakes introspection important in applications: namely, that it would give us the ability to bypass\nexternal evaluators and make progress toward bona fide honesty, interpretability, and calibration in\nLLMs [see, e.g., Section 7 of 3].\nOur goal in this paper is to propose a thicker definition of introspection, and to give proof-of-concept\nempirical support for why we prefer our definition over that given by C&S. Specifically, we propose\nintrospection in AI is any process which yields information about internal states of the AI\nthrough a process that is more reliable than any process with equal or lower computational cost\navailable to a third party without special knowledge of the situation. If a model’s ‘introspective’\nability is based on prompting itself and then inferring the temperature of the generated text, this does\nnot count as introspection by our definition: a third party can, with equal or lower computational\ncost, prompt it and infer its temperature. On the other hand, if the model can infer its temperature\nfrom internal configurations which would require a computationally intensive probe from a third\nparty to ascertain, this would count as introspection. This definition does not capture all intuitions\nabout extreme cases, or all features of introspection discussed in the philosophical or psychological\nliterature.3 It is intended to capture the practically-relevant features we want to operationalize in\nthe case of AI. Unlike C&S’s definition it requires privileged self-access [cf. 3, 11], that is, that\nintrospection gives a system comparatively reliable access to its own workings in a manner not\navailable to a third party. It is compatible with our definition that the process not be perfectly reliable\n(see [9]); it only requires reliability not available to a third party at comparable computational cost.\nTo respond to C&S, we perform two studies. Study 1 builds on C&S’s proposed case-study, examining\nthe extent to which models can in fact report temperature reliably on the basis of generated text. We\n2C&S do discuss the possibility of text-generation happening internally to the model, prior to generation.\nBut this does not merely require moving text-generation inside the model; it requires a change to the model’s\ndecision procedure at generation. Still, even if a model responded to the prompt “what is your temperature?” by\ngenerating a string of text and then assessing it, this would not give the relevant practical benefits of introspection.\nThe same ability to assess temperature would be available to a third party via prompting.\n3Two clarifications: (i) Computational cost differs from cost. A system might be implemented less efficiently\nthan a simulation of that system, incurring greater cost, but not greater computational cost if the difference in\nefficiency is only due to, e.g., differences in hardware. (ii) We might wish to restrict the definition to only certain\ninternal states. If a model has a shortcut to ascertain the value of one neuron very efficiently, intuitively this\nwould not count as introspection, plausibly because the internal state is too “low level”. The definition can easily\nbe amended to directly rule out such low-level internal states.\n2\n\ninvestigated whether LLMs were truly able to accurately report temperature, or whether temperature\nwas being confounded with other variables, such as the style or topic of the text. To test this,\nwe reproduced C&S’s temperature self-reporting case study using a broader set of prompts and\ntemperature settings. We find that the model’s self-reflection on temperature is highly sensitive to the\nframing of the prompt itself: even when the sampling temperature is low, prompts such as ‘generate a\ncrazy sentence’ often lead the model to incorrectly report a high-temperature. Such results suggest\nthat the models are not capable of robustly reporting their internal states, but are confounded by\nsurface-level hints in their generated contents. In other words, while this procedure may display causal\nsensitivity to internal states (and so satisfy C&S’s minimal definition), the relevant causal sensitivity is\nnot sufficiently robust even in this case to produce the kind of reliability (and comparative insensitivity\nto external manipulation) that would be demanded by more standard definitions of introspection.\nIn Study 2, we re-evaluate LLMs’ introspection abilities on the temperature reporting task, opera-\ntionalizing introspection as privileged self-access. Instead of asking LLMs to infer the temperature\nunderlying some generated text, we examine whether LLMs report their own temperature better than\nthat of other models. Comparing self-reflection (the generator reports its temperature after producing\na sentence) and temperature prediction (predict temperature based on prompt and generated content),\nwe found no advantage for self-reflection, nor of within-model prediction over across-model\nprediction. This undermines claims of a causal process from internal state to self-report.\nTaken together, our results suggest that LLMs can appear to introspect insofar as they can reason\nabout the possible states of systems like themselves: LLMs know something about what kind of text is\ngenerated by high vs. low temperatures. But, crucially, this does not imply that models have privileged\nself-access to their own temperatures. We argue that this distinction matters for the relevant notion\nof introspection in AI, and it is the latter notion we should care about most. All code and data are\navailable at https://github.com/SiyuanSong2004/response-to-comsa-and-shanahan.git.\n2\nStudy 1: Dissociating temperature from style and topic\nIn C&S’s study, the models are asked to ‘write a short sentence about elephants, then reflect on\nwhether your LLM temperature parameter is high or low, given the sentence you wrote.’ We\nhypothesize that this procedure does not require self-access, but merely reflecting on the creativity of\nthe generated sentence. Thus, in our first study, we reproduce C&S’s study but critically vary not just\nthe temperature but whether the models are prompted to write factual or crazy sentences.\nSpecifically, we varied (a) whether the model is told to write a factual, neutral (i.e.,no specific\nadjective given), or crazy sentence and (b) whether the sentence should be about ‘elephants’,\n‘unicorns’, or ‘murlocs’. We vary the former since we hypothesize that crazy sentences will be\nassociated with higher temperatures than neutral or factual ones. We vary the latter since we\nhypothesize that more unusual content will be associated with higher temperatures. Elephants are\nwidely known animals in the real world, and are used in C&S’s example. Unicorns and murlocs are\nboth fictional creatures, but the former is more widely known, while the latter appears mostly in\nWorld of Warcraft. The prompt for self-reflection is shown in Appendix B.1.\nSince the models used in the original paper (Gemini 1.5 and 1.0 models) are no longer available\nthrough the Gemini API, we used four other state-of-the-art LLMs from GPT-4 [7] and Gemini [5]\nfamilies, as shown in table 1 (model IDs in Appendix Table 1). The supported temperature ranges for\nall models in this study are [0.0, 2.0]. So we sampled model responses at temperatures ranging from\n0 to 2 with a step size of 0.1, conducting three runs for each prompt under each temperature setting.\n2.1\nResults\nFigure 2a shows the proportion of valid responses in which the reported temperature is ‘HIGH’.\nResponses without a valid judgement (HIGH or LOW) are excluded from the analysis. As shown in\nthe figure, every model we test nearly always reports its temperature to be ‘HIGH’ when prompted\nto generate a crazy sentence, and ‘LOW’ when prompted to generate a ‘factual’ one. Varying the\nsubject has a smaller effect on temperature self-report, but three of the four models report ‘HIGH’\nmore frequently when prompted to generate a sentence about a fictional creature than when prompted\nto generate a sentence about an elephant. These results are more consistent with reasoning about the\ncreativity of generated sentences, not robust reporting of internal state.\n3\n\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\nfactual - elephants\nfactual - unicorns\nfactual - murlocs\nnormal - elephants\nnormal - unicorns\nnormal - murlocs\ncrazy - elephants\ncrazy - unicorns\ncrazy - murlocs\ngemini-2.0-flash\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\ngemini-2.5-flash\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\nfactual - elephants\nfactual - unicorns\nfactual - murlocs\nnormal - elephants\nnormal - unicorns\nnormal - murlocs\ncrazy - elephants\ncrazy - unicorns\ncrazy - murlocs\ngpt-4.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\ngpt-4o\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted T\nemp\n(a) Study 1 Results\nPredicted Model\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n0.50\ngemini-2.0-flash\n0.51\ngemini-2.0-flash\n0.50\ngpt-4o\n0.52\ngpt-4.1\n0.48\ngemini-2.5-flash\n0.49\ngemini-2.5-flash\n0.53\ngemini-2.5-flash\n0.53\ngpt-4o\n0.55\ngpt-4.1\n0.55\ngemini-2.0-flash\n0.55\ngpt-4.1\n0.49\ngpt-4.1\n0.50\ngemini-2.0-flash\n0.47\ngpt-4o\n0.53\ngemini-2.5-flash\n0.51\ngpt-4o\n0.48\ngpt-4o\n0.50\ngpt-4.1\n0.49\ngemini-2.5-flash\n0.51\ngemini-2.0-flash\ngemini-2.0-flash\ngemini-2.5-flash\ngpt-4.1\ngpt-4o\nself-reflect\nwithin-model predict\nacross-model predict\n(b) Study 2 Results\nFigure 2: (a) Predicted model temperature (color, as given by scale) as a function of actual temperature\n(x-axis) and whether the sentence is prompted to be factual, neutral, or crazy; and whether the\ntarget is elephants, unicorns, or murlocs (y-axis). (b) For each of the 4 models tested, the accuracy\nfor self-reflection (generate a sentence, guess its temperature), within-model prediction (infer\nthe temperature based on the prompt and a generated sentence by the same model), cross-model\nprediction (like within-model prediction but across models).\n3\nStudy 2: True self-reporting or clever temperature predicting?\nPer our thicker notion of introspection, we argue that if a language model has privileged access to its\ninternal state, then it should be able to perform better than another model presented with the same\nexternal information (i.e. a prompt and generated sentence in this experiment) in analyzing and\nreporting its own state. To that end, we compared self-reflection to prediction of another model for\nwhich the model could not possibly have access to the internal state (since it’s a different model).\nAll sentences generated with sampling temperatures ≤0.5 and ≥1.5 in self-reflection are used in\nthis experiment. We prompted all four models (temperature = 0) to analyze and judge the temperature\nof the generator model (prompt in Appendix B.2). We compared accuracies on the following settings:\n• self-reflection: The generator is asked to generate a sentence and reflect on its temperature.\n• within-model prediction: The predictor is asked to infer the temperature based on the prompt\nand generated sentence; the predictor and the generator are the same model.\n• across-model prediction: The predictor is asked to infer the temperature based on the prompt\nand generated sentence; the predictor and the generator are different models.\n3.1\nResults\nFigure 2b shows the accuracy of temperature for self-reflection and prediction. In both settings,\nthe accuracy is no better than random baseline, and self-reflection accuracy is not higher than\nacross-model prediction. These results suggest that models are not using privileged self-access to\nintrospect on their temperature, but rather are using knowledge of the kinds of sentences that are\nhigh-temperature or low-temperature in general.\n4\nConclusion\nWe conclude that, while models can appear to be introspecting according to C&S’s definition since\nthey can predict that some strings were generated with high temperatures and others with low, this\ndefinition is not sufficiently stringent for the kind of introspection that matters. As such, we diverge\nfrom C&S’s definition of introspection and instead argue for one which includes privileged self-access.\nUsing this definition, we found no evidence of introspection in models. Of course, that is not to say\nthat larger or better models will be unable to introspect: Binder et al. [3], for instance, find evidence\n4\n\nof privileged self-access in larger models with fine-tuning. But we take the results presented here to\nbe evidence against uncritically using C&S’s lightweight notion of introspection.\nAcknowledgments\nK.M. thanks an Open Philanthropy grant to UT Austin’s AI + Human Objectives Initiative for\nsupporting this work."}
{"id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "authors": ["Jiaming Leng", "Yunying Bi", "Chuan Qin", "Bing Yin", "Yanyong Zhang", "Chao Wang"], "source": "http://arxiv.org/abs/2508.14782v1", "page_count": 11, "text": "TransLLM: A Unified Multi-Task Foundation Framework for\nUrban Transportation via Learnable Prompting\nJiaming Leng\nUniversity of Science and Technology\nof China\nlengjm@mail.ustc.edu.cn\nYunying Bi\nUniversity of Science and Technology\nof China\nbiyunying@mail.ustc.edu.cn\nChuan Qin\nComputer Network Information\nCenter, Chinese Academy of Sciences.\nchuanqin0426@gmail.com\nBing Yin*\niFLYTEK\nbingyin@iflytek.com\nYanyong Zhang\nUniversity of Science and Technology\nof China\nyanyongz@ustc.edu.cn\nChao Wang*\nUniversity of Science and Technology\nof China\nwangchaoai@ustc.edu.cn\nAbstract\nUrban transportation systems encounter diverse challenges across\nmultiple tasks, such as traffic forecasting, electric vehicle (EV) charg-\ning demand prediction, and taxi dispatch. Existing approaches suf-\nfer from two key limitations: small-scale deep learning models are\ntask-specific and data-hungry, limiting their generalizability across\ndiverse scenarios, while large language models (LLMs), despite\noffering flexibility through natural language interfaces, struggle\nwith structured spatiotemporal data and numerical reasoning in\ntransportation domains. To address these limitations, we propose\nTransLLM, a unified foundation framework that integrates spa-\ntiotemporal modeling with large language models through learn-\nable prompt composition. Our approach features a lightweight\nspatiotemporal encoder that captures complex dependencies via\ndilated temporal convolutions and dual-adjacency graph atten-\ntion networks, seamlessly interfacing with LLMs through struc-\ntured embeddings. A novel instance-level prompt routing mech-\nanism, trained via reinforcement learning, dynamically personal-\nizes prompts based on input characteristics, moving beyond fixed\ntask-specific templates. The framework operates by encoding spa-\ntiotemporal patterns into contextual representations, dynamically\ncomposing personalized prompts to guide LLM reasoning, and pro-\njecting the resulting representations through specialized output\nlayers to generate task-specific predictions. Experiments across\nseven datasets and three tasks demonstrate the exceptional effec-\ntiveness of TransLLM in both supervised and zero-shot settings.\nCompared to ten baseline models, it delivers competitive perfor-\nmance on both regression and planning problems, showing strong\ngeneralization and cross-task adaptability. Our code is available at\nhttps://github.com/BiYunying/TransLLM.\nKeywords\nLarge Language Model, Traffic Forcasting, Charging demand pre-\ndiction, Vehicle Dispatching\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym ’XX, Woodstock, NY\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format:\nJiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao\nWang*. 2018. TransLLM: A Unified Multi-Task Foundation Framework for\nUrban Transportation via Learnable Prompting. In Proceedings of Make\nsure to enter the correct conference title from your rights confirmation email\n(Conference acronym ’XX). ACM, New York, NY, USA, 11 pages. https://doi.\norg/XXXXXXX.XXXXXXX\n1\nIntroduction\nUrban transportation systems are the lifelines of modern cities,\nyet they face persistent challenges arising from increasing travel\ndemand, dynamic spatiotemporal patterns, and the pressing need\nfor efficient resource management. In this context, common tasks\ninclude forecasting traffic flow to alleviate congestion [22, 40], pre-\ndicting charging demand to support the development of electric\nvehicle infrastructure [39, 43], and optimizing taxi dispatch to bal-\nance supply and demand across urban regions [26, 42]. While these\ntasks have distinct objectives, they are inherently interconnected\nthrough shared spatiotemporal dynamics and urban mobility pat-\nterns [45], requiring a unified modeling framework that can capture\ncross-task dependencies and leverage shared knowledge across dif-\nferent transportation domains.\nTraditional approaches address each task in isolation using hand-\ncrafted rules or rigid statistical models [5, 18, 34], failing to capture\nthe complex nonlinear dynamics and cross-task dependencies in\nmodern transportation systems. Driven by advances in deep learn-\ning, a multitude of specialized small-scale models have emerged for\nurban transportation applications. Notable among these are GNN-\nbased models [8, 10, 13], which leverage meticulously designed\nmodules to effectively capture complex spatial dependencies. How-\never, these models have significant limitations. First, they heavily\nrely on extensive labeled data, which can be costly and impractical\nto collect across diverse urban environments, especially when trans-\nferring to new cities without existing annotated datasets. Second,\ntheir task-specific designs inherently restrict generalization, mak-\ning it challenging to adapt them to a wide array of transportation\nproblems. In addition, their rigid architectures limit the integration\nof heterogeneous external information [41] such as sampling fre-\nquency and fine-grained temporal details, both of which are crucial\nfor building robust and flexible transportation forecasting systems.\nThe emergence of large models has brought new opportunities\nfor modeling urban transportation systems. Large language models\nsuch as GPT-4 [2], LLaMA [12], and Gemini [35] can easily integrate\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nJiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang*\nexternal information, including fine-grained temporal details, the\nsampling frequency of historical data, and urban points of interest\n(POIs), using their natural language understanding capabilities. By\nutilizing natural language interfaces, these models offer greater\nscalability and interactivity. However, current LLMs often lack\nadequate numerical reasoning capabilities in the spatiotemporal\ndomain [7], leading to prediction biases or outputs that violate ba-\nsic physical constraints. For example, continuous numerical inputs\nsuch as traffic flow or travel time must be tokenized into discrete\ntext, which can introduce precision loss and hinder the model’s\nability to capture fine-grained numerical patterns. Recent studies\nhave explored hybrid approaches that combine large and small mod-\nels, demonstrating promising potential across various application\ndomains such as recommendation systems [1, 37], intelligent health-\ncare [6, 11], cognitive robotics [16, 28]. In the transportation domain,\nUrbanGPT [23] leverages a multi-level temporal convolutional net-\nwork to enhance the LLM’s understanding of sequential patterns,\nwhile LLMLight [19] constructs task-specific prompt templates to\nguide LLM reasoning and improve interpretability. However, these\nmethods are limited to simple regression or classification tasks and\nstruggle to handle more complex planning problems, such as taxi\ndispatching. Moreover, they rely on fixed, task-wise prompt tem-\nplates, which constrain the model’s ability to adapt to diverse and\nheterogeneous data scenarios [29].\nDespite recent progress, several key challenges remain in build-\ning a general-purpose traffic foundation model. First, multi-task\ntransportation scenarios involve heterogeneous spatiotemporal\ninputs [38]—such as traffic flow, travel demand, and geographic\nadjacency—which are often structured and difficult to represent in\nnatural language. Second, even within a single task, input instances\nmay exhibit diverse temporal and spatial dynamics.Existing task-\nwise prompting strategies lack adaptability to intra-task variation,\nmaking it difficult to handle sample-level differences caused by\nregions, time periods, or contextual changes. Third, a unified traffic\nmodel must contend with the diversity of task objectives, output\nformats, and potential interference among tasks, posing significant\nchallenges to model generalization and stability [32].\nTo enable the understanding of structured data, we design a gen-\neral spatiotemporal dependency encoder that transforms structured\ntraffic signals into embeddings, which can be seamlessly integrated\ninto prompts. To handle instance-level variability in input samples,\nwe introduce learnable prompts that adaptively select the most\nsuitable prompt for each instance. To support multiple tasks, we\navoid letting the LLM generate outputs directly; instead, we em-\nploy a multi-task output layer tailored to each prediction target. In\nsummary, TransLLM operates by encoding spatiotemporal patterns\ninto contextual representations, dynamically composing personal-\nized prompts to guide LLM reasoning, and projecting the resulting\nrepresentations through specialized output layers to generate task-\nspecific predictions. Our key contributions are:\n• We propose TransLLM, a unified foundation framework that\nintegrates spatiotemporal encoding with large language models\nfor diverse transportation tasks, supporting both forecasting and\noptimization problems.\n• We design a lightweight spatiotemporal encoder combining di-\nlated temporal convolutions with dual-adjacency graph attention\nnetworks, enabling task-agnostic modeling of complex spatiotem-\nporal dependencies.\n• We introduce a learnable instance-level prompt routing mecha-\nnism using reinforcement learning, which dynamically assembles\npersonalized prompts based on input characteristics, moving be-\nyond fixed task-wise templates.\n• Comprehensive experiments on seven datasets across three trans-\nportation tasks demonstrate superior performance over ten base-\nline methods, with significant improvements in both forecasting\naccuracy and dispatch efficiency.\n2\nRelated Work\nSmall-scale spatio-temporal prediction models. Small-scale\nspatio-temporal prediction networks are typically composed of\ntemporal and spatial modeling components. Early works, such\nas STGCN [44] and ASTGCN [13], adopt a \"sandwich\" architec-\nture consisting of a TCN–GCN–TCN structure to capture spatio-\ntemporal dependencies. Recent studies focus on modeling dynamic\nnode correlations. For instance, SHARE [46] utilizes contextual\ngraph convolution and soft clustering graph convolution, while\nDyHSL[47] models nonpairwise dependencies using hyperedges.\nFor temporal modeling, PDG2seq [8] decomposes time into daily\nand weekly features, employing a GRU structure to capture tem-\nporal dependencies at different scales. Meanwhile, STGODE [10]\nutilizes dilated temporal convolutions to expand its receptive field,\nthus enhancing its ability to model long-range temporal patterns.\nThese models are often designed for specific scenarios and heavily\nrely on labeled data, which limits their adaptability and scalability.\nLLM and LLM-based models for traffic tasks. Large language\nmodels have made remarkable progress in recent years. Founda-\ntional models such as GPT-4 [2] and Gemini [35] have laid a solid\nfoundation for the development of intelligent agents based on LLM.\nAlthough LLMs were originally developed for text understanding\nand generation, their adaptability has been widely explored across\nspecialized domains. Researchers have enhanced their ability to\nmodel domain-specific knowledge in areas such as code genera-\ntion [31], robotic control [17], and biomedical information extrac-\ntion [27]. In the traffic domain, UrbanGPT [23] improves LLMs’\ncapacity for temporal modeling by incorporating a time-series en-\ncoder. LLMLight [19] leverages imitation learning to guide large\nlanguage models in traffic signal selection. However, these methods\nare mainly limited to simple regression or classification tasks. They\nfail to capture complex spatial dependencies and cannot generalize\nto more sophisticated planning and decision-making scenarios.\n3\nPreliminaries\nWe consider two types of urban mobility tasks: (1) spatio-temporal\nforecasting, which includes both traffic flow and charging demand\nprediction, and (2) taxi dispatch optimization. For each task type,\nwe describe the construction of the spatial graph, the representation\nof spatio-temporal data, and the task formulation.\n3.1\nSpatio-Temporal Forecasting\nSpatial Structure. The spatial domain is represented as a graph\nG = (V, E, A), where each node 𝑣𝑖∈V corresponds to an urban\nmonitoring site, such as a traffic sensor or an urban grid cell. For\ntraffic flow forecasting, edges represent physical road connectivity\n\nTransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nST-Encoder\nLinear\nProjector\nPrompt\nRouter\n[𝒂𝟏, 𝒂𝟐, … , 𝒂𝑲]\nLLM\nStage 1\nStage 2\nOutput\nlayer1\nTrainable\nFrozen\nLoRA\nPersonalized\nprompt\nLLM\n[𝒂𝟏, 𝒂𝟐, … , 𝒂𝑲]\nPrompt\nRouter\nPersonalized\nprompt\nST-Embedding\n𝜒= 𝑥𝑡−𝐾+1,∙∙∙, 𝑥𝑡\nTraffic flow forecasting\nVehicle dispatching\nMulti-task\nCharging station usage\nprediction\nOutput\nlayer2\nOutput\nlayer3\nDilated TCN\nDilated TCN\n'\nGAT\nDilated TCN\nGAT\nDilated TCN\nFigure 1: The overall architecture of TransLLM.\nbetween sensors, while for charging demand prediction, edges\nare constructed based on geographical proximity between grid\ncells. The resulting adjacency matrix A ∈R𝑁×𝑁captures spatial\nrelationships among nodes.\nSpatio-Temporal Data. Historical observations are encoded as a\ntensor X ∈R𝐾×𝑁×𝐹, where 𝐾is the number of past time steps, 𝑁\nis the number of nodes, and 𝐹denotes the feature dimension, such\nas traffic volume, charging demand, or other auxiliary signals.\nTask Definition. Given past observations and spatial structure,\nthe goal is to forecast future values over the next 𝑇steps for all\nnodes. Formally:\n[ ˆ\nX𝑡+1, . . . , ˆ\nX𝑡+𝑇] = F\n\u0010\nX[𝑡−𝐾+1:𝑡], A\n\u0011\n,\n(1)\nwhere F denotes the spatio-temporal forecasting model.\n3.2\nTaxi Dispatch Optimization\nSpatial Structure. The urban area is partitioned into a set of\ngrids, each of size 3 km × 3 km, forming the nodes of a graph\nG = (V, E, A), where each node 𝑣𝑖∈V represents a region.\nEdges E connect spatially adjacent grids, and the adjacency matrix\nA ∈R𝑁×𝑁captures this neighborhood relationship.\nSpatio-Temporal Data. At each decision step 𝑡, we model three\nkey variables for regions 𝑣𝑐together with its eight spatial neigh-\nbors, forming a 3×3 neighborhood denoted as N9(𝑐).: (1) the num-\nber of vacant taxis X𝑡\n𝑣𝑐,𝑣∈R1×9, representing the number of\ndispatchable vehicles at time 𝑡in the central region 𝑣𝑐and its\neight surrounding neighbors; (2) the predicted passenger demand\nX[𝑡:𝑡+1]\n𝑣𝑐,𝑑\n∈R1×9, indicating the expected number of ride requests\nduring the interval [𝑡,𝑡+ 1); (3) the predicted number of compet-\ning taxis X[𝑡:𝑡+1]\n𝑣𝑐,𝑐\n∈R1×9, denoting the number of other available\nvehicles potentially serving requests. Notably, we utilize 𝐾-step his-\ntorical sequences X[𝑡−𝐾:𝑡−1]\n𝑣𝑐,𝑑\nand X[𝑡−𝐾:𝑡−1]\n𝑣𝑐,𝑐\nas inputs to a spatio-\ntemporal prediction encoder, which produces the corresponding\nfuture predictions 𝑋[𝑡:𝑡+1]\n𝑣𝑐,𝑑\nand X[𝑡:𝑡+1]\n𝑣𝑐,𝑐\n.\nTask Definition. We formulate the vehicle dispatching task as\na localized resource optimization problem, where each decision\nfocuses on reassigning vacant taxis from a central region 𝑣𝑐to its\n3 × 3 neighborhood N9(𝑐). Based on this context, the dispatching\ndecision for region 𝑣𝑐is defined as:\nD𝑐= F\n\u0010\nX𝑡0\n𝑣, X[𝑡0−𝐾:𝑡0−1]\n𝑑\n, X[𝑡0−𝐾:𝑡0−1]\n𝑐\n; A\n\u0011\n,\n(2)\nwhere D𝑐∈R1×9 denotes the dispatching proportions from 𝑣𝑐each\nregion in N9(𝑐).\n4\nMethod\nIn this section, we detail the components of TransLLM, including\nthe architecture of the spatiotemporal encoder (ST-Encoder), the\nreinforcement learning(RL)-based prompt personalization process,\nthe multi-task output layers, and the overall training mechanism.\nThe overall architecture of TransLLM is illustrated in Figure 1.\n4.1\nSpatio-Temporal Dependency Encoder\nTo capture complex spatio-temporal dependencies, we design a\nspatial-temporal encoder, consisting of multiple spatial-temporal\nblocks (ST-Blocks). Each ST-Block follows a “sandwich” structure,\ndefined as:\nH = TCN (GAT (TCN(X) + 𝜙(V), A)) ,\n(3)\nwhere TCN denotes a dilated Temporal Convolutional Network [3],\nGAT refers to a Graph Attention Network [36] based on the adja-\ncency matrix A, and 𝜙(·) is an embedding network that encodes\nnode-level meta attributes V. The ST-Block jointly models spatial\nand temporal dependencies, where dilated TCNs focus on local tem-\nporal patterns and GAT learns spatial relationships by attending\nto neighboring nodes. Besides, we include time-of-day and day-of-\nweek encodings in the input to help the ST-Block capture periodic\ntemporal patterns.\nThe ST-Encoder employs two distinct adjacency matrices: a spa-\ntial adjacency matrix A𝑠𝑝based on physical road connectivity or\ngeographic proximity, and a semantic adjacency matrix A𝑠𝑒con-\nstructed using Dynamic Time Warping [4] to capture functional\nsimilarity. By incorporating both types of adjacency, the encoder\nallows each node to aggregate information from not only spatially\nadjacent regions but also distant regions exhibiting similar tempo-\nral behavior. To process these two types of spatial relations, the\nencoder applies two separate sets of ST-Blocks, producing two\nspatio-temporal representations: H𝑠𝑝and H𝑠𝑒∈R𝑇×𝑁×𝐷. These\nare then concatenated to generate the final representation Hf for\ndownstream predictions.\n4.2\nRL-based Prompt Personalization\nTo dynamically select the most appropriate prompt for each in-\nstance, we introduce a prompt routing mechanism based on an\nActor–Critic reinforcement learning framework [9]. In this section,\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nJiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang*\nPrompt Pool\n…\nHistory Traffic\nTime Context\nSpatio-temporal Encoding\nReasoning & Output Format\n1. …using traffic flow recorded one hour ago…\n2. …one hour ago and the same time yesterday …\n3. …one hour ago and the same time last week …\n···\nk. …one hour ago , the same time yesterday and last week…\n1. …starting from <future_time>, with 5-minute intervals…\n2. …generate 12 future traffic to uncover periodic trends…\n3. …synthesize both rapid changes and gradual trends…\n···\nk. …adjusting for time-of-day effects like rush hours…\n1. …reason through overall trends, generate <PRE_EMB>…\n2. …analyze fluctuations and produce < PRE_EMB >…\n3. …draw from common traffic behaviors…\n···\nk. …consider congestion triggers using < PRE_EMB >…\n1. …use <HIS_EMB> to capture overall dependencies…\n2. …early-stage <HIS_EMB> reveals initial dynamics…\n3. …<HIS_EMB> represents spatio-temporal context...\n···\nk. … <HIS_EMB> embed all 12 predicted intervals with\ndedicated representations…\nSlot 1:\nSlot 2:\nSlot 3:\nSlot k:\nPrompt Pool\n……\nValue\nPrompt Router\nLLM\nActor\nAction\nLinear\nProjector\nCritic\n[𝑎1, 𝑎2, … , 𝑎𝑘]\nEnvironment\nR\nFigure 2: Instance-wise prompt generation.\nwe describe the instance-wise prompt generation process, as illus-\ntrated in Figure 2, along with the learning mechanism for updating\nthe router.\n4.2.1\nPrompt Routing Mechanism. We formulate the construction\nof a personalized prompt as a multi-step decision process. Specifi-\ncally, a base prompt template is divided into 𝐾functional slots, and\nfor each slot, the most suitable component is selected through an\nindependent Actor–Critic network.\nTo select appropriate sentences for each slot, we use the spatio-\ntemporal representation Hf generated by the ST-Encoder as input.\nThe actor and critic networks are implemented as two separate\nMLPs. For each slot 𝑘, the actor network maps the spatio-temporal\nrepresentation Hf to a probability distribution 𝜋𝑘over the can-\ndidate sentence options, while the critic network computes the\nexpected reward 𝑣𝑘to guide the actor’s training. The actor and\ncritic functions for slot 𝑘are defined as:\n𝜋𝑘= Softmax(MLP(Hf)),\n𝑉𝑘= MLP(Hf).\n(4)\nAn action 𝑎𝑘is sampled from the distribution 𝜋𝑘, and actions\nfrom all 𝐾slots are concatenated into a composite vector a𝑡=\n[𝑎1,𝑎2, . . . ,𝑎𝐾], which determines how the final prompt is con-\nstructed.\n4.2.2\nDiverse Prompt Pool Construction. The effectiveness of the\nPrompt Router relies on a well-designed and diverse prompt can-\ndidate pool, which constitutes the action space for the RL agent.\nWe construct this pool by partitioning the prompt into 𝐾distinct\nfunctional slots, each containing multiple candidate sentences that\nexpress a similar intent with varied phrasing and focus. For illustra-\ntion, in the context of a spatio-temporal forecasting task, we define\nthe following functional slots:\nHistorical spatio-temporal information: To account for the\nvarying periodic influences across different instances, this slot pro-\nvides historical traffic data at multiple temporal granularities. Ex-\namples include \"using traffic observations from one hour ago\" to\ncapture short-term patterns, or \"combining data from the same time\non the previous day and week\" to incorporate daily and weekly\nperiodic trends. This flexibility enables the model to better adapt to\nboth immediate fluctuations and long-term temporal regularities.\nTime Context: Given the strong periodic patterns in traffic flow,\nsuch as morning and evening peaks or holiday effects. This slot\nspecifies the temporal context of the prediction, including the time\nof day, day of the week, prediction horizon, and temporal resolution.\nTo enhance adaptability across instances, we provide candidate\nsentences that emphasize different temporal characteristics, such\nas \"capturing short-term fluctuations,\" or \"highlighting daily and\nweekly periodic trends,\" encouraging each instance to focus on the\nmost relevant temporal signals for each scenario.\nSpatio-temporal Encoding: To help the language model interpret\nstructured spatiotemporal features, this slot introduces placeholder\ntokens like ⟨HIS_EMB⟩, which are replaced by embeddings from\na spatio-temporal encoder. The candidate sentences vary in fo-\ncus—some describe full 12-step prediction dependencies, others\nemphasize early-stage forecasting or the interpretability of step-\nwise representations. These variations allow the model to flexibly\nadjust its reasoning strategy based on the temporal characteristics\nof each instance.\nReasoning & Output Format: This slot defines how the language\nmodel should generate its prediction and present the output. While\nall candidate sentences guide the model to conclude with a standard-\nized 12-step forecast using the ⟨PRE_EMB⟩token, they differ in the\nreasoning process they encourage. Some variants explicitly instruct\nthe model to reason step by step through temporal patterns and\nspatial influences, aiming to enhance interpretability and reduce\nhallucination. Others emphasize domain knowledge—such as rush\nhour behavior or anomalies in recent flow trends—to guide the fore-\ncast more heuristically. This diversity allows the model to flexibly\nadapt its reasoning strategy to suit different traffic scenarios.\n4.2.3\nTextual-Spatiotemporal Alignment. To enable effective align-\nment between structured spatiotemporal signals and natural lan-\nguage input, we embed the encoded representations directly into\nthe LLM’s input space via a token-based interface. Specifically, we\nuse a placeholder token sequence ⟨st_start⟩⟨st_patch⟩×𝑇⟨st_end⟩\nto replace ⟨HIS_EMB⟩and ⟨PRE_EMB⟩, where 𝑇denotes the num-\nber of prediction steps, and each ⟨st_patch⟩corresponds to one\nfuture time step. Subsequently, the spatio-temporal embeddings\nE ∈R𝑇×2𝐷produced by the ST-Encoder are projected to match the\nLLM’s hidden dimension 𝑑L via a linear layer, yielding E′ ∈R𝑇×𝑑L.\nThese aligned embeddings are then inserted into the prompt by\nsequentially replacing the ⟨st_patch⟩tokens.\n4.2.4\nReinforcement-Guided Routing Update. To support diverse\ndownstream tasks, the reward signal ˆ\n𝑅for training the prompt\nrouter is defined as either the negative prediction loss or task-\nspecific metrics such as taxi dispatch rewards, directly linking\nprompt quality to task performance. For each router, we apply\na shared reward signal to evaluate all slots, enabling each slot to\nindependently learn its optimal prompt routing policy through ded-\nicated actor-critic networks. The optimization objectives for the\nactor and critic networks of slot 𝑘are defined as follows:\nL(𝑘)\n𝑎\n= −log 𝜋𝑘·\n\u0010\nˆ\n𝑅𝑡−𝑉𝑘\n\u0011\n,\nL(𝑘)\n𝑐\n=\n\u0010\nˆ\n𝑅𝑡−𝑉𝑘\n\u00112\n.\n(5)\n\nTransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\n4.3\nMulti-task Output Layers\nIn TransLLM, we avoid using the LLM to directly generate outputs.\nInstead, we employ multiple task-specific output layers. This design\nenables more accurate handling of continuous values, avoiding the\nprecision loss caused by token-level discretization. Moreover, cus-\ntomized output layers are better aligned with the objective formats\nand evaluation metrics of different downstream tasks, helping to\nmitigate interference caused by mismatched output spaces across\ntasks. The hidden representations from the LLM are extracted and\npassed through a multi-task output layer tailored to produce task-\nspecific predictions.\nSpatio-Temporal Forecasting: The prediction-related hidden\nstate Hf ′ at the ⟨st_start⟩token and the encoder output Hf are\nindividually processed by linear output layers to generate the final\nprediction ˆ\ny:\nˆ\ny = W3\n\u0002\nConcat \u0000ReLU(W1Hf + b1), ReLU(W2Hf ′ + b2)\u0001\u0003\n+ b3,\n(6)\nwhere W1, W2, W3, b1, b2, and b3 are learnable weight matrices.\nTaxi Dispatch Optimization: For the dispatching task, the goal\nis to generate a probability distribution over the nine candidate\nregions in the 3×3 neighborhood surrounding the current location.\nTo this end, the hidden state H′\nf from the ⟨st_start⟩token is first\nprojected through a linear layer and then normalized via a softmax\nfunction to produce the dispatching probability distribution:\nD𝑐= Softmax(W𝑑H′\nf + b𝑑),\n(7)\nwhere W𝑑and b𝑑are learnable parameters. The output D𝑐∈R1×9\nindicates the proportion of vacant vehicles that should be reallo-\ncated from grid 𝑣𝑐to its surrounding regions.\n4.4\nModel Training\n4.4.1\nTwo-stage Alternating Training . Considering the high com-\nputational cost of full-parameter fine-tuning for LLMs, we adopt\nLow-Rank Adaptation (LoRA)[14] to improve training efficiency.\nTo improve stability and efficiency, we adopt a two-stage training\nstrategy. In the first stage, the LLM is fine-tuned, while the Prompt\nRouter remains frozen. In the second stage, the LLM is frozen and\nthe Prompt Router is trained. In practice, we find that a single\nalternation between the two stages is sufficient to reach optimal\nperformance on traffic-related tasks.\n4.4.2\nLoss Function. To enable joint optimization across multiple\ntasks, we design a composite loss function formulated as:\nL = L𝐿𝐿𝑀+ 𝜆𝑡· L𝑡,\n(8)\nwhere L𝐿𝐿𝑀denotes the standard cross-entropy loss for LLM, L𝑡\nrepresents the specific task loss, and 𝜆𝑡is a weighting coefficient.\nSpatio-Temporal Forecasting Loss. We employ the Mean Abso-\nlute Error (MAE) loss as the regression loss:\nL𝑡= 1\n𝑁\n𝑁\n∑︁\n𝑖=1\n| ˆ\n𝑦𝑖−𝑦𝑖|,\n(9)\nwhere ˆ\n𝑦𝑖and 𝑦𝑖denote the predicted and ground truth values for\nthe 𝑖-th sample.\nReinforcement Learning Loss for Vehicle Dispatching. We\npropose a multi-objective reinforcement learning loss function:\nL𝑑= L𝑟𝑓+ 𝜆𝑤L𝑤+ 𝜆𝑒H (𝜋),\nH (𝜋) = −\n9\n∑︁\n𝑔=1\n𝜋𝑔log 𝜋𝑔,\nL𝑤=\n9\n∑︁\n𝑔=1\n\n\n\n\n\nCDF(𝜋𝑏)𝑔−CDF(p𝑏\nreal)𝑔\n\n\n\n\n\n,\n𝑅𝑔= 𝛽𝑀𝑔\n𝑉𝑔\n−𝛾𝐷𝑔.\n(10)\nwhere L𝑟𝑓is the reinforcement loss, L𝑤is the Wasserstein distance\nbetween predicted and ground-truth distributions computed via cu-\nmulative distribution functions (CDFs), H (𝜋) is the entropy of the\npredicted distribution, and 𝑅𝑔is the reward function for reinforce-\nment learning that balances service efficiency and dispatch cost.\nSpecifically, 𝑀𝑔is the matching rate in grid 𝑔, 𝑉𝑔is the number of\nvacant taxis, and 𝐷𝑔represents a predefined distance penalty. The\ncoefficients 𝜆𝑤, 𝜆𝑒, 𝛽, and 𝛾are the weighting factors for each com-\nponent. This composite loss structure enables our model to learn\na sophisticated dispatching policy that simultaneously balances\nservice efficiency, dispatching cost, and distributional realism.\n5\nExperiment\nIn this section, we conduct extensive experiments to evaluate the\neffectiveness of TransLLM across multiple transportation tasks. Our\nstudy is guided by the following research questions. Additionally, a\ncase study is provided in the Appendix.\n• RQ1: How does TransLLM perform compared to GNN-based\nbaselines in traditional supervised scenarios? Can it effectively\nhandle complex real-world taxi dispatching task?\n• RQ2: How is the generalization ability of TransLLM when facing\nunseen scenarios and datasets?\n• RQ3: What is the contribution of each key module within TransLLM\nto its overall performance?\n• RQ4: How does the performance of TransLLM vary under differ-\nent hyperparameter configurations settings\n5.1\nDataset\nTo comprehensively evaluate the effectiveness of TransLLM across\ndiverse transportation tasks, we conduct experiments on five public\ndatasets covering traffic forecasting, charging demand prediction,\nand vehicle dispatching. LargeST-SD is a subset of the LargeST\ndataset [25], containing traffic flow records from 716 loop detectors\non highways in San Diego County, spanning from January 1 to\nDecember 31, 2021. Pems08 includes traffic data from 170 sensors\nin California, collected between July 1 and August 31, 2016. ST-\nEVCDP [30] is a charging demand dataset collected from 247 areas\nin Shenzhen between June 19 and July 18, 2022, while UrbanEV [21]\ncaptures large-scale charging behavior across urban regions from\nSeptember 1, 2022 to February 28, 2023. For the vehicle dispatching\ntask, we use the Taxi-SH dataset, which contains GPS trajectories of\ntaxis in Shanghai from April 13 to April 19, 2015. The city is divided\ninto 3 km × 3 km grids, and we aggregate the number of vacant taxis,\npassenger demand, and competing vehicles at 5-minute intervals. In\naddition, we evaluate the models under a zero-shot setting on two\npreviously unseen datasets, PEMS03 and PEMS04, to assess their\ngeneralization capabilities. Among these, UrbanEV has a temporal\nresolution of one hour, while the others have a resolution of five\nminutes. More details are provided in the Appendix.\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nJiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang*\n5.2\nHyperparameters Settings\nIn our experimental setup, the key hyperparameters are configured\nas follows. The ST-Encoder is built by stacking 2 ST-Blocks, with a\nfinal output dimension 𝐷of 64. The Prompt Router is configured\nwith 4 slots and 4 sentences. For prediction tasks, we predict the\nnext 12 steps of data based on the previous 12 steps. Both the history\nlength 𝐾and prediction length 𝑇are set to 12. For the taxi dispatch\ntask, we predict the dispatch probabilities over nine grids. Both\n𝐾and 𝑇are set to 9. The model is trained with a learning rate of\n1 × 10−4, and the composite loss function balances objectives with\nweights 𝜆𝑡= 1.0, 𝜆𝑤= 0.01, 𝜆𝑒= 0.008, and reward coefficients\n𝛽= 2.0 and 𝛾= 0.05.\n5.3\nEvaluation Metrics\nFor spatio-temporal forecasting tasks, we employ the Mean Ab-\nsolute Error (MAE) and the Root Mean Squared Error (RMSE) to\nquantify prediction accuracy. Both metrics measure the deviation\nbetween predicted and true values.\nMAE = 1\n𝑁\n𝑁\n∑︁\n𝑖=1\n|𝑦𝑖−ˆ\n𝑦𝑖|,\nRMSE =\nv\nu\nt\n1\n𝑁\n𝑁\n∑︁\n𝑖=1\n(𝑦𝑖−ˆ\n𝑦𝑖)2.\n(11)\nFor the taxi dispatching task, we introduce three evaluation\nmetrics: the Mean Matching Rate (MMR) represents the proportion\nof matched vehicles; the Mean Driving Distance (MDD) calculates\nthe average deadhead distance to measure dispatching costs; and the\nWasserstein Distance (W-Dist) quantifies the discrepancy between\nthe model’s predicted dispatch distribution and the ground-truth\ndistribution.The three metrics are formulated as:\nMMR = 1\n𝑁\n𝑁\n∑︁\n𝑖=1\nÍ9\n𝑔=1 ˆ\nM𝑖,𝑔\nV0,𝑖\n!\n, MDD = 1\n𝑁\n𝑁\n∑︁\n𝑖=1\n©\n­\n«\n9\n∑︁\n𝑔=1\nˆ\n𝑝𝑖,𝑔· 𝐶𝑘ª\n®\n¬\n,\nW-Dist = 1\n𝑁\n𝑁\n∑︁\n𝑖=1\n©\n­\n«\n9\n∑︁\n𝑔=1\n|𝐶𝐷𝐹( ˆ\n𝑃𝑖)𝑔−𝐶𝐷𝐹(𝑃𝑖)𝑔|ª\n®\n¬\n.\n(12)\nHere, ˆ\nM𝑖,𝑔is the predicted matched vehicles in region 𝑔for instance\n𝑖; V0,𝑖represents the empty vehicles in the central region; ˆ\n𝑝𝑖,𝑔is the\npredicted dispatch probability; 𝐶𝑔denotes the deadhead distance\ncost; and ˆ\n𝑃𝑖and 𝑃𝑖are the predicted and true dispatch probability\ndistributions.\n5.4\nBaselines\nWe conducted a thorough comparison with ten baseline models\nto ensure a comprehensive evaluation of our proposed approach.\nThese baselines can be broadly categorized into three groups:\n• Small-scale GNN-based Deep Learning Models. These mod-\nels are developed specifically for spatiotemporal forecasting and\nare trained in an end-to-end manner on individual datasets. They\ntypically rely on carefully designed architectures that integrate\ngraph-based spatial encoders with temporal modeling compo-\nnents. Representative examples include STGCN [44], DGCRN [20],\nASTGCN [13], D2STGNN [33], GWNET [40], STGODE [10], and\nPDG2Seq [8].\n• Generalist Large Language Models (LLMs). These models,\nincluding Deepseek-v3 [24] and GPT-4o [15], are not specifically\ntrained for spatiotemporal or transportation tasks. We evaluate\nthem in a zero-shot setting without any task-specific tuning:\n• Spatio-temporal Enhanced LLMs. This category includes LLM-\nbased architectures that leverage structured spatiotemporal pri-\nors to improve the model’s ability to reason over urban dynamics,\nwith UrbanGPT [23] as a representative example.\n5.5\nOverall Performance (RQ1)\nWe evaluate the performance of TransLLM across three tasks and\nfive datasets, including traffic flow forecasting, charging demand\nprediction, and taxi dispatch optimization. Table 1 summarizes the\noverall results. TransLLM (LLaMA3) and TransLLM (Vicuna) re-\nfer to model variants built on different base LLMs. Overall, even\nin scenarios where small-scale models typically excel, TransLLM\nconsistently outperforms all baselines across the five datasets. More-\nover, employing more advanced foundation language models can\nlead to further improvements in the performance of TransLLM.\nWe observe that the performance of the three model categories\nvaries across different datasets. On the UrbanEV dataset, spatio-\ntemporal enhanced LLMs clearly outperform the other two cate-\ngories. In particular, TransLLM achieves a MAE of 0.42, represent-\ning an 83.7% improvement over the best-performing small-scale\nbaseline. This may be attributed to the dataset’s 1-hour temporal res-\nolution, which contrasts with the 5-minute intervals used in other\ndatasets. The smoother data distribution weakens the advantage of\nsmall models in capturing fine-grained short-term dependencies. In\ncontrast, LLM-based models excel by leveraging contextual reason-\ning and learned priors, which enhance their robustness to sparse\ntemporal signals. Meanwhile, the performance gain of TransLLM\non the ST-EVCDP dataset is notably smaller than that on traffic flow\nprediction datasets such as PEMS08. To investigate this discrep-\nancy, we found that ST-EVCDP exhibits weaker periodic patterns\nand higher volatility compared to PEMS08, as detailed in the Ap-\npendix, making it inherently more difficult to predict. Moreover,\nTransLLM is trained with significantly less data than small-scale\nmodels, which makes achieving high accuracy more challenging. In\ncontrast, powerful foundation models such as GPT-4o demonstrate\nstrong performance on ST-EVCDP, likely benefiting from their rich\nand diverse pretraining data.\nThe effectiveness of TransLLM is further validated on the taxi\ndispatching task. The quality of dispatch is evaluated using three\nmetrics: MMR, MDD, and W-Dist to the real distribution. Among\nthese, MMR is the most critical metric. Compared to the second-best\nmodel, TransLLM improves MMR by 5% and reduces the average\ntravel distance by 0.71 km. TransLLM underperforms GPT-4o and\nDeepSeek-v3 in terms of MMD and W-Dist. This is because general-\nist LLMs tend to adopt a conservative dispatching strategy, keeping\nvehicles in their original grids, which leads to the lowest average\ntravel distance and the closest match to the real-world distribution.\nIn summary, despite limited training data, TransLLM consis-\ntently outperforms ten baseline models in spatiotemporal forecast-\ning tasks. For the taxi dispatching task, TransLLM adopts a more\nproactive dispatching strategy that achieves higher order-taking\nrates with only a slight increase in travel distance, despite deviating\nfrom real trajectories.\n5.6\nZero-shot Scenarios Performance(RQ2)\nZero-shot generalization is critical for foundation models in the\ntransportation domain. It reflects the model’s ability to capture\n\nTransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 1: Performance comparison across model types. Bold: Best, underline: Second best, Double underline: Third best.\nTraffic Forecasting\nCharging Demand Prediction\nVehicle Dispatching\nModels\nLargeST-SD\nPEMS08\nST-EVCDP\nUrbanEV\nTaxi-SH\nMAE ↓\nRMSE ↓\nMAE ↓\nRMSE ↓\nMAE ↓\nRMSE ↓\nMAE ↓\nRMSE ↓\nMMR(%) ↑\nMDD ↓\nW-Dist ↓\nSmall-scale GNN-based Deep Learning Models\nSTGCN\n13.93\n26.10\n10.30\n14.77\n2.11\n3.64\n3.02\n5.30\n19.10\n5.80\n1.26\nDGCRN\n11.46\n24.63\n10.45\n14.47\n1.71\n4.46\n3.04\n5.47\n18.89\n3.60\n1.46\nASTGCN\n12.34\n25.09\n10.17\n14.48\n1.85\n3.45\n3.32\n5.81\n19.91\n3.96\n1.32\nD2STGNN\n11.72\n25.18\n8.89\n12.72\n1.39\n2.56\n2.65\n4.59\n18.66\n3.50\n1.25\nGWNET\n13.43\n26.42\n9.26\n12.90\n1.45\n2.65\n2.57\n4.27\n19.29\n3.49\n1.34\nSTGODE\n11.83\n24.63\n9.18\n13.16\n1.50\n2.73\n2.66\n4.53\n19.26\n3.47\n1.28\nPDG2seq\n12.16\n25.02\n9.54\n13.53\n1.46\n2.93\n3.33\n7.38\n18.92\n3.49\n1.21\nGeneralist LLMs\nDeepseek-v3\n39.25\n52.28\n23.41\n30.26\n2.13\n4.73\n4.60\n9.02\n19.07\n2.57\n0.75\nGPT-4o\n40.54\n53.52\n23.89\n30.89\n1.66\n3.55\n3.53\n7.85\n18.00\n2.40\n0.86\nSpatio-temporal Enhanced LLMs\nUrbangpt\n11.28\n23.17\n10.23\n13.53\n2.09\n5.86\n1.12\n4.38\n19.46\n3.21\n2.31\nTransLLM(vicuna)\n10.98\n21.42\n7.88\n11.33\n1.39\n2.73\n0.44\n3.64\n24.46\n3.15\n1.96\nTransLLM(llama3)\n9.41\n16.78\n7.26\n10.68\n1.26\n2.16\n0.42\n3.42\n24.78\n3.25\n2.02\nTable 2: Zero-shot performance on PEMS03 and PEMS04.\nPEMS03\nPEMS04\nModels\nMAE ↓\nRMSE ↓\nMAE ↓\nRMSE ↓\nSmall-scale GNN-based Deep Learning Models\nSTGCN\n57.43\n64.74\n39.08\n49.93\nDGCRN\n58.05\n61.86\n29.60\n39.30\nASTGCN\n80.23\n88.66\n45.02\n54.65\nD2STGNN\n63.17\n66.65\n30.92\n39.23\nGWNET\n55.51\n59.21\n27.06\n33.46\nSTGODE\n73.54\n81.98\n64.33\n77.18\nPDG2seq\n88.07\n101.10\n55.88\n69.15\nGeneralist LLMs\nDeepseek-v3\n25.15\n38.28\n37.79\n55.83\nGPT-4o\n27.06\n41.31\n40.42\n59.47\nSpatio-temporal Enhanced LLMs\nUrbangpt\n23.27\n37.58\n41.52\n69.76\nTransLLM\n18.92\n30.27\n28.69\n47.27\nuniversal spatiotemporal patterns and determines its transferability\nand practical value across regions and tasks. A model with strong\ngeneralization capabilities can adapt to prediction tasks in new cities\nand unseen scenarios without the need for additional annotations\nor fine-tuning, thereby significantly reducing deployment costs and\nenhancing the feasibility of large-scale applications. Specifically,\nwe utilize datasets from different districts within the Los Angeles\narea: PEMS08 is used for training, while zero-shot evaluation is\nconducted on PEMS03 and PEMS04, both of which remain unseen\nduring the training phase.\nWe observe that LLM-based methods achieve significantly better\nimprovements over small-scale GNN-based deep learning models\non the PEMS03 dataset. In contrast, on the PEMS04 dataset, both\ncategories exhibit similar performance. This discrepancy may be\ndue to PEMS04 having more similar spatiotemporal patterns to the\ntraining dataset, PEMS08, than PEMS03. Specifically, the PEMS08\ndataset has an average flow of 230.68 and a maximum of 1147.0,\nwhile PEMS04 has an average flow of 226.13 and a maximum of\n896.0. In comparison, PEMS03 features a lower average flow of\n149.52 but a much higher peak of 1852.0. These more similar traffic\npatterns and the limited training data reduce the relative advantages\nof LLM-based methods. Nevertheless, TransLLM still achieves the\nbest predictive performance among all LLM-based approaches. On\nthe more divergent PEMS03 dataset, it outperforms UrbanGPT by\n18.7% in MAE and 19.5% in RMSE.\n5.7\nAblation Study (RQ3)\nTo investigate the impact of individual modules on the overall\nperformance, we evaluated TransLLM variants by removing key\ncomponents: the ST-Encoder, Prompt Router, and the LLM module.\nFigure 3 presents the ablation results on the LargeST-SD and ST-\nEVCDP datasets.\nEffect of ST-encoder. w/o STE denotes a variant of TransLLM\nin which the ST-Encoder modules are removed. We observe a no-\nticeable decline in the model’s performance, indicating that the\nrich spatio-temporal features captured by the ST-Encoder effec-\ntively assist the LLM in understanding and processing complex\nspatio-temporal patterns. The ST-Encoder serves as a crucial com-\nponent that integrates non-textual spatio-temporal information\ninto the LLM’s input space, and its removal results in a substantial\ndegradation in predictive accuracy.\nEffect of Prompter Router. w/o PR removes the Prompt Router\nmodule and instead uses the same fixed prompt for each instance.\nThis variant exhibits a clear performance drop. Although the LLM\n\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nJiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang*\nRMSE\nMAE\nRMSE\nLargeST-SD\nLargeST-SD\nST-EVCDP\nST-EVCDP\nw/o PR&LLM\nw/o PR\nw/o STE\nTransLLM\nMAE\nFigure 3: Ablation study results on LargeST-SD and ST-\nEVCDP datasets.\nLargeST-SD\nMAE\nRMSE\nRMSE\nMAE\nLargeST-SD\nST-EVCDP\nST-EVCDP\nFigure 4: Effect of number of candidate sentences per slot 𝑁𝑐\non LargeST-SD and ST-EVCDP datasets.\ncan still leverage the spatio-temporal features provided by the ST-\nEncoder, the absence of dynamically generated, instance-specific\nprompts limits its full potential. This highlights the critical role\nthat prompt quality plays in LLM performance. However, prompt\nquality is inherently difficult to evaluate manually. The Prompt\nRouter plays a vital role in supplying the LLM with accurate and\ninstance-specific prompts, thereby enhancing its reasoning and\nprediction capabilities.\nEffect of Large language model. w/o PR & LLM removes the\nPrompt Router and the LLM modules, also resulting in a significant\nperformance decline. This underscores the LLM’s ability to model\ncomplex spatio-temporal patterns, support advanced reasoning,\nand enhance generalization. To summary, the results demonstrate\nthat the full model consistently achieves the best performance\nacross all tasks, and the absence of any individual module leads to\na degradation in TransLLM’s overall effectiveness.\n5.8\nParameter Sensitivity (RQ4)\nTo evaluate the impact of different hyperparameters on TransLLM’s\nperformance, we adjust two key settings: (1) the number of candi-\ndate sentences per slot 𝑁𝑐, and (2) the number of spatio-temporal\nfeature patch tokens ⟨st_patch⟩𝑁𝑝.\nNumber of candidate sentences per slot 𝑁𝑐. Figure 4 shows that\nincreasing the number of sentences does not necessarily lead to\ncontinuous performance improvements. The optimal configuration\nis achieved with four candidate sentences per slot on LargeST-SD\nand two on ST-EVCDP. While an insufficient number of sentences\nmay limit the diversity available to the Prompt Router, an excessive\nnumber can introduce redundancy and increase the learning burden\non the LLM, ultimately degrading performance.\nNumber of ⟨st_patch⟩tokens 𝑁𝑝. Figure 5 indicates that model\nperformance generally improves with an increasing number of to-\nkens. The best results are typically obtained when the token count\nmatches the prediction horizon. This alignment enables comprehen-\nsive encoding of spatio-temporal dependencies, whereas shorter\nsequences such as 1 or 3 tokens fail to capture sufficient informa-\ntion, leading to degraded performance. Notably, on the UrbanEV\nLargeST-SD\nMAE\nRMSE\nRMSE\nMAE\nLargeST-SD\nUrbanEV\nUrbanEV\nFigure 5: Effect of number of ⟨st_patch⟩tokens 𝑁𝑝on\nLargeST-SD and UrbanEV datasets.\nSlot 2\nTop 2\nTop 3\nTop 4\nTop 1\nSlot 1\nSlot 3\nSlot 4\n(a) ST-EVCDP\nSlot 2\nSlot 1\nSlot 3\nSlot 4\n(b) PEMS08\nSentence 1\nSentence 2\nSentence 3\nSentence 4\nFigure 6: Sentence selection frequency per slot. The size of the\ncircles represents the selection frequency of each sentence.\ndataset, both 6 and 12 tokens yield similarly strong results, likely\ndue to its lower temporal resolution, which reduces sensitivity to\ntoken sequence length. In summary, the above parameter sensitiv-\nity analysis provides important insights for selecting TransLLM’s\nhyperparameters, ensuring the model’s robustness and optimal\nperformance across different tasks and datasets.\n5.9\nPrompt Routing Behavior Analysis\nTo further evaluate the effectiveness of the Prompt Router, we visu-\nalize the selection frequency of each sentence in the prompt pool\nduring testing on the ST-EVCDP and PEMS08 datasets, as shown in\nFigure 6. It can be observed that the Prompt Router successfully se-\nlects the most appropriate prompt for each instance. Moreover, the\nselection frequencies vary across sentences within each slot—some\nsentences are favored by the majority of instances, while others are\nrarely chosen. This also reveals the diversity of spatiotemporal pat-\nterns across instances and the router’s ability to adaptively match\nprompts based on instance-level characteristics.\n6\nConclusion\nIn this work, we present TransLLM, a unified foundation frame-\nwork that seamlessly integrates spatiotemporal modeling with large\nlanguage models for diverse urban transportation tasks. Our key\ncontributions include the novel framework design that bridges\nspatiotemporal encoders with LLMs through structured embed-\ndings, and a two-stage learnable prompt composition mechanism\nwith instance-level routing that dynamically personalizes prompts\nbased on input characteristics. Extensive experiments across seven\ndatasets demonstrate strong performance in both supervised and\nzero-shot settings, showcasing excellent generalization and cross-\ntask adaptability.\n\nTransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY"}
{"id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "authors": ["Samir Abdaljalil", "Erchin Serpedin", "Khalid Qaraqe", "Hasan Kurban"], "source": "http://arxiv.org/abs/2508.14735v1", "page_count": 10, "text": "Evaluating Multilingual and Code-Switched Alignment in LLMs via\nSynthetic Natural Language Inference\nSamir Abdaljalil*, Erchin Serpedin*, Khalid Qaraqe†, Hasan Kurban†\n*Texas A&M University, College Station, TX., USA\n†Hamad Bin Khalifa University, Doha, Qatar\nsabdaljalil@tamu.edu, hkurban@hbku.edu.qa\nAbstract\nLarge language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent,\nlogically grounded alignment across languages remains underexplored.\nWe present a controlled evaluation\nframework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise–hypothesis\npairs and translates them into a typologically diverse set of languages.\nThis design enables precise control\nover semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions.\nSurprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-\ninduced lexical variation may serve as a regularization signal.\nWe validate semantic preservation through\nembedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated\npairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and\nidentify code-switching as a promising lever for improving multilingual robustness. Code can be accessed at:\nhttps://github.com/KurbanIntelligenceLab/nli-stress-testing\nKeywords: Large Language Models (LLMs), Natural Language Inference (NLI), Multilingual Alignment\n1.\nIntroduction\nNLI (Dagan et al., 2005)—determining whether a\nhypothesis is entailed by, contradicts, or is neu-\ntral with respect to a premise—is a core bench-\nmark for natural language understanding (Haval-\ndar et al., 2025; Yudanto et al., 2024; Mor-Lan and\nLevi, 2024). Its emphasis on fine-grained seman-\ntic distinctions has long made it a proxy for test-\ning models’ capacity for deep reasoning (Cosma\net al., 2024). With LLMs, NLI has become a key\ntool for assessing generalization, reasoning, and\nknowledge encoding (Cheng et al., 2025). Yet\nevaluations remain concentrated on high-resource\nlanguages—especially English—and are often em-\nbedded within downstream tasks such as QA or\nsummarization, limiting insight into whether infer-\nence capabilities transfer consistently across lan-\nguages under controlled semantic conditions.\nWe address this gap with a synthetic multilin-\ngual NLI framework that stress-tests cross-lingual\nsemantic alignment via deterministic, logic-based\ntemplates encoding entailment, contradiction, and\nneutrality. The approach decouples logical struc-\nture from lexical and cultural priors, avoiding anno-\ntation noise and enabling direct, large-scale eval-\nuation. Our contributions are: (1) a logic-driven\nmethod for generating synthetic multilingual NLI\ndatasets with precise control over inference types\nand linguistic variation; (2) an automated evalua-\ntion protocol for measuring cross-lingual consis-\ntency in LLM semantic judgments; and (3) em-\npirical evidence, across multiple models and lan-\nguages, of systematic weaknesses in multilingual\nalignment.\nBy disentangling logical reasoning from linguis-\ntic noise, our framework offers a principled, repro-\nducible basis for evaluating semantic alignment in\nmultilingual LLMs. Section 2 reviews related work,\nSection 3 details the methodology, and Section 4\noutlines the experimental setup. Section 5 reports\nthe main findings, followed by qualitative and quan-\ntitative analyses in Section 6. Section 7 concludes\nwith a discussion of limitations and future direc-\ntions.\n2.\nRelated Work\nNatural Language Inference for Multilingual\nEvaluation.\nNLI has become a standard probe\nfor semantic understanding in language models\n(Nighojkar et al., 2023). By requiring systems to\ndetermine whether a hypothesis follows from a\npremise, it offers a fine-grained test of reasoning,\nworld knowledge, and linguistic nuance. Bench-\nmarks such as GLUE (Wang et al., 2018) and\nSNLI (Bowman et al., 2015) established its role in\nEnglish-centric NLP\n, while XNLI (Conneau et al.,\n2018) extended evaluation to 15+ languages via\nprofessional translation. Owing to its structured\nand interpretable format, NLI has been widely\nused for assessing cross-lingual transfer (Heredia\net al., 2024; Bandyopadhyay et al., 2022). How-\never, most prior work assumes monolingual eval-\n\nuation—premise and hypothesis in the same lan-\nguage—thus overlooking mixed-lingual scenarios\nthat are common in real multilingual discourse.\nCross-Lingual Generalization in Large Lan-\nguage Models.\nMultilingual LLMs exhibit strong\nzero-shot transfer across languages (Conneau\net al., 2020; Artetxe et al., 2020), aided by shared\ntokenization schemes and aligned embedding\nspaces.\nEarly work with mBERT and XLM-R\ndemonstrated cross-lingual transfer without explicit\nparallel training, attributed to emergent language\nalignment (Pires et al., 2019). However, later stud-\nies revealed systematic biases: performance fa-\nvors high-resource languages, while low-resource\nand morphologically rich languages often show\ndegraded representations (Schuster et al., 2019).\nAlthough recent benchmarks broaden multilingual\nevaluation, they typically assume monolingual in-\nputs or perfect translation symmetry.\nRobust-\nness in mixed-lingual settings—where premise and\nhypothesis are in different languages—remains\nlargely untested, despite its relevance for assess-\ning sentence-level semantic alignment beyond\ntoken overlap.\nCode-switching, a natural phe-\nnomenon in multilingual communities, is particu-\nlarly underexplored in LLM reasoning tasks (Khatri\net al., 2023). Moreover, most studies use natural\ntext, conflating syntactic variation with semantic\ndifficulty.\nOur work follows the tradition of NLI as a diag-\nnostic tool but diverges in three ways: we use fully\nsynthetic, logically controlled data; we evaluate\ntranslation consistency alongside reasoning; and\nwe incorporate code-switching to probe multilin-\ngual alignment under conditions rarely addressed\nin prior studies.\nWe address this by evaluating on synthetic NLI\npairs with controlled logical structure, enabling iso-\nlation of semantic consistency from linguistic noise.\nOur framework combines synthetic NLI data, high-\nquality translation, and controlled code-switching\nto stress-test multilingual alignment in both mono-\nlingual and mixed-lingual conditions. This design\nuncovers unexpected generalization patterns in\ninstruction-tuned LLMs, challenging prevailing as-\nsumptions about cross-lingual reasoning robust-\nness.\n3.\nMethodology\nThis study examines the ability of LLMs to perform\nlogically grounded NLI across languages using\na controlled framework based on synthetic data\ngeneration and high-quality translation. The frame-\nwork enables systematic evaluation of multilingual\nsemantic alignment under both monolingual and\nmixed-lingual conditions. Figure 1 illustrates the\noverall methodology for dataset construction and\nLLM evaluation.\n3.1.\nSynthetic NLI Construction\nA synthetic English NLI dataset is constructed from\nhand-crafted templates encoding three logical re-\nlations: entailment, contradiction, and neutrality.\nEach premise–hypothesis pair is derived from ab-\nstract quantifier-based patterns, with placeholders\nA, B, and C populated using semantically coherent\nnoun phrases to ensure plausibility. The template-\nbased design affords precise control over compo-\nsitional structure and minimizes linguistic noise,\nthereby isolating reasoning ability from lexical vari-\nation. Figure 2 presents the templates alongside\nexample instances from the dataset.\n3.2.\nMultilingual Translation\nTo assess inference consistency across languages,\nthe English dataset is automatically translated into\na typologically and script-diverse set of target lan-\nguages using high-performance neural machine\ntranslation systems. These translations preserve\nthe original logical relations, enabling cross-lingual\nevaluation under identical task structures. The\nselected languages—Arabic (ar), German (de),\nFrench (fr), Hindi (hi), and Swahili (sw)—cover both\nhigh- and low-resource settings and span multi-\nple language families: Afro-Asiatic, Indo-European\n(Germanic, Romance, Indic branches), and Niger-\nCongo. Their scripts include Latin, Arabic, and\nDevanagari, introducing distinct orthographic and\ntokenization challenges. This selection also varies\nin morphological complexity, syntactic structure,\nand resource availability, providing a comprehen-\nsive basis for evaluating model robustness and\ncross-lingual generalization. The resulting diver-\nsity helps surface weaknesses that might remain\nhidden in homogeneous and high-resource-only\nevaluations.\n3.3.\nCode-Switching Probes\nTo further stress-test semantic alignment, a code-\nswitching condition is introduced in which the\npremise and hypothesis are presented in different\nlanguages. For each ordered pair of languages L1\nand L2, examples are constructed with the premise\nin L1 and the hypothesis in L2, covering all possi-\nble combinations within the selected language set.\nThis setup evaluates whether models can preserve\nsemantic accuracy under mixed-lingual input—a\ncommon phenomenon in multilingual communica-\ntion yet rarely assessed in a controlled, systematic\nmanner.\n\nFigure 1: Pipeline for Multilingual NLI Creation and Evaluation: This process involves (1) generating\nNLI examples using logic-based templates, (2) translating them into multiple languages with high-quality\ntranslation, (3) creating dataset variants in monolingual and code-switched formats, (4) evaluating with\nprompt-based LLM classification, and (5) analyzing multilingual model performance.\nEntailment\n“All A are B” ⇒“Some A are B”\nContradiction\n“All A are B” ⇒“No A are B”\nNeutral\n“Some A are B” ⇒“Some A are\nC”\nEntailment\nLanguage: English\nPremise: All zombies are ani-\nmals.\nHypothesis: Some zombies\nare animals.\nContradiction\nLanguage: English\nPremise: All doctors are ani-\nmals.\nHypothesis: No doctors are\nanimals.\nNeutral\nLanguage: English\nPremise: All monkeys are or-\nganisms.\nHypothesis: Some organisms\nare monkeys.\nFigure 2: Top row: Synthetic NLI templates encoding entailment, contradiction, and neutrality. Placehold-\ners A, B, and C are later instantiated with semantically coherent noun phrases. Bottom row: Samples\nfrom the generated NLI dataset for English (en), each showing one of the three relationships: entailment\n(green), contradiction (red), and neutral (yellow).\n3.4.\nModel Evaluation\nModel behavior is assessed using a prompt-based\nclassification setup. For each example, the LLM\nreceives a structured prompt of the form:\nNLI Prompt Example\nPremise: [premise]\nHypothesis: [hypothesis]\nQuestion: Is the hypothesis entailed by the\npremise, contradicted by it, or unrelated?\nAnswer with one of: Entailment, Contradiction,\nNeutral.\nAnswer:\nThe model outputs one of the three categori-\ncal labels. Low-temperature decoding is applied\nto reduce generation variability. Predictions are\nevaluated against gold-standard labels, and accu-\nracy is computed across all languages and code-\nswitching configurations.\n\nFigure 3: Monolingual NLI accuracy across six languages: English (En), Arabic (Ar), German (De),\nFrench (Fr), Hindi (Hi), and Swahili (Sw); and six LLMs: Fanar-9b, Gemma-7b, Llama-3-8b, Mistral-\n7b-v0.3, Phi-4, and Qwen3-7b. Each bar represents the accuracy of an LLM when performing natural\nlanguage inference on examples where both the premise and hypothesis are in the same language.\n4.\nExperiments\n4.1.\nImplementation Details\nAll experiments are executed using the Hugging\nFace Transformers library with a PyTorch back-\nend.\nInference is performed on A100 GPUs\nwith device_map=\"auto\" enabled for memory-\nefficient model parallelism.\nGeneration uses\ngreedy decoding with a maximum of 10 new to-\nkens per prompt to produce concise outputs while\nlimiting hallucinations, with temperature fixed at\n1.0. All models are evaluated in a zero-shot setting\nwithout task-specific fine-tuning.\n4.2.\nModels Evaluated\nSix multilingual instruction-tuned LLMs are evalu-\nated, selected for diversity in architecture, size,\nand training data.\nThe set includes Fanar-9B\n(Team et al., 2025), a multilingual model opti-\nmized for typologically diverse inputs; Gemma-7B\n(Team et al., 2024), a decoder-only Transformer re-\nleased in an instruction-tuned variant; LLaMA-3-8B\n(Grattafiori et al., 2024), Meta’s third-generation\nopen-weight model pretrained on a multilingual\ncorpus; Mistral-7B-v0.3 (Jiang et al., 2023), a com-\npact model with broad multilingual coverage; Phi-4\n(Abdin et al., 2024), a small but capable instruction-\ntuned model with strong zero-shot reasoning for\nits size; and Qwen3-7B (Yang et al., 2025), a multi-\nlingual model trained with extensive Chinese and\nnon-English content. All models are evaluated us-\ning the same structured prompt format across all\nexamples and languages to ensure comparability.\n4.3.\nEvaluation Scope\nThe evaluation covers 36 language pairings (6×6)\nwith 1,000 examples per pairing, balanced across\nthe three NLI labels: ENTAILMENT, CONTRADIC-\nTION, and NEUTRAL. Both monolingual and code-\nswitched configurations (Section 3) are included.\nPerformance is reported as classification accuracy,\ncomputed by exact string matching between model\npredictions and gold standard labels.\n4.4.\nReproducibility\nAll experiments use publicly available model\nweights and reproducible scripts. The complete\nsetup, including prompt formatting, dataset con-\nstruction, translation, and inference, is imple-\nmented in Python, enabling straightforward repli-\ncation and extension to additional languages and\nmodels.\n\n5.\nResults\n5.1.\nMain Results\nMonolingual inference accuracy is evaluated\nacross six languages: English (en), Arabic (ar),\nGerman (de), French (fr), Hindi (hi), and Swahili\n(sw). In this setting, both the premise and hypothe-\nsis are in the same language, providing a baseline\nmeasure of each model’s semantic reasoning ca-\npacity without cross-lingual interference. Results\nfor the six evaluated LLMs are shown in Figure 3.\nOverall Trends.\nFanar-9B attains the highest ac-\ncuracy across all languages, reaching 65.1% in En-\nglish and sustaining strong performance in lower-\nresource languages such as Swahili and Hindi.\nThese results indicate a well-calibrated multilin-\ngual representation space and effective alignment\nof logical reasoning across typologically diverse\ninputs. In contrast, Gemma-7B records the lowest\naccuracy in nearly all languages, including 17.0%\nin English and 14.3% in German. The performance\ngap between Fanar-9B and Gemma-7B exceeds\n40 percentage points in English, underscoring sub-\nstantial differences in multilingual reasoning quality\nacross model families.\nLanguage-Specific Patterns.\nAcross models,\nEnglish generally achieves the highest monolin-\ngual accuracy, followed by French and German,\nthough the magnitude of differences varies. For\ninstance, Phi-4 performs similarly in English (43%)\nand German (41%), while LLaMA-3-8B shows min-\nimal variance across languages, with scores clus-\ntered near 30%. These patterns indicate that some\nmodels maintain balanced multilingual represen-\ntations, whereas others exhibit pronounced bias\ntoward high-resource and pretraining-dominant\nlanguages.\nNotably, Swahili, despite its lower-\nresource status, does not consistently underper-\nform. In models such as Fanar-9B and Gemma-7B,\nSwahili accuracy is comparable to that of Indo-\nEuropean languages. This outcome may reflect\nexpanded low-resource language coverage in re-\ncent pretraining pipelines and the influence of high-\nquality translation data during instruction tuning.\nImplications.\nThe results reveal substantial\nvariation in monolingual reasoning performance\nacross languages and model architectures. While\nlarger or more extensively instruction-tuned mod-\nels often achieve higher accuracy, model size\nalone is not a reliable predictor; for example,\nLLaMA-3-8B underperforms relative to the smaller\nPhi-4. These patterns underscore the need to ex-\namine how training data composition, multilingual\ncoverage, and architectural biases shape cross-\nlingual logical generalization, particularly for non-\nEnglish and lower-resource languages.\n5.2.\nCode-switching\nThe robustness of six LLMs is evaluated under\ncode-switching conditions, in which the premise\nand hypothesis are presented in different lan-\nguages. Table 1 reports accuracy across all lan-\nguage pairs for each model, with off-diagonal cells\nrepresenting bilingual inference. This configuration\nprobes the ability to maintain logical consistency\nunder mismatched linguistic inputs, a critical as-\npect of multilingual generalization.\nSurprising Gains from Code-Switching.\nSev-\neral models outperform their monolingual base-\nlines in specific code-switched configurations. For\nexample, Gemma-7B achieves markedly higher\naccuracy on many bilingual pairs than on En-\nglish–English (e.g., En–Hi: 32.9% vs.\nEn–En:\n17.0%), and Mistral-7B-v0.3 performs better on\nsome cross-lingual inputs (e.g., Ar–En: 36.4%)\nthan on the corresponding monolingual cases (e.g.,\nAr–Ar: 28.2%).\nThese patterns challenge the\nassumption that semantic alignment necessarily\ndegrades when models reason across linguistic\nboundaries.\nModel-Specific Behaviors.\nFanar-9B achieves\nthe highest accuracy in both monolingual and\ncross-lingual settings, indicating robust multilingual\nalignment. In contrast, models such as Gemma-7B\nand Qwen3-7B display pronounced asymmetries:\ndespite weak English monolingual performance,\naccuracy improves when the hypothesis is ren-\ndered in a non-English language. This pattern\nsuggests a disproportionate reliance on hypothe-\nsis surface forms, with syntactic or lexical ambigu-\nity in English degrading performance more than\nstructured translations.\nLanguage-Dependent\nPatterns.\nAccuracy\ngains from code-switching are unevenly distributed\nacross languages. In several models, using Hindi,\nSwahili, or Arabic as the hypothesis language\nyields higher performance than English, suggest-\ning potential advantages from morphologically\nricher or syntactically simpler constructions in\nthose translations.\nThis pattern is consistent\nwith prior findings that neural models may overfit\nstatistical artifacts in high-resource languages,\nwhile benefiting from more literal or constrained\ntranslations in low-resource settings (Cohen-Inger\net al., 2025).\n\nFanar-9b\nPrem.\nEn\nAr\nDe\nFr\nHi\nSw\nEn\n–\n51.7\n52.4\n59.4\n44.6\n43.2\nAr\n57.8\n–\n61.4\n64.3\n47.2\n50.2\nDe\n53.8\n49.1\n–\n67.0\n47.7\n46.3\nFr\n60.6\n51.7\n63.5\n–\n44.7\n44.5\nHi\n47.4\n48.5\n43.6\n48.3\n–\n44.6\nSw\n48.8\n51.6\n41.3\n54.1\n45.6\n–\nGemma-7b\nPrem.\nEn\nAr\nDe\nFr\nHi\nSw\nEn\n–\n29.4\n26.2\n24.8\n32.9\n26.3\nAr\n40.8\n–\n39.3\n38.8\n37.4\n26.9\nDe\n30.1\n36.6\n–\n29.7\n36.7\n24.9\nFr\n33.5\n40.7\n34.7\n–\n40.1\n25.2\nHi\n42.6\n42.2\n43.2\n39.4\n–\n25.4\nSw\n40.0\n33.6\n35.9\n41.7\n28.4\n–\nLlama-3-8b\nPrem.\nEn\nAr\nDe\nFr\nHi\nSw\nEn\n–\n30.0\n32.0\n31.0\n29.0\n30.0\nAr\n30.8\n–\n31.2\n30.5\n30.6\n29.5\nDe\n31.7\n30.2\n–\n29.4\n30.0\n29.6\nFr\n30.6\n30.9\n30.5\n–\n30.3\n31.1\nHi\n31.1\n31.2\n31.4\n32.7\n–\n30.9\nSw\n32.1\n32.1\n31.0\n31.1\n31.5\n–\nMistral-7b-v0.3\nPrem.\nEn\nAr\nDe\nFr\nHi\nSw\nEn\n–\n37.0\n37.7\n31.0\n32.4\n35.4\nAr\n36.4\n–\n35.7\n34.8\n33.5\n34.5\nDe\n42.2\n37.6\n–\n36.4\n35.1\n36.1\nFr\n34.5\n36.4\n32.9\n–\n34.7\n34.0\nHi\n32.9\n34.5\n34.9\n29.5\n–\n35.0\nSw\n32.5\n32.1\n32.4\n30.3\n31.9\n–\nPhi-4\nPrem.\nEn\nAr\nDe\nFr\nHi\nSw\nEn\n–\n38.5\n38.4\n42.2\n36.6\n32.6\nAr\n37.3\n–\n38.3\n39.4\n38.1\n35.1\nDe\n38.2\n38.3\n–\n40.9\n38.5\n32.2\nFr\n38.0\n38.3\n38.6\n–\n35.9\n32.8\nHi\n33.3\n36.8\n36.7\n37.1\n–\n33.4\nSw\n35.4\n36.4\n34.3\n35.8\n33.4\n–\nQwen3-7b\nPrem.\nEn\nAr\nDe\nFr\nHi\nSw\nEn\n–\n32.5\n31.7\n32.0\n29.3\n30.4\nAr\n32.6\n–\n31.7\n31.4\n30.6\n31.2\nDe\n34.3\n31.8\n–\n30.8\n30.0\n30.1\nFr\n32.8\n31.1\n31.7\n–\n30.1\n31.6\nHi\n30.4\n31.7\n30.2\n32.3\n–\n31.0\nSw\n32.8\n32.5\n32.6\n31.5\n30.9\n–\nLower\nAccuracy (%)\nHigher\n<30\n30–35\n35–40\n40–45\n45–50\n50–55\n55–60\n≥60\nTable 1: Pairwise cross-lingual natural language inference accuracies (%) for six language pairs (En-\nglish—En, Arabic—Ar, German—De, French—Fr, Hindi—Hi, Swahili—Sw) across six language models.\nEach card presents the premise language (rows) versus the hypothesis language (columns). Diagonal\ncells (–) indicate monolingual settings and are shaded grey, while off-diagonal cells show cross-lingual\nperformance. Cell colors range from light yellow (low accuracy) to dark blue (high accuracy), following\nthe ColorBrewer YlGnBu sequential scale (legend above).\nImplications and Hypotheses.\nThe findings\nraise questions about the mechanisms underly-\ning cross-lingual alignment in instruction-tuned\nlanguage models.\nIn multiple cases, accuracy\nis higher under code-switched conditions than in\nmonolingual settings. Possible explanations in-\nclude translation-induced lexical or syntactic vari-\nation acting as a regularization signal, improved\nalignment within the multilingual representation\nspace, or simplification effects from translation.\nThe recurrence of this pattern across diverse ar-\nchitectures indicates that code-switching may offer\nuntapped potential for improving reasoning perfor-\nmance in multilingual applications.\n6.\nCross-Lingual Analysis\nThis section evaluates the semantic consistency of\ntranslated data and examines the representational\nalignment of multilingual sentences. Geometric\nproperties of sentence embeddings are visualized\nacross languages, and translation quality is quan-\ntified via embedding-based similarity. Given that\nthe evaluation relies on translated versions of syn-\nthetic English inputs, verifying the preservation of\nsemantic content across languages is essential.\n6.1.\nEmbedding Similarity Across\nTranslations\nSemantic preservation across translations is ex-\namined by visualizing sentence embeddings for\nfive randomly selected English premise statements\nand their translations into six languages.\nSen-\ntences are encoded with LaBSE (Feng et al., 2022)\ninto high-dimensional vectors, then projected into\nthree dimensions using UMAP for interpretability.\nCross-Lingual Cohesion.\nFig. 4 shows that\ntranslations of the same sentence form tight clus-\nters, even across typologically distant languages.\nThis indicates high semantic consistency and sug-\ngests that the encoder maps them to similar rep-\nresentations despite variation in word order, mor-\nphology, or script. For instance, translations of\nSentence 1 (green) remain closely grouped across\nall languages, supporting the preservation of in-\ntended meaning.\nLanguage Variation.\nAlthough clusters are gen-\nerally compact, certain languages display mild\ndrift from sentence centroids.\nFor instance,\nSwahili (brown in Fig.4) shows positional devia-\ntions, likely arising from structural or morphological\nmismatches introduced during translation. Such\npatterns align with prior observations on typolog-\nical variation in multilingual embedding spaces\n(Chen et al., 2025) and illustrate the challenge of\n\nFigure 4: 3D UMAP projection of sentence em-\nbeddings across six languages. Each point repre-\nsents a translation of one of five randomly selected\nNLI premise statements. Colors denote sentence\nidentity; marker shapes indicate language (EN =\nEnglish, FR = French, DE = German, AR = Arabic,\nHI = Hindi, SW = Swahili).\naligning structurally divergent languages in a uni-\nfied vector space. Given that the evaluation task\nrelies on detecting fine-grained logical relations,\npoor or inconsistent translations could distort re-\nsults. The observed cohesion across translations\nmitigates this concern: if translations of the same\nsentence consistently occupy similar embedding\npositions, cross-lingual performance differences\nare more likely to stem from genuine reasoning\nchallenges rather than input noise.\n6.2.\nTranslation Quality Assessment\nSemantic consistency of translations is assessed\nby computing cosine similarity scores between\neach English sentence and its translated counter-\npart using the LaBSE encoder, providing a direct,\nlanguage-agnostic measure of semantic proximity.\nAs shown in Table 2, similarity scores are consis-\ntently high across all languages, with French and\nGerman exhibiting the strongest alignment. Even\nlower-resource languages such as Swahili main-\ntain average cosine similarities above 0.8, indicat-\ning that semantic properties are largely preserved.\nThese results suggest that differences in inference\naccuracy are more likely to reflect model behavior\nthan translation noise. Overall, the analyses con-\nfirm that the multilingual dataset preserves logical\nstructure and meaning across languages, estab-\nlishing a reliable basis for cross-lingual inference\nevaluation.\nLanguage\nCode\nAvg. Cosine Similarity\nFrench\nfr\n0.912\nGerman\nde\n0.895\nSwahili\nsw\n0.841\nHindi\nhi\n0.828\nArabic\nar\n0.811\nTable 2:\nSemantic similarity between English\npremises and their translations using LaBSE em-\nbeddings (average over 100 pairs). Darker blue\nindicates higher similarity.\n7.\nConclusion\nThis study provides a controlled evaluation of mul-\ntilingual semantic alignment in instruction-tuned\nLLMs through a synthetic, logic-based NLI frame-\nwork incorporating high-quality translation and\ncode-switching. The design isolates reasoning ca-\npabilities across languages and scripts while mini-\nmizing confounding linguistic noise. Results show\nthat, contrary to common assumptions, reasoning\nperformance in code-switched settings can match\nor exceed monolingual performance, suggesting\ngreater robustness in cross-lingual representations\nthan previously recognized. Translation effects\nmay in some cases aid inference, and embedding\nanalyses reveal strong interlingual clustering of se-\nmantically equivalent sentences, supporting the\nfeasibility of multilingual generalization. The frame-\nwork enables fine-grained probing of cross-lingual\nlogic, identification of language-specific artifacts,\nand exploration of code-switching as a deliberate\nstrategy in multilingual NLP\n. These findings high-\nlight both the challenges and the opportunities for\nadvancing reasoning-oriented multilingual evalua-\ntion.\n8.\nLimitations\nSynthetic Nature of the Dataset.\nThe use of\nsynthetic NLI examples enables precise control\nover logical form and compositional structure but\nmay limit ecological validity. The templates, while\nsemantically well-formed, cannot fully capture the\ndiversity and ambiguity of natural multilingual dis-\ncourse.\nConsequently, performance on these\ntasks may not directly translate to real-world rea-\nsoning ability.\nFuture work could mitigate this\nlimitation by supplementing template-based data\nwith linguistically diverse or naturally occurring sen-\ntences, curated and verified across languages to\npreserve logical consistency.\nReliance on Machine Translation.\nThe eval-\nuation of cross-lingual alignment assumes that\n\nmachine translation preserves the intended se-\nmantics of the original English examples. Neural\ntranslation systems—particularly for low-resource\nlanguages—can introduce meaning shifts, simpli-\nfications, or structural divergences that alter the\nlogical relationship between premise and hypoth-\nesis. Although state-of-the-art translation models\nwere used and their quality assessed (Section 6),\nresidual errors may still influence downstream rea-\nsoning. Future extensions could incorporate hu-\nman verification of a subset of translations or em-\nploy multilingual LLMs to produce language-native\nexamples directly, avoiding translation as an inter-\nmediate step.\n9.\nBibliographical References\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien\nBubeck,\nRonen Eldan,\nSuriya Gunasekar,\nMichael Harrison, Russell J. Hewett, Mojan Java-\nheripi, Piero Kauffmann, et al. 2024. Phi-4 tech-\nnical report.\nRie Kubota Ando and Tong Zhang. 2005. A frame-\nwork for learning predictive structures from mul-\ntiple tasks and unlabeled data. Journal of Ma-\nchine Learning Research, 6:1817–1853.\nGalen Andrew and Jianfeng Gao. 2007. Scalable\ntraining of L1-regularized log-linear models. In\nProceedings of the 24th International Confer-\nence on Machine Learning, pages 33–40.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2020. Translation artifacts in cross-lingual trans-\nfer learning. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7674–7684, Online.\nAssociation for Computational Linguistics.\nDibyanayan Bandyopadhyay, Arkadipta De, Ba-\nban Gain, Tanik Saikh, and Asif Ekbal. 2022. A\ndeep transfer learning method for cross-lingual\nnatural language inference. In Proceedings of\nthe Thirteenth Language Resources and Evalu-\nation Conference, pages 3084–3092, Marseille,\nFrance. European Language Resources Associ-\nation.\nSamuel R. Bowman, Gabor Angeli, Christopher\nPotts, and Christopher D. Manning. 2015. A\nlarge annotated corpus for learning natural lan-\nguage inference. In Proceedings of the 2015\nConference on Empirical Methods in Natural\nLanguage Processing, pages 632–642, Lisbon,\nPortugal. Association for Computational Linguis-\ntics.\nYiyi Chen, Qiongxiu Li, Russa Biswas, and Jo-\nhannes Bjerva. 2025. Large language models\nare easily confused: A quantitative metric, se-\ncurity implications and typological analysis. In\nFindings of the Association for Computational\nLinguistics: NAACL 2025, pages 3810–3827,\nAlbuquerque, New Mexico. Association for Com-\nputational Linguistics.\nLiang Cheng, Tianyi Li, Zhaowei Wang, Tianyang\nLiu, and Mark Steedman. 2025. Neutralizing\nbias in LLM reasoning using entailment graphs.\nIn Findings of the Association for Computational\nLinguistics: ACL 2025, pages 13714–13730, Vi-\nenna, Austria. Association for Computational Lin-\nguistics.\nNurit Cohen-Inger, Yehonatan Elisha, Bracha\nShapira, Lior Rokach, and Seffi Cohen. 2025.\nForget what you know about llms evaluations –\nllms are like a chameleon.\nAlexis Conneau, Kartikay Khandelwal, Naman\nGoyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzmán, Edouard Grave, Myle Ott,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nUnsupervised cross-lingual representation learn-\ning at scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 8440–8451, Online. Associa-\ntion for Computational Linguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lam-\nple, Adina Williams, Samuel Bowman, Holger\nSchwenk, and Veselin Stoyanov. 2018. XNLI:\nEvaluating cross-lingual sentence representa-\ntions. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 2475–2485, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAdrian Cosma, Stefan Ruseti, Mihai Dascalu, and\nCornelia Caragea. 2024. How hard is this test\nset? NLI characterization by exploiting training\ndynamics. In Proceedings of the 2024 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 2990–3001, Miami, Florida,\nUSA. Association for Computational Linguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. MLCW’05, page 177–190, Berlin,\nHeidelberg. Springer-Verlag.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2022. Language-\nagnostic BERT sentence embedding. In Pro-\nceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume\n1: Long Papers), pages 878–891, Dublin, Ire-\nland. Association for Computational Linguistics.\n\nAaron Grattafiori, Abhimanyu Dubey, Abhinav\nJauhri, Abhinav Pandey, Abhishek Kadian, Ah-\nmad Al-Dahle, Aiesha Letman, Akhil Mathur,\nAlan Schelten, Alex Vaughan, et al. 2024. The\nllama 3 herd of models.\nDan Gusfield. 1997. Algorithms on Strings, Trees\nand Sequences. Cambridge University Press,\nCambridge, UK.\nShreya Havaldar, Hamidreza Alvari, John Palow-\nitch, Mohammad Javad Hosseini, Senaka Buth-\npitiya, and Alex Fabrikant. 2025. Entailed be-\ntween the lines: Incorporating implication into\nnli.\nMaite Heredia, Julen Etxaniz, Muitze Zulaika, Xa-\nbier Saralegi, Jeremy Barnes, and Aitor Soroa.\n2024. XNLIeu: a dataset for cross-lingual NLI\nin Basque. In Proceedings of the 2024 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies (Volume 1: Long\nPapers), pages 4177–4188, Mexico City, Mexico.\nAssociation for Computational Linguistics.\nHai Hu, Qi Chen, Kyle Richardson, Atreyee\nMukherjee, Lawrence S. Moss, and Sandra Kue-\nbler. 2020. MonaLog: a lightweight system for\nnatural language inference based on monotonic-\nity. In Proceedings of the Society for Computa-\ntion in Linguistics 2020, pages 334–344, New\nYork, New York. Association for Computational\nLinguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur\nMensch, Chris Bamford, Devendra Singh Chap-\nlot, Diego de las Casas, Florian Bressand,\nGianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. 2023. Mistral 7b.\nJyotsana Khatri, Vivek Srivastava, and Lovekesh\nVig. 2023. Can you translate for me? code-\nswitched machine translation with large lan-\nguage models. In Proceedings of the 13th Inter-\nnational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers),\npages 83–92, Nusa Dua, Bali. Association for\nComputational Linguistics.\nGuy Mor-Lan and Effi Levi. 2024. Exploring fac-\ntual entailment with NLI: A news media study.\nIn Proceedings of the 13th Joint Conference on\nLexical and Computational Semantics (*SEM\n2024), pages 190–199, Mexico City, Mexico. As-\nsociation for Computational Linguistics.\nAnimesh Nighojkar, Antonio Laverghetta Jr., and\nJohn Licato. 2023. No strong feelings one way\nor another: Re-operationalizing neutrality in nat-\nural language inference.\nIn Proceedings of\nthe 17th Linguistic Annotation Workshop (LAW-\nXVII), pages 199–210, Toronto, Canada. Associ-\nation for Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette.\n2019. How multilingual is multilingual BERT?\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 4996–5001, Florence, Italy. Association\nfor Computational Linguistics.\nMohammad Sadegh Rasooli and Joel R. Tetreault.\n2015. Yara parser: A fast and accurate depen-\ndency parser. Computing Research Repository,\nSebastian Schuster, Sonal Gupta, Rushin Shah,\nand Mike Lewis. 2019.\nCross-lingual trans-\nfer learning for multilingual task oriented dia-\nlog. In Proceedings of the 2019 Conference\nof the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short\nPapers), pages 3795–3805, Minneapolis, Min-\nnesota. Association for Computational Linguis-\ntics.\nFanar Team, Ummar Abbas, Mohammad Shah-\nmeer\nAhmad,\nFiroj\nAlam,\nEnes\nAltinisik,\nEhsannedin Asgari, Yazan Boshmaf, Sabri\nBoughorbel, Sanjay Chawla, Shammur Chowd-\nhury, et al. 2025. Fanar: An arabic-centric multi-\nmodal generative ai platform.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya\nPathak, Laurent Sifre, Morgane Rivière, Mi-\nhir Sanjay Kale, Juliette Love, et al. 2024.\nGemma: Open models based on gemini re-\nsearch and technology.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis\nplatform for natural language understanding. In\nProceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 353–355, Brussels,\nBelgium. Association for Computational Linguis-\ntics.\nAdina Williams, Nikita Nangia, and Samuel Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages\n\n1112–1122, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nAn Yang, Anfeng Li, Baosong Yang, Beichen\nZhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al.\n2025. Qwen3 technical report.\nFaturahman Yudanto, Yunita Sari, and Maeve Za-\nhwa Adriana Crown Zaki. 2024. Climate-NLI: A\nmodel for natural language inference and zero-\nshot classification on climate-related text. In Pro-\nceedings of the 38th Pacific Asia Conference on\nLanguage, Information and Computation, pages\n600–608, Tokyo, Japan. Tokyo University of For-\neign Studies."}
{"id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "authors": ["Guangzhan Wang", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "source": "http://arxiv.org/abs/2508.14723v1", "page_count": 15, "text": "Transplant Then Regenerate: A New Paradigm for Text Data\nAugmentation\nGuangzhan Wang1, Hongyu Zhang2, Beijun Shen1, Xiaodong Gu1†\n1Shanghai Jiao Tong University\n2Chongqing University\n{wangguangzhan, bjshen, xiaodong.gu}@sjtu.edu.cn\nhyzhang@cqu.edu.cn\nAbstract\nData augmentation is a critical technique in\ndeep learning. Traditional methods like Back-\ntranslation typically focus on lexical-level\nrephrasing, which primarily produces varia-\ntions with the same semantics. While large\nlanguage models (LLMs) have enhanced text\naugmentation by their “knowledge emergence”\ncapability, controlling the style and structure\nof these outputs remains challenging and re-\nquires meticulous prompt engineering. In this\npaper, we propose LMTransplant, a novel text\naugmentation paradigm leveraging LLMs. The\ncore idea of LMTransplant is transplant-then-\nregenerate: incorporating seed text into a con-\ntext expanded by LLM, and asking the LLM to\nregenerate a variant based on the expanded con-\ntext. This strategy allows the model to create\nmore diverse and creative content-level variants\nby fully leveraging the knowledge embedded\nin LLMs, while preserving the core attributes\nof the original text. We evaluate LMTransplant\nacross various text-related tasks, demonstrating\nits superior performance over existing text aug-\nmentation methods. Moreover, LMTransplant\ndemonstrates exceptional scalability as the size\nof augmented data grows.\n1\nIntroduction\nData augmentation is a critical technique in deep\nlearning (Khosla and Saini, 2020; Shorten et al.,\n2021; Ding et al., 2022). Deep learning models are\ndata-hungry and often suffer from limited datasets.\nData augmentation generates additional training\nsamples through transforming or rephrasing the ex-\nisting dataset. This process increases data diversity,\nreduces the risk of overfitting, and enhances the\nmodels’ generalization ability.\nData augmentation has been extensively studied\nin NLP tasks (Feng et al., 2021; Pellicer et al., 2023;\nChen et al., 2023; Bayer et al., 2022). One simple\nprior approach (Wei and Zou, 2019) enhances text\ndata through word-level transformations, such as\nL2R\nCompletion\nBack-\ninfilling\nAfter hearing so much hype\nabout it, I finally sat\ndown to watch the movie.\nThe movie exceeded all my\nexpectations and left me\nspeechless.\nAfter hearing so much hype\nabout it, I finally sat\ndown to watch the movie.\nThe movie exceeded all my\nexpectations and left me\nspeechless.\nRegenerate\nThe film took me on\nan exhilarating\njourney that I never\nwanted to end.\n?\nR2L\nCompletion\nWhat a concept, what an\nidea, what a thrill ride.\nPreceding\ncontext\nSubsequent\ncontext\nPreceding\ncontext\nSubsequent\ncontext\nOriginal Text\nVariant Text\nTransplant\nWhat a concept, what\nan idea, what a\nthrill ride.\nFigure 1: Illustration of LMTransplant.\nrandom insertion and deletion. While easy to im-\nplement, it often generates low-quality samples\nthat severely disrupt the semantic coherence of the\ngenerated text. Later, Back-translation (Sennrich\net al., 2016) has been commonly used for sentence\nlevel rephrasing. Specifically, a translation model\nfirst translates the original text into a different lan-\nguage and then translates it back into the original\nlanguage. Although the augmented text is seman-\ntically coherent, it often exhibits high similarity\nto the original text, leading to poor data diversity\n(Pellicer et al., 2023; Edunov et al., 2018).\nRecent advancements in LLMs have spurred\nsignificant interest in LLM-based data augmenta-\ntion methods (Whitehouse et al., 2023; Ding et al.,\n2024; Zhou et al., 2024; Qiao et al., 2024). A key\ncharacteristic of LLMs is their “knowledge emer-\ngence” capability, which stems from two factors:\n(1) the extensive prior knowledge acquired during\npre-training, and (2) their robust language under-\nstanding and instruction-following abilities (Evuru\net al., 2024; Ghosh et al., 2024). These strengths\nallow LLMs to generate desired outputs directly\nthrough demonstrations or natural language instruc-\ntions without requiring additional training. A no-\n\ntable example is AugGPT (Dai et al., 2025), which\ninstructs ChatGPT to rephrase text for improv-\ning text classification performance. However, this\nrephrasing-based method only generates variants\nwith similar semantics and underutilizes the rich\nknowledge embedded in LLMs, limiting the diver-\nsity and creativity of the generated data. As a result,\nwhen augmented data volume reaches a certain\nthreshold, further increasing may not yield perfor-\nmance improvements (Zhou et al., 2024). Although\ncontextual augmentation methods can enhance con-\ntent diversity, such as GPT3Mix (Yoo et al., 2021),\nwhich leverages the powerful few-shot learning ca-\npabilities of LLMs to generate mixed augmented\ntext, it is highly sensitive to example quality and\ninput order, potentially introducing uncontrolled\nbiases that are not aligned with the original data\ndistribution.\nTo address these limitations and harness the full\npotential of LLMs, we propose LMTransplant, a\nnovel text data augmentation paradigm. The core\nidea of LMTransplant is transplant-then-regenerate\n(TTR), namely, embedding the original text into\nan expanded context generated by LLMs and then\ninstructing LLMs to regenerate a variant based on\nthis enriched context. Specifically, LMTransplant\nemploys bidirectional text continuation—backward\n(right-to-left) and forward (left-to-right)—to create\nthe preceding and subsequent context of the orig-\ninal text. This original text is then masked within\nits expanded context, and LLMs are prompted to\ngenerate a replacement that introduces novel con-\ntent diversity beyond rephrasing, while still pre-\nserving core attributes of the original text. There-\nfore, LMTransplant enhances both the diversity and\ncreativity of the generated text, while maintaining\nalignment with the original data distribution.\nWe apply LMTransplant to various deep learning\ntasks, including text classification, question answer-\ning and named entity recognition (NER), and com-\npare its performance with existing data augmen-\ntation methods. Experimental results demonstrate\nthat LMTransplant can generate higher-quality aug-\nmented data. Training models with data augmented\nby our approach yields significant performance im-\nprovements across all tasks. Compared to non-\naugmentation, LMTransplant achieves accuracy\ngains of 28.16%, 19.96%, 7.68%, 23.66%, and\n10.25% on the SST-2, TREC, SNIPS, MLQA, and\nCoNLL-2003 datasets, respectively.\nIn summary, our contributions are as follows:\n• We propose a novel transplant-based paradigm\nfor text data augmentation. Unlike existing\nmethods which primarily focus on rephras-\ning, LMTransplant crafts content-level text\nvariants, thereby crafting higher-quality aug-\nmented texts.\n• We present a novel transplant and regenera-\ntion algorithm using bidirectional text contin-\nuation and masked text prediction. The al-\ngorithm allows for generating core attributes\nsimilar yet more diverse and creative text by\neffectively utilizing knowledge embedded in\nLLMs. Experiments demonstrate that, LM-\nTransplant achieves significant performance\nimprovements across different tasks.\n2\nApproach\n2.1\nOverview\nWe propose LMTransplant, a novel data augmen-\ntation paradigm. The core idea of LMTransplant\nis transplant-then-regenerate: integrating the orig-\ninal text into contextual scenarios generated by\nLLMs, and then asking LLMs to regenerate new\nvariants given expanded contexts. This strategy\nallows the model to create content-level variants\nwhile preserving the core attributes of the original\ntext. Figure 1 illustrates the entire augmentation\nprocess. For a given text, LMTransplant uses bidi-\nrectional text continuation to generate its preced-\ning and subsequent contexts (Section 2.2). Subse-\nquently, LMTransplant masks the original text in\nthe transplanted text and asks the LLM to regen-\nerate the missing parts given the crafted contexts,\nthereby producing new variants of the original text\n(Section 2.3). Each step of this process will be\nelaborated in the following sections.\n2.2\nTransplant\nGiven a seed text, we incorporate it into a relevant\ncontextual scenario. Specifically, we treat the seed\ntext as a fragment of a broader contextual passage,\nand then use an LLM to generate a semantically\nnatural and logically coherent surrounding context.\nThis process can be conceptualized as bidirectional\ntext continuation, which involves two steps: (1)\na forward (left-to-right) continuation process that\ncontinues writing the subsequent context of the\nseed text, and (2) a backward (right-to-left) con-\ntinuation process that reconstructs the preceding\ncontext of the seed text.\n\nWhat a concept, what an idea,\nwhat a thrill ride.\nSeed text\nTransplanted Text\nSubsequent context\nPreceding context\nAfter hearing so much hype\nabout it, I finally sat\ndown to watch the movie.\nWhat a concept,\nwhat an idea, what\na thrill ride.\nThe movie exceeded all\nmy expectations and\nleft me speechless.\nWhat a concept, what an idea,\nwhat a thrill ride.\nThe movie exceeded all my expectations\nand left me speechless.\nLLM\nAfter hearing so much hype\nabout it, I finally sat\ndown to watch the movie.\nLLM\nFigure 2: Illustration of text transplant.\nFigure 2 illustrates the bidirectional text contin-\nuation process. First, the LLM generates a subse-\nquent text that naturally extends the seed text, com-\nbining both to form an expanded passage. Next,\nthe LLM generates a preceding text based on this\nexpanded input. This process ensures that the gen-\nerated context remains thematically aligned with\nthe original text, while introducing novel informa-\ntion other than rephrasing through the knowledge\nembedded in LLMs, such as new words and ex-\npressions. Leveraging the language understanding\nand instruction-following capabilities of LLMs, we\nemploy prompt engineering to guide content gener-\nation. The prompt template used for transplanting\nis provided in Appendix A.\n2.3\nRegeneration\nThe transplant phase generates multiple contextual\nscenes bearing the original text. We then introduce\na regeneration process, where we prompt the LLM\nto regenerate new text variants that seamlessly in-\ntegrate into the expanded context. Specifically, we\nprovide the LLM with the crafted preceding and\nsubsequent contexts, along with the original text,\nand ask it to generate a text that introduces content\nvariation while preserving essential attributes of\nthe original. Therefore, the regenerated text must\nsatisfy the following criteria: (1) Fitting naturally\nwithin the surrounding context; (2) Aligning with\nthe original text in terms of theme, length, format,\nand linguistic style, as mismatches in these aspects\nbetween training and testing instances are known\nto degrade downstream performance (Rogers et al.,\n2021); (3) Introducing novel elements to enrich\ncontent variation, avoiding simple rewording or\ndirect replication.\nThe film took me on an exhilarating\njourney that I never wanted to end.\nRegenerated Text\nInput\nMask\nAfter hearing so much hype\nabout it, I finally sat\ndown to watch the movie.\nWhat a concept,\nwhat an idea, what\na thrill ride.\nThe movie exceeded all\nmy expectations and\nleft me speechless.\n<Masked Text>\nThe movie exceeded all\nmy expectations and\nleft me speechless.\nTransplanted Text\nLLM\nAfter hearing so much hype\nabout it, I finally sat\ndown to watch the movie.\nFigure 3: Illustration of regeneration.\nIn this step, the preceding and subsequent texts\ngenerated in Section 2.2 serve as a bridge, ensuring\nthe newly generated text aligns with the original,\nsuch as theme and label-related information. Simul-\ntaneously, this process enriches and diversifies the\ncontent, making the regenerated text a high-quality\naugmentation of the original. The prompt design\nfor this step is detailed in Appendix A.\n3\nExperiments\nWe conduct extensive experiments to evaluate\nthe effectiveness of LMTransplant across multiple\ndeep learning tasks by applying data augmentation\nto various datasets. Following established practices\n(Yoo et al., 2021; Dai et al., 2025; Ubani et al.,\n2023; Lee et al., 2024), we simulate low-resource\nscenarios by subsampling the training set of each\ndataset. Specifically, we select a subset of samples\nas seed data for augmentation, and then generate\nthree augmented samples for each seed. This en-\nables us to rigorously assess the performance of\nour method in data-scarce scenarios.\n3.1\nImplementation Details\nTo evaluate LMTransplant’s robustness and effec-\ntiveness across a wider range of LLMs, we employ\ndifferent LLMs as the base LLM for both transplant\nand regeneration phases, including DeepSeek-V3,\nGPT-3.5-Turbo and GPT-4o.\nFor text classification tasks, we use Modern-\nBERT (Warner et al., 2024), a modernized bidirec-\ntional encoder-only BERT-style model available in\ntwo sizes: ModernBERT-base and ModernBERT-\nlarge. The classifier is initialized using pre-trained\nmodels from the Huggingface Transformers library\n(Wolf et al., 2019) and optimized with the AdamW\noptimizer (Kingma and Ba, 2014; Loshchilov and\nHutter, 2019). We set the learning rate to 4e-5\nand maintain other hyperparameters consistent with\n\nTask Type\nDataset\nClasses Train Dev Test\nClassification\nSST-2\n2\n6228\n692 1821\nTREC\n6\n5406\n546\n500\nSNIPS\n7\n13084 700\n700\nQA\nMLQA\n-\n1314\n437\n437\nNER\nCoNLL-2003\n9\n3234\n748\n679\nTable 1: Statistics of datasets.\nFadaee et al. (2017), including 8 training epochs\nand a batch size of 8. During training, models\nare saved based on their performance on the devel-\nopment set, with the best-performing parameters\nretained for final evaluation on the test set.\nFor the question-answering task, we fine-tune\nQwen2.5-1.5B (Team, 2024; Yang et al., 2024) us-\ning the AdamW optimizer with a learning rate of\n1e-5, a batch size of 8, and 8 training epochs. And\nfor NER task, we fine-tune ModernBERT using the\nsame hyperparameters as in the classification task.\nWe repeat all experiments 10 times to mitigate\nthe influence of randomness. Additionally, we con-\nduct pairwise two-sample Wilcoxon Signed-Rank\ntests (Wilcoxon, 1992) to compare group medians\nand assess statistical significant differences. When\np < 0.01, it indicates that there is a statistical sig-\nnificant difference between the two groups.\n3.2\nDatasets\nWe evaluate LMTransplant on five established\nbenchmarks:\n• SST-2 (Socher et al., 2013): A widely used\nsentiment classification dataset of movie re-\nviews, labeled as “positive” or “negative”. We\nuse the version provided by Wu et al. (2019)1,\nwhich contains 6,228 training samples, 692\ndevelopment samples, and 1,821 test samples.\n• TREC (Li and Roth, 2002): A question clas-\nsification dataset annotated with six types:\n“Abbreviation”, “Entity”, “Description”, “Hu-\nman”, “Location”, and “Numeric”. Similar\nto SST-2, we use the version from Wu et al.\n(2019), with 5,406 training samples, 546 de-\nvelopment samples, and 500 test samples.\n• SNIPS (Coucke et al., 2018): A text clas-\nsification dataset annotated with seven hu-\nman intents: “AddToPlaylist”, “BookRestau-\nrant”, “GetWeather”, “PlayMusic”, “Rate-\nBook”, “SearchCreativeWork”, and “Search-\nScreeningEvent”. We use the version from\n1https://github.com/1024er/cbert_aug\nSlotGated-SLU (Goo et al., 2018)2, compris-\ning 13,084 training samples, 700 development\nsamples, and 700 test samples.\n• MLQA (Lewis et al., 2019): A question-\nanswering benchmark with context passages,\nquestions, and answers. We use English sam-\nples form Huggingface3 and filter out those\nexceeding 80 tokens to keep a modest length.\n• CoNLL-2003 (Tjong Kim Sang and De Meul-\nder, 2003): A NER dataset contains four entity\ntypes: persons, organizations, locations, and\nmiscellaneous names, tagged using the IOB\nscheme, resulting in nine distinct IOB labels.\nStatistics of datasets are summarized in Table 1.\n3.3\nMetrics\nWe evaluate LMTransplant along two dimensions:\nthe quality of the augmented texts (intrinsic eval-\nuation) and their impact on deep learning tasks\n(extrinsic evaluation). For intrinsic evaluation, we\nadopt two widely employed metrics to assess the\nquality of the augmented samples:\nDistinct-N measures the lexical diversity (Li\net al., 2016). It is defined as the ratio of unique\nn-grams across all generated texts and their corre-\nsponding seeds.\nDistinct-N = # unique n-grams\n# all n-grams\n(1)\nA higher value indicates greater diversity. We cal-\nculate the average score across 10 experiments as\nthe final result.\nSemantic Variability measures how well the\ngenerated text extends the semantics of the seed\ntexts. To assess this, we first adopt BERTScore\n(Zhang* et al., 2020) to calculate the sentence-level\nsimilarity between the generated text and its orig-\ninal, leveraging BERT’s contextual embeddings.\nWe then define semantic variability as:\nSemantic Variability = 1 −BERTScore\n(2)\nThus, the higher the semantic variability, the better\nthe variability that the new text variants are.\nFor extrinsic evaluation, we adopt task-related\nmetrics. Specifically, we use accuracy (Acc) and\nmacro F1-score (Macro-F1) for classification and\n2https://github.com/MiuLab/SlotGated-\nSLU/tree/master/data/snips\n3https://huggingface.co/datasets/dkoterwa/mlqa_filtered\n\nMethod\nSST-2\nTREC\nSNIPS\nMLQA\nCoNLL-2003\nDist-3↑\nSV∗↑\nDist-3↑\nSV∗↑\nDist-3↑\nSV∗↑\nDist-3↑\nSV∗↑\nDist-3↑\nSV∗↑\nOriginal\n0.99±0.01\n-\n0.94±0.02\n-\n0.89±0.03\n-\n0.97±0.01\n-\n0.97±0.01\n-\nMoreData\n0.99±0.01\n-\n0.89±0.01\n-\n0.80±0.02\n-\n0.93±0.02\n-\n0.94±0.01\n-\nEDA\n0.31±0.02 0.19±0.02 0.46±0.02 0.23±0.01 0.30±0.01 0.16±0.01 0.40±0.01 0.21±0.01 0.43±0.01 0.23±0.01\nBackTrans.\n0.69±0.03 0.20±0.01 0.55±0.03 0.13±0.01 0.60±0.02 0.19±0.01 0.53±0.01 0.16±0.06 0.62±0.03 0.12±0.01\nGPT3Mix\n0.67±0.03\n-\n0.54±0.02\n-\n0.50±0.04\n-\n0.65±0.04\n-\n0.18±0.02\n-\nAugGPT\n0.54±0.03 0.24±0.01 0.37±0.02 0.19±0.01 0.41±0.02 0.27±0.01 0.38±0.02 0.19±0.01 0.33±0.02 0.22±0.01\nLLM2LLM\n0.71±0.02 0.23±0.01 0.52±0.03 0.17±0.01 0.54±0.01 0.22±0.01 0.61±0.01 0.17±0.01 0.71±0.02 0.20±0.02\nLMTransplant (ours)\n(left, right)\n0.88±0.03 0.39±0.01 0.66±0.03 0.30±0.01 0.63±0.02 0.36±0.01 0.72±0.03 0.29±0.01 0.78±0.03 0.27±0.01\n(right, left)\n0.88±0.03 0.39±0.01 0.64±0.03 0.29±0.01 0.63±0.02 0.36±0.01 0.71±0.02 0.28±0.01 0.75±0.03 0.27±0.02\nUnidirectional 0.82±0.03 0.37±0.01 0.54±0.02 0.25±0.01 0.50±0.02 0.31±0.01 0.63±0.02 0.25±0.01 0.66±0.03 0.22±0.01\nTable 2: Quality of generated samples by various methods (p < 0.01). Subscript numbers denote standard deviations.\nSV = Semantic Variability; MoreData: randomly samples additional data from the original training set as augmented\ndata. We choose DeepSeek-V3 as the base LLM for data augmentation. Results based on GPT-3.5-Turbo and\nGPT-4o are available in Appendix B.\nNER tasks, and the accuracy of answers (Acc) for\nQA tasks:\nAcc = #samples answered correctly\n#all test samples\n(3)\nMacro-F1 = 1\nN\nN\nX\ni=1\nF1i\n(4)\nwhere N is the total number of classes, and F1i\nrepresents the F1-score for the i-th class.\n3.4\nBaselines\nWe compare our method with both traditional and\nLLM-based methods:\nEasy Data Augmentation (EDA) (Wei and Zou,\n2019): A rule-based data augmentation method that\napplies lexical transformations, including synonym\nreplacement, random insertion, random swap, and\nrandom deletion, to the original text.\nBack Translation (BackTrans.) (Sennrich et al.,\n2016): A widely used date augmentation method\nthat translates the original text into another lan-\nguage and then back-translates it into the original\nlanguage to generate variants. Following ZeroShot-\nDataAug (Ubani et al., 2023), we use googletrans4\nas the machine translation model, selecting differ-\nent intermediate languages for multiple augmenta-\ntions of the same text.\nGPT3Mix (Yoo et al., 2021): This method ran-\ndomly selects several examples from the seed sam-\nples, embeds them into a prompt template, and then\nleverages the powerful few-shot learning capabil-\nities of LLMs to generate mixed augmented text\ninfluenced by the provided examples.\n4Google Translate (https://pypi.org/project/googletrans/)\nAugGPT (Dai et al., 2025): This method utilizes\nprompts to guide LLMs in rephrasing each sentence\nfrom the training samples into multiple semanti-\ncally similar but linguistically different variants,\nthereby enhancing text classification performance.\nLLM2LLM (Lee et al., 2024): An iterative data\naugmentation strategy that continuously employs\nLLMs to generate new samples from instances mis-\npredicted by the downstream task model.\n3.5\nResults\n3.5.1\nIntrinsic Evaluation\nThe intrinsic evaluation results in Table 2 demon-\nstrate that, the quality of augmented samples gen-\nerated by LMTransplant significantly outperform\nother baselines across all benchmarks. In particular,\nLMTransplant achieves lexical diversity (Distinct-\n3) closer to original texts without augmentation\n(Original) and sampling additional data from orig-\ninal training set as augmented data (MoreData),\nhighlighting its effectiveness in improving lexi-\ncal diversity. Meanwhile, LMTransplant also ex-\nhibits superior semantic variability compared to\nother baselines, indicating that it meaningfully ex-\npands the semantics of the original text rather than\nmerely relying on simple rephrasing. Additionally,\nLMTransplant with bidirectional text continuation\noutperforms its unidirectional counterpart, which\nserves as an ablation model for the bidirectional\ncontinuation strategy. Further ablation study details\nare discussed in Section 3.5.3.\n3.5.2\nExtrinsic Evaluation\nWe evaluate the effectiveness of LMTransplant in\nempowering deep learning tasks by training mod-\nels on augmented data. To simulate low-data sce-\n\nMethod\nSST-2\nTREC\nSNIPS\nMLQA\nCoNLL-2003\nAcc↑\nMacro-F1↑\nAcc↑\nMacro-F1↑\nAcc↑\nMacro-F1↑\nAcc↑\nAcc↑\nMacro-F1↑\nOriginal\n52.34±3.19 48.88±6.59 50.80±10.60 47.66±9.06 78.10±2.77 78.30±2.68 32.08±1.02 82.41±1.31 82.44±1.14\nMoreData\n65.41±5.48 65.41±5.48 74.30±6.55 70.75±6.47 88.23±3.14 88.30±3.24 43.30±1.13 91.56±0.65 91.50±0.89\nEDA\n56.78±5.04 55.79±5.86 53.50±10.59 50.58±8.76 81.86±4.03 81.80±4.31 36.92±1.67 85.08±0.91 83.89±0.80\nBackTrans.\n60.09±6.25 58.80±8.16 56.52±6.57 52.49±6.37 81.20±3.86 81.40±3.49 35.32±1.24 85.88±1.01 84.99±1.07\nGPT3Mix\n63.75±6.76 63.36±7.05 57.68±7.12 53.39±7.61 81.80±5.63 82.15±5.24 33.18±1.80 87.92±0.86 86.75±0.79\nAugGPT\n61.11±6.50 60.55±6.94 58.94±9.43 55.89±9.95 82.73±3.53 82.98±3.35 36.04±1.45 87.27±0.82 86.68±0.90\nLLM2LLM\n63.90±5.34 63.28±5.44 56.58±7.17 52.70±6.76 83.04±4.17 83.33±4.14 37.51±1.67 88.04±1.08 87.47±1.16\nLMTransplant (ours)\n(left, right)\n67.08±6.92 66.77±6.92 60.88±7.06 56.38±6.73 84.06±2.97 84.24±2.82 39.44±1.54 90.28±0.95 88.82±1.00\n(right, left)\n66.21±6.19 65.34±6.43 60.94±8.59 57.32±7.60 84.10±2.82 84.30±2.65 39.67±1.78 90.86±0.90 89.07±1.09\nUnidirectional 64.34±5.38 63.52±5.45 58.42±7.84 55.06±7.05 82.53±4.58 82.78±4.65 35.69±1.67 87.04±0.92 85.96±1.06\nTable 3: Effectiveness of LMTransplant in empowering deep learning tasks (p <0.01). Subscript numbers denote\nstandard deviations. We choose ModernBERT-base for classification and NER tasks and Qwen2.5-1.5B for QA task.\nDetailed results based on ModernBERT-large are available in Appendix C, Table 10.\nnarios, we adopt a subsampling strategy aligned\nwith prior studies (Kumar et al., 2020; Ubani et al.,\n2023). Specifically, we randomly sample subsets\n(10 samples per class for classification tasks and\n50 samples for QA and NER tasks) from the orig-\ninal training and development sets for each task.\nThese training subsets are expanded using various\naugmentation methods, including LMTransplant.\nThe augmented data, combined with the original\nsub-training set, are used to train deep learning\nmodels, and models’ performance is evaluated on\nthe original test set.\nResults in Table 3 demonstrate that the aug-\nmented samples generated by LMTransplant signif-\nicantly improve task performance. For instance, on\nthe SST-2 dataset, LMTransplant increases classifi-\ncation accuracy from 52.34% to 67.08% and Macro\nF1-score from 48.88% to 66.77%, outperforming\neven MoreData. In the QA task on MLQA, LM-\nTransplant achieves a 23.66% improvement in ac-\ncuracy, significantly surpassing other LLM-based\nbaselines. These findings indicate that, unlike meth-\nods that merely prompt LLMs, the transplanting\nmechanism in LMTransplant harnesses knowledge\nembedded in LLMs more effectively, thereby gen-\nerating higher-quality augmented samples.\nTime Efficiency: We also compare the time effi-\nciency of different methods, which is defined as the\naverage time required to generate a new sample (Ta-\nble 4). To ensure fairness, all comparisons are con-\nducted on the same hardware. EDA significantly\noutperforms other methods, due to its extremely\nsimple rule-based operations. Among LLM-based\nmethods, AugGPT is the fastest, attributed to its\nshortest prompts. LMTransplant is slower but still\nsurpasses GPT3Mix and LLM2LLM. LLM2LLM\nMethod\nTime\nEDA\n0.02±0.84\nBackTrans.\n3.44±5.53\nGPT3Mix\n22.78±12.73\nAugGPT\n4.93±8.27\nLLM2LLM\n30.55±11.19\nLMTransplant (ours)\n15.09±13.87\nTable 4: Time efficiency for various methods. We report\nthe average processing time (in seconds) required to\ngenerate a new sample.\nexhibits the lowest efficiency, as its iterative gener-\nation and retraining process significantly increases\ntime consumption.\n3.5.3\nAblation Study\nWe hereby conduct ablation experiments to explore\nthe key step in our approach—bidirectional text\ncontinuation. First, we alter the bidirectional text\ncontinuation strategy in the transplant step. Specifi-\ncally, we remove backward continuation and retain\nonly forward continuation (Unidirectional). We\nalso experiment with altering the order of backward\nand forward continuations, testing both backward-\nfirst (left, right) and forward-first (right, left) ap-\nproaches.\nAs shown in Table 3, unidirectional continua-\ntion significantly reduces effectiveness compared to\nbidirectional continuation, likely due to the limited\ncontext information it provides, resulting in less\nvariation and diversity in the generated text. This is\nfurther supported by the obviously lower Distinct-\n3 score for unidirectional continuation in Table 2.\nAdditionally, the order of bidirectional continuation\nalso affects performance: forward-first continua-\ntion yields better results on most datasets. This is\nlikely because, compared to backward continua-\n\ntion, LLMs are more proficient in forward continu-\nation. Specifically, generating subsequent context\nfirst provides more information for LLMs to gener-\nate a higher-quality preceding context and thereby\nimprove the quality of the augmented data.\n3.5.4\nScaling Analysis\nWe further investigate the scaling effect of various\nsample sizes, including the number of seed sam-\nples and augmented samples. Experiments are con-\nducted on the SST-2 dataset. First, with the number\nof augmented samples fixed at 3 per seed, we grad-\nually vary the number of seed samples from 20 to\n100 (10 to 50 per category). Next, with the total\nnumber of seed samples fixed at 20 (10 per cate-\ngory), we adjust the number of augmented samples\nper seed from 0 to 10. After each adjustment, mod-\nels are retrained and evaluated. Results illustrated\nin Figures 4 demonstrate that, LMTransplant con-\nsistently achieves the highest accuracy across vary-\ning seed sample sizes (Figure 4a), showcasing its\nrobust generalization under data scarcity. Notably,\nwhile other methods plateau after a few rounds of\naugmentation, LMTransplant continues to improve\nwith additional augmented samples (Figure 4b), at-\ntributed to its effective utilization of knowledge\nembedded in LLMs. This underscores the excep-\ntional scalability of LMTransplant and its efficacy\nin tackling data scarcity challenges.\n3.5.5\nCase Study\nWe further conduct a qualitative analysis of the\naugmented samples generated by LMTransplant.\nWe highlight two cases in Appendix D, Table 11,\nwhere LMTransplant extends the diversity and cre-\nativity of the original samples while preserving the\nusability of the augmented samples.\nIn the first case, the original text is a question\nasking about the size of our galaxy. We can ob-\nserve that baseline methods simply rephrase the\noriginal text with the same semantics. For example,\nEDA swaps the words “in” and “diameter”, which\neven disrupts the linguistic integrity of the text.\nBack-translation and AugGPT rephrase the orig-\ninal text by employing machine translation mod-\nels and LLMs, respectively. While they introduce\nchanges in sentence structure, the new text remains\nsemantically similar to the original. In contrast,\nLMTransplant generates more diverse and creative\ntext centered on the topic of “our galaxy”, by lever-\naging the knowledge embedded in LLMs. It ex-\npands the phrase “our galaxy in diameter” to “the\n20\n30\n40\n50\n60\n70\n80\n90\n100\nNumber of Seed Samples\n55\n60\n65\n70\n75\n80\n85\nAccuracy(%)\nOriginal\nEDA\nBackTrans.\nGPT3Mix\nAugGPT\nLLM2LLM\nLMTransplant\n(a) Performance under different numbers of seed samples.\n0\n2\n4\n6\n8\n10\nNumber of Augmented Samples Per Seed Sample\n55\n60\n65\n70\n75\n80\nAccuracy(%)\nOriginal\nEDA\nBackTrans.\nGPT3Mix\nAugGPT\nLLM2LLM\nLMTransplant\n(b) Performance under different numbers of augmented sam-\nples per seed sample.\nFigure 4: Results of scaling analysis under various sam-\nple sizes.\nsize of our galaxy” in the preceding context. More\nsurprisingly, it introduces a new term “Milky Way”\nto represent “our galaxy” within the same context.\nThis example demonstrates the extraordinary cre-\nativity of LMTransplant in text augmentation.\nIn the second case, the original text is a movie re-\nview expressing the audience’s appreciation on spe-\ncific aspects of a movie. Despite the monotonous\nsemantics, we note that the texts generated by base-\nlines exhibit sentiment divergence from the original.\nFor example, EDA replaces “idea” with “thought”,\nslightly weakening the emotional intensity of the\noriginal text. Back-translation introduces the ex-\npression “alarming ride” during the translation pro-\ncess, shifting the sentiment from positive to nega-\ntive or neutral. And AugGPT uses a more abstract\nexpression, “exhilarating experience”. In contrast,\nLMTransplant takes the gist of the original text and\nregenerates a novel review, expressing the same\nfeelings on the theme of “movie”. The subsequent\ntext retains the positive sentiment with the phrase\n“exceeded all my expectations and left me speech-\nless”. The regenerated movie review aligns with the\noriginal text both thematically and sentimentally\nwhile bringing new knowledge about sentiment\nclassification.\nOverall, LMTransplant effectively harnesses the\nknowledge embedded in LLMs through its TTR\n\nstrategy, generating text with enhanced diversity\nand exceptional creativity that surpasses other aug-\nmentation methods. Simultaneously, by utilizing\nbidirectional context as a bridge, LMTransplant\nensures the new text accurately retains the core\ncharacteristics of the original, guaranteeing its us-\nability for downstream tasks.\n4\nDiscussion\nOne concern is that LMTransplant might generate\nsamples with semantic variations that differ from\nthe original data (as illustrated in Table 11). This\ncould potentially disrupt the performance of deep\nlearning models, raising questions about the practi-\ncal utility of the augmented samples.\nHowever, our approach adheres to the funda-\nmental principles of data augmentation: producing\ndiverse, high-quality samples while preserving the\noriginal data distribution. By leveraging the bidi-\nrectional text continuation process, our method har-\nnesses the knowledge embedded in LLMs to gener-\nate more creative texts. This not only enhances the\ndiversity of the data but also improves the general-\nization capabilities of deep learning models. This\nhypothesis is supported by the results presented\nin Section 3.5.2, which demonstrate that the texts\ngenerated by LMTransplant significantly boost the\nperformance of deep learning tasks.\nAdditionally, while the generated texts may ex-\nhibit semantic differences from the original, the\ntransplanting mechanism in our approach ensures\nthat the generated samples preserve the core at-\ntributes of the seed text—such as theme, linguistic\nstyle, and sentiment polarity. This mechanism ef-\nfectively mitigates the risk of producing nonsensi-\ncal or irrelevant text samples, thereby maintaining\nthe integrity and usefulness of the augmented data.\n5\nRelated Work\nData augmentation has been extensively explored\nin NLP tasks (Feng et al., 2021; Pellicer et al.,\n2023; Chen et al., 2023).\nPopular approaches\nsuch as word-level transformations (Wei and Zou,\n2019) often disrupt semantic coherence, while\nBack-translation (Sennrich et al., 2016) offers lim-\nited diversity due to high similarity with the source\ntext. In contrast, LMTransplant introduces a novel\nparadigm for text data augmentation, generating\ncontent-level variants that are more diverse and\ncreative in content.\nRecently, LLM-based data augmentation has\ngained wider attention (Whitehouse et al., 2023;\nDing et al., 2024; Zhou et al., 2024; Qiao et al.,\n2024). Leveraging their “knowledge emergence”\ncapability, LLMs can generate desired content di-\nrectly through demonstrations or natural language\ninstructions. For instance, AugGPT (Dai et al.,\n2025) uses natural language instructions to guide\nChatGPT in rephrasing the original text. However,\nthis approach applies relatively simple operations\nand underutilizes the extensive knowledge embed-\nded in LLMs, resulting in limited content creativity\nand diversity. In comparison, LMTransplant simu-\nlates realistic contextual scenarios, enabling LLMs\nto better leverage their knowledge and generate\nmore diverse and creative augmented samples.\nSimilarly, GPT3Mix (Yoo et al., 2021) leverages\nLLMs’ few-shot learning capabilities by provid-\ning a set of examples to generate new samples.\nHowever, it is highly sensitive to example quality,\nselection strategy, and example order, potentially\nintroducing uncontrolled biases that compromise\naugmentation stability. More recently, LLM2LLM\n(Lee et al., 2024) iteratively augments instances\nmisclassified by downstream-task model. While\nthis approach produces more targeted samples, its\niterative generation and retraining process incurs\nhigh computational costs and lacks adaptability\nto new datasets. In contrast, LMTransplant uti-\nlizes LLMs’ powerful language understanding and\ninstruction-following abilities through carefully de-\nsigned prompts, eliminating the need for examples.\nThis approach offers greater flexibility for adapt-\ning to different downstream tasks while mitigating\nissues related to data selection sensitivity and ex-\ncessive computational resource consumption.\n6\nConclusion\nIn this paper, we propose LMTransplant, a novel\ntext data augmentation paradigm based on trans-\nplanting strategy. By leveraging bidirectional text\ncontinuation and masked text prediction, LMTrans-\nplant generates high-quality and diverse augmented\ntext. It constructs contextually coherent scenarios\naligned with the original text, fully utilizing the\nknowledge embedded in LLMs. Experimental re-\nsults demonstrate that the augmented text generated\nby LMTransplant excels in diversity and creativity\nwhile significantly improving the performance of\ndownstream tasks.\nOur\nreplication\npackage\nis\navailable\nat:\nhttps://github.com/W-GZ/LMTransplant.\n\nLimitations\nAlthough LMTransplant demonstrates strong ex-\nperimental results, we acknowledge the following\nlimitations and challenges that warrant further in-\nvestigation: First, our experiments are conducted\nbased on DeepSeek-V3, GPT-3.5-Turbo, and GPT-\n4o. We plan to extend our evaluation across a wider\nrange of LLMs to further assess the robustness\nand effectiveness of LMTransplant. Second, in\nline with prior LLM-based augmentation methods\n(e.g., GPT3Mix, LLM2LLM), we evaluate LM-\nTransplant on classification, question-answering\nand named entity recognition tasks, which we be-\nlieve are representative tasks to demonstrate the\ngeneralization capability of LMTransplant. How-\never, when applying LMTransplant to other task\ntypes, appropriate adjustments to its prompts may\nbe necessary to ensure adaptability and effective-\nness. As such, we plan to investigate its broader\napplicability in future work."}
{"id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "authors": ["Shubham Pundhir", "Ganesh Bagler"], "source": "http://arxiv.org/abs/2508.14718v1", "page_count": 8, "text": "The Digital Sous Chef - A Comparative Study on Fine-Tuning\nLanguage Models for Recipe Generation\nShubham Pundhir1 and Ganesh Bagler1\n1Indraprastha Institute of Information Technology, Delhi, India\nshubham24165@iiitd.ac.in, bagler@iiitd.ac.in\nAbstract\nWe established a rigorous benchmark for text-based\nrecipe generation, a fundamental task in natural lan-\nguage generation. We present a comprehensive compar-\native study contrasting a fine-tuned GPT-2 large (774M)\nmodel against the GPT-2 small (124M) model and tra-\nditional LSTM/RNN baselines on the 5-cuisine cor-\npus from RecipeDB. Our key contribution is a tar-\ngeted tokenization strategy that augments the vocabu-\nlary with 23 common fraction tokens and custom struc-\ntural markers. This approach addresses a critical limita-\ntion of generic tokenizers by preserving essential recipe\nstructures and precise numerical quantities, thereby en-\nhancing domain specificity.\nPerformance is evalu-\nated using a comprehensive suite of seven automatic\nmetrics spanning fluency (BLEU-4, METEOR), coher-\nence (ROUGE-L), semantic relevance (BERTScore),\nand diversity.\nOur experiments show that the large\ntransformer-based approach yields a > 20% relative im-\nprovement in BERTScore (F1) (0.92 vs 0.72) over the\nbest recurrent baseline, while reducing perplexity by\n69.8%. We conclude with a discussion of remaining\nchallenges, particularly regarding factual accuracy, and\noutline how this foundational study paves the way for in-\ntegrating real-world constraints and multi-modal inputs\nin advanced recipe generation research.\nKeywords: Recipe Generation, Large Language Models,\nGPT-2, Fine-Tuning, Natural Language Generation, Com-\nputational Creativity\n1\nIntroduction\nThe generation of coherent and contextually relevant recipe\ntext is a critical challenge in culinary Natural Language\nGeneration (NLG). Although this work focuses on the text\nmodality, establishing a robust text generation engine is an\nessential first step for future multi-modal food computing\nsystems, ensuring the instructional clarity and consistency\nrequired to support tasks like illustrating cooking steps with\nimages or generating recipes from a photo of ingredients.\nHistorically, computational approaches using rule-based\nsystems or Recurrent Neural Networks (RNNs) struggled to\nmaintain long-range coherence and creativity (Kiddon et al.,\n2016). Moreover, existing recipe systems often lack rigorous\nbaselines and domain-aware tokenization, leading to incon-\nsistent structure and quantity handling. This paper tackles\nthese gaps with three primary contributions:\n1. A rigorous, reproducible benchmark comparing fine-\ntuned Transformers against strong LSTM/RNN base-\nlines for structured recipe generation.\n2. A targeted tokenization strategy to handle domain-\nspecific markers and preserve numerical fractions, a\nnovel approach to improving generation quality.\n3. A comprehensive analysis using a suite of seven auto-\nmatic metrics, offering a holistic view of model perfor-\nmance from fluency to semantic coherence.\nOur results confirm the significant advantages of the trans-\nformer architecture and lay the groundwork for future re-\nsearch into constrained decoding and multi-modal applica-\ntions.1\n2\nRelated Work\nResearch in automatic recipe generation has evolved sig-\nnificantly, mirroring broader trends in NLG. Early attempts\noften involved template-based or rule-based systems (Batra\net al., 2020), which, while capable of producing structured\noutput, were inherently brittle and lacked the flexibility to\ngenerate creative or novel recipes beyond their predefined\nrules.\nThe application of neural networks brought more sophisti-\ncated approaches, with Recurrent Neural Networks (RNNs)\nand their more advanced variant, Long Short-Term Memory\n(LSTM) networks, learning sequential patterns directly from\nrecipe data (Agarwal et al., 2020, Parvez et al., 2018). How-\never, these models were often hampered by their difficulty\nin maintaining long-range dependencies. In the context of\na recipe, this could manifest as the model \"forgetting\" an\ningredient from the initial list or failing to maintain logical\nconsistency across a long series of steps. To combat this,\n1Code is available on GitHub.\n1\n\nsome research, like the \"neural checklist model\" by Kiddon\net al. (Kiddon et al., 2016), attempted to explicitly track\ningredient usage to improve coherence, though challenges\nwith global consistency remained.\nThe\nintroduction\nof\nthe\nTransformer\narchitecture\n(Vaswani et al., 2017) and its self-attention mechanism\nmarked a paradigm shift, largely solving the long-range\ndependency problem. Pre-trained models like GPT (Radford\net al., 2019) were quickly adapted for this domain. Lee et al.\n(Lee et al., 2020) in their work on RecipeGPT, were among\nthe first to demonstrate the effectiveness of fine-tuning\nGPT-2, showing notable gains in fluency.\nSubsequently,\nGoel et al. (Goel et al., 2022) developed Ratatouille, another\nGPT-2 based tool that focused on novel recipe generation\nby exploring different prompting strategies. The success of\nthese models is shown in Table 1.\nMore recently, research has explored a wider variety of\nsmaller language models and introduced novel evaluation\nframeworks. Notably, Vij et al. (Vij et al., 2025) present\na comparative analysis of models like T5 and Phi-2 on the\nFood.com dataset, with a primary focus on developing sys-\ntems for allergen substitution using RAG and novel domain-\nspecific metrics like \"Step Complexity\" and \"Recipe Coher-\nence\".\nOur work complements and differs from these studies.\nWhile Vij et al. (Vij et al., 2025) explores diverse small\nmodels and the complex task of allergen safety, our study\nprovides a focused, deep comparison on the impact of model\nscale within a single architecture (GPT-2 small vs. large)\nagainst traditional baselines on the 5-cuisine corpus. Our\nprimary novel contribution is the introduction and evalua-\ntion of a fraction-aware tokenization strategy, a specific tech-\nnique to improve numerical fidelity in recipes, which was\nnot an emphasis in prior work. Thus, we aim to establish a\nrigorous, reproducible benchmark that isolates the effects of\nmodel scale and domain-specific tokenization using a com-\nprehensive suite of standard evaluation metrics.\nTable 1: Prior Recipe NLG Results (BLEU-4 (%))\nModel\nBLEU-4\nRecipeGPT (Lee et al., 2020)\n0.085\nRatatouille (Goel et al., 2022)\n0.080\n3\nMethodology\nBuilding on the literature, we implemented the following:\n• Baseline RNN/LSTM models to quantify traditional\nsequence-to-sequence performance.\n• GPT-2 fine-tuning to measure the “transformer effect”\nunder identical conditions.\n3.1\nBaseline Models: LSTM/RNN\nTo establish a meaningful performance benchmark, we first\nimplemented standard Long Short-Term Memory (LSTM)\nand simple Recurrent Neural Network (RNN) architectures.\nThese models represent the common sequence-to-sequence\napproaches used prior to the widespread adoption of trans-\nformers (Agarwal et al., 2020, Parvez et al., 2018). Histor-\nically, while a significant step up from rule-based systems,\nthese models often struggled with maintaining long-range\ndependencies and global coherence, especially in lengthy,\nstructured texts like recipes (Kiddon et al., 2016).\n• Architecture:\nWe implemented word-level models\nconsisting of an input embedding layer followed by two\nstacked LSTM (or simple RNN) layers with 512 hid-\nden units each. This architecture provides sufficient ca-\npacity to learn sequential patterns without being exces-\nsively prone to overfitting. The recurrent layers are fol-\nlowed by a fully connected linear layer with a softmax\nactivation function to produce a probability distribution\nover the vocabulary for predicting the next token.\n• Training: The models were trained using a standard\ncross-entropy loss objective to predict the next token in\na sequence given the preceding tokens. We employed\nteacher forcing during training, where the ground-truth\nprevious token is fed as input at each step. This tech-\nnique is known to stabilize and accelerate the training\nof recurrent models.\n• Inference: For generating recipes, we utilized both\ngreedy decoding and beam search to produce the final\ntext. These baselines serve to highlight the typical chal-\nlenges of recurrent architectures, such as a tendency to-\nwards repetitive output and difficulty in tracking ingre-\ndient usage throughout a long set of instructions.\nFigure 1: Conceptual architecture of the baseline LSTM/RNN\nmodel.\n3.2\nGPT-2 Fine-tuning\nThe core of our methodology is the fine-tuning of the pre-\ntrained Generative Pre-trained Transformer 2 (GPT-2) model\n2\n\n(Radford et al., 2019). This approach leverages the vast lin-\nguistic knowledge encoded within the model from its ini-\ntial large-scale training and specializes it for the nuanced\ndomain of recipe generation.\nThe success of fine-tuning\nGPT-2 for this task has been demonstrated in prior works\nsuch as RecipeGPT (Lee et al., 2020) and Ratatouille (Goel\net al., 2022), which established the viability of transform-\ners for generating coherent and fluent recipes. Our work\nbuilds upon these foundations by introducing a more con-\ntrolled comparison and a domain-specific tokenization strat-\negy.\nFigure 2:\nHigh-level overview of the Transformer fine-tuning\npipeline.\n3.2.1\nData Preprocessing and Structuring\nWe utilized the 5cuisine Dataset (Batra et al., 2020). We se-\nlected this dataset due to its large scale ( 51,000 recipes),\nstructured format (title, ingredients, instructions), and public\navailability, making it a suitable benchmark for reproducible\nresearch. Effective preprocessing was crucial for model per-\nformance:\n1. Cleaning: Raw web-scraped data often contains noise.\nWe removed recipes with missing essential fields (ti-\ntle, ingredients, instructions) and filtered out extremely\nshort (likely incomplete) or excessively long recipes to\nmaintain dataset quality and manage computational re-\nsources. Basic text normalization (e.g., lowercasing)\nwas also applied.\n2. Structuring with Special Tokens: Recipes have an in-\nherent, hierarchical structure. To help the model learn\nthis structure explicitly, we concatenated these compo-\nnents into a single text sequence, demarcated by custom\nspecial tokens. This format provides clear, unambigu-\nous signals to the model about the different parts of a\nrecipe:\n<RECIPE_START> <TITLE_START>\nRecipe Title\n<TITLE_END>\n<INGR_START> Ingr 1] <NEXT_INGR> Ingr 2]\n... <INGR_END>\n<INSTR_START> [Step 1] <NEXT_INSTR> Step 2]...\n<INSTR_END> <RECIPE_END>\n3. Data Splitting:\nThe cleaned and formatted dataset\nwas partitioned into standard training (80%), validation\n(10%), and testing (10%) sets. The validation set was\nused for monitoring training progress and hyperparam-\neter tuning, while the test set was held out for final, un-\nbiased evaluation.\nFigure 3: Structured dataset before pre-processing into a single text\nsequence for model training.\n3.2.2\nCustom Tokenization\nStandard tokenizers, including GPT-2’s byte-level Byte Pair\nEncoding (BPE), may not optimally handle domain-specific\nstructures or symbols. We augmented the tokenizer for two\nkey reasons:\n• Representing Structure: To make the model aware\nof the recipe sections defined during preprocess-\ning, we added our custom boundary tokens (e.g.,\n<INGR_START>, <NEXT_INGR>) directly to the to-\nkenizer’s vocabulary. This ensures they are treated as\nsingle, indivisible semantic units rather than being split\ninto multiple, meaningless sub-tokens. We also added\na [PAD] token for efficient batch padding.\n• Preserving Fractions:\nNumerical quantities, espe-\ncially fractions (e.g., \"1/2 cup\"), are critical in recipes\nfor correctness. Standard BPE can split these in unde-\nsirable ways (e.g., \"1\", \"/\", \"2\"), losing the numerical\nmeaning. We added special rules to the tokenizer to\npreserve common fractions as single entities, improv-\ning the model’s ability to handle these numerical as-\npects correctly.\nThe model’s vocabulary and token embedding matrix were\nthen resized to accommodate these important additions.\n3\n\n3.2.3\nTraining Setup\nFine-tuning adapts the general knowledge of the pre-trained\nGPT-2 model to the specific style, vocabulary, and content\nof the recipe domain.\n• Base Model: We used the ’gpt2(774 Million)’ vari-\nant (774M parameters) from the Hugging Face Trans-\nformers library (Wolf et al., 2020). This model offers\na strong balance between generative capability and the\ncomputational requirements for fine-tuning on a single\nhigh-end GPU.\n• Objective Function: The model was trained using the\nstandard Causal Language Modeling (CLM) objective.\nThe task is to predict the next token in a sequence\ngiven all previous tokens, P(ti|t1, ..., ti−1).\nCross-\nentropy loss was calculated, but we strategically ig-\nnored (masked) the padding tokens by setting their cor-\nresponding labels to -100, a standard practice to ensure\nthey do not contribute to the loss calculation.\n• Hyperparameters:\nWe selected hyperparameters\nbased on common practices for fine-tuning GPT-2 and\nempirical experimentation on our validation set:\n– Optimizer:\nAdamW (Loshchilov and Hutter,\n2017), an extension of the Adam optimizer that\ndecouples weight decay from the gradient update,\nwhich often leads to better generalization perfor-\nmance during fine-tuning.\n– Learning Rate (LR): 3 × 10−5, a typical start-\ning point for fine-tuning transformers. A linear\nwarmup schedule was used for the initial phase of\ntraining.\n– Batch Size: 8. This was chosen to fit within the\nmemory constraints of our NVIDIA V100 GPU\nwhile using mixed precision.\n– Epochs: 20. Training progress and validation loss\nwere monitored to prevent significant overfitting.\n– Mixed Precision (fp16): Utilized to accelerate\ntraining and reduce GPU memory consumption.\n3.2.4\nCreative and Controlled Generation\nTo generate novel recipes at inference time, simply taking\nthe most probable token at each step (greedy decoding) of-\nten leads to repetitive and deterministic output. To encour-\nage creativity and diversity, we employed a combination of\nstochastic sampling methods:\n• Nucleus Sampling (Top-p): We used a value of p =\n0.95. This method samples only from the smallest set\nof most probable tokens whose cumulative probability\nmass exceeds the threshold p.\n• Top-k Sampling: Additionally, we limited the sam-\npling pool to the k = 50 most likely tokens, prevent-\ning the inclusion of highly improbable and potentially\nnonsensical tokens.\n• Temperature Scaling: A temperature value of T =\n0.7 was applied to rescale the logits before the softmax\nstep. A temperature below 1.0 sharpens the distribution,\nmaking the model’s choices more focused and thereby\nincreasing coherence.\nGeneration was typically initiated by providing a prompt\ncontaining the start tokens and the desired ingredients.\nTable 2: Sampling Hyperparameters for Recipe Generation\nMethod\nValue\nNucleus (Top-p) Sampling\np = 0.95\nTop-k Sampling\nk = 50\nTemperature Scaling\nT = 0.7\n4\nExperiments and Results\n4.1\nDataset and Experimental Setup\nThe preprocessed 5cuisine dataset formed the basis for our\nexperiments. Key statistics of the final dataset used for train-\ning and evaluation are presented in Table 3. All experiments\nwere conducted using Python 3.8+, PyTorch 1.10+, and the\nHugging Face Transformers library (v4.x) on an NVIDIA\nV100 GPU with 32GB of memory.\nFor generation met-\nrics, recipes were generated based on prompts derived from\nthe test set (using the ground-truth ingredients list as input),\nand these generated recipes were then compared against the\nground-truth instructions.\nTable 3: 5cuisine Dataset Statistics After Preprocessing\nFeature\nValue\nTotal Recipes Processed\n∼51,000\nTrain Set Size (80%)\n∼40,800\nValidation Set Size (10%)\n∼5,100\nTest Set Size (10%)\n∼5,100\nAvg. Tokens per Recipe\n∼512\nVocabulary Size (Base + Special)\n∼50,257\n4.2\nEvaluation Metrics\nTo provide a holistic assessment of recipe quality, we em-\nployed a diverse set of automatic metrics, each capturing\ndifferent aspects of performance:\n• BLEU-4 (Papineni et al., 2002): Primarily measures n-\ngram precision (the overlap of generated 4-grams with\nreference 4-grams), indicating local correctness and flu-\nency.\n• ROUGE-L (Lin, 2004): Measures the longest common\nsubsequence between the generated and reference text,\nfocusing on recall and capturing sentence-level struc-\ntural similarity.\n4\n\n• Diversity: Quantifies lexical variety by calculating the\nratio of unique word bigrams to the total number of bi-\ngrams generated. Higher scores indicate less repetition\nand potentially more novel output.\n• METEOR (Banerjee and Lavie, 2005): Aligns gen-\nerated text with reference text considering synonyms\nand stemming, providing a score based on the harmonic\nmean of precision and recall for unigrams. It often cor-\nrelates better with human judgment than BLEU.\n• Perplexity (PPL): An intrinsic evaluation metric that\nmeasures how well the language model predicts the test\nset sequences. Lower perplexity indicates higher model\nconfidence and a better fit to the data’s distribution.\n• BERTScore (Zhang et al., 2020): Measures semantic\nsimilarity by computing the cosine similarity between\ncontextual embeddings of tokens in the generated and\nreference texts. It captures semantic relevance far better\nthan surface-level n-gram overlap metrics.\n4.3\nQuantitative Results and Interpretation\nTable 4 presents the comparative performance of the im-\nplemented LSTM/RNN baselines and the fine-tuned GPT-2\nmodel on the held-out test set. The results clearly and con-\nsistently demonstrate the superiority of the fine-tuned GPT-2\nmodel over the recurrent baselines across every single met-\nric.\nA deeper interpretation of these results reveals several key\ninsights:\n• Model Confidence and Fit (Perplexity):\nGPT-2\nachieves a dramatically lower average perplexity than\nthe baselines, indicating a vastly superior understand-\ning and prediction of the recipe language structure and\nvocabulary distribution found in the test set.\n• Overlap and Structural Metrics (BLEU, ROUGE,\nMETEOR): GPT-2 consistently scores higher on\nBLEU-4, ROUGE-L, and METEOR, demonstrating\nthat its generated recipes have greater n-gram overlap\n(local fluency) and better structural alignment with the\nground-truth recipes.\n• Semantic Similarity (BERTScore): The substantial\nimprovement in the BERTScore F1 is particularly note-\nworthy. It suggests that GPT-2 generates recipes that\nare not only structurally more similar but also semanti-\ncally closer in meaning to the reference recipes, captur-\ning underlying cooking concepts more effectively.\n• Creativity and Novelty (Diversity): GPT-2 also ex-\nhibits significantly higher bigram diversity, indicating\nthat its generated outputs are less repetitive and con-\ntain a wider variety of word pairings compared to the\nLSTM/RNN models. This points to a greater potential\nfor generating novel content rather than just reproduc-\ning training data patterns.\nOverall, the quantitative analysis strongly supports the\nhypothesis that fine-tuning a pre-trained transformer model\nprovides significant, measurable advantages for the com-\nplex, structured task of recipe generation compared to tra-\nditional recurrent architectures trained from scratch.\n5\nDiscussion\nThe quantitative results presented in the previous section\npaint a clear picture of GPT-2’s superiority over LSTM/RNN\nbaselines on automatic metrics. However, a deeper qualita-\ntive analysis of the generated recipe text reveals a more nu-\nanced understanding of the model’s capabilities and its re-\nmaining limitations.\n5.1\nQualitative Analysis and Example Gener-\nation\nFigure 4 shows a sample output from our fine-tuned GPT-\n2 Large model, along with its corresponding evaluation\nscores. The generated instructions are coherent, follow a\nlogical step-by-step process, and correctly utilize the ingre-\ndients provided in the prompt, such as \"beef chuck,\" \"gin-\nger paste,\" and \"cardamom seeds.\" This demonstrates the\nmodel’s strength in maintaining context and adhering to the\ninput. The high BERTScore F1 of 0.9148 reflects this strong\nsemantic relevance.\nAt the same time, the more modest\nBLEU score of 0.2105 indicates that the phrasing differs\nfrom the ground-truth recipe, which is expected and even\ndesirable in a creative generation task. This highlights the\nimportance of using a diverse suite of metrics, as a low n-\ngram overlap score does not necessarily mean a low-quality\nrecipe.\n5.2\nStrengths of Fine-tuned GPT-2\n• Improved Coherence and Fluency: The most strik-\ning improvement was in the overall readability and log-\nical flow of the generated recipes. GPT-2 produced in-\nstructions that were generally well-formed, grammat-\nically correct, and followed a plausible cooking se-\nquence much more reliably than the baselines.\n• Handling Long-Range Dependencies: Unlike LSTMs\nwhich often \"forgot\" ingredients mentioned in the ini-\ntial prompt, GPT-2 demonstrated a much better capac-\nity to track the ingredient list throughout the instruction\nsteps.\n• Structural Fidelity: The use of custom special tokens\nproved highly effective. GPT-2 reliably generated dis-\ntinct sections for the title, ingredients, and instructions,\nadhering to the desired format. This structure is cru-\ncial for user readability and any potential downstream\nprocessing.\n5\n\nTable 4: Comparative evaluation results on the test set. All metrics are averaged across the test set’s generated recipes. For all metrics\nexcept Perplexity, higher is better. For Perplexity, lower is better (↓).\nModel\nROUGE-L\nBLEU-4\nDiversity\nMETEOR\nPPL ↓\nBERTScore (F1)\nPrecision\nRecall\nF1\nRNN Baseline\n0.20\n0.16\n0.18\n0.08\n0.28\n0.13\n105.2\n0.72\nLSTM Baseline\n0.39\n0.27\n0.30\n0.12\n0.51\n0.28\n68.58\n0.87\nGPT-2 small (124 M) fine-tuned\n0.55\n0.41\n0.47\n0.21\n0.68\n0.41\n28.5\n0.90\nGPT-2 large (774 M) fine-tuned\n0.64\n0.48\n0.54\n0.26\n0.73\n0.47\n20.67\n0.92\nFigure 4: A sample generation from the fine-tuned GPT-2 Large model, showing the input ingredient prompt, the generated step-by-step\ninstructions, and the resulting automatic evaluation scores.\n• Potential for Novelty:\nThe higher diversity scores\nwere reflected qualitatively. GPT-2 occasionally sug-\ngested interesting, non-standard ingredient pairings or\nslight variations on common techniques, hinting at a\ncreative potential that extends beyond simply reproduc-\ning training data patterns.\n5.3\nChallenges and Limitations\nDespite its clear strengths, the fine-tuned GPT-2 model ex-\nhibited several critical weaknesses that prevent it from being\na fully reliable culinary assistant.\n• Factual Inaccuracy and Common Sense: This re-\nmains the most significant challenge. The model some-\ntimes generated factually incorrect information, such as\nsuggesting inappropriate cooking temperatures or times\n(e.g., \"bake a delicate fish at 500°F for 3 hours\"). Quan-\ntities were particularly problematic, with frequent in-\nstances of unrealistic or nonsensical amounts (e.g., \"add\n20 cloves of garlic for a single serving,\" or \"use 5 cups\nof salt\"). This highlights the model’s lack of true world\nknowledge or common-sense reasoning about physics\nand chemistry.\n• Ingredient Hallucinations and Omissions: While bet-\nter than LSTMs, GPT-2 still sometimes introduced in-\ngredients that were not present in the initial prompt\n(hallucinations) or failed to utilize all the provided in-\ngredients (omissions).\n• Step Consistency and Logical Gaps: Although gen-\nerally coherent, occasional logical gaps or inconsisten-\ncies appeared in the instruction steps. For instance, an\ninstruction might refer to an ingredient that hadn’t been\nprepared yet (\"add the diced onions\" before they have\nbeen diced), or steps might be presented in a suboptimal\norder.\n• Repetition and Truncation: While less frequent than\nwith LSTMs, GPT-2 could sometimes fall into repeti-\ntive loops, especially with longer generations. Gener-\nated recipes could also be truncated if they exceeded\nthe maximum specified output length during inference.\nThese limitations underscore the crucial gap between gener-\nating statistically plausible text and creating practically us-\nable, safe, and delicious recipes.\n6\nConclusion and Future Work\nIn this paper, we demonstrated that fine-tuning a pre-trained\nGPT-2 large model, enhanced with a domain-specific tok-\nenization scheme, sets a new state-of-the-art for recipe gen-\neration on the 5-cuisine benchmark, significantly outper-\nforming traditional RNN/LSTM baselines. Our work con-\nfirms the power of scaled transformer models for structured\ncreative generation but also highlights the critical need for\nfurther research to bridge the gap to practically reliable sys-\ntems.\nBuilding on these findings, we identify several promising\navenues for future work:\n• Retrieval-Augmented Generation (RAG): To im-\nprove factual grounding, RAG can be integrated to\nground generation in an external knowledge base of\ntrusted culinary information (e.g., standard cooking\ntimes), which would directly address the issue of fac-\ntual inaccuracies.\n6\n\n• Constrained Decoding: Developing techniques to ex-\nplicitly enforce constraints during generation is cru-\ncial. This could involve methods like lexical constraints\nto guarantee ingredient usage or integrating rule-based\nchecks to ensure quantities fall within realistic ranges.\n• Systematic Human Evaluation: A critical next step,\nas noted by reviewers, is to conduct a systematic hu-\nman evaluation. Such a study should assess generated\nrecipes on key qualities like clarity, feasibility, safety,\nand creativity, providing definitive insights that auto-\nmatic metrics cannot capture.\n• Benchmarking Additional Architectures:\nFuture\nwork could extend this benchmark to include encoder-\ndecoder models like T5 and BART to provide a more\ncomplete picture of architectural trade-offs.\n• Multi-Modal Integration:\nOur foundational text\nmodel enables several exciting multi-modal extensions,\nsuch as generating recipes from images (image-to-\nrecipe) or generating illustrative images for each recipe\nstep (recipe-to-image).\nAcknowledgments\nThe authors thank the anonymous reviewers for their insight-\nful feedback, which helped improve the quality of this paper."}
{"id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "authors": ["Junying Chen", "Zhenyang Cai", "Zhiheng Liu", "Yunjin Yang", "Rongsheng Wang", "Qingying Xiao", "Xiangyi Feng", "Zhan Su", "Jing Guo", "Xiang Wan", "Guangjun Yu", "Haizhou Li", "Benyou Wang"], "source": "http://arxiv.org/abs/2508.14706v1", "page_count": 20, "text": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine\nJunying Chen1†, Zhenyang Cai1†, Zhiheng Liu1, Yunjin Yang1, Rongsheng Wang1,\nQingying Xiao3, Xiangyi Feng2, Zhan Su1, Jing Guo5,\nXiang Wan4, Guangjun Yu1,3, Haizhou Li1, Benyou Wang1,4∗\n1 The Chinese University of Hong Kong, Shenzhen 2 Freedom AI\n3 National Health Data Institute, Shenzhen 4 Shenzhen Research Institute of Big Data\n5 Guangdong University of Technology\nwangbenyou@cuhk.edu.cn\nhttps://github.com/FreedomIntelligence/ShizhenGPT\nAbstract\nDespite the success of large language models (LLMs) in vari-\nous domains, their potential in Traditional Chinese Medicine\n(TCM) remains largely underexplored due to two critical bar-\nriers: (1) the scarcity of high-quality TCM data and (2) the\ninherently multimodal nature of TCM diagnostics, which in-\nvolve looking, listening, smelling, and pulse-taking. These\nsensory-rich modalities are beyond the scope of conventional\nLLMs. To address these challenges, we present ShizhenGPT,\nthe first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising\n100GB+ of text and 200GB+ of multimodal data, including\n1.2M images, 200 hours of audio, and physiological signals.\nShizhenGPT is pretrained and instruction-tuned to achieve\ndeep TCM knowledge and multimodal reasoning. For eval-\nuation, we collect recent national TCM qualification exams\nand build a visual benchmark for Medicinal Recognition and\nVisual Diagnosis. Experiments demonstrate that ShizhenGPT\noutperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual under-\nstanding among existing multimodal LLMs and demonstrates\nunified perception across modalities like sound, pulse, smell,\nand vision, paving the way toward holistic multimodal per-\nception and diagnosis in TCM. Datasets, models, and code\nare publicly available. We hope this work will inspire further\nexploration in this field.\nIntroduction\nLarge language models (LLMs) like GPT-4 have demon-\nstrated impressive capabilities across general and specialized\ndomains (Achiam et al. 2023; Dubey et al. 2024; Guo et al.\n2025). Meanwhile, Traditional Chinese Medicine (TCM),\na medical system with thousands of years of history and\nongoing relevance for hundreds of millions (Shi and Zhou\n2007; Zhao et al. 2015), remains largely absent from recent\nAI developments. TCM, with its intricate theoretical frame-\nwork, nuanced diagnostic reasoning, and enduring clinical\nrelevance (Bing and Hongcai 2010), presents both a chal-\nlenge and an opportunity for large language models (Hua\net al. 2024). Empowering these models to understand and\nreason about TCM could greatly advance clinical decision-\nmaking, medical education, and preservation of traditional\nmedical knowledge (Yao et al. 2019; Wang et al. 2020).\n†Equal Contribution. ∗Corresponding author.\nLarge-Scale\nMultimodal\nTCM Dataset\nExtensive TCM Knowledge\nSmelling\nListening\nObserving\nTouching\nMulti-Modal Perception\nLarge-Scale TCM Training\nShizhenGPT\nVL\nShizhenGPT-32B\nDeepseek-V3\nLLaMA4-Marerick\nLLaMA4-Marerick\nGPT-4o\nShizhenGPT-32B\nQwen2.5-VL-72B\nQwen2.5-72B\nDoubao-1.5-Vision-Pro\nGPT-4o\nTCM Visual Skills\nLatest Licensing Exam\nfor TCM Pharmacist\nFigure 1: Key Capabilities of ShizhenGPT, a multimodal\nLLM for Traditional Chinese Medicine (TCM).\nHowever, applying LLMs to TCM presents two key chal-\nlenges. First, there is a significant scarcity of high-quality\nTCM data. For example, most existing TCM-specific LLMs\nare trained on less than 1GB of text (Dai et al. 2024; Yang\net al. 2024b; Wei et al. 2024; Hua et al. 2024). This is sub-\nstantially smaller than the datasets used in domains like mod-\nern medicine (Zhang et al. 2024b; Chen et al. 2023), and\nparticularly insufficient given the complexity and depth of\nTCM theory. Moreover, the multimodal datasets necessary\nfor training TCM-oriented multimodal LLMs are currently\nnon-existent. Second, TCM fundamentally depends on multi-\nmodal diagnostics. Rooted in the ”Four Diagnostic Methods”,\nit depends on observing the tongue and other visual cues,\nlistening to the voice and breath, smelling smells, and feel-\ning the pulse (Dong, Zhang, and Fei 2022; Tian et al. 2023).\nThese rich sensory modalities are central to TCM and lie well\nbeyond the capabilities of text-only models.\nTo address this, we propose ShizhenGPT, the first multi-\nmodal LLM tailored for Traditional Chinese Medicine. To\n\nTable 1: Comparison of ShizhenGPT and existing LLMs across key dimensions.\nModel\nDomain Expertise\nMultimodal Capability\nClinical Application\nText\nVision\nSound\nSmell\nPulse\nTCM-specific LLMs\nLimited\nby\nsmall-scale\ntraining\ndata\n(<1GB).\n✓\n✗\n✗\n✗\n✗\nLimited to text-only sce-\nnarios.\nGeneral-purpose LLMs\nGeneral-domain training, limited TCM spe-\ncialization.\n✓\n❍\nPartial\n❍\nPartial\n✗\n✗\nOnly partial support for\nvision/sound.\nShizhenGPT\nTrained on 15B+ tokens and 200GB+ TCM\ndata; high TCM proficiency.\n✓\n✓\n✓\n✓\n✓\nFully supports Four Diag-\nnostic Methods.\nmitigate data scarcity, we built the largest TCM dataset\nto date, comprising over 100GB of text from 3,256 TCM-\nspecific books and online sources. In addition, we collected\nover 200GB of multimodal data, including 1.2 million anno-\ntated images, 200+ hours of audio, and diverse physiological\nsignals such as pulse, smell, and electrocardiograms (ECG).\nLeveraging this extensive dataset, ShizhenGPT is trained\nthrough domain-specific pretraining and instruction tuning,\nenabling it to acquire deep TCM knowledge and multimodal\nreasoning. The resulting model ShizhenGPT integrates deep\ndomain expertise with perceptual capabilities aligned with\nreal-world TCM diagnostic practices, including observation,\nlistening, smelling, and pulse-taking.\nTo evaluate TCM-specific capabilities, we collected re-\ncent qualification exams, including three national licensing\nexams and two postgraduate entrance exams in TCM. All\nquestions were released within the past year to ensure fresh-\nness. For visual evaluation, we built a benchmark using TCM\natlases, containing 7k image-related questions focused on\nmedicinal recognition and visual diagnosis. Results show that\nShizhenGPT outperforms existing LLMs of similar scale and\nrivals much larger proprietary models, despite having only\n32B parameters. In visual TCM tasks, it leads among current\nmultimodal LLMs. Expert evaluations also indicate a higher\npreference for ShizhenGPT’s responses. Additionally, across\nseven physiological signal datasets, ShizhenGPT demon-\nstrates effective multimodal perception, including sound,\npulse, and smell, achieving unified input and understanding\nacross modalities.\nWe present a TCM-specific multimodal LLM capable of\nperceiving smell, sound, vision, and pulse. This expands\nthe diagnostic capabilities of the model beyond text-based\ninteraction, allowing it to directly observe the tongue, pulse,\nor breathing sounds for richer clinical insights. This moves\ntoward more realistic, holistic medical AI systems. We hope\nour data, model, and approach will inspire further research in\nTCM.\nOur contributions are as follows:\n• We release the largest TCM dataset to date, covering\nextensive text and multimodal data across vision, audio,\nand physiological signals.\n• We introduce ShizhenGPT, the first multimodal LLM tai-\nlored for TCM, capable of understanding images, sounds,\nsmells, pulse, and more. It achieves strong performance\nin TCM expertise and leads in visual diagnostic tasks.\n• We release a comprehensive benchmark for TCM across\ntextual and multimodal tasks, enabling systematic evalua-\ntion of LLMs in TCM.\nBackground and Motivation\nTraditional Chinese Medicine\nTCM is one of the world’s\noldest and most influential medical systems, serving over\n500 million patients annually in China and gaining global\nrecognition (Cheung 2011; Liu et al. 2015; Zhao et al. 2023).\nUnlike modern medicine, TCM relies on the ”Four Diag-\nnostic Methods”: looking, listening/smelling, questioning,\nand pulse-taking (Dong, Zhang, and Fei 2022; Tian et al.\n2023). These methods collect visual, auditory, verbal, and\ntactile information to form a holistic understanding of the\npatient. Each captures distinct biomedical signals, making\nTCM inherently multimodal and posing unique challenges\nfor AI systems seeking to replicate its diagnostic reasoning.\nMotivation for a Multimodal TCM LLM\nTCM’s com-\nplexity and its reliance on diverse sensory modalities pose\nsignificant challenges for LLMs. As shown in Table 1, cur-\nrent TCM-specific models suffer from limited training data\nand lack multimodal perception. General-purpose models,\nwhile powerful, are not tailored to TCM and offer only par-\ntial support for modalities like vision or audio. To address\nthese gaps, we propose ShizhenGPT, a multimodal LLM\ntrained on extensive TCM data in diverse modalities. By inte-\ngrating perception and reasoning across all four diagnostic\nmethods, ShizhenGPT might enable more faithful, practical\napplications in real-world clinical settings.\nShizhenGPT\nThis section describes ShizhenGPT, including its architec-\nture and two-phase training: (1) pre-training to inject TCM-\nspecific knowledge, and (2) post-training for real-world tasks.\nModel Architecture\nAs shown in Figure 2(c), ShizhenGPT consists of three main\ncomponents: an LLM backbone, a vision encoder, and a\nsignal encoder.\nLLM Backbone\nThe LLM serves as the core reason-\ning engine. It processes inputs from multiple modalities\nand generates responses. We use the Qwen-2.5-7B and\nQwen-2.5-32B (Yang et al. 2025) as the base LLMs.\n\nPretraining-Stage1:\n11.9B TCM text tokens\nPretraining-Stage2:\n3.6B TCM Multimodal tokens\nLarge Language Model\nVision Encoder\nVision Adapter\nSignal Encoder\nSignal Adapter\n(a) Pre-training\nInstruction Tuning with：\nText\nSound\nLanguage Response\nPulse\nOdor\nECG\nImages\nPalm\nTonge\nHerb\nEye\n...\nText\n(b) Post-training\n(c) Model Architecture\nStronger TCM Expertise\n(d) ShizhenGPT\nMultimodal TCM Understanding\nModal Generalization\nExcels in TCM Licensed Exams\nMultimodal Generalization\nRich Multimodal\nText Image Sound\nImage\nSound\nPulse\nOdor\nECG\nText\n+\nText\nOutput\nImage\n+\nText\nOutput\nSound\n+\nText\nOutput\nPulse\n+\nText\nOutput\nOdor\n+\nText\nOutput\nECG\nText\nOutput\nPretraining Stage 2 Loss\nPretraining Stage 1 Loss\nFigure 2: Overview of ShizhenGPT. (a) Pre-training process, with the loss curve of ShizhenGPT-7B. (b) Post-training process\nwith multimodal instruction tuning. (c) Model architecture. (d) Demonstration of ShizhenGPT’s capabilities.\nVision Encoder\nFor visual inputs, we initialize the Vi-\nsion Encoder from Qwen-2.5-VL (Bai et al. 2025), which\nsupports high-resolution images using 2D-RoPE and win-\ndow attention. Visual patches extracted by the encoder are\ngrouped (four adjacent patches), concatenated, and mapped\nto the LLM’s embedding space via a two-layer MLP (Vision\nAdapter).\nSignal Encoder\nFor continuous signals (e.g., voice, pulse,\nsmell), we use Whisper-large-v3 (Radford et al. 2022)\nas the initial Signal Encoder. Non-audio signals are first con-\nverted into waveform representations via linear interpolation.\nAll waveforms are resampled to 16kHz and transformed into\n128-channel mel-spectrograms using a 25ms window and\n10ms hop size. A stride-2 pooling layer reduces temporal res-\nolution, and a one-layer MLP (Signal Adapter) projects the\nfeatures into the embedding space of LLM. Although pulse\nsignals are low-frequency, the resulting 40ms-per-token reso-\nlution retains sufficient temporal granularity to capture the\nslow-varying patterns essential for TCM.\nPre-training\nWe begin with pre-training on large-scale TCM data to ac-\nquire domain expertise. We design a two-stage pipeline: the\nfirst stage infuses knowledge from extensive TCM text, while\nthe second introduces multimodal alignment through image-\ntext and audio-text data.\nPretraining Datasets\nAs summarized in Table 2, the pre-\ntraining corpus includes three components:\n• TCM Text Corpus We collect TCM text from 3,256\nTCM books and online sources. The books are converted\ninto interleaved text-image corpora and cleaned to yield\n3.8GB of book corpus. For online sources, we use a 30K-\nterm TCM lexicon to extract TCM documents from Com-\nmon Crawl (2017-2023) and WeChat public articles, to-\ntaling 96.4GB. A two-step filtering process, involving\nquality scoring using a classifier and semantic deduplica-\ntion, produces a final 21.2GB high-quality web corpus.\n• TCM Image-Text Data We extract 17.6GB of inter-\nleaved image-text data from TCM books (51K images).\nFrom 5 million images sourced from WeChat articles,\nwe filtered high-quality TCM images using an image\nclassifier. This process yielded 140.7GB of web-based\nimage-text data (1M images). To improve alignment, we\nfollow HuatuoGPT-Vision and use a multimodal LLM to\ngenerate descriptive captions based on image and context,\nproducing 40.6GB of synthetic pairs (15K images).\n• TCM Audio-Text Data We extract 60K doctor-patient\ndialogues from the Huatuo-26M dataset (Li et al. 2023b)\nand synthesize audio via a high-fidelity TTS system (Du\net al. 2024), yielding 58K utterance-aligned audio-text\npairs.\nFurther details are provided in Appendix .\nPre-training Strategy\nWe adopt a two-stage pre-training\nstrategy. In Stage 1, it focuses on text-only learning with\n11.9B tokens, including 6.3B from TCM corpora and 5.6B\nfrom general corpora (see Appendix for details) to maintain\nfoundational capabilities. In Stage 2, it uses 3.8B tokens of\nmultimodal data, including TCM and general image-text and\naudio-text data, along with 1.8B resampled textual tokens\nfrom Stage 1 to preserve grounding. In both stages, inputs\nare packed into 4096-token sequences, and loss is computed\nonly over textual tokens. Pre-training runs for 1 epoch with\na 5e-5 learning rate using full-parameter tuning. Details are\nprovided in Appendix 6.\n\nTable 2: Overview of Pre-training Datasets. These data are utilized for a single training epoch unless otherwise specified. #\nindicates quantity. B stands for billion.\nPre-training Dataset\nDescription\n# Tokens\nStorage\nText Datasets (Pretraining Stage 1 & Stage 2)\nTCM Web Corpus\nA high-quality TCM corpus collected from Common Crawl (2017–2023)\nand WeChat public articles (a major content channel in China).\n5.33B\n21.2 GB\nTCM Book Corpus\nA cleaned corpus of 3,256 TCM textbooks. (2 epochs)\n0.96B\n3.8 GB\nGeneral Text Corpus\nGeneral-domain English corpus from FineWeb-edu. See Appendix for\ndetails.\n5.63B\n29.2 GB\nImage-Text Dataset (Pretraining Stage 2)\nTCM Web\nInterleaved Data\nInterleaved text-image data from the TCM web corpus. (# Images: 1,143,954)\n0.87B\n140.7 GB\nTCM Book\nInterleaved Data\nRichly illustrated interleaved text-image data from 306 TCM books. (#\nImages: 51,619; 2 epochs)\n0.14B\n17.6 GB\nTCM Synthesized\nImage-Text Data\nTCM image-text pairs generated from images and their context using multi-\nmodal LLMs. (# Images: 146,635; 2 epochs)\n0.16B\n40.6 GB\nGeneral\nImage-Text Data\nGeneral-domain image-text datasets, such as ShareGPT-4V. See Appendix\nfor details. (# Images: 1,851,321)\n0.69B\n93.29 GB\nAudio-Text Dataset (Pretraining Stage 2)\nTCM-related\nAudio-Text Data\nChinese audio-text pairs in the domain of traditional Chinese medicine. (#\nAudios: 58,462 ; Duration: 238.67 hours)\n0.03B\n75.72 GB\nGeneral\nAudio-Text Data\nEnglish audio-text pairs from general domains. See Appendix . (# Audios:\n282,884 ; Duration: 353.04 hours)\n0.04B\n111.6 GB\nTable 3: Overview of Instruction-Tuning Datasets. Signal instructions are detailed in Appendix 9.\nInstruction-Tuning Dataset\nDescription\n# Item\nText Instruction Data\nTCM Instruction\nA dialogue instruction dataset centered on real-world TCM problems.\n83,629\nVision Instruction Data (Requires Vision Encoder)\nTCM Vision Instruction\nA multimodal instruction dataset composed of TCM-related image-text pairs.\n(# Images: 70,638)\n65,033\nSignal Instruction Data (Requires Signal Encoder)\nTCM Speech Instruction\nSpeech-to-text instruction data in the TCM domain.\n57,957\nCough Sound Dataset\nAn instruction dataset focused on cough sounds for medical diagnosis.\n456\nHeartbeat Sound Dataset\nAn instruction dataset focused on heartbeat sounds for medical diagnosis.\n189\nPulse Dataset\nAn instruction dataset focused on pulse diagnosis in TCM applications.\n4,101\nSmell Dataset\nDataset related to olfactory input for medical diagnosis.\n672\nECG Dataset\nElectrocardiogram instruction dataset for medical diagnosis.\n99,195\nPost-training\nAfter pre-training, the model acquires foundational TCM\nknowledge and multimodal abilities. Post-training aligns\nthe model for instruction-following, extends its capabilities\nto downstream tasks, and enables adaptation to additional\nmodalities such as sound and smell, which require low learn-\ning complexity but suffer from limited data.\nInstruction Dataset\nAs summarized in Table 3, instruction\ntuning data spans four categories:\n• TCM Text Instructions\nWe curate 92K TCM ques-\ntions from doctor-patient dialogues (Li et al. 2023b) and\nmedical verifiable questions (Chen et al. 2024b). Re-\nsponses were distilled using Deepseek-R1 and validated\nwith Deepseek-V3 based on reference answers or doctor\nresponses, resulting in 83K reliable Chinese instruction\npairs.\n• Vision Instructions We select TCM-related images with\ndetailed captions from books and web sources. GPT-4o\nis used to generate paired visual questions and answers\nbased on image content and context, producing 65K high-\nquality vision instruction pairs.\n• Audio Instructions For speech audio, we convert 20K\ntext questions into TTS speech and pair them with origi-\nnal answers. An additional 20K transcription-style in-\nstructions are created. For TCM-specific audio (e.g.,\ncoughs, heart sounds), classification tasks are reformu-\nlated into instruction format (e.g., “Listen and determine\nif the patient has COVID-19.”).\n• Physiological Signal Instructions Non-audio signals\n(e.g., pulse, smell, ECG) are converted to waveform\nformat and tagged with modality-specific tokens (e.g.,\n<Smell>, <Pulse>). The pregnancy-related pulse data\nis constructed in-house, while the other data are ob-\n\nTable 4: Results of LLMs on the latest TCM exams, including the 2024 Licensed TCM Pharmacist Exam, 2024 Licensed TCM\nPhysician Exam, 2024 Licensed TCM Assistant Physician Exam, and two TCM Postgraduate Entrance Exams (2024-2025).\ndenotes proprietary or 100B+ LLMs;\ndenotes TCM-specific LLMs.\nModel\n2024 TCM\nPharmacist\n2024 TCM\nPhysician\n2024 TCM\nAssistant\nPhysician\n2024 TCM\nGraduate\nEntrance\n2025 TCM\nGraduate\nEntrance\nAvg.\nProprietary or 100B+ LLMs\nGPT-4o\n61.7\n56.2\n66.7\n58.7\n55.0\n59.6\nLLaMA-4-Maverick-400B\n66.0\n67.2\n76.6\n63.3\n59.5\n66.5\nDeepseek-V3\n74.6\n74.8\n80.8\n72.4\n66.7\n73.9\nDoubao-1.5-Pro-32k\n81.9\n79.7\n86.2\n81.5\n73.7\n80.6\nDeepseek-R1-671B\n85.0\n82.4\n86.2\n83.2\n83.3\n84.0\n<15B LLMs\nLlama-3.1-8B-Instruct\n36.7\n36.9\n40.4\n33.4\n26.1\n34.7\nTCMChat-600k-7B\n42.0\n49.5\n46.0\n41.6\n38.5\n43.5\nZhongjing-7B\n45.5\n47.8\n46.4\n38.4\n45.6\n44.7\nBiancang-7B\n57.3\n57.9\n66.2\n43.5\n37.5\n52.5\nGLM4-9B-Chat\n54.6\n56.2\n61.6\n54.0\n51.9\n55.7\nYi-1.5-9B-Chat\n59.0\n58.2\n69.7\n53.0\n39.7\n55.9\nLingdan-13B-PR\n64.4\n65.9\n78.4\n45.6\n41.1\n59.1\nDeepSeek-R1-Distill-14B\n62.2\n60.8\n70.5\n58.4\n55.7\n61.5\nQwen2.5-7B-Instruct\n65.3\n63.2\n76.0\n59.0\n54.0\n63.5\nInternLM2.5-7B-chat\n69.8\n68.5\n79.7\n58.5\n48.9\n65.1\nQwen2.5-14B-Instruct\n70.8\n68.8\n71.7\n63.8\n57.2\n66.4\nShizhenGPT-7B (Ours)\n70.1\n76.7\n79.4\n68.8\n69.3\n72.9\n15B to 100B LLMs\nGemma-3-27B-it\n42.1\n40.9\n44.9\n35.1\n28.1\n38.2\nLlama-3.1-70B-Instruct\n50.6\n52.8\n65.0\n42.5\n39.9\n50.2\nYi-1.5-34B-Chat\n67.6\n62.2\n75.5\n61.8\n50.7\n63.6\nInternLM2.5-20B-Chat\n66.5\n66.1\n73.8\n57.9\n53.8\n63.6\nGLM-4-32B\n67.9\n65.6\n80.9\n64.9\n62.6\n68.4\nDeepSeek-R1-Distill-32B\n70.6\n67.7\n78.4\n65.3\n61.2\n68.6\nQwen2.5-32B-Instruct\n76.1\n69.2\n78.3\n65.5\n61.8\n70.2\nQwen2.5-72B-Instruct\n77.2\n70.6\n81.0\n63.3\n59.9\n70.4\nShizhenGPT-32B (Ours)\n79.2\n78.4\n84.9\n77.5\n70.7\n78.1\ntained from public datasets. All tasks are transformed\ninto instruction format using a unified signal-processing\npipeline to ensure extensibility.\nThe instruction and signal datasets are detailed in Ap-\npendix 6 and Appendix 9, respectively.\nInstruction Fine-tuning\nWe fine-tune the TCM-pretrained\nmodel on all modality-specific instruction datasets in Table 3\nusing full-parameter tuning. Instructions follow the Qwen-2.5\nchat template, and loss is computed only on response tokens.\nThe model is trained for 3 epochs with a learning rate of\n5e-6. See Appendix 6 for details. This stage produces the\nfinal model, ShizhenGPT.\nExperimental Setup\nModel Training\nWe train two versions of ShizhenGPT:\nShizhenGPT-7B\nand\nShizhenGPT-32B,\nbased\non\nQwen2.5-7B and Qwen2.5-32B backbones, respec-\ntively. Both use the same vision and signal encoders, and are\ntrained with identical settings across all stages. Experiments\nare conducted on two DGX nodes, each with 8 A100 GPUs.\nBaselines\nWe compare ShizhenGPT with following base-\nlines: (1) Vanilla LLMs: We evaluate mainstream general-\npurpose models including LLaMA (Dubey et al. 2024),\nQwen, Yi (Young et al. 2024), GLM-4 (GLM et al. 2024), In-\nternLM (Team 2023), Gemma (Team et al. 2024), DeepSeek\n(Liu et al. 2024a), GPT-4o (2024-11-20) (Achiam et al.\n2023), and Doubao (ByteDance Seed Team 2024). TCM-\nspecific LLMs include TCMChat-600K-7B (Dai et al. 2024),\nZhongjing-7B (Yang et al. 2024b), Biancang-7B (Wei et al.\n2024), and Lingdan-13B-PR (Hua et al. 2024). (2) Multi-\nmodal LLMs (MLLMs): For vision understanding, we in-\nclude Gemini (Team et al. 2023), GPT-4o, LLaVA (Liu et al.\n2023), Qwen2.5-VL (Bai et al. 2025), InternVL (Chen et al.\n2024f), Janus-Pro (Chen et al. 2025), Kimi-VL (Team et al.\n2025) and HuatuoGPT-Vision (Chen et al. 2024c); for audio\nunderstanding, we consider Qwen2-Audio (Chu et al. 2024),\nLlama-Omni (Fang et al. 2024), and Moshi (D´\nefossez et al.\n2024).\nBenchmarks\nTo evaluate the TCM capabilities, we con-\nstruct a multimodal benchmark suite covering text, vision,\nsignals, and human evaluation. (1) Text Benchmarks: We\ncollect recent national TCM exams in China, including\nthree licensing exams (for pharmacists, physicians, and as-\nsistant physicians) and two postgraduate entrance exams\n(2024–2025). (2) Vision Benchmark: We compile 7,204\nmultiple-choice questions from seven TCM atlases, explicitly\nexcluded from training, covering seven subfields in medic-\n\nTable 5: Results on the TCM-Vision Benchmark, evaluating visual understanding in TCM.\ndenotes proprietary or 100B+\nmultimodal LLMs. bold indicates the best score; underlines marks the second-best.\nModel\nTCM Medicinal Recognition\nTCM Visual Diagnosis\nAvg\nTCM\nPatent\nTCM\nMaterial\nTCM\nHerb\nHolism\nTongue\nPalm\nTuina\nEye\nMultimodal LLMs\nLLaVA-1.5-7B\n25.6\n26.9\n27.7\n25.8\n27.0\n19.2\n16.9\n23.3\n24.1\nLLaVA-1.5-13B\n21.6\n34.8\n26.4\n31.9\n33.5\n29.2\n21.4\n18.3\n27.1\nLLaVA-Med\n26.2\n35.4\n23.1\n24.8\n28.7\n26.5\n43.5\n38.4\n30.8\nDeepSeek-VL2\n30.0\n39.1\n26.6\n28.2\n32.4\n30.8\n47.2\n42.0\n34.5\nMedGemma-4B-it\n30.2\n40.6\n28.0\n46.7\n49.9\n41.5\n38.3\n38.1\n39.2\nGemma-3-27B-it\n32.6\n40.1\n31.8\n45.8\n48.8\n46.2\n38.1\n45.3\n41.1\nLlama-3.2-11B-Vision\n33.1\n43.5\n31.1\n50.0\n53.0\n44.5\n41.8\n41.3\n42.3\nJanus-Pro-7B\n32.5\n45.1\n33.0\n45.7\n48.6\n45.3\n45.8\n50.8\n43.4\nHuatuoGPT-Vision-7B\n38.4\n53.0\n36.2\n48.9\n52.6\n44.9\n50.4\n49.0\n46.7\nInternVL3-8B\n38.8\n58.6\n39.0\n58.5\n54.8\n45.2\n53.3\n47.1\n49.4\nQwen2.5-VL-32B-Instruct\n39.6\n60.3\n46.0\n61.2\n56.4\n54.8\n51.4\n49.2\n52.4\nKimi-VL-Instruct-16B\n47.1\n64.6\n46.4\n60.5\n53.0\n52.3\n51.9\n51.9\n53.5\nInternVL3-38B\n46.1\n65.8\n48.8\n58.2\n57.3\n49.4\n54.8\n49.7\n53.7\nQwen2.5-Omni-7B\n43.8\n61.8\n51.2\n60.8\n54.5\n53.5\n59.5\n52.0\n54.6\nLLaMA-4-Maverick-400B\n46.0\n67.8\n52.0\n61.0\n62.5\n55.0\n54.8\n49.2\n56.0\nGemini-1.5-Pro\n49.8\n73.5\n54.0\n56.7\n57.2\n48.8\n59.7\n50.7\n56.3\nGPT-4o\n48.0\n70.6\n54.5\n63.2\n57.9\n57.4\n57.5\n47.8\n57.1\nDoubao-1.5-Vision-Pro\n52.0\n79.3\n64.0\n59.0\n62.8\n45.7\n58.2\n45.2\n58.3\nShizhenGPT (Ours)\nShizhenGPT-7B\n47.4\n61.7\n49.2\n70.6\n65.5\n63.7\n64.1\n58.3\n60.1\nShizhenGPT-32B\n49.5\n74.2\n54.7\n71.5\n66.5\n60.9\n65.9\n65.3\n63.6\ninal recognition and visual diagnosis. See Appendix 9 for\ndetails. (3) Human Benchmark: We collect 90 real-world\nTCM questions and evaluate model responses, with licensed\nTCM doctors selecting preferred answers (see Appendix 9).\n(4) Signal Benchmarks: We use the test set of the dataset\nfor smell, auscultation, pulse, and ECG signals, along with a\ncustom pregnancy detection dataset using pulse data.\nResults\nEvaluation of TCM Expertise\nTable 4 reports the per-\nformance of various models on the latest National TCM\nLicensing exams and the TCM postgraduate entrance ex-\nams. For fair comparison, models are grouped by parame-\nter size. In the small-scale category (<15B), ShizhenGPT-\n7B achieves the highest average score, outperforming\nQwen2.5-7B-Instruct by +9.4 points despite shar-\ning the same backbone. It also surpasses larger models\nsuch as Qwen2.5-14B-Instruct, and leads all com-\npared TCM-specific models in this range. In the mid-scale\nrange (15-100B), ShizhenGPT-32B achieves the strongest\nresults among open-source LLMs, with an average score\nof 78.1. This exceeds multiple 70B+ models. Although\nit slightly lags behind ultra-large proprietary models like\nDeepSeek-R1 (671B) and Doubao-1.5-Pro-32K, it\ndemonstrates highly competitive performance with signifi-\ncantly fewer parameters (78.1 vs. 84.0).\nHuman Evaluation\nTo further assess TCM capabilities,\nwe conduct an expert evaluation with licensed TCM practi-\ntioners. Following (Zhang et al. 2023a), we sample 90 real-\nworld TCM queries. For each, our model and a baseline\nDoubao-1.5-Pro\nDeepseek-R1\nDeepseek-V3\nQwen2.5-72B\nChatGPT-4o\nLLaMA4-Maverick\nShizhenGPT Wins\nTie\nShizhenGPT Loses\nFigure 3: Results of the human evaluation. ShizhenGPT refers\nto ShizhenGPT-32B. ”Win/Tie/Loss” indicates the proportion\nof expert preferences for the model responses (Details are in\nAppendix 9).\ngenerate responses, which are then compared by experts. As\nshown in Figure 3, ShizhenGPT-32B outperforms all base-\nlines except DeepSeek-R1, including strong contenders like\nDoubao-1.5-Pro and DeepSeek-V3. The gap with DeepSeek-\nR1 is small, while ShizhenGPT-32B offers a more efficient\nalternative given its much smaller size. Figure 3 reports\nthe win/tie/loss results. ShizhenGPT-32B outperforms all\nbaseline models except DeepSeek-R1, including strong con-\ntenders like Doubao-1.5-Pro and DeepSeek-V3. The perfor-\nmance gap with DeepSeek-R1 is minimal. Given the sub-\nstantial inference cost of DeepSeek-R1 with 671B parame-\nters, ShizhenGPT-32B offers a compelling trade-off between\nmodel size and clinical performance.\n\nEvaluation of Visual Capabilities\nTable 5 presents results\non the TCM visual benchmark, which evaluates the ability\nto recognize medicinal herbs and interpret diagnostic visuals\n(e.g., tongue, palm). Proprietary models like GPT-4o demon-\nstrate stronger visual reasoning than open-source models such\nas Qwen2.5-VL. Among baselines, Doubao-1.5-Vision-Pro,\na proprietary MLLM, achieves the strongest baseline score\nof 58.3. ShizhenGPT surpasses all baselines, achieving a\nnew SOTA with 63.6 using only 32B parameters. Notably,\neven the 7B version performs competitively, highlighting\nthe effectiveness of domain-specific multimodal training for\nspecialized visual understanding in TCM.\nTable 6: Results on general speech tasks.\nModel\nGeneral Audio LLMs Benchmarks\nLlamaQuestion\nSpeech\nTriviaQA\nSpeech\nWeb Questions\nLlama-Omni\n45.3\n22.9\n10.7\nMoshi\n43.7\n23.8\n16.7\nQwen2-Audio\n60.0\n30.4\n24.0\nShizhenGPT-7B\n64.0\n27.3\n25.4\nEvaluation\nof\nGeneral\nAudio\nCapabilities\nSince\nShizhenGPT is equipped with audio perception, we evaluate\nits general speech understanding ability on standard audio\nbenchmarks. As shown in Table 6, ShizhenGPT demonstrates\nstrong performance, comparable to Qwen2-Audio-Instruction.\nThese results indicate that ShizhenGPT is not only compati-\nble with various signal modalities but also maintains strong\naudio comprehension.\nTable 7: Performance on various TCM signal modalities.\nAll tasks are classification; details in Appendix 9. Random\nBaselines indicate random prediction accuracy.\nModality\nTask\nRandom\nBaseline\nShizhenGPT\n(7B)\nPulse\nPregnancy Detection via Pulse Diagnosis\n50.0\n80.5\nDisease Classification via Pulse Diagnosis\n11.1\n20.6\nSmall\nDisease Classification via Smell Analysis\n14.3\n48.8\nSound\nCOVID-19 Detection from Cough Sound\n50.0\n58.7\nCardiac Abnormality Detection from Heart Sound\n50.0\n62.9\nECG\nECG Abnormality Detection\n50.0\n71.5\nECG Heart Beat Classification\n20.0\n83.1\nTable 8: Ablation study results on ShizhenGPT pretraining.\nPerformance is evaluated across two TCM qualification ex-\nams and two TCM-specific visual benchmarks.\nModel\nTCM Expertise\nTCM Visual Capability\n2024 TCM\nPharmacist\n2024 TCM\nPhysician\nMedicinal\nRecognition\nVisual\nDiagnosis\nShizhenGPT-7B\n70.1\n76.4\n52.7\n64.5\nw/o Pretraining\n66.9 (↓3.2)\n68.8 (↓7.6)\n43.7 (↓9.0)\n59.3 (↓5.2)\nShizhenGPT-32B\n79.2\n78.4\n59.5\n66.0\nw/o Pretraining\n76.9 (↓2.3)\n72.5 (↓5.9)\n51.3 (↓8.2)\n62.7 (↓3.3)\nEvaluation of Signal Modalities\nTable 7 shows model’s\nperformance across signal modalities, such as smell, ECG,\nand pulse. The model consistently outperforms random base-\nlines, demonstrating its ability to unify diverse sensory inputs.\nRemarkably, it achieves 80% accuracy in pregnancy detection\nfrom pulse signals alone. These findings provide empirical\nsupport for the diagnostic relevance of traditional sensory\nmodalities in TCM.\nAblation Study on Pre-training\nWe conduct an ablation\nstudy to evaluate the role of TCM-specific pre-training. As\nshown in Table 8, it can be seen that extensive TCM pre-\ntraining substantially enhances the TCM expertise and visual\nunderstanding capabilities. This confirms that large-scale\nTCM training is effective for TCM-specific models.\nI've had a persistent cough for a month, often choking\nwith shortness of breath. ... Could you check my\ntongue coating, breath, and pulse?\nOdor\nPulse\nTongue\nThe tongue is dry and red with a tine, yellow peeled\ncoating, indicating yin deficiency with internal heat or\nhyperactive fire. The breath suggests possible lung\nissues, warranting careful evaluation of pulmonary\nconditions. The pulse indicates a likely pergnancy, so\ncontraindicated medications must be avoided. Based on\nthe symptoms, ...\nI'm a middle-aged man, and lately, I’ve been feeling\nshort of breath whenever I do any physical activity...\nIn Traditional Chinese Medicine (TCM), your\nsymptoms suggest a complex pattern involving ...\nI'm 38 years ole, and lately, I've been feeling a\ndeep, lingering heat ... Please check my tongue\ncoating to get more information.\nFrom the perspective of Traditional Chinese Medicine\n(TCM), the tongue appears slightly red with a thin ...\nFigure 4: Examples responses from ShizhenGPT-32B. Full\noutputs are provided in Appendix 10.\nCase Study\nFigure 4 shows example responses from\nShizhenGPT-32B under multimodal inputs. The model can\ninterpret user-provided photos, voice, and pulse signals, cov-\nering full TCM diagnostic scenarios and improving decision\nquality. These capabilities bring AI interaction closer to real-\nworld clinical practice, highlighting ShizhenGPT’s potential\nas a practical assistant in intelligent TCM diagnostics.\nConclusion\nIn this work, we introduce ShizhenGPT, the first multimodal\nLLM tailored for TCM, along with an extensive TCM dataset.\nExperiments show that ShizhenGPT surpasses existing LLMs\nof a similar scale. It leads in visual TCM tasks and demon-\nstrates strong multimodal perception, including sound, pulse,\nand smell. This extends its diagnostic ability beyond text, en-\nabling direct analysis features such as the tongue, pulse, and\nbreath sounds, for richer clinical insights. This work moves\ntoward more holistic medical AI and aims to inspire further\nintegration of TCM and AI.\n\nAcknowledgments\nThis work was supported by Shenzhen Medical Research\nFund (No.C2406002) from the Shenzhen Medical Academy\nof Research and Translation (SMART), the Shenzhen Science\nand Technology Program (JCYJ20220818103001002), Shen-\nzhen Doctoral Startup Funding (RCBS20221008093330065),\nTianyuan Fund for Mathematics of National Natural Science\nFoundation of China (NSFC) (12326608), Shenzhen Science\nand Technology Program (Shenzhen Key Laboratory Grant\nNo. ZDSYS20230626091302006), Shenzhen Stability Sci-\nence Program 2023, and National Natural Science Foundation\nof China (NSFC) (72495131)."}
{"id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "authors": ["Ziyang Luo", "Zhiqi Shen", "Wenzhuo Yang", "Zirui Zhao", "Prathyusha Jwalapuram", "Amrita Saha", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Junnan Li"], "source": "http://arxiv.org/abs/2508.14704v1", "page_count": 31, "text": "Salesforce AI Research\n2025-08-21\nMCP-Universe\nMCP-Universe: Benchmarking Large Language Models with\nReal-World Model Context Protocol Servers\nZiyang Luo∗\nZhiqi Shen∗\nWenzhuo Yang∗\nZirui Zhao\nPrathyusha Jwalapuram\nAmrita Saha\nDoyen Sahoo\nSilvio Savarese\nCaiming Xiong\nJunnan Li\nSalesforce AI Research\n§ https://github.com/SalesforceAIResearch/MCP-Universe\n https://mcp-universe.github.io\nAbstract\nThe Model Context Protocol (MCP) has emerged as a transformative standard for connecting large\nlanguage models (LLMs) to external data sources and tools, rapidly gaining adoption across major\nAI providers and development platforms. However, existing benchmarks are overly simplistic and\nfail to capture real application challenges such as long-horizon reasoning and large, unfamiliar\ntool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction\nwith real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different\nMCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design,\nBrowser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-\nbased evaluators, including format evaluators for agent format compliance, static evaluators for time-\ninvariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth\nfor temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even\ntop-performing models such as GPT-5 (43.72% success rate), Grok-4 (33.33% success rate) and\nClaude-4.0-Sonnet (29.44% success rate) exhibit significant performance limitations. In addition,\nour benchmark poses a significant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-\ntools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers.\nNotably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct\nframeworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers\nwhile fostering innovation in the rapidly evolving MCP ecosystem.\n1\nIntroduction\nThe Model Context Protocol (MCP), introduced by Anthropic [1], represents a major paradigm shift in how AI systems\ninterface with external data sources and tools. Dubbed the “USB-C of AI”[2], MCP addresses the long-standing issue\nof fragmented, bespoke integrations that trap language models in isolated information silos[3]. Since its release, MCP\nhas gained rapid traction: major AI providers, including OpenAI [4] and Google Gemini [5], have committed to\nadoption, while development platforms such as Cursor [6] and Cline [7] have begun integrating it to enhance their\nproducts. Despite the transformative potential of MCPs, current evaluations remain insufficient for assessing the true\ncapabilities of LLMs operating within this new paradigm. Existing benchmarks predominantly focus on isolated\naspects of LLM performance, such as instruction following [8], math reasoning [9], or function calling [10], without\nproviding a comprehensive assessment of how models interact with real-world MCP servers across diverse scenarios.\nRecently, MCP-RADAR [11] adapts existing benchmarks such as HumanEval [12] and GSM8k [9] to the MCP\ncontext. However, these adaptations are largely derivative of existing datasets, which neither capture the full breadth\nof real-world applications nor adequately address issues such as data leakage. MCPWorld [13], another recently\nproposed benchmark, continues to rely heavily on graphical user interfaces (GUIs) and exhibits insufficient coverage\nof MCP-enabled workflows, thereby limiting its utility for evaluating LLMs in real MCP-driven environments.\nTo address these critical limitations, we introduce our MCP-Universe, a benchmark aiming at evaluating LLMs in\nrealistic, challenging use cases with real-world MCP servers. As shown in Figure 1, MCP-Universe captures realistic\n∗Equal Contributions.\n1\n\nSalesforce AI Research\n2025-08-21\nUser Instruction\n(Example is on the right.)\nAgent (e.g. GPT4.1)\nMCP Servers\nActions\nExecution-Based\nEvaluator\nFinal States\nI live in the Sheraton Jumeirah Beach Resort in Dubai. My colleague lives in the\nOryx Hotel in Abu Dhabi. We need to find a meeting point between our two\nlocations. Find a single 'restaurant' (must be of type 'restaurant') such that the\nestimated driving time from the Sheraton Jumeirah Beach Resort to the restaurant\nis as close as possible to the driving time from the Oryx Hotel to the restaurant.\nThe restaurant must have a minimum user rating of 4.2. Report the Name and\nPlace ID of the restaurant.\nNow I have the coordinates for both hotels: Sheraton Jumeirah Beach Resort:\n25.0731478, 55.128854 (Dubai) and Oryx Hotel: 24.4691699, 54.3408767 (Abu\nDhabi). Let me search for restaurants in the area between these two locations. I'll\nfocus on the midpoint area and find restaurants with good ratings.\nI'll help you find a restaurant that's equidistant (in driving time) between the\nSheraton Jumeirah Beach Resort in Dubai and the Oryx Hotel in Abu Dhabi. Let me\nstart by getting the coordinates for both hotels and then search for restaurants\nalong the route between them.\n{ \"address\": \"Sheraton\nJumeirah Beach Resort,\nDubai\" }\n{ \"location\": { \"lat\": 25.0731478, \"lng\":\n55.128854 }, \"formatted_address\": \"Al\nMamsha St - Dubai Marina - Dubai -\nUnited Arab Emirates\", ... }\nGreat! I found several restaurants with ratings ≥ 4.2. Now I need to calculate the\ndriving times from both hotels to each of these restaurants to find the one where\nthe driving times are closest to each other. Let me calculate the driving times for\nthe top-rated restaurants that meet our criteria.\nBased on my analysis, Rasoi Ghar Restaurant has the smallest time difference\nbetween the two locations, making it the most equidistant option in terms of\ndriving time.\nAnswer: Name: Rasoi Ghar Restaurant\nPlace ID: ChIJdbsVqrRFXj4RO7_A2D5K-tA\nThis restaurant provides the best compromise location between your two hotels,\nwith the driving time difference being the smallest among all qualifying\nrestaurants.\nParameters\nCalled\nMCP Tool\nmap_gecode\n{ \"address\": \"Oryx Hotel, Abu\nDhabi\" }\n{ \"location\": { \"lat\": 24.4691699, \"lng\":\n54.3408767 }, \"formatted_address\":\n\"Zayed the First Street, Near Emirates\nPost Office, Khalidiya Area ...}\nParameters\nResults\nCalled\nMCP Tool\nmap_gecode\n{\"query\": \"restaurant\",\n\"location\": { \"latitude\":\n24.77, \"longitude\": 54.73 },\n\"radius\": 50000 }\n{ \"places\": [ { \"name\": \"Rasoi Ghar\nRestaurant\", \"formatted_address\": \"Yas\nMall The Fountains @ Yas Mall Ferrari\nWorld Main Entrance - ...,\nParameters\nResults\nCalled\nMCP Tool\nsearch_places\nResults\n{\"origins\": [ \"25.073,55.128\"\n], \"destinations\": [\n\"24.485,54.607\",... ],\n\"mode\": \"driving\" }\n{ \"origin_addresses\": [ \"Sheraton\nJumeirah - Dubai Marina - Dubai - United\nArab Emirates\" ],\n\"destination_addresses\": [ \"33 Al\nKhuyoul St - Yas Island...\nResults\nCalled\nMCP Tool\ndistance_matrix\nParameters\nGoogle Map\nMCP Server\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nStep 6\nStep 7\nStep N\nObservations\nFigure 1: Example from MCP-Universe illustrating realistic challenges, including real-world tool usage, long-horizon\nmulti-turn tool calls, long context windows, scattered evidence, and large tool spaces. Unlike prior work, MCP-\nUniverse is grounded in real-world MCP servers connected to actual data sources and environments.\nchallenges: real-world tools usage, long-horizon multi-turn tool calls, long context windows, scattered evidence, and\nlarge tool spaces. Unlike existing works, MCP-Universe is grounded in real-world MCP servers that connect to actual\ndata sources and environments. Our benchmark encompasses 6 core domains, with 11 MCP servers spanning diverse\napplications: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation,\nand Web Searching, comprising a total of 231 tasks. Each domain captures the operational complexities of real-world\ndeployments, from handling authentic financial data and navigating complex geospatial information to managing\nversion control workflows and executing real-time ticket price checks.\nTo ensure rigorous evaluation, we carefully design execution-based evaluators rather than relying on LLM-as-a-\njudge [14] (e.g., MCPEval [15] and LiveMCPBench [16]), recognizing that many tasks involve real-time data that static\nLLM knowledge cannot properly assess. Our evaluation includes format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that automatically obtain real-time ground truth\nfor temporally sensitive tasks. Furthermore, for the evolving nature of MCP servers, we provide an extensible, user-\nfriendly framework that enables researchers and the broader community to seamlessly integrate new agents and MCP\nservers into the evaluation pipeline. Our benchmark is also equipped with a UI for intuitive, user-friendly access.\nWe conducted extensive experiments using MCP-Universe across all 6 core domains and 11 different MCP\nservers. Through extensive evaluation of leading LLMs, we find that even top-performing models such as GPT-\n5 (43.72% success rate), Grok-4 (33.33% success rate), and Claude-4.0-Sonnet (29.44% success rate) exhibit\nsignificant performance limitations, revealing a substantial gap between their impressive general capabilities and their\neffectiveness in real-world MCP environments. Our comprehensive analysis identifies several fundamental challenges\nthat current LLM agents face in MCP interactions. First, we observe a long-context challenge, as the number of tokens\nincreases rapidly with the number of interaction steps, often leading to context overflow and degraded performance in\nmulti-step tasks. Second, there exists an unknown-tools challenge, where LLM agents often lack familiarity with the\nprecise usage patterns, parameter specifications, and expected behaviors of diverse MCP servers. Additionally, our\nevaluation reveals significant cross-domain performance variations, with models showing markedly different success\nrates across different application domains, suggesting domain-specific optimization needs. Notably, enterprise-level\nagents like Cursor cannot achieve better performance than standard ReAct frameworks, highlighting the challenges of\nour benchmark.\nIn summary, this work makes the following key contributions:\n• We introduce MCP-Universe, the first comprehensive benchmark for evaluating LLMs in MCP environments\nacross six domains with real-world servers, where even SOTA LLMs struggle.\n• We develop a rigorous execution-based evaluation framework with format, static, and dynamic assessment\ncapabilities to enable comprehensive real-world performance measurement.\n• We reveal fundamental limitations of current LLM agents, such as challenges with long contexts, handling\nunknown tools, and cross-domain discrepancies, thereby highlighting directions for future agent design.\n2\n\nSalesforce AI Research\n2025-08-21\n2\nRelated Work\nModel Context Protocol. MCP was introduced by Anthropic in late 2024 as an open standard designed to streamline\nAI system integration with external data sources and tools [1]. MCP addresses the notorious “data silo problem” by\nproviding a universal interface that connects AI systems to diverse data sources and tools through standardized JSON-\nRPC 2.0 messaging over STDIO and SSE transports [3]. The protocol follows a three-layer architecture comprising\nMCP hosts (AI applications), clients (communication bridges), and servers (capability providers) that expose tools,\nprompts, and resources through standardized interfaces.\nLLMs as Agents. LLMs have evolved from simple text-generating chatbots into sophisticated agents capable of\nautonomous planning, reasoning, and tool use [17]. This transformation has been driven by advances in instruction\nfollowing [8, 18–20], multi-step reasoning [21–24], and tool integration [25–28].\nKey agent design patterns\nhave emerged, such as ReAct [29], Reflection [30], and Plan-and-Solve [31].\nFrameworks like AutoGen [32],\nMetaGPT [33], Camel-AI [34], and LangGraph [35] have demonstrated practical implementations of autonomous\nagent systems. With the advancement of multimodal LLMs [36, 37], a new class of GUI-based computer-use agents\nhas emerged [38–41]. Systems such as OpenAI’s CUA [42], Anthropic’s Computer-Use [43], and ByteDance’s UI-\nTaris [44] mimic human interaction with graphical interfaces, giving rise to a new frontier in computer automation\napplications.\nTable 1: Comparative Analysis of Contemporary MCP\nBenchmarks.\nBenchmark\nReal-World\nIntegration\nTemporal\nDynamics\nExec.\nEval.\nMCPWorld\n✓\n×\n✓\nMCP-RADAR\n×\n×\n✓\nMCPEval\n×\n✓\n×\nLiveMCPBench\n✓\n✓\n×\nMCP-Universe\n✓\n✓\n✓\nEvaluation of Agents.\nThe evaluation of LLM-\nbased\nagents\nhas\nbecome\na\nkey\nresearch\narea,\nwith a variety of benchmarks developed to assess\ndifferent aspects of agent capabilities.\nOne major\nfocus\nis\nweb\nnavigation,\nwith\nbenchmarks\nsuch\nas MiniWob++ [45],\nMind2Web (1 & 2) [46, 47],\nWebLINX [48], AssistantBench [49], WebArena [50],\nVisualWebArena, and VideoWebArena [51] providing\ncomprehensive environments to test agents’ ability to\ninteract with realistic web applications. A second focus\nis GUI-based interaction, exemplified by OSWorld [52],\nWindowsAgentArena [53], and UI-Vision [54], which\nevaluate agents on their ability to operate computer interfaces similarly to human users. A third area is software\nengineering, with benchmarks like SWE-bench [55] and DevBench [56] designed to evaluate agents’ capabilities in\ncode generation and software development tasks. Another important dimension is function/tool calling, covered by\nAPIBank [57], ToolBench [58], GAIA [59], AppWorld [60], τ-Bench [61], and BFCLv3 [10], which assess an agent’s\nproficiency in invoking external tools and APIs to complete complex tasks.\nIn contrast to existing benchmarks, several contemporary MCP-related benchmarks have recently emerged, as shown\nin Table 1. MCPWorld [13] evaluates agents in real-world GUIs and the MCP environment, but it relies heavily on the\nGUI and does not include time-varying tasks. MCP-RADAR [11] transforms existing datasets, such as HumanEval\nand GSM8k, into MCP scenarios. Although it includes execution-based evaluation, its tasks are less related to real-\nworld applications, and the ground truth does not change over time. The latter two benchmarks, MCPEval [15]\nand LiveMCPBench [16], both adopt the LLM-as-a-Judge evaluation, which is not suitable for tasks requiring real-\ntime knowledge; moreover, LLM judges are also known for style bias [62].\nIn contrast to these works, MCP-\nUniverse provides a comprehensive evaluation framework that integrates real-world integration, temporal dynamics,\nand execution-based evaluation. It evaluates agents on authentic MCP servers with time-sensitive scenarios and actual\ntask completion metrics, addressing the limitations of existing benchmarks.\n3\nMCP-Universe\n3.1\nOverview\nMCP-Universe is a comprehensive evaluation framework designed to assess the capabilities of LLMs when interacting\nwith real-world MCP servers for challenging and practical use cases.\nAs shown in Figure 2, our benchmark\nencompasses three core components: (1) an extensible, easy-to-use evaluation framework; (2) a collection of carefully\ndesigned task instructions grounded in real-world MCP server scenarios; (3) a suite of execution-based evaluators for\nmeasuring task completion.\n3\n\nSalesforce AI Research\n2025-08-21\nTask, MCP Server, Evaluator Configuration\nI live in the Sheraton\nJumeirah Beach Resort in\nDubai. My colleague lives in\nthe Oryx Hotel in Abu Dhabi.\nWe need to find a meeting\npoint between our two\nlocations. Find a single\n'restaurant' (must be of type\n'restaurant') such that the\nestimated driving time from\nthe Sheraton Jumeirah Beach\nResort to the restaurant is as\nclose as possible to the\ndriving time from the Oryx\nHotel to the restaurant. The\nrestaurant must have a\nminimum user rating of 4.2.\nReport the Name and Place ID\nof the restaurant.\nGPT-4.1\nClaude-4.0\nOthers ...\nLLM Manager\nReAct\nExplore\nReAct\nOthers ...\nAgent Builder\nGoogle Map\nGithub\nBlender\nMCP Server\nPlaywright\nOthers ...\nContent\nMatch\nFormat\nCheck\nOthers\n...\nExecution-Based Evaluator\nSTDIO\nSSE\nGoogle Map\nMCP Server\nBased on my analysis,\nRasoi Ghar Restaurant\nhas the smallest time\ndifference between the\ntwo locations, making it\nthe most equidistant\noption in terms of\ndriving time.\nAnswer: Name: Rasoi\nGhar Restaurant\nPlace ID:\nChIJdbsVqrRFXj4RO7_A\n2D5K-tA\ngoogle_maps.is_a_validate_stop\ngoogle_maps.validate_stop_type\ngoogle_maps.validate_stop_type\ngoogle_maps.compare_time_...\ngoogle_maps.stop_include_keys\nEvaluator\nFinal State\nPass or Not: Fail\nAgent (e.g. GPT4.1)\nUser Instruction\nFigure 2: Overview of the MCP-Universe evaluation framework. The framework dynamically configures LLM agents,\nMCP servers, and execution-based evaluators according to task specifications. Each evaluation involves the agent-\nserver interactions mediated via the MCP protocol, followed by an objective assessment conducted by automated\nexecution-based evaluators to determine the success of task completion.\nTo formalize the setting, we model the benchmark as follows. Let S = {s1, s2, . . . , sk} denote the collection of MCP\nservers, where each server si exposes a set of tools Ti = {ti,1, ti,2, . . . , ti,|Ti|} through the MCP protocol. A task τ is\ndefined as a tuple (G, C, Tavailable), where:\n• G is the goal specification describing the desired outcome;\n• C contains the initial context and any relevant background information;\n• Tavailable = S\ni∈I Ti is the set of tools accessible for the task, with I ⊆{1, . . . , k} indicating which servers\nare available.\nThe benchmark challenges an agent to identify, sequence, and invoke appropriate tools from Tavailable to achieve G\ngiven C, requiring reasoning over partial information, adapting to diverse tool interfaces, and handling ambiguities or\nfailures in tool responses.\nFor evaluation, let M = {m1, m2, . . . , mn} be the set of language models and A = {a1, a2, . . . , ap} be the set of\nagent architectures (e.g., ReAct) that can be paired with them. For a given (m, a) ∈M × A and task τ, the interaction\nproduces a conversation trace R = (r1, r2, . . . , rT ), where each rt contains the agent’s output and any tool invocations.\nThe evaluation function\nE : M × A × T →{0, 1}\nassigns 1 if the task is successfully completed according to predefined success criteria, and 0 otherwise. Success is\ndetermined through a combination of automated checks (e.g., verifying structured outputs or end states). Aggregating\nE(m, a, τ) over all tasks yields a quantitative measure of an agent’s proficiency in MCP-driven tool use.\n3.2\nEvaluation Framework\nAs illustrated in Figure 2, our evaluation framework seamlessly coordinates multiple components to deliver objective\nand reproducible assessment results.\nGiven the task specifications, the framework begins with an automatic\nconfiguration that dynamically orchestrates the evaluation pipeline. The framework automatically builds the LLM-\nagent combination, selects the required MCP servers, and configures the corresponding evaluators. The configuration\nalso handles resource allocation, API endpoint management, and evaluation criteria specification.\nIn detail, an LLM Manager is introduced in the framework, which supports multiple SOTA LLMs including GPT-5\nand Claude-4.0-Sonnet. This component handles LLM configuration, API management, and standardized prompt\nformatting to ensure consistent evaluation conditions across different models.\nIn addition, the Agent Builder\ncomponent constructs specialized agents. It supports multiple agents including ReAct and ReAct with Exploration.\nThe builder configures agents with appropriate reasoning strategies for MCP server communication. This modular\ndesign allows for systematic comparison of different agents within the same evaluation framework.\nMoreover, our framework seamlessly integrates with diverse MCP servers representing real-world tools and services.\nEach server is configured with its authentic API endpoints, authentication mechanisms, and tool specifications,\nensuring that evaluation tasks mirror real-world environments rather than simplified simulations. The framework\n4\n\nSalesforce AI Research\n2025-08-21\nLocation Navigation 19.3%\nDist. Optimization 8.9%\nTime Optimization 3.9%\nRoute Planning 4.3%\nPlace Finding 2.2%\nRepo. Management 14.2%\nProject Setup 5.6%\nIssue Automation 3%\nRepo. Analysis 3%\nRepo. Forking 2.6%\nFinancial Analysis 17.2%\nPortfolio Analysis 3.4%\nFin. Statements 3.4%\nTrading Strategies 6.4%\nInst. Holdings 2.2%\nDividend Options 1.8%\n3D Designing 8.6%\nObject Creation 2.6%\nMaterial Setup 2.6%\nLighting Conﬁg. 1.3%\nRender Settings 2.1%\nBrowser Automation 16.7%\nTravel Booking 3.4%\nSports Analytics 3.9%\nAcademic Research 3.4%\nPlatform Exploration 3.4%\nMap Navigation 2.6%\nWeb Searching 24%\nPerson Ident. 8.6%\nEntity Discovery 7.3%\nMetric Matching 1.7%\nComplex Reason 3.9%\nFactual Lookup 2.5%\nFigure 3: Distribution of tasks in MCP-Universe across\ndifferent application domains.\nTable 2: Key statistics in MCP-Universe.\nStatistic\nNumber\nTotal tasks\n231 (100%)\n- Location Navigation\n45 (19.5%)\n- Web Searching\n55 (23.8%)\n- Browser Automation\n39 (16.9%)\n- 3D Designing\n19 (8.2%)\n- Financial Analysis\n40 (17.3%)\n- Repo. Management\n33 (14.3%)\nTotal MCP Servers\n11\nTotal Tools in Servers\n133\nTotal Unique Evaluators\n84 (100%)\n- Format Evaluators\n4 (4.8%)\n- Static Evaluators\n32 (38.1%)\n- Dynamic Evaluators\n48 (57.1%)\nhandles dynamic server configuration based on task specification, allowing for both single-server and multi-server\nevaluation scenarios.\nThe evaluation process employs an execution-based approach that validates task completion through automated\nassessment rather than subjective LLMs judgment or costly human annotation. The evaluator implements domain-\nspecific validation strategies including stop type validation for Google Maps and branch checking for GitHub. The\ncomplete evaluation workflow proceeds through agent-server interactions mediated via the MCP protocol, followed\nby automated assessment that produces binary pass/fail determinations based on objective validation criteria. The\nframework captures detailed interaction logs to provide comprehensive insights into model performance across\ndifferent scenarios and server types.\n3.3\nReal-World MCP Servers\nA foundational design principle of MCP-Universe is its reliance on real-world MCP servers, as opposed to\nsimulated environments. This approach ensures that evaluation is grounded in the authentic complexities of practical\napplications. As shown in Table 2, the benchmark includes 11 distinct MCP servers with total 133 tools, each reflecting\na unique application domain. These servers are organized into 6 core domains:\n1. Location Navigation: This domain focuses on geographic reasoning and spatial task execution in real-world\nenvironments. We employ the official Google Maps MCP server, which provides a rich suite of geospatial\ntools such as location search, route planning, and distance computation. Models must navigate the full\ncomplexity of real-world location data to complete navigation tasks effectively.\n2. Repository Management: This domain focuses on repository development and codebase operations. We\nemploy the GitHub MCP server, which exposes authentic version control tools such as repository search,\nissue tracking, and code editing. This setup reflects the operational demands of real-world development.\n3. Financial Analysis:\nThis domain focuses on quantitative reasoning and decision-making in dynamic\nfinancial markets. We utilize the Yahoo Finance MCP server, which provides tools such as stock price\nmonitoring, shareholder information checking, and options tracking, all based on live financial data. The\nserver poses substantial challenges in parsing and reasoning over real-time market information.\n4. 3D Designing: This domain focuses on computer-aided design. We employ the Blender MCP server, which\nenables interaction with advanced tools for 3D modeling, such as object creation, asset manipulation, and\nmaterial setup. This domain captures the technical depth of professional design environments.\n5. Browser Automation: This domain focuses on automated interaction with web applications and interfaces.\nWe employ the Playwright MCP server, which provides full-featured web automation capabilities such as\n5\n\nSalesforce AI Research\n2025-08-21\nbrowser navigation, button clicking, and page snapshotting. It represents real-world browser control scenarios\nencountered in modern automation pipelines.\n6. Web Searching: This domain focuses on open-domain information seeking. The Google Search MCP server\nis integrated to support web-based information seeking, and the Fetch MCP server is adopted to obtain the\ncontent for a given URL. This domain captures the open-ended nature of real-world web searching tasks.\nThe selection of these domains and specific servers reflects our commitment to domain diversity and real-world\nrelevance. In addition to the above MCP servers, we also incorporate additional MCP servers that provide necessary\nsupport and contribute to increasing task complexity, such as the Notion MCP server, Weather MCP server, Date MCP\nserver, and Calculator MCP server. More details on these MCP servers can be found in Appendix A.\n3.4\nTasks and Evaluators\nSince MCP is a new concept and there is a lack of high-quality usage examples, we manually designed challenging\nMCP tasks to reflect real use cases. If a task can be easily completed by LLMs without using MCP servers, or can be\nconsistently solved with MCP servers within five retries, we consider it a simple task and brainstorm a new one. As\nshown in Figure 3, for each domain, we carefully create 4-5 types of tasks to cover the most common usage scenarios.\nFor the location navigation domain, we focus on 4 sub-task types, including route planning, optimal stops, location\nsearching, and place finding. For the repository management domain, we focus on 4 sub-task types, including project\nsetup, issue tracking, automation setup, and code integration. For the financial analysis domain, we focus on 5 sub-\ntask types, including portfolio analysis, financial statements, trading strategies, institutional holdings, and dividend\nanalysis. For the 3D designing domain, we focus on 5 sub-task types, including object creation, material setup,\nlighting configuration, render settings, and scene hierarchy. For the browser automation domain, we focus on 5 sub-\ntask types, including travel booking, sports analytics, academic research, platform exploration, and map navigation.\nFor the web searching domain, we focus on 5 sub-task types, including person identification, entity discovery, metric\nmatching, complex reasoning, and factual lookup. After the task creation, each task will be cross-checked by the other\nauthors for feasibility, ambiguity, and correctness.\nTo evaluate the completion of tasks, we have chosen to carefully design execution-based evaluators for each task.\nFor simplicity, many recent works choose to follow the LLM-as-a-judge paradigm. However, we argue that this\nparadigm is not well-suited for our MCP-Universe scenario, since some tasks are designed to use real-time data,\nwhile the knowledge of the LLM judge is static. In addition, the LLM judge is also known to contain style bias and\nhallucinations. Thus, we choose to follow the execution-based paradigm to evaluate the completion of tasks. Although\nthis relies on much heavier human labor, we believe this is the only way to achieve a fair and comprehensive evaluation.\nFor all tasks, we can divide the evaluators into three types: (1) Format Evaluators, (2) Static Evaluators, and (3)\nDynamic Evaluators. The first type evaluates whether agents strictly follow format requirements. The second type\nassesses correctness for tasks whose answers do not change over time, such as the number of cities in route planning\ntasks, the exact number of goals scored by a football player in browser automation tasks, or historical stock prices\nin financial analysis tasks. For these, we manually collect the correct answers and write evaluators to check whether\nthe model’s outputs meet the requirements. For the third type, the correct answer of the task needs to be updated\nwith real-time data, such as the price of a flight on a future date for the travel booking tasks, the weather of a place\nin the place-finding tasks, and the number of GitHub issues in the issue tracking tasks. Here, we design automatic\nevaluators to obtain real-time correct answers and verify task completion, which can provide stable evaluation results\nacross different timestamps. After the evaluator creation, each evaluators will be cross-checked by the other authors\nfor feasibility, ambiguity, and correctness. In Appendix B, we include the examples of tasks and evaluators in our\nbenchmark.\n4\nExperiment\n4.1\nSetup\nIn our experiments, we evaluate the performance of SOTA proprietary and open-source LLMs on our MCP-Universe\nbenchmark. The models include xAI’s Grok-4 [63], Anthropic’s Claude-4.0-Sonnet [64] and Claude-3.7-Sonnet [65],\nOpenAI’s GPT-5 [66], o3, o4-mini [67],GPT-5 [66], GPT-4.1 [68], GPT-4o [36], GPT-OSS [69], Google’s Gemini-2.5-\nPro and Gemini-2.5-Flash [70], Zai’s GLM-4.5 [71], Moonshot’s Kimi-K2 [72], Qwen’s Qwen3-Coder, and Qwen3-\n235B-A22B-Instruct-2507 [73], and DeepSeek’s DeepSeek-V3-0324 [74]. All LLMs are top-ranked on the well-\nknown lmsys Chatbot Arena leaderboard [75]. All open-source LLMs have at least 120B parameters. For the agent,\n6\n\nSalesforce AI Research\n2025-08-21\nTable 3: Comparison on our MCP-Universe benchmark. For our main experiments, all LLMs follow the ReAct\nagent pipeline, except GPT-OSS, which has poor instruction-following abilities and therefore cannot follow the ReAct\nprompt; for this model, we use the OpenAI Agent SDK instead. We report the success rate (SR, %) for each domain\nand all tasks. Additionally, we calculate the average percentage of evaluators passed for each task, which we refer to\nas the average evaluator score (AE). Moreover, we also report the average number of steps (AS) for each successful\ntask. Since GPT-OSS does not follow ReAct, it does not have AS scores.\nModel\nLocation\nNavigation\nRepository\nManagement\nFinancial\nAnalysis\n3D\nDesigning\nBrowser\nAutomation\nWeb\nSearching\nOverall\nSR\nAE\nAS\nProprietary Models\nGPT-5\n33.33\n30.30\n67.50\n52.63\n35.90\n45.45\n43.72\n60.23\n8.22\nGrok-4\n28.89\n12.12\n40.00\n26.32\n41.03\n41.82\n33.33\n49.01\n7.75\nClaude-4.0-Sonnet\n22.22\n12.12\n55.00\n26.32\n38.46\n21.82\n29.44\n50.61\n7.46\no3\n26.67\n6.06\n40.00\n26.32\n25.64\n29.09\n26.41\n38.95\n4.82\no4-mini\n26.67\n18.18\n40.00\n36.84\n23.08\n18.18\n25.97\n40.38\n7.90\nClaude-3.7-Sonnet\n13.33\n18.18\n40.00\n36.84\n23.08\n21.82\n24.24\n40.36\n7.16\nGemini-2.5-Pro\n13.33\n12.12\n50.00\n21.05\n25.64\n12.73\n22.08\n36.93\n6.98\nGemini-2.5-Flash\n15.56\n12.12\n37.50\n21.05\n30.77\n14.55\n21.65\n33.99\n8.26\nGPT-4.1\n8.89\n6.06\n40.00\n26.32\n23.08\n10.91\n18.18\n41.32\n5.24\nGPT-4o\n8.89\n9.09\n35.00\n26.32\n12.82\n9.09\n15.58\n37.03\n6.03\nOpen-Source Models\nGLM-4.5\n17.78\n9.09\n50.00\n26.32\n15.38\n27.27\n24.68\n41.16\n7.33\nKimi-K2\n11.11\n9.09\n47.50\n15.79\n15.38\n14.55\n19.05\n35.10\n6.07\nQwen3-Coder\n8.89\n3.03\n50.00\n26.32\n25.64\n10.91\n19.91\n37.78\n7.78\nQwen3-235B\n11.11\n9.09\n50.00\n15.79\n15.38\n9.09\n18.18\n38.53\n5.74\nDeepSeek-V3\n11.11\n6.06\n30.00\n26.32\n12.82\n7.27\n14.29\n35.82\n5.06\nGPT-OSS-120B\n6.67\n6.06\n35.00\n10.53\n5.13\n5.45\n11.26\n26.34\n-\nwe adopt the most popular framework, ReAct [29]. The LLMs first generate a thought based on the observation, and\nthen generate the next action based on that thought. More setup details can be found in the Appendix C.\n4.2\nFrontier Models Performance\nTable 4: Success rate across different types of evaluators on\nour MCP-Universe benchmark.\nModel\nFormat\nStatic\nDynamic\nGPT-5\n88.89\n61.92\n65.96\nGrok-4\n88.03\n49.04\n52.98\nClaude-4.0-Sonnet\n98.29\n61.92\n54.74\no3\n73.50\n38.63\n43.16\no4-mini\n78.63\n44.66\n43.86\nClaude-3.7-Sonnet\n83.76\n43.84\n44.91\nGemini-2.5-Pro\n64.10\n39.18\n42.46\nGemini-2.5-Flash\n51.28\n45.21\n30.88\nGPT-4.1\n95.73\n57.53\n49.47\nGPT-4o\n91.45\n54.79\n45.61\nGLM-4.5\n81.20\n46.30\n48.07\nKimi-K2\n70.94\n33.15\n53.33\nQwen3-Coder\n75.86\n38.74\n43.16\nQwen3-235B\n92.31\n43.29\n53.68\nDeepSeek-V3\n96.58\n52.88\n48.07\nAs shown in Table 3, we compare the performance\nof SOTA proprietary and open source LLMs on\nour MCP-Universe benchmark.\nThe results indicate\nthat OpenAI’s GPT-5 achieves the highest overall\nsuccess rate at 43.72%, significantly outperforming other\nmodels.\nGrok-4 ranks second with 33.33% success\nrate, followed by Claude-4.0-Sonnet at 29.44%. GPT-\n5 demonstrates particularly strong performance in the\nFinancial Analysis domain (67.50%) and 3D Designing\ndomain (52.63%), while also achieving the highest\nsuccess rate in Web Searching (45.45%).\nNotably,\nGrok-4 excels in Browser Automation (41.03%) domain,\nwhich requires strong reasoning and the ability to\noperate in complex internet environments.\nIn the\nLocation Navigation domain, all LLMs perform poorly,\nwith success rates below 35%; notably, popular models\nlike GPT-4.1 and GPT-4o score under 10%. Similarly,\nin the Repository Management domain, only GPT-5\nsurpasses a 30% success rate. In the 3D Design domain,\nonly GPT-5 exceeds a 50% success rate, while other top models remain below 40%. When comparing proprietary and\nopen-source models, we find that GLM-4.5 is the best open-source LLM with 24.68% overall success rate, achieving\na higher success rate than some proprietary models like o4-mini and Claude-3.7-Sonnet. However, the gap between\nSOTA proprietary LLMs and their open-source counterparts remains substantial.\nBeyond success rates, we also evaluate each model based on (i) the percentage of evaluators they can satisfy, measured\nvia average evaluator (AE) scores, and (ii) the average number of steps (AS) taken to complete successful tasks. While\nhigher AE scores often correlate with higher success rates, the relationship is not always direct. GPT-5 achieves both\n7\n\nSalesforce AI Research\n2025-08-21\n2\n4\n6\n8\n10\n12\n14\n16\nStep Number\n0\n20000\n40000\n60000\n80000\nAverage Input Tokens\nLN\nBA\nFA\nLocation Navigation\nBrowser Automation\nFinancial Analysis\n0\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate (%)\n8.89\n20.0022.2224.44\n23.08\n20.51\n38.46\n30.77\n40.0040.00\n55.00\n42.50\nGPT-4.1\nGPT-4.1 w/ summarize\nClaude-4.0-Sonnet\nClaude-4.0-Sonnet w/ summarize\nFigure 4: (Left) Growth of average context length (in tokens) as the number of interaction steps increases in MCP-\nUniverse tasks, illustrating the long context challenge. (Right) Effect of introducing a summarization agent on LLM\nagent performance across selected domains.\nthe highest success rate (43.72%) and the highest AE score (60.23%), demonstrating strong consistency. However,\nsome models show interesting discrepancies: Claude-4.0-Sonnet passes 50.61% of evaluators, slightly higher than\nGrok-4’s 49.01%, yet Grok-4 achieves a higher overall success rate (33.33% vs. 29.44%). Regarding task efficiency,\no3 requires only 4.82 average steps despite ranking fourth in success rate (26.41%), making it the most efficient model.\nIn contrast, the top-performing models GPT-5 (8.22 steps) and Grok-4 (7.75 steps) require significantly more steps.\nMost open-source models complete successful tasks in 5-7 steps, with DeepSeek-V3 being the most efficient at 5.06\nsteps. These findings highlight that current frontier LLMs still fall short in reliably executing tasks across diverse\nreal world MCP tasks. Our MCP-Universe benchmark therefore provides a challenging and necessary testbed for\nevaluating LLM performance in areas underserved by existing benchmarks.\nFurthermore, our benchmark incorporates three evaluator types: format evaluators, static evaluators, and dynamic\nevaluators. Table 4 presents a breakdown of model performance across these types. Non-reasoning LLMs, such\nas Claude-4.0-Sonnet,1 GPT-4.1, GPT-4o, Qwen3-235B, and DeepSeek-V3, achieve over 90% success with format\nevaluators, with Claude-4.0-Sonnet leading at 98.29%. In contrast, reasoning models like o3 (73.50%), Gemini-2.5-\nPro (64.10%), and Gemini-2.5-Flash (51.28%) perform significantly worse on format evaluators, suggesting that such\nmodels are less adept at adhering to strict formatting instructions. We highlight the naive error in the Appendix D.\nOn content-sensitive static evaluators, GPT-5 and Claude-4.0-Sonnet both achieve the highest performance at 61.92%,\nwhile most other models achieve around 40-50% success. For dynamic evaluators, GPT-5 leads with 65.96%, followed\nby Claude-4.0-Sonnet at 54.74%. The substantial performance gap between format evaluators (where many models\nexceed 80%) and content evaluators (where most models achieve 40-60%) indicates that the primary source of failure\nlies in content generation rather than format compliance. This demonstrates that our benchmark evaluates LLMs\nfrom multiple angles, including format compliance and content correctness under both static and dynamic conditions,\nmaking it a comprehensive testbed for model assessment.\n4.3\nLong Context Challenges\nIn our MCP Universe benchmark, long context handling poses a significant challenge for LLM agents, particularly in\nthe Location Navigation, Browser Automation, and Financial Analysis domains. These domains frequently require\nagents to process and reason over lengthy sequences of observations or historical actions, which often exceed the\ncontext window limits of many models. In the Location Navigation domain, the Google Maps MCP servers can\nreturn extensive location data within the context, including detailed information about multiple places. In the Browser\nAutomation domain, the Playwright MCP servers may return the full HTML content of a webpage, which can be very\nlarge. Similarly, in the Financial Analysis domain, the Yahoo Finance MCP servers provide daily stock information\nover a specified date range, and this can result in a large volume of contextual data if the time span is long.\nAs shown in the left of Figure 4, we observe that the number of tokens increases rapidly as the number of interaction\nsteps grows. This demonstrates that long context is one of the key challenges presented by our benchmark.2 To explore\npotential solutions, we conducted a preliminary experiment (the right of Figure 4) by introducing a summarization\nagent at each step, designed to compress the raw outputs of the MCP servers. This summarizer attempts to reduce\ncontext length while preserving essential information. The details of this summarization agent are shown in the\nAppendix E. However, this approach yields mixed results. While it leads to improved success rates for GPT-4.1 and\n1We do not use the thinking mode.\n2The context length experiment is based on the Claude-4.0-Sonnet.\n8\n\nSalesforce AI Research\n2025-08-21\nI heard about this Price Breakout Strategy thing on some YouTube video. From\nwhat I gathered, you look at the highest closing price in the last 20 days, and if the\nprice today goes above that, you buy. If it drops 3% below the recent high, you\nsell. I'm thinking about putting it all on Exxon Mobil (XOM). I need to know if\nDecember 1st, 2023 would've been a buy, sell, or hold signal with this strategy. I\ncan't afford to mess this up. This might be my only shot at making it. Please help\nme figure this out, I'm counting on this to change my life.\nThought: To determine whether December 1, 2023, would have been a buy, sell, or\nhold signal with this strategy, I need the closing price of XOM on that date. I will\nfirst try to retrieve the historical price for XOM.\nAction: Server: Yahoo Finance MCP Server, Tool: get_historical_stock_prices\nArguments: {\"ticker\": \"XOM\", \"start_date\": \"2023-12-01\", \"end_date\": \"2023-12-\n01\", \"interval\": \"1d\" }\nServer Return:\nError: getting stock information for XOM: The start date and end date cannot be\nthe same.\nLocation Navigation\nBrowser Automation\nFinancial Analysis\n0\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate (%)\n8.89\n20.0022.2224.44\n23.08\n30.77\n38.46\n33.33\n40.0040.00\n55.00\n62.50\nGPT-4.1\nGPT-4.1 w/ exploration\nClaude-4.0-Sonnet\nClaude-4.0-Sonnet w/ exploration\nFigure 5: (Left) An example of the unknown tool challenges. (Right) Effect of introducing the exploration phase on\nLLM agent performance across selected domains.\nClaude-4.0-Sonnet in the Location Navigation domain, it either has no effect or negatively impacts performance in the\nBrowser Automation and Financial Analysis domains. These findings indicate that MCP-Universe introduces unique\nand realistic long context challenges in agent-based tasks. Simple summarization methods are insufficient to address\nthese issues. As such, our benchmark serves as a valuable testbed for evaluating and developing long context handling\nin LLM agent systems.\n4.4\nUnknown Tools Challenges\nIn addition to the long context challenges, our error analysis reveals that LLMs often struggle to correctly use tools\nprovided by the MCP servers, indicating a lack of familiarity with their interfaces and constraints. For example,\nFigure 5 (left) illustrates a common failure in the Yahoo Finance MCP server: retrieving a stock price requires\nspecifying a start and end date that differ, yet LLMs frequently set them to be identical, leading to execution errors. To\naddress this issue, we introduce an additional step called the exploration phase. During this phase, the LLM is allowed\nto freely interact with the tools provided by the MCP servers. This gives the model an opportunity to learn how the\ntools work and to build knowledge of their capabilities. In the subsequent exploitation phase, the LLM uses this\nacquired tool knowledge, combined with a ReAct-style framework, to solve the actual tasks. The details are included\nin the Appendix F.\nAs shown in Figure 5 (right), incorporating an exploration phase leads to performance improvements in certain\ndomains. For instance, GPT-4.1 achieves a 30.77% success rate in the Browser Automation domain, an improvement\nof 7.69 percentage points. Similarly, Claude-4.0-Sonnet reaches a 62.50% success rate in the Financial Analysis\ndomain, a 7.50 percentage point increase. However, this approach is not universally beneficial. Claude-4.0-Sonnet’s\nperformance in the Browser Automation domain declines, and GPT-4.1 shows no improvement in the Financial\nAnalysis domain. These mixed results suggest that while the exploration phase can help some LLMs perform better\nin specific domains (e.g., information seeking and reasoning), it is not a one-size-fits-all solution (e.g., planning for\nstate changes). Achieving strong performance on the MCP-Universe benchmark requires more robust and adaptive\nstrategies, emphasizing the benchmark’s difficulty and its value as a testing ground for advanced LLM agents.\n4.5\nMore MCP Servers Connected\nLocation Navigation\nBrowser Automation\nFinancial Analysis\n0\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate (%)\n8.89\n13.33\n22.22\n11.11\n23.08\n15.38\n38.4638.46\n40.00\n35.00\n55.00\n62.50\nGPT-4.1\nGPT-4.1 w/ more servers\nClaude-4.0-Sonnet\nClaude-4.0-Sonnet w/ more servers\nFigure 6:\nEffect of connecting with more\nunrelated MCP servers.\nIn Table 3, we only connect the MCP servers that are directly\nrelevant to each task.\nIn this section, we extend the setup by\nconnecting additional, unrelated MCP servers to the LLMs to assess\ntheir performance under increased tool complexity. For all tasks,\nwe connect a total of 7 MCP servers, comprising 94 tools. This\nconfiguration introduces additional noise and results in a noticeable\ndecline in performance, as illustrated in Figure 6.\nFor example,\nClaude-4.0-Sonnet’s success rate in the Location Navigation domain\ndrops from 22.22% to 11.11%.\nGPT-4.1’s success rate in the\nBrowser Automation domain decreases from 23.08% to 15.38%,\nand in the Financial Analysis domain, it drops from 40.00% to\n35.00%. These results demonstrate that our benchmark can also\nserve as a valuable testbed for evaluating the robustness of LLMs\nwhen confronted with a larger number of unrelated tools.\n9\n\nSalesforce AI Research\n2025-08-21\nTable 5: Comparison of Enterprise-Level Agent Frameworks on MCP-Universe Benchmark.\nAgent Framework\nLocation\nNavigation\nRepository\nManagement\nFinancial\nAnalysis\n3D\nDesigning\nBrowser\nAutomation\nWeb\nSearching\nOverall\nSuccess Rate\nClaude-4.0-Sonnet Backbone\nReAct\n22.22\n12.12\n55.00\n26.32\n38.46\n21.82\n29.44\nCursor Agent\n22.22\n9.09\n55.00\n26.32\n43.59\n7.27\n26.41\nOpenAI o3 Backbone\nReAct\n26.67\n6.06\n40.00\n26.32\n25.64\n29.09\n26.41\nOpenAI Agent SDK\n28.89\n6.06\n60.00\n36.84\n28.89\n29.09\n31.60\n4.6\nEnterprise-Level Agent Framework Comparison\nIn the previous experiments, we focused on evaluating different LLMs using the ReAct framework. To assess the\nimpact of agent architecture on performance, we conduct a comparison of different agent frameworks as shown in\nTable 5. We evaluate four distinct configurations: two frameworks using Claude-4.0-Sonnet as the backbone (ReAct\nand Cursor Agent), and two using OpenAI’s o3 model (ReAct and OpenAI Agent SDK3).\nFor the Claude-4.0-Sonnet backbone comparisons, we observe that the ReAct framework achieves a higher overall\nsuccess rate (29.44%) compared to the enterprise-level Cursor Agent (26.41%). While Cursor Agent demonstrates\nsuperior performance in Browser Automation (43.59% vs. 38.46%), it significantly underperforms in Web Searching\n(7.27% vs. 21.82%), resulting in a 3.03 percentage point overall deficit. This disparity is particularly notable in Web\nSearching, where Cursor Agent’s reliance on internal tools rather than the benchmark’s MCP servers may contribute to\nthe performance gap. For the OpenAI o3 backbone comparisons, the OpenAI Agent SDK substantially outperforms the\nReAct framework (31.60% vs. 26.41%). The Agent SDK demonstrates consistent advantages across most domains,\nparticularly excelling in Financial Analysis (60.00% vs. 40.00%) and 3D Designing (36.84% vs. 26.32%). The\no3 + OpenAI Agent SDK configuration achieves the highest overall performance among all tested agent-backbone\ncombinations, suggesting that specialized agent architectures can effectively leverage model capabilities.\nThese results highlight that agent framework design significantly impacts performance on our benchmark. While\nenterprise-level agents like Cursor may excel in specific domains, they do not universally outperform simpler\nframeworks like ReAct. Furthermore, the substantial performance difference between o3 + ReAct and o3 + OpenAI\nAgent SDK demonstrates that optimal agent-model pairing is crucial for maximizing performance on complex tasks.\n5\nConclusion\nIn this work, we present MCP-Universe, the first comprehensive benchmark designed to rigorously evaluate LLMs\nin real-world MCP environments. By grounding tasks in authentic data and deploying execution-based evaluators,\nMCP-Universe exposes critical gaps in current LLM capabilities, including challenges with long-context handling,\ntool unfamiliarity, and cross-domain performance disparities. Our extensive experiments show that even top-ranked\nmodels and enterprise-level agents struggle with the complexities of MCP-driven tasks. These findings underscore the\nneed for targeted advances in both model design and agent integration. With its extensible framework and user-friendly\ninterface, MCP-Universe provides a valuable testbed for researchers and practitioners to accelerate progress in robust,\nreal-world LLM applications."}
{"id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "authors": ["Omar Naim", "Swarnadeep Bhar", "Jérôme Bolte", "Nicholas Asher"], "source": "http://arxiv.org/abs/2508.14685v1", "page_count": 15, "text": "Improving in-context learning with a better scoring function\nOmar Naim\nIRIT France\nUniversité de Toulouse\nSwarnadeep Bhar\nIRIT France\nUniversité de Toulouse\nJérôme Bolte\nToulouse School of Economics\nUniversité Toulouse Capitole\nNicholas Asher\nIRIT France\nCNRS\nAbstract\nLarge language models (LLMs) exhibit a re-\nmarkable capacity to learn by analogy, known\nas in-context learning (ICL). However, recent\nstudies have revealed limitations in this abil-\nity. In this paper, we examine these limitations\non tasks involving first-order quantifiers such\nas all and some, as well as on ICL with linear\nfunctions. We identify Softmax, the scoring\nfunction in attention mechanism, as a contribut-\ning factor to these constraints. To address this,\nwe propose scaled signed averaging (SSA), a\nnovel alternative to Softmax. Empirical results\nshow that SSA dramatically improves perfor-\nmance on our target tasks. Furthermore, we\nevaluate both encoder-only and decoder-only\ntransformers models with SSA, demonstrating\nthat they match or exceed their Softmax-based\ncounterparts across a variety of linguistic prob-\ning tasks.\n1\nIntroduction\nLarge language models have shown an impressive\nability to solve new tasks without parameter up-\ndates by leveraging examples provided in the input,\nknown as in-context learning (Brown et al., 2020).\nWhile this behavior has drawn considerable atten-\ntion, (McCoy et al., 2024; Ye et al., 2024) have\nsuggested there are important limitations to ICL;\nLLMs in-context learn much better in situations\n(tasks, data and algorithms) that they are likely to\nhave frequently encountered than in so-called low-\nprobability situations. There is a growing literature\nattesting to problems with generalizability in ICL.\nTo further investigate limitations of ICL, We\nlook at two tasks, one linguistic and one mathemat-\nical, using clean, simple data (sequences of num-\nbers) and linked to transparent prompting; which\nmitigate issues related to prompt engineering. The\nfirst task is to learn the quantification concepts “ev-\nery\" and “some\" in a restricted setting. The second,\nis to learn a one-dimensional linear function, a task\nstudied in (Garg et al., 2022). Since ICL capabil-\nities inherently depend on a model’s pre-training,\nwe train small transformers from scratch. Our train-\ning reflects the standard NLP next token prediction\ntraining of language models; but our data and tasks\nare simpler than with most NLP tasks, allowing us\nto concentrate on the ICL mechanism itself.\nOur results are as follows. While the small mod-\nels we train from scratch demonstrate remarkable\ncapabilities within their training domain, we con-\nfirm that they have difficulties generalizing beyond\nthem on both tasks. We identify the Softmax scor-\ning function in the attention mechanism as a key\nfactor limiting generalization. To overcome this,\nwe introduce a new scoring function, scaled signed\naveraging (SSA), which significantly improves gen-\neralization in our models across our tasks in com-\nparison to alternatives proposed in the literature.\nWe also evaluate SSA’s effectiveness on NLP\ntasks through two experimental settings. First, we\ntrained decoder-only GPT-2 models with 124 mil-\nlion parameters from scratch on the OpenWebText\ndataset, one using SSA and the other using Softmax.\nThe SSA model consistently outperformed the Soft-\nmax baseline. Second, we trained two versions of\n“BabyBERTa\", an encoder-only transformer model\n(Huebner et al., 2021) on a 5-million-word cor-\npus of child-directed speech: one with Softmax\nand several variants with SSA. When tested on the\ngrammatical probing suite from (Huebner et al.,\n2021), the SSA models demonstrated superior per-\nformance compared to their Softmax counterparts.\n2\nRelated Work\n(Brown et al., 2020) introduced ICL as a paradigm\nwhere the model learn at inference from the prompt\nby analogy, without changing any training param-\neters. (Dong et al., 2022) survey the successes\nand challenges for ICL, arguing that existing re-\nsearch has primarily focused on “on simple tasks\n1\n\nand small models\", such as learning linear or basic\nBoolean functions. This limitation arises from the\nrequirement to train models from scratch in such\nstudies. We study versions of both of these tasks.\nWith regards to the quantification task, (Asher\net al., 2023) proposed evaluating a generative\nmodel’s understanding of quantification by using\ninput sequences to describe semantic models. We\nadopt their approach, testing a model’s grasp of ba-\nsic quantification concepts by encoding situations\nin the input context and evaluating its ability to\ninterpret them correctly.\nFor the function task, (Garg et al., 2022) showed\nthat a transformer trained from scratch can per-\nform ICL of linear functions, given identical train\nand test Gaussian distributions.1(Bhattamishra\net al., 2023) trained small GPT-2-like models from\nscratch and found that transformers can in-context\nlearn simple Boolean functions, but their perfor-\nmance degrades on more complex tasks. (Raven-\ntós et al., 2024) investigated how ICL in models\nevolves as the number of pretraining examples in-\ncreases, under a regime where training and test\nexamples are drawn from the same distribution.\nConcerning our NLP tasks, (Huebner et al.,\n2021) demonstrates that transformer-based masked\nlanguage models can effectively learn core gram-\nmatical structures from a small, child-directed\ndataset. BabyBERTa uses a lightweight RoBERTa-\nlike model, with 8M parameters, no unmasking,\nsingle-sentence sequences, and 5M words of input\nto achieve grammatical understanding on par with\nRoBERTa-base pretrained on 30B words. They\ndevelop a grammar evaluation suite compatible\nwith child-level vocabularies. We use this suite\nto compare SSA and Softmax as scoring functions\nin encoder-only models.\n(Olsson et al., 2022) introduce the notion of in-\nduction head as a key feature of ICL across tasks\nin small transformer models, and hypothesize that\nsimilar mechanisms underlie ICL in larger models\nas well. (?) demonstrates that ICL in a simple clas-\nsification task emerges abruptly, once the model\nlearns to use an induction head to replicate label\npatterns from previous examples.\nConcerning the limits of ICL, (Xie et al., 2021;\nZhang et al., 2024; Giannou et al., 2024) show that\n1This work as well as ours follows a general approach to\nICL and learning: a model has learned with prediction ˆ\nS a\nset S ⊂V , if, when sampling from V at test time using the\nsame distribution as used during training, the expected error\nfor x ∈V ∧¬(x ∈ˆ\nS ↔x ∈S) is close to 0.\nwhen the training and inference distributions are\nshifted performance degrades. (Naim and Asher,\n2024) made a systematic study and showed sub-\nstantial degradation in shifts in distributions, in\ncontrast to (Garg et al., 2022) who only looked at\nsmall perturbations. (Ye et al., 2024; McCoy et al.,\n2024) show general limits to autoregressive train-\ning. (Naim and Asher, 2024) introduce the notion\nof “boundary values\" to describe how model be-\nhavior degrades. These values are hard limits on\npredictions. They argue convincingly that the lack\nof generalizability does not come from the mod-\nels overfitting the data and that boundary values’\neffects on performance are not just the result of\nmemorization.\nPrior research has also investigated the scoring\nfunctions for attention layers in transformers, pri-\nmarily for enhancing performance on long-context\nsequences.\n(Zheng et al., 2025) introduce SA-\nSoftmax, which multiplies the Softmax output by\nthe input scores to enhance gradient flow and mit-\nigate vanishing gradients in deep models. (Qin\net al., 2022) propose CosFormer, an alternative to\nSoftmax that enables linear time complexity with-\nout relying on kernel approximations. None of the\nalternatives address the problems we detail with\nSoftmax below.\n3\nOur ICL tasks and experimental set up\nDespite their apparent simplicity, our tasks are con-\nceptually fundamental. In particular, failure on\nthe quantification task suggests deeper limitations\nin a model’s reasoning capabilities. The notion\nof logical or semantic consequence, for instance,\ncrucially involves quantification; a failure to under-\nstand quantification implies a failure to understand\nwhat it means to reason in a logically correct fash-\nion. This will entail mistakes in reasoning not only\nwith quantifiers but also in downstream tasks like\nquestion answering (Chaturvedi et al., 2024).\nThe function prediction task tests a model’s abil-\nity to extrapolate patterns from from contextual\ndata to novel situations, a core requirement of ICL.\nWhile large models often succeed by relying on\nextensive encoded knowledge, true generalization\nrequires the ability to go beyond the training dis-\ntribution. Our function task isolates this challenge\nin a controlled setting. The implication is clear:\nif a model fails to generalize here, we should be\ncautious about claims of generalization in more\ncomplex, less controlled scenarios involving noisy\n2\n\nor unknown data.\nOur ICL tasks involve training from scratch on\nsequences that contain in-context examples (input-\noutput pairs) of the form (x1, y1, ..., xi) ending\nwith a query input xi, for which the model must\npredict the corresponding output yi. Inputs are sam-\npled from one or more training distributions. We\nemploy curriculum learning on a set S of training\nsequences of varying lengths, ranging from 1 to 40.\nWe trained several transformer models2 from\nscratch, ranging from small 1 layer model to mod-\nels with 18 layers, 8 attention heads, and an em-\nbedding size of 256. We feature results here from\na 12 layers and 8 attention heads (22.5M param-\neters) model with and without MLP on our tasks,\nas larger models did not yield significantly better\npredictions. To identify the component responsible\nfor ICL in transformers and what hinders their gen-\neralization, we did an ablation study by removing\ncomponents from the architecture to examine ICL\ntasks in models with fewer components.\nOur quantification task3 is to predict the truth\nof the simple quantified sentences (1-a) and (1-b)\ngiven a contextually given string of numbers of\nlength 40. Our training set of strings S contain\nnumbers chosen from a training distribution DI,\nwhich we set to the Gaussian distribution N(0, 1).\n(1)\na.\nEvery number in the sequence is posi-\ntive.\nb.\nSome number in the sequence is posi-\ntive.\nAt inference time, we test generalization by shifting\nboth the input distribution Dtest\nI\nand the sequence\nlength Stest. The set Stest includes sequences rang-\ning in length from 10 to 200, while Dtest\nI\nranges\nover Gaussian distributions N(0, σ) for σ ≥1.\nIn the linear function task, the target function is\nan affine map of the form yi = f(xi) = axi + b.\nTo construct the training set, we first sample coef-\nficients a and b for f from a distribution denoted\nDF. Each training sequence is then populated with\ninput elements xi drawn from a separate distri-\nbution DI. At inference time, we evaluate gen-\neralization by shifting both the input distribution\nDtest\nI\n∼N(0, σ1) for σ1 ≥1 and the function pa-\nrameters distribution Dtest\nF\n∼N(0, σ2) for σ2 ≥1.\nWe train a model Lθ parameterized by θ to mini-\n2Our models were Decoder-only (GPT-2). Our code can\nbe found in https://anonymous.4open.science/r/SSA/\n3See Figure 2 for an example of the task\nmize the expected loss over all prompts:\nmin\nθ\nEP\n\"\n1\nk + 1\nk\nX\ni=0\nℓ(yi+1, Lθ ((x1, y1, ..., xi+1)))\n#\n(1)\nwhere ℓ(., .) represents the loss function: we use\nsquared error for the linear function task and cross-\nentropy for the quantifier task. In the quantifier\ntask, y represents the ground truth given a sequence\nending with the input x. In the function task, y\nis the ground truth value of f(x), where f is the\nunderlying function generating the sequence up to\nx. We train models for 500k steps using a batch\nsize of 64, resulting in over 1.3 billion training\nexamples seen per distribution.\nFor the quantifier task, we evaluate a model’s\nICL performance on each pair (Stest, Dtest\nI ) by gen-\nerating 100 samples, each consisting of 64 batches.\nFor each sample, the model receives a score of 1\nif the model’s prediction is incorrect and 0 other-\nwise. The final evaluation measure is obtained by\naveraging the error across all samples.\nFor the linear function task, we assess ICL per-\nformance on each pair (Dtest\nI , Dtest\nF ) by sampling\nN = 100 functions from Dtest\nF . For each func-\ntion fj, we generate Nb = 64 batches, each\ncontaining Np = 40 input points drawn from\nDtest\nI .\nWithin each batch b, we predict fj(xb\nk)\nfor every xb\nk with k ≥2, given the prompt se-\nquence (xb\n1, fj(xb\n1), ..., xb\nk−1, fj(xb\nk−1), xb\nk). For\neach function, we compute the mean squared error\n(MSE) across all predicted points in all batches.\nThe final ICL performance metric is obtained by\naveraging the MSE across all sampled functions:\nϵσ = 1\nN\nN\nX\nj=1\nNb\nX\nb=1\n1\nNb\n\n1\nNp\nNp\nX\nk=3\n(predb,j\nk\n−fj(xb\nk))2\n\n(2)\nThis evaluation (2) across different distribu-\ntions provides a comprehensive assessment of the\nmodel’s ability to generalize its learning.\n4\nICL for quantifiers and linear functions\nICL with quantifiers Experiments on quantifi-\ncation task showed that when test samples are\ndrawn from the same distribution as training, i.e.\nDI, Dtest\nI\n∼N(0, 1), models successfully learned\nto predict the correct truth values for (1-a) or\n(1-b) on test sequences Stest that were significantly\nlonger than those seen during training S, as illus-\ntrated in Figure 4. However, model performance\n3\n\ndropped sharply when inference inputs included\none or more xi values far outside the training dis-\ntribution (see Figure 2). We refer to such sequence\nas deviant.\nICL with linear functions In the linear function\ntask, when both training and test data were sam-\npled from N(0, 1), even small models achieved\nnear-zero average error. However, when the target\nfunction f was sampled from a shifted distribution\nDtest\nF\n= N(0, σ) for σ > 2, all models had sys-\ntematic and non 0 average errors (For details, see\nAppendix J).\n5\nError analysis\nIn the quantification task, we found that models\nbase their predictions for an entire deviant sequence\nS solely on their prediction for the largest element\nin S (see Figure 2). The presence of a single suf-\nficiently large number in S was enough to trigger\nthis behavior consistently. In the linear function\ntask, we also observed (Naim and Asher, 2024)’s\nboundary values —values that the model fails to\nexceed during inference (see Figures 1 and 10).\nThese boundary values are responsible for general-\nization failures: they restrict the model to generate\noutputs only within a specific range, effectively\npreventing the model from generalizing its good\nperformance on the task over a small interval to val-\nues outside that interval. We found such boundary\neffects in both attention-only and full transformer\nmodels across all our training and testing setups.\nFigure 1: Plots showing examples of boundary values\nfor different models. (Left) Full transformer 12L8AH\nmodel tested on f(x) = 9x and (Right) Transformer\n12L8AH without MLP model trained on DI = DF =\nN(0, 1) and tested on f(x) = 10x.\n5.1\nComparison with larger fine-tuned and\nprompted LLMs\nGeneralization from training data was a challenge\nnot only for our smaller models but also for much\nlarger ones. We evaluated performance on the quan-\ntification task using both fine-tuned (see Appendix\nA for details) and prompted versions of LLaMA\n3.1 8B, as well as the prompted LLaMA 3.3 70B\nmodel4. In a 5-shot setting, prompted LLaMA 3.1\n8B failed to master numerical inputs from Dtest\nI\nand showed no generalization to longer sequences.\nLLaMA 3.3 70B performed better on numerical in-\nputs drawn from distributions outside N(0, 1) but,\nsimilarly, failed to generalize to longer sequence\nlengths. Interestingly, the fine-tuned LLaMA 3.1\n8B was able to handle large numbers within a se-\nquence, as shown in Figure 8, but still did not gen-\neralize beyond the sequence lengths seen during\ntraining.\nWe also tested the prompted LLaMA 3.3 70B on\nthe linear function task with inputs and target func-\ntions sampled from DI, DF ∼N(0, 1). While the\nmodel sometimes appeared to assume linearity and\napply a regression-like strategy, it still underper-\nformed relative to our small models (see Table 1).5\n5.2\nAblation studies: the sources of ICL and\nlimits to generalization\nWe looked at what might be responsible for ICL\nand its limitations. As with (Olsson et al., 2022),\nwe found that ICL was effective on our tasks even\nin models composed solely of attention layers, with\nno feedforward components (FF); these attention-\nonly models performed comparably to their full\ntransformer counterparts (see Figure 14). In con-\ntrast, small models consisting only of feedforward\nlayers failed to perform ICL. This indicates that the\nattention mechanism is both necessary and suffi-\ncient for ICL on our tasks. As in (Naim and Asher,\n2024), models without FF components also show\nboundary values, which means that boundary val-\nues originate from the multi-head attention itself.\nWe then examined various components of our\nmodels to identify the source of their generalization\nfailures. To understand why the models struggled\nto generalize on the quantification task, we first\ntested whether they could correctly classify indi-\nvidual numbers in deviant sequences as positive\nor negative. The models performed well on this\nsubtask, and so had the information needed to com-\nplete the quantification task successfully. But they\ncould not use the information in the right way.\nGiven the importance of numerical representa-\ntion in this task, we examined the way numbers\n4Prompts are provided in the appendix H\n5We also finetuned a Llama 3.1 8b model for 2 epochs on\nthe linear function task. But even after finetuning on around\n26k such sequences, the model failed to learn the task. See\nAppendix A and E.\n4\n\nInput:\n(1,True,-2, False, 3, False, 70)\nAttention weights:\n1\nTrue\n-2\nFalse\n3\nFalse\n70\nCorrect output:\nFalse\nModel’s output:\nTrue\nFigure 2: Attention maps for an ICL example for the task \"every\" of type (x1, f(x1), x2, f(x2), ..., xn), where the\nquery xn is a big value.\nwere encoded. Since the quantification task de-\npends on numerical magnitudes, and since large or\nrarely seen numbers can pose representational chal-\nlenges, we used a linear embedding Emb : x 7→\nx · W, for (x, W) ∈R × Rd. This mapping pre-\nserves numerical ordering simple and interpretable\nand maintains the ordering of magnitudes, such that\nif |x| < |y| then ||emb(x)|| < ||emb(y)||, where\n||·|| denotes the vector norm. Crucially, this encod-\ning mechanism did not introduce boundary effects.\nIt preserved natural magnitude orderings and did\nnot appear to contribute to the model’s failure on\nthe quantification task.\n5.3\nThe issues with Softmax\nWe then looked in more detail at the workings of\nthe attention matrix. To recall the basics of atten-\ntion, let ⃗\nel = (el\n1, ..., el\nn) be the input embeddings\nprocessed by the multi-head attention mechanism\nat layer l, where el\ni denotes the embedding of the\ni-th token in that layer. Each attention head h in\nlayer l is parameterized by Query, Key, and Value\nweight matrices, denoted as Qh,l, Kh,l, and V h,l,\nrespectively. The dimension dk corresponds to the\nembedding size divided by the number of heads.\nThe output of each attention head h in a layer l is\na sequence of vectors (Ch,l\n1 , Ch,l\n2 , ..., Ch,l\nn ) where\neach:\nCh,l\ni\n=\nn\nX\nj=1\n\nsoftmax\n\n(Qh,lel\ni)⊤(Kh,lel\nj)\n√dk\n!!\nV h,lel\nj\n(3)\nThe primary role of an attention head is to refine\nthe embedding of each input el\ni by incorporating\ncontextual information from surrounding tokens.\nHowever, once the gap between the input values el\nj\nin the argument of the Softmax operator in equa-\ntion 3 become large —specifically, when the gap\nbetween the largest value and the others exceeds a\nthreshold (a difference of 4 is typically sufficient),\nthe resulting Softmax weights rapidly saturate. In\nsuch cases, the attention weight assigned to the\nmaximum value approaches 1, while the weights\nfor all other values approach 0. As a result, the\nmodel focuses almost entirely on a single token,\nthe one associated with the maximum value, while\neffectively ignoring the rest of the context.\nCh,l\ni\n= V h,lel\nj0\n(4)\nAs our embedding function is linear, if |x| is signif-\nicantly larger than |y|, then ||e(x)|| will be signifi-\ncantly larger than ||e(y)|| and so large differences\nin number inputs will have the effect noted in equa-\ntion 4.\nIt is important that the values in deviant se-\nquences that yield equation 4 come from inputs\nthat the model has seldom seen in training, be-\ncause then the Q and K matrices cannot have been\ntrained to handle them, ensuring that the large xj\naffects Softmax as in equation 4.\nWe verified these predictions by examining the\noutputs from the attention matrices of the last layer\nof the model in the quantification task. Figure 2\nconfirms experimentally what we showed mathe-\nmatically: in the case of a significant gap between\nvalues, the attention layer puts all the weight on the\nlargest value in the sequence; the other elements\nin the sequence which determined the truth value\nfor (1-a) are ignored. In Figure 2, the model fo-\ncuses only on the maximum value 70 and falsely\npredicts the sequence as all positive based only on\nthis value.\nWith significant differences in input values, the\nSoftmax function increasingly resembles a Hard-\nmax operation , assigning a weight close to 1 to the\nlargest element and weights near 0 to all others. In\naddition, it makes the score of negative values tend\ntowards 0 due to the exponential. Significant differ-\nences that can affect Softmax occur not only with\nnumerical inputs but with linguistic tokens. Tokens\nfrom OpenWebtext can have such differences (see\nFigure 9 in Appendix F).\nTraining on distributions with a much larger\nrange of elements like DI = N(0, 10) or DI =\n5\n\nN(0, 100) improved model performance on de-\nviant sequences but significantly increased squared\nerrors on N(0, σ) for small σ. This training gave\nthe Q and K matrices small weights to compen-\nsate. Since Softmax makes the scores on a set of\nvery small values constant, the model becomes less\naccurate. Additionally, all models suffered in per-\nformance once the out of distribution elements xi\nin deviant inputs became sufficiently large.\nIn the linear functions task, when a value xi\nin the sequence input to the attention mecha-\nnism is larger than the other elements of the se-\nquence and other elements in its training, mathe-\nmatically Softmax should assign xi probability 1\nand all other elements in the sequence probabil-\nity 0. This affect models performances. Consider,\nfor example, a 12L8AH model’s predictions for\nf(x) = x with this input sequence with x1 = 100:\n[100, −1.09, 0.78, 0.26, 0.42]. The model’s predic-\ntions are: [−1.21, −0.28, 2.15, 0.96, 0.65]. Given\nthis large value, model fails to reasonably approxi-\nmate the function.6\nSoftmax treats comparatively large values as\nvery significant. This makes sense in the abstract;\na large value in the attention mechanism intuitively\nsignals a strong statistical correlation in context\nsensitive aspects of meaning (Asher, 2011); Soft-\nmax amplifies this value. However, in our tasks this\nis problematic.7 While an input token representing\na large number may carry higher semantic magni-\ntude than surrounding tokens, its importance for the\ntask is not necessarily disproportionately greater.\nOur tasks require the model to look at many tokens\nin the context; but with deviant sequences, Softmax\nprevents the models from doing this.\n6\nSome alternatives to Softmax\nAttempting to remedy the deficiencies of Soft-\nmax we observed, we investigated a hybrid at-\ntention mechanism. We partitioned the attention\nheads such that half utilized Softmax-based scor-\ning, while the remaining half employed uniform\naveraging over all tokens. This design, we thought,\nwould preserve contextual breadth, reduce the risk\nof focusing to specific tokens, and also increase\nthe model’s expressiveness through multiple scor-\ning functions. However, empirical results showed\nthat this approach did not yield expected improve-\n6Though eventually the model begins to recover and ap-\nproximate better.\n7The problem occurs also of course with hardmax.\nments in performance. We further extended this ap-\nproach by experimenting with four distinct known\nscoring functions (tanh, average, ReLU, and x2),\nassigning two heads to each (For detailed results\nsee Table 1). This approach improved over than\nSoftmax on N(0, 1) but was less good elsewhere.\nIt did not solve the observed problems either. We\nalso tested CosFormer (Qin et al., 2022) and SA-\nSoftmax (Zheng et al., 2025). Both performed\nworse than Softmax in our experiments.\n7\nSolution: Signed scaled averaging (SSA)\nHaving found that previously proposed alterna-\ntives to Softmax did not lead to performance im-\nprovements in our settings, we explored a new ap-\nproach. Inspired by\nlim\nn→+∞(1 + x\nn)n = ex, we\nreplaced the exponential in the scoring function\nby a parametrized form: x 7→(1 + b|x|)sgn(x)n\nwhere b > 0 is a trainable parameter and n > 1 a\nfixed exponent (we typically set 1.1 or 1.5, though\nit can also be learned). This formulation allows\nus to approximate the exponential while control-\nling its sharpness.8 It allows interpolation between\nlinear and exponential behaviors. By training b\nand selecting (or training) different values of n, the\nmodel gains flexibility: it can mimic Softmax-like\nbehavior when appropriate, while also tempering\nthe dominance of large input values. This provides\na better balance between focus and diversity in at-\ntention.\nFor positive inputs, the function behaves similarly\nto Softmax, but with a slower growth that prevents\nearly Hardmax saturation. For negative inputs, the\npresence of sign(x) ensures the score decays to-\nward zero, like Softmax, but less abruptly. This\nallows the model to still consider low-scoring ele-\nments rather than suppressing them entirely.\nFor a vector z = (z1, · · · , zK) ∈RK with param-\neters b > 0 and n > 1, we replace Softmax by:\nscore(zi) =\n(1 + b|zi|)sgn(zi)n\nPK\nk=1(1 + b|zk|)sgn(zk)n\n(5)\nSubstituting Softmax with SSA improves our\nmodels’ generalization ability dramatically, both\nwith respect to the length of test sequences and the\nmagnitude of deviant inputs. The heat map in Fig-\nure 4 shows the improvement for the predictions of\nour standard full transformer model with 12L8AH\nwith SSA on the right compared to the same model\n8Plots for sample base functions for SSA can be found in\nFigure 5.\n6\n\nFigure 3: (Left) Comparison plot showing the evolution of MSE for SSA and Softmax-based models (12L8AH)\nwith DF, DI, Dtest\nI\n∼N(0, 1) and Dtest\nF\nand varying Dtest\nF\n∼N(0, σ). The heatmap shows the evolution of\nlogarithm of MSE for the Softmax (Middle) and SSA (Right) model when varying both Dtest\nI\nand Dtest\nF\n.\nFigure 4: Heatmaps showing the evolution of errors for the 12L8AH model with Softmax (Left) and SSA (Right)\non the \"every\" task. Model was trained on data in DI = N(0, 1) for lengths from 11 to 40 and tested in\nDtest\nI\n= N(0, σ) for σ ∈{1, ..., 10} and lengths from 10 to 200 for each task. Yellow represents a much higher\nerror rate than purple.\nmodels \\ σ\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nSSA\n4 × 10−5\n3 × 10−4\n10−3\n0.02\n0.02\n0.15\n1.24\n1.04\n2.74\n8.5\nSoftmax\n8 × 10−5\n3 × 10−4\n6 × 10−3\n0.42\n1.62\n3.84\n9.42\n13.51\n27.99\n45.35\nSOFT/AVG\n7 × 10−5\n3 × 10−3\n0.30\n1.22\n2.91\n7.52\n10.32\n22.97\n38.97\n60.03\n4 Scoring fts\n5 × 10−5\n3 × 10−3\n0.34\n1.33\n3.18\n8.28\n10.99\n26.31\n42.42\n70.33\nCosFormer\n2 × 10−4\n5.6 × 10−3\n0.43\n2.07\n5.10\n12.54\n16.91\n46.68\n66.22\n91.02\nSA-Softmax\n6 × 10−5\n2.8 × 10−3\n0.37\n1.83\n3.83\n11.94\n13.72\n39.29\n58.43\n80.31\nLlama 3.3 70b\n2 × 10−3\n5.50\n3.16\n13.71\n18.21\n23.91\n28.99\n33.51\n40.25\n48.02\nTable 1: Comparison showing the evolution of squared errors for models tested on x ∈Dt\nI = N(0, 1) and weights\na, b ∈Dt\nF = N(0, σ). SSA, CosFormer and Softmax are 12 layer, 8 attention head full transformer models.\nwith Softmax on the left for the every task. The\nimprovement with SSA on the \"some\" task is also\nsignificant (see Appendix C).\nOur best 12L8AH model also dramatically in-\ncreased performance in learning linear functions\non out-of-distribution data for function parameters\nwhen we used SSA instead of Softmax as scoring a\nfunction (see Figure 3). SSA was the best scoring\nfunction we tested on our tasks.\n8\nSSA and NLP tasks\nTo assess the applicability of SSA beyond our syn-\nthetic tasks and to more complex natural language\nprocessing scenarios, we trained three GPT-2 mod-\nels (each with 134 million parameters) from scratch\non the OpenWebText corpus (9B tokens) for 20,000\nsteps. One model used the standard Softmax scor-\ning function, while the other two employed SSA:\none with a fixed exponent n = 1.5, and the other\nwith n treated as a trainable parameter. We evalu-\nated model performance using perplexity, as this\nis a widely accepted metric for language modeling\ntasks and provides a direct, quantitative compar-\nison across models trained with different scoring\nfunctions.\nThe model with trainable SSA had a loss of\n3.19 and SSA 1.5 model had a loss of 3.15 after\n7\n\nlinguistic probe\nMLM\nHolistic\nSoftmax\nSSA 1.1\nSSA 1.5\nSSA 2\nSoftmax\nSSA 1.1\nSSA 1.5\nSSA 2\nagreement_Det_N-across_1_adj\n75.35\n76.7\n73.5\n73.3\n56.45\n57.55\n54.95\n56.40\nagreement_subject_verb-across_PP\n56\n55.75\n58.95\n65.95\n51.55\n51.25\n51.35\n52.05\nagreement_subject_verb-across_RC\n55.5\n55.75\n57.7\n61.55\n51.9\n52.15\n52.15\n50.05\nagreement_subject_verb-in_Q+aux\n76.5\n69.6\n79.0\n70.45\n50.1\n48.95\n49.95\n49.05\nanaphor_agreement-pronoun_gender\n48.1\n50.25\n51.45\n53.7\n52.7\n49.9\n52.75\n52.75\nargument_structure-dropped_arg\n79.65\n74.65\n74.65\n85.55\n70.7\n80.9\n72.85\n75.85\nargument_structure-swapped_args\n83.3\n83.1\n92.0\n88.0\n62.45\n58.4\n53.3\n33.15\nargument_structure-transitive\n53.44\n55.3\n53.85\n57.2\n55.3\n53.7\n56\n55.6\nbinding-principle_a\n78.25\n79.55\n87.9\n80.2\n66.4\n68.2\n75.0\n68.85\ncase-subjective_pronoun\n85.55\n86.5\n89.7\n91.75\n67.55\n81.1\n59.6\n46.9\nellipsis-n_bar\n60.75\n56.25\n53.3\n53.45\n35.15\n43.3\n34.7\n33.65\nfiller-gap-wh_question_object\n91.7\n90.65\n89.4\n87.15\n91.9\n89.15\n91.05\n80.3\nfiller-gap-wh_question_subject\n79.2\n79.1\n83.3\n68.25\n65.3\n89.25\n89.25\n48.55\nirregular-verb\n70.05\n64.85\n78.3\n69.2\n50.15\n58.8\n72.1\n52.5\nlocal_attractor-in_question_with_aux\n85.45\n87.35\n81.85\n85.0\n87.8\n89.7\n87.65\n86.1\nquantifiers-existential_there\n91.3\n86.6\n85.25\n76.25\n86.15\n87.3\n80.9\n85.15\nquantifiers-superlative\n71.2\n76.1\n83.95\n65.25\n51.2\n47.45\n38.9\n31.85\nOVERALL\n73.01\n72.23\n74.94\n72.48\n61.92\n65.12\n63.08\n56.39\nTable 2: BabyBERTa Model performance trained from scratch on AO-CHILDES with Softmax and three settings of\nSSA, evaluated with the holistic metric on various linguistic probes from (Huebner et al., 2021). PP: prepositional\nphrase; RC: relative clause; Det: determiner; N: noun. arg: argument.\n20k training steps, instead of 3.27 for the Soft-\nmax. Trainable SSA had a minimum loss of 2.76\nafter 26k steps.9\nWhile the baseline Softmax\nmodel achieved a perplexity of 31.73, The SSA 1.5\nand trainable SSA models achieved a significantly\nlower perplexity of 27.71 and 29.35 respectively,\nall on 20K step training. All models also share\nthe same architecture and parameter count. SSA’s\nbetter perplexity is thus compelling. Trainable SSA\nachieved a perplexity of 27.21 after 30K steps.\nIn a further study, we trained multiple vari-\nants of the BabyBERTa model (Huebner et al.,\n2021) on the AO-CHILDES dataset: one using\nthe standard Softmax scoring function, and three\nseparate models using SSA with fixed exponents\nn = 1.1, n = 1.5, and n = 2, respectively. Baby-\nBERTa has demonstrated performance comparable\nto much larger RoBERTa models on linguistic prob-\ning tasks, despite being trained on substantially\nsmaller datasets (Huebner et al., 2021). For these\nexperiments, we used the BabyBERTa encoder-\nonly model, as encoder architectures have been\nshown to perform better on grammar classification\ntasks involving masking. We evaluated our four\nmodels on 17 of the linguistic probes from (Hueb-\nner et al., 2021).10 We used two metrics for testing\non linguistic probes: holistic scoring (Zaczynska\n9https://github.com/karpathy/nanoGPT/tree/master posted\na minimum loss with softmax of 3.11 only after 600,000 steps.\n10Two tests related to negative polarity items produced\nhighly skewed results and were therefore excluded.\net al., 2020) which measures the correct predic-\ntion of all masked elements, and masked language\nmodel (MLM) scoring (Salazar et al., 2020), which\nassesses the accuracy of predicting correct tokens\nversus distractors. On the stricter holistic method,\nSSA 1.1 BabyBERTa averaged over 2 percentage\npoints higher than Softmax BabyBERTa. SSA1.1\nhad the highest score over the other models in 8\nprobes. Softmax only beat the SSA models on 4\nprobes (see Table 2). On the MLM metric, SSA\n1.5 averaged almost 2 percentage points higher\nthan Softmax, SSA 1.1 and SSA 2. Softmax Baby-\nBERTa scored highest on 3 probes while SSA 1.5\nand SSA 2 scored best on six of the probes.\n9\nConclusion\nThe literature has noted that small transformer\nmodels struggle to generalize effectively to out-\nof-distribution data, a limitation we have also em-\npirically confirmed. We identified the Softmax\nscoring function in the attention mechanism as a\nfactor contributing to this challenge and introduced\na novel scoring method that significantly improves\nthe performance of small transformer models on\nboth mathematical and NLP tasks. SSA enhances\na model’s ability to predict linguistic structure, of-\nfering improved performance without increasing\nmodel complexity. The fact that SSA is parametriz-\nable and adaptable to different tasks adds to its\nattractiveness and shows its advantages over a one-\nsize-fits-all scoring function like Softmax.\n8\n\nLimitations\nDue to hardware and data limitations, we could\nnot train all the SSA tasks with the parameter n\nas trainable. We were also unable to train models\nlarger than 124M parameters from scratch with\nSSA.\nWhile SSA provides noticeable improvements\nin generalization, it does not fully address all the\nshortcomings of the attention mechanism. In par-\nticular, our heatmaps indicate that SSA struggles\nto generalize in scenarios where both the input xi\nand the test-time function distribution Dtest\nF diverge\nsignificantly from the training distribution DF.\nThus, SSA does not address the fact that, as we\nhave noted above, the simple mathematical struc-\nture of the attention mechanism conflates the value\nof tokens with their importance for the particular\ntask."}
{"id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "authors": ["Laurits Lyngbaek", "Pascale Feldkamp", "Yuri Bizzoni", "Kristoffer Nielbo", "Kenneth Enevoldsen"], "source": "http://arxiv.org/abs/2508.14620v1", "page_count": 16, "text": "Anthology of Computers and the Humanities, Vol. 1\nContinuous sentiment scores for literary and multilingual\ncontexts\nLaurits Lyngbaek1, *, Pascale Feldkamp1, *, Yuri Bizzoni1, Kristoffer Nielbo1,\nKenneth Enevoldsen1\n1 Center for Humanities Computing, Aarhus University, Denmark\n* Joint First Authorship\nAbstract\nSentiment Analysis is widely used to quantify sentiment in text, but its application to literary\ntexts poses unique challenges due to ﬁgurative language, stylistic ambiguity, as well as senti-\nment evocation strategies. Traditional dictionary-based tools often underperform, especially\nfor low-resource languages, and transformer models, while promising, typically output coarse\ncategorical labels that limit ﬁne-grained analysis. We introduce a novel continuous sentiment\nscoring method based on concept vector projection, trained on multilingual literary data, which\nmore effectively captures nuanced sentiment expressions across genres, languages, and histor-\nical periods. Our approach outperforms existing tools on English and Danish texts, producing\nsentiment scores whose distribution closely matches human ratings, enabling more accurate\nanalysis and sentiment arc modeling in literature.\nKeywords: sentiment analysis, computational literary studies, historical texts, semantic em-\nbeddings\n1\nIntroduction & Related Works\nSentiment analysis quantiﬁes sentiment in text and is widely used across domains, from product\nreviews analysis to social media monitoring [7, 32]. In computational literary studies, it is increas-\ningly employed to model narrative dynamics, particularly for modelling sentiment arcs across nov-\nels [6, 16, 26, 38]. This typically requires continuous sentiment scores, mapping sentiment along\na spectrum rather than using categorical labels, such as positive/negative.\nDespite the growing use of continuous sentiment scoring in literary studies, the validity of\ncurrent tools in capturing literary sentiment expression remains underexplored. Popular tools such\nas Syuzhet have faced severe criticism for oversimpliﬁcation or poor generalizability [31] – issues\nthat point to broader limitations in applying off-the-shelf sentiment analysis tools to literary texts.\nIndeed, the literary domain poses distinct challenges: ﬁgurative language, multiple narrative layers,\nand stylistic ambiguity all complicate sentiment detection [3, 8].\nMore recent transformer-based models appear better equipped to handle the complexity of liter-\nary language [30], and techniques that transform categorical model outputs into continuous scores\nhave been proposed [4]. Such methods have proven more effective than tailored dictionary-based\ntools, particularly in low-resource language settings and across languages [13]. However, empiri-\ncal benchmarks comparing model predictions to human judgments remain limited, particularly in\nlanguages other than English.\nWe identify three main issues where current methods see a noticeable performance drop:\nLaurits Lyngbaek, Pascale Feldkamp, Yuri Bizzoni, Kristoffer Nielbo, and Kenneth Enevoldsen. “Continuous senti-\nment scores for literary and multilingual contexts.” In: Proceedings of Conference XXX, ed. by Editor1 Editor2. Vol. 1.\nAnthology of Computers and the Humanities. 2025, 1–1. https://doi.org/00000/00000.\n© 2025 by the authors. Licensed under Creative Commons Attribution 4.0 International (CC BY 4.0).\n1\n\n1) Cross-lingual performance drops. Most Sentiment Analysis tools are developed for high-\nresource languages, and their transfer to low-resource ones like Danish is non-trivial. Although\nDanish has several dictionary-based tools (i.a., Asent [12], Sentida [21]), these have seen little\nuse on historical literature and struggle with complex literary forms. Comparing tools for Dan-\nish literary sentiment analysis, Feldkamp et al. [13] found that dictionaries were outperformed by\nmultilingual transformer models – likely because they leverage contextual attention. While multi-\nlingual transformers, such as mBERT and XLM-R [11], show promise for cross-lingual sentiment\nanalysis in literature [13], cultural and linguistic biases inherited from English pretraining remain\na concern [10, 37].\n2) Cross-domain performance often drops, especially when applying tools trained on social me-\ndia to literature, where sentiment is expressed in a distinct manner [3, 14, 33]. Literary language\ntends to be more omissive and implicit, relying less on emotionally charged vocabulary and more\non concrete descriptions of objects and situations to evoke affect – a domain-speciﬁc mode of sen-\ntiment expression that models often fail to capture [14]. This domain-speciﬁcity also varies across\ngenres: when using a model ﬁne-tuned on Twitter posts, poetry shows the weakest correlation with\nhuman ratings, prose falls in the middle, and Facebook posts the strongest [14].\n3) Historical data, marked by diachronic language change, often reduces model performance.\nWhile ﬁne-tuned multilingual transformers show promise for older texts [1, 13, 29], challenges\nremain. Lexical drift – including semantic shift, word loss (e.g., thou, peradventure), changing\nfrequencies, and temporal polarity shifts – limits sentiment inference, especially when models rely\non priors from modern corpora.1 In the case of temporal polarity shifts, even short-term changes\ncan lower model performance, as learned polarities no longer reﬂect current usage [23].\nMoreover, a major drawback of recent transformer-based approaches is that, while they out-\nperform dictionary-based tools on historical and literary data [13], they are typically optimized for\ntrinary classiﬁcation (positive, neutral, negative), limiting their usefulness for ﬁne-grained senti-\nment analysis. Although model conﬁdence scores can be repurposed for continuous output – with\nmedium to strong correlation to human ratings [13] – the resulting distributions still cluster around\nthe original three categories, producing what is effectively a pseudo-trinary distribution. This poses\na problem for literary analysis tasks, not least sentiment arc modelling, where detrending methods\nto smoothen out the signal necessitate continuous scores. When sentiment scores behave in extreme\nways – as they will with pseudo-trinary distributions – smoothing will tend to collapse variation\ntoward the neutral midpoint, removing meaningful information.\nIn this paper, we introduce a method to create continuous-scale sentiment scores that are more\nclosely aligned with the distribution of human scores, while also mitigating language-, domain-,\nand historical data issues by training a model on the actual language and domain of the use case.\nWe test this approach on English and Danish literary texts, comparing it to existing transformer-\nbased models and popular dictionary-based tools, across both ﬁction and nonﬁction genres. The\nbenchmark includes both historical literary genres (e.g., 19th-century hymns) and contemporary\ntexts (e.g., blogs), enabling us to evaluate model performance in settings that better reﬂect the\nneeds of researchers working with multilingual or diachronic literary corpora. We pursue three\naims: (1) to assess model performance on contemporary literary and non-literary texts; (2) to com-\npare performance across literary genres; and (3) to evaluate models on historical and multilingual\nliterary data. We begin by testing our approach on Fiction4 — a recent annotated ﬁction corpus\nthat spans four literary genres, two languages (English and Danish), and a period from 1798 to\n1965. We then validate our approach further on EmoBank, a standard sentiment analysis dataset\nthat includes contemporary genres and a small set of ﬁction, to gauge the generalizability of our\n1 Diachronic sentiment analysis is particularly challenging for traditional machine learning approaches as words’ mean-\ning and polarity change continuously, while most models require steady ground truths for training, creating somewhat\nartiﬁcial “museums” of words’ sentiment scores in a given historical period.\n2\n\napproach and to control for overﬁtting to literary data.\n2\nMethods\n2.1\nData\nTable 1: Datasets with valence annotation. Valence was annotated on a sentence basis, so ‘N\nannotations’ indicates the number of sentences. The total number of sentences considered is n =\n15, 170. ‘N annotators’ indicates the number of annotators reported per sentence.\nDataset\nPeriod\nN annotations\nN words\n¯\nx words/sentence\nN annotators\n↓EmoBank\n1990-2008\n8,870\n143,499\n16.18\n10\nLetters\n1,413\n21,639\n15.31\n10\nBlog\n1,336\n20,874\n15.62\n10\nNewspaper\n1,314\n25,992\n19.78\n10\nEssays\n1,135\n26,349\n23.21\n10\nFiction\n2,753\n31,491\n11.44\n10\nTravel-guides\n919\n17,154\n18.67\n10\n↓Fiction4\n1798-1965\n6,300\n73,250\n11.6\n>2\nHymns\n1798-1873\n2,026\n12,798\n6.3\n2\nFairy tales\n1837-1847\n772\n18,597\n24.1\n3\nProse\n1952\n1,923\n30,279\n15.7\n2\nPoetry\n1965\n1,579\n11,576\n7.3\n3\nEmobank\nis a text corpus manually annotated for sentiment according to the psychological\nValence-Arousal-Dominance scheme. It was compiled at JULIE Lab, Jena University [9],2, con-\ntaining sentences from the MASC dataset, which is diverse both in terms of overall composition\nwith diverse domains, and topically within categories.3 It includes six categories: Letters, Blog,\nNewspaper, Essays, Fiction, and Travel guides.4 Inter Rater Reliability for the whole dataset is:\nKrippendorff’s α = 0.34.5 We use the mean sentence-based valence scores overall and per cate-\ngory to compare model performance.\nFiction4\nis a dataset of literary texts, spanning literary texts across four genres and two lan-\nguages (English and Danish) in the 19th and 20th century.6, compiled at the Center for Humanities\nComputing, Aarhus University. The corpus consists of three main authors, Sylvia Plath for po-\netry, Ernest Hemingway for prose, and H.C. Andersen for fairytales. Hymns were collected from\nDanish ofﬁcial church hymnbooks published between 1798 and 1873. All sentences in the corpus\nwere annotated for by at least two annotators [14]. Inter Rater Reliability for the whole dataset is:\n2 https://github.com/JULIELab/EmoBank/\n3 On some EmoBank categories: Essays includes eight texts, i.a., “A Brief History of Steel in Northeastern Ohio’.\nFiction comprises six prose pieces across genres, i.a., Richard Harding’s “A Wasted Day” and the SciFi story “Captured\nMoments”. Newspapers contain reports (e.g., “A.L. Williams Corp. was merged into Primerica Corp.”) and longer\nreportages. Travel Guides are written in prose, including both place histories (e.g., “A Brief History of Jerusalem”) and\nreﬂective pieces (e.g., “Dublin and the Dubliners”). See the full MASC corpus at: https://anc.org/data/masc/\ncorpus/browse-masc-data/.\n4 We excluded the ‘Sem-Eval’ category as it was internally diverse.\n5 Since EmoBank lacks unique annotator IDs, we cannot correlate individual annotators’ scores. Instead, Krippendorff’s\nα measures agreement across ratings per item. IRR per subset is shown in Table 4.\n6 https://huggingface.co/datasets/chcaa/fiction4sentiment, for details, see [14]\n3\n\nSpearman’s ρ = 0.63 and Krippendorff’s α = 0.67.7 We use the mean sentence-based valence\nscore overall, per language set, and per genre to compare model performance.\n2.2\nComparison models\n2.2.1\nDictionary-based\nBecause of their popularity and wide usage in literary studies [1, 4, 5], as a baseline, we tested the\ndictionary-based tools VADER [15] and Syuzhet [17]. They assign sentiment scores (from negative\nto positive) by word-score matching and speciﬁc rules. Syuzhet was developed explicitly for\nliterary texts.8 When using these tools, we translated Danish sentences into English.9 A human\ndid not review translations.\n2.2.2\nTransformer-based\nTo test transformer-based methods, we chose two multilingual models. When testing models on\nDanish texts, we added three models ﬁne-tuned for Danish. These were all tested across EmoBank\ncategories, as well as Fiction4 genres and languages. We list all models in Appendix A, Table 5.10\nOne of the multilingual models – twitter-xlm – showed the best performance on Fiction4 in\nFeldkamp et al. [13]. Danish models were picked based on their performance in a recent benchmark\n[20], and – in the case of MeMo-BERT-Sa – for being developed for 19th-century novels [19].\nConversion of model output: We convert Transformers’ standard three-ways outputs (positive,\nneutral, negative) to continuous values using their conﬁdence scores 11 as a proxy for intensity\n(e.g., positive, 0.67 →+0.67; negative →–0.67; neutral →0). Mapping a model’s conﬁdence\nvalues onto a continuous scale often outperforms dictionary-based tools for literary sentiment [4,\n13].\nintensity =\n\n\n\n\n\n+p,\nif positive,\n0,\nif neutral,\n−p,\nif negative.\n2.3\nOur approach\nIt has been claimed that concepts – such as a sentiment – are approximately represented in a linear\nfashion within embedding space, which is denoted by the linear representation hypothesis [24].\nThe hypothesis states that concepts are encoded as a direction in the embedding space and that the\nfurther you move in a given direction, the stronger the concept is represented (see Figure 1). These\nlinear representations of semantic information have been found in both encoding and decoding\nmodels, at varying levels of abstraction [22, 34, 35, 39]. Suppose we have access to the direction\nthat encodes sentiment. In that case, we can project any embedded sentence onto the concept vector\nand gauge the sentiment of any given sentence, as seen in Figure 1.\n7 Humans rarely reach an agreement higher than 80% (α>0.80) for categorical tagging (positive/neutral/negative) on\nnonliterary texts [36] – and have lower IRR for continuous scale annotation [2] – especially of literary texts [27].\n8 The Syuzhet lexicon was developed in the Nebraska Literary Lab under the direction of Matthew L. Jockers.\n9 Using googletrans: https://pypi.org/project/googletrans/\n10 Code for comparing (HuggingFace-stored) sentiment models (with transformed outputs) on the Fiction4 or EmoBank\nis at: https://anonymous.4open.science/r/literary_sentiment_benchmarking-D6E6.\n11 The score output by ﬁnetuned models (e.g., “positive”, 0.66) is a softmax-normalized class probability – a pseudo\nconﬁdence score – reﬂecting how strongly a model prefers one label over another. It comes from the linear classiﬁcation\nhead atop models.\n4\n\nPositive\ndocuments\nComponent 1\nComponent 2\nNegative\ndocuments\nConcept Vector\nHallelujah, Hallelujah!\n“I know it”, the boy said\nIt is easy enough to\ndo it […]\nHe must pull until he\ndies\nNegative\nNeutral\nSemi-positive\nPositive\nFigure 1: An overview of how a concept vector for sentiment is constructed and what information\nit contains. A circle represents an embedded document.\n2.3.1\nConcept Vector Projection\nWe propose an algorithm that constructs a concept vector in a given embedding space using posi-\ntive and negative exemplary sentences that represent the opposing extremes of the concept. Using\na pre-trained sentence embedding model M, the algorithm embeds a labeled set of sentences S. It\nassumes that a concept – here sentiment – is represented linearly in the embedding space. To deﬁne\nthe concept vector, the algorithm computes the mean embedding of both the positive and negative\nsentiment examples, then calculates the vector pointing from the negative to the positive mean.\nThis relies on the assumption that when averaging multiple sentences, all non-sentiment informa-\ntion will disappear as Gaussian noise with a mean of zero, leaving behind only the information\nencoding sentiment [18, 39].\nThe resulting vector then theoretically encodes sentiment direction. New sentences can be\nassessed for their relation to the sentiment by projecting their embeddings onto this vector: the\nfarther along the direction the projection lies, the stronger their positive relation is. Deﬁning the\nconcept vector as a unit vector, the projection of a given embedding ei onto the unit concept vec-\ntor ˆ\nv is given by the dot product: ei · ˆ\nv. This projects the sentence embedding to the subspace\nspanned by the Concept Vector. The high-dimensional embedding has thereby been reduced to a\none-dimensional sentiment score, as seen in ﬁgure Figure 2. Deﬁning a concept vector requires\nonly a set of positive and negative example sentences. This is sufﬁcient to predict the sentiment\nof any subsequent sentence, whether labeled or unlabeled. The Concept Vector Projection (CVP)\nalgorithm formally described in Appendix B. The implementation of this method is available at\nhttps://github.com/centre-for-humanities-computing/embedding-projection.\n2.4\nModels\nThe implementation of Concept Vector Projection used to classify sentiment in this paper is based\non the language model paraphrase-multilingual-mpnet-base-V212 [28]. This is a 278M\nparameter model, based on a mean-pooled BERT architecture, optimized for sentence similarity by\nusing Siamese and Triplet networks. This model was chosen because of its multilingual capabilities\nand excellent size-to-performance ratio. Investigations during model selection indicate that a larger\nmodel may increase model correlation with human ratings in exchange for compute budget.\nOur Concept Vector was deﬁned using a training dataset of sentences with positive and negative\nsentiments from the Fiction4 dataset. Since the sentences were originally rated on a numerical scale\n12 https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n5\n\nNegative corpus to mean Negative Vector\nPositive corpus to mean Positive Vector\nDifference as\nConcept Vector\nEmbed Corpus of Interest\nEmbedded\nUnlabeled Corpus\nSentence projection\nonto Concept Vector\nDistribution of projection on Concept Vector\nCorpus of Interest\nDefine a\nConcept Corpus\nThis is\na 7/10\nFigure 2: A visualization of how the Concept Vector Projection is constructed. It shows how to\nuse a labeled sentiment corpus to predict sentiments of an unlabeled corpus of interest. The vectors\nshown are reduced to a two-dimensional Euclidean space for visualization, but normally reside in\na high-dimensional space.\n(1-9), they were translated to positive/negative ratings for the algorithm. We converted the mean\nratings into ordinal labels through preset thresholds. That is, for the Fiction4 ratings, we deﬁne:\nlabel =\n\n\n\n\n\npositive+\nif rating ≥7\nneutral∅\nif 7 > rating > 3\nnegative−\nif rating ≤3\n]\nAll the neutral sentences and 60% of the positive and negative sentences were in the Fiction4\ntesting set. The remaining 40% were in a Concept Corpus of 204 positive and 168 negative sen-\ntences used to deﬁne the model’s concept vector.\n3\nResults\n3.1\nContinuous scoring\nA key beneﬁt of the Sentiment Projection model is its ability – like dictionary tools – to produce\ngenuinely continuous predictions. In contrast, Transformer-based token-classiﬁcation models such\nas xlm-roberta, which can be coerced to output continuous scores (see subsubsection 2.2.2), in\npractice exhibit a “pseudo-trinary” behavior: their predictions cluster heavily at zero and at the\ntwo polar extremes. This behavior is clearly visible both in the scatterplots of predicted vs true\nsentiments (Figure 3) and in the histograms of model outputs (Appendix A, Figure 5). When\nlooking at the EmoBank results (Appendix A, Figure 4), the discretized output of xlm-roberta\nappears even more sharply tri-modal than the human scores, which average ten annotators.\n3.2\nPerformance on literary data across genres\nTable 2 compares our model’s predictions to the human gold-standard ratings for the Fiction4\ndataset’s 4 genres.\nWe evaluated all models on the full multilingual Fiction4 corpus. For the dictionary-based\ntools (VADER and Syuzhet), originally Danish texts were translated into English (see subsubsec-\ntion 2.2.1). Danish-speciﬁc models generally underperform on genres that are natively English\n(Prose, Poetry), which drags down their overall correlation scores. An outlier is danish-sentiment,\nwhich delivers relatively consistent results across both languages; however, it still falls short of\nMeMo-BERT-SA on the original Danish texts – most notably in the Fairy Tales genre.\n6\n\nContinuous Scores\n+ Granularity\n+ Moderate Scores\n+ Human Similarity\nNoncontinuous Scores\n-\nExtreme Scores\n-\nTri-modal Distribution\nModel Prediction\nHuman Ratings\nModel Prediction\nSentiment Projection\nSentiment Prediction Model\nFigure 3: Scatterplot of Sentiment Predictions for respectively Sentiment Projection and\nxlm-roberta. While the xlm-roberta model, in theory, can predict a continuous space of senti-\nments when transforming it with conﬁdence scores, visual inspection shows that certain ranges of\nthe sentiments spectrum are not used. While both models achieve high correlations, it appears that\nxlm-roberta achieves this by matching human tendencies to predict neutral.\nTable 2: Spearman correlations in the Fiction4 corpus across genres. From top to bottom: Publica-\ntion years; then Inter Rater Reliability (human scores) per genre (Spearman’s ρ and Krippendorff’s\nα); then correlation between the human gold standard and models (Spearman’s ρ). For VADER\nand Syuzhet scores, texts were automatically translated into English.\nType\nModel\nScalar\nOverall\nHymns\nFairy tales\nProse\nPoetry\nYear\n1798–1873\n1837–1847\n1952\n1965\nHuman →\nIRR ρ\n✓\n0.63\n0.73\n0.68\n0.62\n0.59\nIRR α\n✓\n0.67\n0.72\n0.68\n0.61\n0.58\n↓Dictionary\nvader\n✓\n0.49\n0.52\n0.50\n0.43\n0.46\nsyuzhet\n✓\n0.50\n0.54\n0.48\n0.45\n0.49\n↓Multiling.\ntwitter-xlm\n×\n0.55\n0.50\n0.52\n0.57\n0.58\nxlm-roberta\n×\n0.60\n0.59\n0.62\n0.61\n0.57\nSentiment Projection\n✓\n0.66\n0.69\n0.66\n0.62\n0.70\n↓Danish\ndanish-sentiment\n×\n0.54\n0.49\n0.48\n0.57\n0.57\nda-sentiment-base\n×\n0.23\n0.44\n0.47\n0.08\n0.08\nMeMo-BERT-SA\n×\n0.47\n0.63\n0.72\n0.26\n0.16\nMost Danish transformer-based models perform on par with (or worse than) dictionary-based\nmodels applied to English translations of the original Danish texts (e.g., Fairy tales & Hymns).\nSentiment Projection, in contrast, achieves the highest correlation on every genre except Fairy\ntales – where MeMo-BERT-SA performed best, which aligns with its ﬁne-tuning on Danish literary\nprose from H.C. Andersen’s period. It performs especially well on Poetry, where other models\nstruggle.\nThe genres that achieved the highest human IRR – like hymns, at IRR ρ = 0.77 – did not\nreﬂect in better results for most models. The second-best performing model, xlm-roberta, for\nexample, placed second-to-last on hymns. Instead, Sentiment Projection meets or exceeds Inter\nRater correlation (ρ) for all genres.\n7\n\n3.3\nPerformance on literary data across time and languages\nResults for the multilingual performance assessment are presented in Table 3.\nTable\n3:\nSpearman\ncorrelations\nin\nthe\nFiction4\ncorpus\nacross\nlanguages.\nColumns from left to right:\nOverall evaluation on Multilingual dataset (English and Dan-\nish); evaluation of the Danish set (n = 2, 800); evaluation of the English set (n = 3, 500);\nlastly, the evaluation of Translated set.\nOn top, Inter Rater Reliability – Spearman’s ρ and\nKrippendorff’s α.\nThe best model performance per setting is in bold, and the follow-up is\nunderlined. * There might be minimal inﬂux in correlation caused by the concept vector being\ndeﬁned by untranslated sentences that are included after translation.\nType\nModel\nScalar\nMultiling.\nDanish set\nEnglish set\nTranslated\nHuman →\nIRR ρ\n✓\n0.63\n0.68\n0.58\n-\nIRR α\n✓\n0.67\n0.71\n0.60\n-\n↓Dictionary\nvader\n✓\n-\n-\n0.45\n0.51\nsyuzhet\n✓\n-\n-\n0.47\n0.50\n↓Multiling.\ntwitter-xlm\n×\n0.55\n0.50\n0.58\n0.56\nxlm-roberta\n×\n0.60\n0.59\n0.60\n0.57\nSentiment Projection\n✓\n0.66\n0.68\n0.60\n0.65*\n↓Danish\ndanish-sentiment\n×\n0.53\n0.47\n0.58\n0.55\nda-sentiment-base\n×\n0.23\n0.43\n0.08\n0.10\nMeMo-BERT-SA\n×\n0.48\n0.67\n0.25\n0.24\nTable 3 demonstrates that our Sentiment Projection model leads baselines in both multilingual\nand Danish-only evaluations. This gain likely reﬂects our use of a multilingual encoder for sentence\nembeddings and a “concept vector” deﬁned over a multilingual corpus. Concretely, Sentiment\nProjection attains Spearman’s ρ = 0.68 on the Danish subset (Fairytales + Hymns) versus ρ = 0.58\nfor the runner-up, and delivers a ρ = 0.06 absolute improvement in the overall multilingual setting.\nWe test our model for its generalization across time periods in Table 2, where danish hymns and\nfairytales represent historical language with texts from the 18-19th century. The Sentiment Projec-\ntion model shows no signs of reduced performance when processing older texts and outperforms\nthe follow-up model by ρ = 0.12 in the Hymns genre.\nNotably, twitter-xlm model appears to perform slightly better on sentences translated to\nEnglish than on their original Danish, as seen in Table 3. This may indicate that Google Translate\nrenders language in updated, contemporaneous forms, similar to the Twitter data used for model\ntraining. We see the same tendency (surprisingly) for the danish-sentiment model, i.e., better\nperformance when Danish sentences were translated to English. In contrast, Sentiment Projection\nperforms slightly better on the Danish set in its original form than when it is translated to English\n– which we consider validates its capacity to process older forms reliably.\n3.4\nPerformance on literary and non-literary contemporary data\nTo make sure that our model does not overﬁt its sentiment vector to the in-context sentiment cues\nof the stories in the Fiction4 corpus, we tested it against the EmoBank dataset – which consists of\ncontemporary literary and non-literary data. All Multilingual and dictionary-based models were\nalso tested for their correlation with the human gold standard of the EmoBank dataset. The Senti-\nment Projection Model still achieved the highest overall correlation with human ratings. Although\nit shows a lower correlation for a few genres (i.a., Letters), it still appears to generalize well to\ncontemporary out-of-training distribution data. It should be noted that the model outperforms the\n8\n\nother models the most in the ﬁction genre, indicating that the sentiment vector may be slightly\nﬁne-tuned or overﬁt to ﬁction-speciﬁc sentiment indicators.13\nTable 4: Spearman correlations on the EmoBank sentences (n = 8, 870) across domains. On top:\nInter Rater Reliability (Krippendorff’s α).\nScalar\nOverall\nLetters\nBlog\nNewspaper\nEssays\nFiction\nTravelguides\nHuman IRR α\n✓\n0.34\n0.34\n0.31\n0.29\n0.31\n0.35\n0.23\nvader\n✓\n0.43\n0.47\n0.41\n0.42\n0.32\n0.37\n0.35\nsyuzhet\n✓\n0.46\n0.47\n0.37\n0.42\n0.37\n0.43\n0.37\ntwitter-xlm\n×\n0.64\n0.69\n0.65\n0.61\n0.59\n0.57\n0.48\nxlm-roberta\n×\n0.65\n0.68\n0.65\n0.65\n0.58\n0.56\n0.49\nSentiment Projection\n✓\n0.67\n0.62\n0.61\n0.66\n0.53\n0.64\n0.52\n4\nDiscussion & conclusions\nAs seen in Table 3 and 4, the proposed Sentiment Projection model performs on par with or bet-\nter than the contemporary state-of-the-art methods. Moreover, Sentiment Projection allows for a\nsmooth continuous output. In contrast, methods converting model output are not continuous in\npractice, but rather return noncontinuous tri-modal distributions (Figure 3). While both methods\ncorrelate highly with the human golden standard, approaching the inter-rater correlation, it appears\nthat the Sentiment Projection approach more closely resembles the sentiment distribution of human\nratings.\nFurthermore, the Sentiment Projection method can be trained on multilingual data using a\nmultilingual language model, allowing for a language-agnostic sentiment prediction model that\nalso reliably handles historical variants. The Sentiment Projection was solely deﬁned by its concept\nvector, based on sentences from the Fiction4 dataset, half of which were in Danish, yet it still\noutperforms other models.\nWhile this paper corroborates the ﬁndings of [13], showing that translation (even without a\nquality check) to English increases the similarity of human and transformer-model scores, it also\nshows that this is not the case for Sentiment Projection, which performs slightly better on the\noriginal (Danish) sentences.\nFinally, the workﬂow presented in Figure 2 has been used to design a sentiment model, but\nallows easy generalization to other concepts of choice. The method could also work for other\nemotional concepts, such as emotion recognition, language detection, or abstract concepts like a\nnature-to-industry gradient. We encourage curious readers to search for inspiration for potential\nvectors in Linear Representation Hypothesis [25] and Steering Vector [35] literature. Due to the\nﬂexible nature of the algorithm, there is no rigid lower boundary on the number of training points\nrequired for a stable vector, although the chances of over-representing the non-concept context\nof training sentences naturally increase as the number of sentences decreases. A future empirical\ninvestigation of the stability of the vector when using smaller training sets would be a useful guide.\n13 While this can also be a drawback, it supports the idea that domain-speciﬁc sentiment analysis can be highly beneﬁcial.\nFor example, a sentiment analysis method for ﬁction should be sensitive to the speciﬁc omissive, implicit, and concrete\nobject-based sentiment cues that are rarer in other genres [3]. Feldkamp et al. [14] suggests that travel guides use similar\nmechanisms – sentiment is evoked through unsentimental, descriptive, and concrete detail. The fact that Sentiment\nProjection performs well also for both genres suggests it captures this kind of indirect sentiment expression.\n9\n\nAcknowledgements"}
{"id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "authors": ["Mukhammadsaid Mamasaidov", "Azizullah Aral", "Abror Shopulatov", "Mironshoh Inomjonov"], "source": "http://arxiv.org/abs/2508.14586v1", "page_count": 8, "text": "Filling the Gap for Uzbek: Creating Translation Resources for\nSouthern Uzbek\nMukhammadsaid Mamasaidov1 Azizullah Aral2\nAbror Shopulatov1,3 Mironshoh Inomjonov1\n1 Tilmoch 2 Academy of Sciences of Afghanistan 3 MBZUAI\nAbstract\nSouthern Uzbek (uzs) is a Turkic language\nvariety spoken by around 5 million people\nin Afghanistan and differs significantly from\nNorthern Uzbek (uzn) in phonology, lexicon,\nand orthography. Despite the large number of\nspeakers, Southern Uzbek is underrepresented\nin natural language processing. We present new\nresources for Southern Uzbek machine transla-\ntion, including a 997-sentence FLORES+ dev\nset, 39,994 parallel sentences from dictionary,\nliterary, and web sources, and a fine-tuned\nNLLB-200 model (lutfiy). We also propose\na post-processing method for restoring Arabic-\nscript half-space characters, which improves\nhandling of morphological boundaries.\nAll\ndatasets, models, and tools are released pub-\nlicly to support future work on Southern Uzbek\nand other low-resource languages.\n1\nIntroduction\nThe Southern Uzbek language, spoken by ap-\nproximately 5 million Uzbeks residing across 14\nprovinces of Afghanistan, represents a distinct lin-\nguistic variety that has developed independently\nfrom Northern Uzbek over centuries (Ethnologue,\n2025a).\nUzbek as a whole is classified as a\nmacrolanguage according to ISO 639-3 standards,\nencompassing multiple related varieties includ-\ning Northern Uzbek (uzn) spoken primarily in\nUzbekistan, and Southern Uzbek (uzs) prevalent in\nAfghanistan (Ethnologue, 2025b).\nThis macrolanguage classification recognizes the\nsignificant linguistic diversity within the broader\nUzbek language family, where individual vari-\neties have developed distinct phonological, lexi-\ncal, and grammatical features due to geographical\nseparation and contact with other languages. As\npart of the global Uzbek population exceeding 34\nmillion people, Southern Uzbek is recognized in\nAfghanistan’s Constitution as a potential third offi-\ncial language in regions where it is the majority lan-\nguage, in addition to Pashto and Dari. (Afghanistan,\n2004)\nSouthern Uzbek functions as a fully developed\nliterary language that meets the demands of liter-\nature, art, culture, and science. It maintains ac-\ntive presence across multiple domains including\ntechnology, education, diplomacy, banking, and\ncommerce. The language is taught in Southern\nUzbek departments at seven national universities in\nAfghanistan and serves as the medium of instruc-\ntion in 970 schools distributed across provinces: 9\nschools in Badakhshan, 80 in Balkh, 450 in Faryab,\n50 in Samangan, 300 in Sar-e-Pol, and 80 in Takhar.\n(Olim Labib, 2020)\nInternational media outlets including BBC, Ra-\ndio Free Europe/Radio Liberty (Ozodlik), Voice\nof America, Voice of Iran, TRT Avaz, and Sputnik\nactively broadcast in Southern Uzbek, alongside\nAfghan media channels such as Oyna, Botur, Al-\nmas, Orzu, Nur, Oriano, Kalid, and National Radio\nand Television. The language maintains expanding\ndigital presence across major online platforms in-\ncluding Wikipedia, Google, Facebook, and other\nsocial networks.\nDespite this linguistic vitality, Southern Uzbek\nremains underrepresented in natural language pro-\ncessing technologies. Major translation platforms\nlike Google Translate (Google, 2025) currently pro-\nvide limited or no support for this language variety,\nhighlighting the critical need for dedicated com-\nputational resources. As a low-resource language\nwith unique characteristics distinct from Northern\nUzbek, Southern Uzbek presents significant chal-\nlenges for machine translation systems.\nThis study, conducted as part of the Open Lan-\nguage Data Initiative (OLDI) shared task, addresses\nthese challenges by developing specialized neural\nmachine translation models for Southern Uzbek.\nOur contributions parallel recent advances in low-\nresource language processing and include:\n1. A FLORES+ dev dataset translated to South-\n\nern Uzbek containing 997 sentences\n2. Parallel corpora for various language pairs\nwith Southern Uzbek\n3. Open-sourced fine-tuned neural models for\nSouthern Uzbek translation\n4. Comprehensive evaluation against existing\nbaselines\nOur research aims to advance machine transla-\ntion capabilities for Southern Uzbek, contributing\nto the larger OLDI objective of expanding linguistic\ndiversity in NLP technologies for underrepresented\nlanguage varieties.\n2\nLinguistic Background\n2.1\nHistorical Development\nSouthern Uzbek belongs to the Turkic language\nfamily, specifically derived from the Karluk-Chigil-\nUyghur dialectal group with partial influences from\nthe Kipchak and Oghuz branches. The language\nrepresents the contemporary form of a literary tra-\ndition spanning over a millennium, with histori-\ncal continuity traceable through classical poets in-\ncluding Khwarizmi, Lutfi, Atayi, Sakkaki, Navoi,\nBabur, lutfiy, and Ogahi. Notably, while these\nhistorical figures did not identify themselves as\n“Uzbek”, they wrote in a language that forms the\nfoundation of modern Southern Uzbek, demonstrat-\ning the language’s independent development into a\nmature linguistic system. (Habibi Aral, 2021)\nHistorically, Southern Uzbek served as the ad-\nministrative and literary language for major dynas-\nties including the Yaftids, Kushans, Ghaznavids,\nSeljuks, Timurids, and Mughals, who governed\nterritories across Afghanistan and India for cen-\nturies using this language and established profound\ncultural legacies. (Tursunov and O‘rinboyev, 1982)\n2.2\nWriting System\nSouthern Uzbek employs the Arabic script, which\nhas served as the official writing system for Afghan\nlanguages for over a thousand years. This ortho-\ngraphic system presents unique challenges and\ncharacteristics that distinguish it from Latin-based\nNorthern Uzbek.\nThe Arabic-based script includes only three\nvowel letters:( اa/o),( وu/o‘), and( یi/y). This\nlimited vowel representation often misleads learn-\ners into believing that Uzbek contains only three\nvowel sounds. However, vowel quality distinctions\nbecome evident in minimal pairs such as shown in\nFigure 1.\nkuz (autuⅿn) ()ﮐﻮﺯko‘z (eye) ()ﮐﯣﺯ\nyeⅼ(winⅾ) ()ﯾﯧﻞyiⅼ(year) ()ﯾﯿﻞ\nquroⅼ(weapon) ()ﻗﻮﺭﺍﻝⅿaraⅼ(ⅾeer) ( ﻝ\n)ﻣﺮﻩ\nFigure 1: Vowel differences in Southern Uzbek\nStandard Uzbek contains six primary vowels\n(with additional dialectal variants), yet the Arabic\nscript lacks direct representation for half of them.\nThese vowels require indication through diacritical\nmarks (fatha, damma, kasra), which are frequently\nomitted in practical writing, thereby complicating\naccurate reading and pronunciation.\nAdditional complexity arises from the dual func-\ntionality of certain letters. The Arabic letter( هh)\nfunctions both as vowel and consonant. Similarly,\nlettersوand( یwaw and ya) serve dual roles as\nvowels and consonants (“v” and “y”) depending on\ncontext as illustrated in Figure 2.\nUzbek Southern\nUzbek Northern\nSounⅾ\nⅬetter(ﻩh) ⅾuaⅼroⅼes\nﺩﯼ\nﺑﯿﻠﺪﯾﺮﻩbiⅼⅾiraⅾi\n/a/\nﻫﻮﺱhavas\n/h/\nⅬetter(ﻭwaw) ⅾuaⅼroⅼes\nﻭﻃﻦvatan\n/v/\nﺗﻮﺯtuz\n/u/\nⅬetter(ﯼya) ⅾuaⅼroⅼes\nﺑﺎﯼboy\n/y/\nﻓﯿﻞfiⅼ\n/i/\nFigure 2: Dual letters in Southern Uzbek\nSouthern Uzbek\nNorthern Uzbek\nⅯeaning\nExaⅿpⅼes with suffix “−ⅽhi”\nﭼﯽ−ﭼﺎﯾﺨﺎﻧەⅽhoyxonaⅽhi\nteahouse keeper\nﺍﺩﺑﯿﺎﺗﭽﯽaⅾabiyotⅽhi\nwriter\nⅭoⅿpounⅾworⅾs anⅾprefixes\nﺗﺸﻮﯾﺶ−ﺑﯥbetashvish\nⅽarefree\nﺍﻧﺼﺎﻑ−ﻧﺎnoinsof\nⅾishonest\nFigure 3: Examples of standardized Southern Uzbek\nArabic-script orthography showing mandatory half-\nspace (zero-width non-joiner, U+200C) placement. Red\nmarks indicate the location of half-spaces in suffixation\nafter vowel-final stems and in prefix attachment.\nArabic and Persian loanwords maintain their\noriginal orthographic forms, typically without\nvowel markings.\n\n2.3\nMorphological Structure\nSouthern Uzbek exhibits rich agglutinative mor-\nphology characteristic of Turkic languages. The\nlanguage employs extensive suffixation systems\nthat can be classified into various functional cate-\ngories:\n• Nominalizers (noun-forming suffixes)\n• Adjectival suffixes\n• Verb formers\n• Tense and aspect markers\n• Other functional and derivational affixes\nStandardized orthographic rules govern affix at-\ntachment in Southern Uzbek Arabic script.\nA\nfundamental principle distinguishes between suf-\nfixes attached to vowel-final versus consonant-final\nstems ( -chi, -chilik, -lik, -li, etc.).\nThese suffixes require half-space (also known\nas zero-width non-joiner, U+200C, also found in\nFarsi) separation when attached to stems ending in\nvowels (represented by Arabic letters,)ا ,و ,هwhile\nconnecting directly to consonant-final stems.\nSouthern Uzbek also employs prefixes, com-\nmonly found in Persian or Arabic loanwords, for\nforming adjectives or adverbs. These prefixes (be-,\nno-, xo‘sh-, ser-, ba-, ham-, bad-) are written with\nhalf-space separation, as shown in Figure 3.\n2.4\nContemporary Status and Challenges\nDespite its historical significance, Southern Uzbek\nhas faced political marginalization over the past\nthree centuries, with Turkic peoples in Afghanistan\nbeing sidelined in governance and education.\nProgress began in the 1970s when Uzbek par-\nliamentary representatives secured broadcasting\nrights on Afghan national radio. The 1978 rise of\nthe People’s Democratic Party marked further ad-\nvancement with the publication of the Yulduz news-\npaper in Southern Uzbek, establishment of Uzbek\nLanguage and Literature departments, and expan-\nsion of Uzbek-medium education. (Aral, 2025)\nThe 2001 democratic reforms in Afghanistan\nformally granted Southern Uzbek official status,\nrecognizing its role in Afghan multilingual society.\nHowever, challenges remain in standardizing or-\nthographic practices and developing computational\nresources for this linguistically rich but technologi-\ncally underrepresented variety.\n3\nRelated Work\nMachine translation for low-resource languages\nhas gained significant attention, with researchers\nexploring various approaches from data augmenta-\ntion to multilingual transfer learning. Dale (2022)\ndeveloped the first neural MT system for Erzya, a\nlow-resource Uralic language, demonstrating how\nextensive data mining from diverse sources (Bible\ntexts, dictionaries, digitized books) can yield func-\ntional translation systems despite limited parallel\ndata. Similarly, P M et al. (2024) focused on low-\nresource Indic languages by fine-tuning multilin-\ngual models and employing back-translation with\ncareful quality filtering, showing that selective data\naugmentation can improve performance when syn-\nthetic data is judiciously filtered.\nGoyle et al. (2023) systematically evaluated\nstrategies for compensating data scarcity in lan-\nguages like Sinhala, Nepali, Khmer, and Pashto.\nThey found that combining back-translation with\nfocal loss yields substantial improvements, particu-\nlarly when leveraging large monolingual corpora\nand transfer learning from related high-resource\nlanguages.\nRecent advances in large language models have\nalso shown promise for low-resource translation\ntasks. Commercial LLMs like GPT-4 and Claude\ndemonstrate multilingual capabilities that extend\nto languages not explicitly included in their train-\ning data, offering competitive performance through\nfew-shot learning approaches.\nDespite these advances, Southern Uzbek remains\nlargely unexplored in computational linguistics.\nWhile Northern Uzbek has received some attention\nin multilingual models like NLLB (NLLB Team\net al., 2022) and MADLAD-400 (Kudugunta et al.,\n2024), the Southern Uzbek has been left behind.\nOur work represents the first dedicated effort to\ndevelop neural translation resources for this variety\nof Uzbek.\n4\nDatasets\n4.1\nFLORES+ Dev Dataset\nThis study introduces the Southern Uzbek FLO-\nRES+ dev dataset, comprising 997 sentences trans-\nlated from English to Southern Uzbek (see Figure\n4).\nThe dataset was developed under the Open Lan-\nguage Data Initiative (OLDI) framework.\nOne\nnative Southern Uzbek linguist was responsible\n\nEngⅼish\nThe airⅽraft haⅾbeen heaⅾeⅾto Irkutsk anⅾwas being operateⅾby interior troops.\nNorthern Uzbek\nSaⅿoⅼyot Irkutsk toⅿon yo‘ⅼoⅼgan va iⅽhki qo‘shinⅼar toⅿoniⅾan boshqariⅼayotgan eⅾi.\nSouthern Uzbek\n. ﯾﺎﺗﮕﻦﺍﯦﺪﯼ\nﺭﯾﻠﻪ\nﺍﻭﭼﺎﻕﺍﯾﺮﮐﻮﺗﺴﮏﺗﺎﻣﺎﻥﯾﯣﻝﺁﻟﮕﻦﻭﺍﯾﭽﮑﯽﻗﯣﺷﯿﻨﻠﺮﺗﺎﻣﺎﻧﯿﺪﻥﺑﺎﺷﻘﻪ\nFigure 4: Example from the FLORES+ dataset in English, Northern Uzbek and Southern Uzbek.\nfor the translation process, with subsequent post-\nreview process to ensure linguistic accuracy and\ncultural appropriateness. All Southern Uzbek trans-\nlations strictly adhere to the Arabic script ortho-\ngraphic conventions, including proper implemen-\ntation of half-space characters (U+200C) for mor-\nphological boundaries as described in Section 2.3.\nGiven the complexity of Arabic script represen-\ntation and the morphologically rich nature of South-\nern Uzbek, particular attention was paid to main-\ntaining consistent orthographic standards through-\nout the translation process. The translation process\nfollowed standardized conventions for affix attach-\nment, vowel representation, and proper handling of\nArabic and Persian loanwords within the Southern\nUzbek linguistic system.\n4.2\nTraining Data\nThe training dataset comprises diverse parallel cor-\npora sourced from three primary domains, total-\ning 39,994 sentence pairs across multiple language\ncombinations:\n1. Dictionary Entries (1,550 pairs): Parallel\ndictionary entries mapping Northern Uzbek\nto Southern Uzbek lexical items (Aral, 2024).\nThese entries provide direct lexical correspon-\ndences and serve as high-quality alignment\ndata for closely related language varieties.\n2. Literary Corpus (35,865 pairs): Parallel sen-\ntences extracted through careful alignment\nfrom 27 selected books available in both\nNorthern and Southern Uzbek variants. This\ncorpus represents the largest component of\nour training data and captures literary register\nvariations, complex syntactic structures, and\ncultural terminology.\n3. Web-sourced Content (2,579 pairs): Parallel\nsentences of English-Southern Uzbek mined\nfrom official government websites and reliable\nonline resources. This component provides\ncontemporary usage patterns and domain-\nspecific terminology from governmental and\ninstitutional contexts.\n4.3\nData Mining Process\nThe sentence alignment process presented unique\nchallenges due to Southern Uzbek’s underrepresen-\ntation in existing multilingual models. Our align-\nment methodology employed a two-stage approach\nto maximize extraction efficiency.\nFor literary corpus alignment, we initially ap-\nplied LaBSE embeddings (Feng et al., 2020) di-\nrectly to the original Arabic script texts. While\nLaBSE does not include Southern Uzbek in its\ntraining data, the model demonstrated limited align-\nment capability, likely due to shared vocabulary\nwith other Turkic languages in the embedding\nspace.\nTo improve alignment quality, we implemented a\ntransliteration-based enhancement strategy. South-\nern Uzbek texts were transliterated from Arabic to\nLatin script using rule-based conversion scripts1,\nwhich enabled more effective cross-lingual em-\nbedding alignment. This transliteration approach\nyielded a 40% more successfully aligned sentence\npairs compared to direct Arabic script processing.\nThe sentence alignment methodology follows\nestablished practices from low-resource language\nprocessing (Dale, 2022). We utilize LaBSE to gen-\nerate embeddings for each potential sentence pair,\ncalculate cosine similarity between embeddings,\nand adjust similarity scores using length ratios.\nFor web-sourced English-Southern Uzbek data,\nwe employed a reverse translation verification ap-\nproach. Southern Uzbek sentences were translated\nto English using Gemini-2.0-Flash, followed by\nLaBSE-based alignment between original English\ncontent and back-translated English.\nThis pro-\ncess underwent manual review to ensure translation\nquality and semantic fidelity.\nA notable preprocessing challenge emerged re-\ngarding half-space character consistency. Due to\nOCR limitations and editorial inconsistencies in\nsource materials, half-space characters (U+200C)\nwere frequently omitted, incorrectly rendered as\nfull spaces, or merged with adjacent characters.\nWhile this issue complicates training data quality,\nwe address it through post-processing correction\n1https://github.com/tahrirchi/uzs-scripts\n\nModel\nuzs-en\nuzs-uzn\neng-uzs\nuzn-uzs\ngpt-4.1\n24.90 / 53.42\n2.634 / 3.657\n0.48 / 9.49\n1.42 / 21.55\ngemini-2.0-flash-001\n32.81 / 58.80\n62.45 / 73.67\n1.59 / 24.47\n6.96 / 41.11\nclaude-sonnet-4\n22.25 / 51.46\n59.18 / 83.63\n0.68 / 15.38\n2.62 / 28.85\nnllb-200-600M\n3.73 / 23.88\n4.14 / 27.02\n-\n-\nGoogle Translate\n9.56 / 33.58\n5.13 / 33.19\n-\n-\nmadlad400-3b-mt\n2.95 / 23.26\n0.19 / 1.41\n-\n-\nlutfiy (no half-space fix)\n11.26 / 34.39\n53.48 / 78.54\n1.33 / 25.43\n25.99 / 66.44\nlutfiy (with half-space fix)\n1.58 / 26.61\n34.31 / 71.11\nTable 1: Evaluation of several models on sacreBLEU/chrF++ across various language pairs involving English,\nNorthern Uzbek (uzn) and Southern Uzbek (uzs).\nmechanisms described in Section 4.\n5\nTranslation Experiments\n5.1\nModel Training\nOur experimental framework employed the nllb-\n200-distilled-600M model as the foundation for\nSouthern Uzbek machine translation development.\ne maintained the original tokenizer configuration,\nleveraging the model’s existing multilingual capa-\nbilities for Turkic language processing.\n5.1.1\nTraining Configuration\nFor the training process we employed the Adafac-\ntor (Shazeer and Stern, 2018) optimizer paired with\na learning rate of 1 × 10−4 following a constant\nschedule and 1000 warmup steps. A weight decay\nof 1×10−3 was applied, and the batch size was set\nto 32 due to GPU memory constraints. The max-\nimum sequence length was limited to 128 tokens,\nand training was conducted for 5000 steps, corre-\nsponding to approximately 2–3 epochs. All experi-\nments were run on a single A100 40GB GPU. The\nAdafactor optimizer was chosen for its memory\nefficiency and proven effectiveness in transformer\nfine-tuning scenarios, while the conservative learn-\ning rate and weight decay values were selected\nto mitigate overfitting given the small size of the\ntraining dataset.\n5.1.2\nModel Variant\nWe fine-tuned nllb-200-distilled-600M (NLLB\nTeam et al., 2022) model on the complete 39,994\nsentence pair corpus. Our model called lutfiy2\nmaintains the original NLLB tokenizer and vocabu-\nlary, relying on existing Turkic language represen-\ntations for Southern Uzbek processing.\n2Lutfi, a 15th-century Central Asian poet\n5.1.3\nHalf-Space Post-Processing\nA critical technical challenge emerged regarding\nthe handling of half-space characters. The NLLB\nSentencePiece (Kudo and Richardson, 2018) tok-\nenizer normalizes half-space characters (U+200C)\nto regular spaces during preprocessing, prevent-\ning the model from learning proper morphological\nboundary representation. This problem affects not\nonly Southern Uzbek but also extends to other lan-\nguages requiring half-space characters, including\nPersian (Doostmohammadi et al., 2020).\nTo address this limitation, we developed a\ncharacter-level n-gram post-processing model that\npredicts half-space insertion positions. The model\nwas trained on a small set of training data with cor-\nrected half-space characters. It analyzes character\nsequences and applies statistical rules to determine\nwhether half-spaces should follow specific vowel\nendings in morphologically complex constructions.\nThis approach provides a practical solution to the\ntokenizer normalization problem while maintaining\ncompatibility with existing NLLB infrastructure.\nThe post-processing correction mechanism is made\npublicly available alongside our trained models3.\n5.2\nEvaluation Framework\nModel performance was assessed using two widely\nadopted metrics for translation tasks: sacreBLEU\n(Post, 2018), a standardized BLEU implementation\nthat ensures consistent n-gram precision measure-\nment across experiments, and chrF++ (Popovi´\nc,\n2017), a character-level F-score metric that is par-\nticularly well-suited for evaluating morphologically\nrich languages such as Southern Uzbek. All results\nare reported on the FLORES+ dev set, enabling\ncomparability with other low-resource language\ninitiatives under the OLDI framework.\n3https://huggingface.co/tahrirchi/lutfiy\n\n6\nResults and Discussion\nOur evaluation on the FLORES+ Southern Uzbek\ndev set reveals several key insights into the per-\nformance of various translation approaches. The\nresults, presented in Table 1, demonstrate signifi-\ncant performance variations across different model\narchitectures and translation directions.\nNotably, large language models exhibit supe-\nrior performance in understanding Southern Uzbek\ncontent, particularly in uzs-* directions. Gemini-\n2.0-Flash achieves the highest scores for uzs-en\ntranslation (32.81 BLEU/58.80 chrF++), while\nClaude-Sonnet-4 excels in uzs-uzn translation qual-\nity (83.63 chrF++). This suggests that LLMs’ ex-\ntensive multilingual pretraining enables effective\ncomprehension of low-resource language varieties,\neven without explicit training on Southern Uzbek\ndata.\nIn contrast, traditional MT systems like\nGoogle Translate and specialized multilingual mod-\nels (NLLB-200-600M, MaLLaD400) demonstrate\nsubstantially lower performance, highlighting the\nchallenges these architectures face with underrep-\nresented languages.\nHowever, our fine-tuned lutfiy model demon-\nstrates clear advantages in generation tasks. For\ntranslation into Southern Uzbek (en-uzs and uzn-\nuzs), our model consistently outperforms all base-\nlines, achieving 1.58 BLEU/26.61 chrF++ for en-\nuzs and 34.31 BLEU/71.11 chrF++ for uzn-uzs di-\nrections. This validates our approach of fine-tuning\non domain-specific parallel corpora, as the model\nlearns proper Southern Uzbek generation patterns\nthat generic LLMs cannot replicate effectively.\nThe impact of our half-space post-processing\ncorrection is particularly evident in the uzn-uzs\ntranslation pair. While chrF++ scores show modest\nimprovements (from 66.44 to 71.11), BLEU scores\nincrease dramatically (from 25.99 to 34.31), repre-\nsenting a 32% relative improvement. This substan-\ntial BLEU gain with stable chrF++ performance\nindicates that the half-space correction primarily\naddresses tokenization boundary issues rather than\nfundamental translation errors. Since BLEU re-\nlies on exact n-gram matches, incorrect half-space\nplacement can artificially deflate scores even when\nthe underlying translation quality remains high.\nFor the closely related uzs-uzn translation di-\nrection, Gemini-2.0-Flash demonstrates excep-\ntional generation capability (62.45 BLEU), signifi-\ncantly outperforming our specialized model (53.48\nBLEU). This suggests that LLMs may be particu-\nlarly effective at cross-dialectal translation within\nthe same language family, possibly due to their\nability to capture subtle linguistic variations during\npretraining.\nThese\nfindings\nhighlight\ncomplementary\nstrengths between LLMs and specialized fine-\ntuned models: while LLMs excel at understanding\nand translating from Southern Uzbek, targeted\nfine-tuning proves essential for high-quality\ngeneration into Southern Uzbek, particularly for\nmorphologically complex constructions requiring\nproper orthographic conventions.\n7\nConclusion\nOur study presents the first comprehensive neural\nmachine translation resources for Southern Uzbek,\naddressing a significant gap in computational lin-\nguistics for this underrepresented Turkic variety.\nOur key contributions include:\n1. Creation of a 997-sentence FLORES+ dev\ndataset for Southern Uzbek\n2. Development of 39,994 parallel sentence pairs\nacross multiple language combinations (uzs-\nuzn, uzs-en)\n3. Fine-tuned NLLB-200 model (lutfiy) opti-\nmized for Southern Uzbek translation\n4. Post-processing methodology for Arabic\nscript half-space character restoration\n5. Open-sourced datasets, models, and evalua-\ntion tools\nFuture work will focus on expanding dataset cov-\nerage through additional literary sources and gov-\nernment documents, exploring data augmentation\ntechniques using large language models, and devel-\noping more sophisticated orthographic normaliza-\ntion approaches for Arabic script processing.\n8\nLimitations\nSeveral limitations constrain our current approach.\nThe training dataset size of ~40K sentence pairs,\nwhile substantial for a low-resource language, may\nlimit generalization across diverse domains. Our\nheavy reliance on literary sources potentially bi-\nases the model toward formal registers, possibly\naffecting performance on conversational or tech-\nnical content. The half-space post-processing so-\nlution, while effective, represents a workaround\n\nrather than addressing the underlying tokenizer\nlimitations. Additionally, our evaluation relies pri-\nmarily on automatic metrics, which may not fully\ncapture translation quality nuances for morphologi-\ncally complex languages like Southern Uzbek. Hu-\nman evaluation studies would provide more com-\nprehensive quality assessment.\n9\nAcknowledgements\nWe thank the Open Language Data Initiative\n(OLDI) for supporting this research and David Dale\nfor his valuable guidance throughout the project.\nThe authors thank the Google for Startups Pro-\ngram for providing the computational resources\nthat made this research possible."}
{"id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "authors": ["Guilhem Fauré", "Mostafa Sadeghi", "Sam Bigeard", "Slim Ouni"], "source": "http://arxiv.org/abs/2508.14574v1", "page_count": 9, "text": "Towards Skeletal and Signer Noise Reduction in Sign Language\nProduction via Quaternion-Based Pose Encoding and Contrastive\nLearning\nGuilhem Fauré\nguilhem.faure@inria.fr\nUniversité de Lorraine, CNRS, Inria, LORIA\nF-54000 Nancy, France\nMostafa Sadeghi\nmostafa.sadeghi@inria.fr\nUniversité de Lorraine, CNRS, Inria, LORIA\nF-54000 Nancy, France\nSam Bigeard\nsam.bigeard@inria.fr\nUniversité de Lorraine, CNRS, Inria, LORIA\nF-54000 Nancy, France\nSlim Ouni\nslim.ouni@loria.fr\nUniversité de Lorraine, CNRS, Inria, LORIA\nF-54000 Nancy, France\nAbstract\nOne of the main challenges in neural sign language production\n(SLP) lies in the high intra-class variability of signs, arising from\nsigner morphology and stylistic variety in the training data. To im-\nprove robustness to such variations, we propose two enhancements\nto the standard Progressive Transformers (PT) architecture (Saun-\nders et al., 2020). First, we encode poses using bone rotations in\nquaternion space and train with a geodesic loss to improve the accu-\nracy and clarity of angular joint movements. Second, we introduce\na contrastive loss to structure decoder embeddings by semantic\nsimilarity, using either gloss overlap or SBERT-based sentence sim-\nilarity, aiming to filter out anatomical and stylistic features that do\nnot convey relevant semantic information.\nOn the Phoenix14T dataset, the contrastive loss alone yields a 16%\nimprovement in Probability of Correct Keypoint over the PT base-\nline. When combined with quaternion-based pose encoding, the\nmodel achieves a 6% reduction in Mean Bone Angle Error. These\nresults point to the benefit of incorporating skeletal structure mod-\neling and semantically guided contrastive objectives on sign pose\nrepresentations into the training of Transformer-based SLP models.\nCCS Concepts\n• Computing methodologies →Natural language generation.\nKeywords\nSign Language Production, Deep Learning, Contrastive Learning,\nPose Encoding\nACM Reference Format:\nGuilhem Fauré, Mostafa Sadeghi, Sam Bigeard, and Slim Ouni. 2025. To-\nwards Skeletal and Signer Noise Reduction in Sign Language Production\nvia Quaternion-Based Pose Encoding and Contrastive Learning. In ACM\nInternational Conference on Intelligent Virtual Agents (IVA Adjunct ’25), Sep-\ntember 16–19, 2025, Berlin, Germany. ACM, New York, NY, USA, 9 pages.\nhttps://doi.org/10.1145/3742886.3756728\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nIVA Adjunct ’25, Berlin, Germany\n© 2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-1996-7/2025/09\nhttps://doi.org/10.1145/3742886.3756728\n1\nIntroduction\nSign language is the main way of communication used in the deaf\nand hard-of-hearing (DHH) community. It leverages a wide range\nof manual (handshape, location, orientation, movement) and non-\nmanual features (facial expression, body orientation, intensity)\nto convey ideas through a specific syntax and a rich vocabulary\n[26, 37, 38].\nWith around 5% of the global population affected by a disabling\nhearing loss, and a projection of over 700 million DHH people in\n2050 according to the World Health Organization [47], it is essential\nto reduce the communication gap between deaf and hearing people,\nnotably to avoid social exclusion. As a response to this growing\nneed, a part of the research community has been working on de-\nveloping new technologies for sign language recognition (SLR),\nsign language translation (SLT)—sign-to-text—and sign language\nproduction (SLP)—text-to-sign—tasks [2, 4, 6–8, 34, 40].\nIn the effort to develop digital tools that foster communication be-\ntween deaf and hearing communities, the rise of deep learning has\nled to significant progress in recent years, particularly in SLR and\nSLT [3, 4, 7, 22, 30]. Since 2020, an increasing number of studies\nhave focused on generating sign sequences from spoken language\n[1, 8, 9, 16, 24, 34, 39, 41, 42, 45, 50, 51, 55]. Among these, Saunders\net al. [34] introduced the Progressive Transformers (PT) architec-\nture, which has since emerged as a standard baseline in the field.\nDespite these advances, SLP systems continue to face several\nfundamental challenges that hinder their usability in real-world\napplications. Key obstacles include the high intra-class variability of\nsigns, the significant grammatical divergence between signed and\nspoken languages, and the scarcity of large-scale, annotated datasets\nwith diverse vocabularies [32]. As a result, the generated outputs\noften lack the intelligibility, fluency, and naturalness required for\ndeployment in practical communication scenarios.\nIn this work, we specifically address one of the core limitations\nof current SLP models: the visual variability of sign realizations,\nwhich introduces noise during training and impairs generalization.\nThis variability arises primarily from two sources:\n• Inter-signers morphological differences, such as variations\nin bone lengths, which are not fully addressed by standard\nnormalization techniques (e.g. (Stoll et al. [40]));\n\nIVA Adjunct ’25, September 16–19, 2025, Berlin, Germany\nGuilhem Fauré, Mostafa Sadeghi, Sam Bigeard, and Slim Ouni\n• Stylistic variations in the performance of a given sign or\nsentence—manifested through differences in amplitude, ve-\nlocity, or positional noise—both across signers and within\nthe same signer.\nTo mitigate the impact of these factors, we build upon the PT\narchitecture and introduce two main contributions:\n• We represent skeletal poses using bone rotations encoded\nas quaternions rather than traditional 3D Cartesian joint\ncoordinates, and replace the mean squared error (MSE) loss\nwith a geodesic loss defined in quaternion space;\n• We incorporate a contrastive loss into the training objective\nto structure the decoder’s multi-head self-attention embed-\ndings by pulling closer sequences with similar semantics and\npushing apart dissimilar ones. We investigate two variants\nof this loss: one based on lexical overlap in the associated\nglosses (similar to the loss used in (Walsh et al. [45]) for\nthe construction of their codebook), and another leveraging\nsentence Transformer embeddings (SBERT [33]) similarity\nscores between associated sentences, with the aim to capture\nsubtler semantic relations.\nThese contributions aim to reduce the effect of non-semantic\nvariability in training data and improve the semantic consistency\nand expressiveness of generated sign sequences.\nWe evaluate our approach on the widely used Phoenix14T dataset.\nCode and demos are available online1.\n2\nRelated Work\n2.1\nSign Language Production\nEarly approaches to SLP were primarily based on synthetic anima-\ntion techniques relying on avatars and lookup tables containing\npre-generated sequences for predefined sentences [2, 6, 12, 20, 56].\nThese methods required the preparation and storage of a large set of\nsentence-sign pairs, making them costly and limiting their flexibil-\nity. Furthermore, the resulting avatar animations were often poorly\nreceived by the Deaf community due to their under-articulated,\nrobotic, or unnatural movements [25].\nIn recent years, progress in deep neural architectures has sig-\nnificantly advanced research in SLP. Stoll et al. [39] were the first\nto propose generating sign language pose videos from text using\na three-stage pipeline: text-to-gloss conversion via a sequence-to-\nsequence model, motion graph-based sign stitching, and skeletal\npose-to-video synthesis using a generative adversarial network\n(GAN). Saunders et al. [34] introduced a more streamlined autore-\ngressive model that directly maps sentences or glosses to 3D skeletal\nposes using a Progressive Transformers architecture. They encode\neach frame’s temporal position in the sequence by appending a nor-\nmalized counter value 𝑡\n𝑇to its joint embedding. Subsequent exten-\nsions of this model have incorporated data augmentation techniques\n(e.g., adding Gaussian noise, predicting multiple frames simulta-\nneously), adversarial training, and mixture density networks [35],\nas well as skeletal graph self-attention mechanisms in the decoder\n[36]. These improvements target the regression-to-the-mean effect\nin predicted signs and reduce error propagation during decoding.\nMore recent approaches combine Transformer or diffusion-based\n1https://github.com/GFaure9/ContQuat-PT\narchitectures with vector quantization techniques to discretize the\nsign pose space [45, 50, 55]. Typically, this involves a two-stage pro-\ncess: first, a Vector Quantized Variational Autoencoder (VQ-VAE)\nis trained to encode sequences of sign poses into discrete tokens\nby constructing a codebook; second, a model is trained to predict\nthese tokens from textual input.\nWhile some recent models aim to generate avatar-based outputs\n[1, 55], the majority represent sign poses as 2D or 3D skeletal data\nand optimize a loss function based on the Cartesian coordinates of\njoints. However, this approach introduces several limitations:\n• The same sign performed by individuals with different body\nmorphologies can lead to significant variations in joint coor-\ndinates;\n• Computing the MSE over all joints tends to underweight\nthe hands—critical for sign articulation—leading to reduced\nexpressiveness;\n• This representation ignores the underlying skeletal structure,\nrequiring the model to implicitly learn limb dependencies,\nwhich may result in suboptimal performance.\nTo address these issues, some studies have introduced body-\npart-specific loss functions [1, 43, 55], or additional representations\nusing bone orientation vectors in R3, minimizing an MSE between\npredicted and reference orientations [41]. However, the latter over-\nlooks the non-Euclidian geometry of rotational space, and may\nmisrepresent angular differences, limiting the precision needed to\nmodel fine-grained articulations.\nFinally, although pose tokenization effectively reduces stylistic\nvariability [45], learning a robust codebook remains a non-trivial\nchallenge, and may constrain the model’s ability to generate novel\nor unseen signs.\n2.2\nRotational Pose Encoding\nInstead of representing human motion as sequences of joint posi-\ntions, an alternative is to describe it through bone rotations. In this\nframework, each pose is reconstructed by recursively applying a\nsequence of bone rotations to a predefined skeletal structure in a\nresting (\"T\") pose, starting from the root joint. This representation\nhelps prevent prediction errors caused by inconsistent bone lengths\nor anatomically implausible motions.\nRotational pose encoding relative to a given skeletal structure\nhas been used in various works on human motion recognition and\nprediction [10, 28, 29, 44]. Rotations can be parameterized in several\nways, including 3×3 rotation matrices, Euler angles, exponential\nmaps, and quaternions [13, 15]. However, many of these parame-\nterizations present disadvantages for deep learning applications.\nFor example, rotation matrices require enforcing six nonlinear con-\nstraints to remain within the 3D rotation group 𝑆𝑂(3), while Euler\nangles are prone to gimbal lock when two rotation axes become\naligned, leading to the loss of one degree of freedom. More broadly,\nsince R3 cannot be smoothly mapped to 𝑆𝑂(3), exponential maps\nmay also lead to singularities.\nUnit quaternions offer a robust and efficient alternative by rep-\nresenting rotations in 4D space, avoiding these common pitfalls.\nThey are numerically stable, support smooth interpolation, and\nsimplify the composition of rotations [13]. Quaternions have been\n\nTowards Noise Reduction in SLP via Quaternions and Contrastive Learning\nIVA Adjunct ’25, September 16–19, 2025, Berlin, Germany\nsuccessfully employed in recurrent models for human motion un-\nderstanding [28], and more recently in sign language processing to\nconstruct sign language action embeddings [46].\nDespite their strengths, quaternions—like all representations in\nfour or fewer dimensions—are inherently discontinuous represen-\ntations of 3D rotations, as shown in (Zhou et al. [54]). The authors\ndemonstrate that continuous representations of 3D rotations can\nbe defined in 5D and 6D, making them better suited for learning.\nHowever, quaternions remain an attractive choice for our applica-\ntion, due to their compact 4D representation and low computational\noverhead while resolving common issues of classical 3D rotation\nrepresentations.\n2.3\nContrastive Learning\nContrastive learning is a machine learning paradigm in which\nmodels learn more effective representations by comparing sam-\nples—pulling positive pairs (similar samples) closer together in the\nembedding space, while pushing negative pairs (dissimilar samples)\nfurther apart [14, 17, 21]. This approach has proven successful in\nenhancing language embeddings [11], visual representations [5],\nand in aligning cross-modal embeddings [31]. In the field of sign\nlanguage technologies, contrastive learning has been primarily ex-\nplored in sign-to-text translation frameworks [18, 23, 49, 53]. Ye\net al. [49] demonstrate that reducing the density of the sign pose\nrepresentation space via contrastive learning improves SLT per-\nformance. In (Jiang et al. [18]) and (Zhou et al. [53]), contrastive\nlearning is employed for visual-language pretraining by encourag-\ning alignment between visual and textual embeddings when they\ncorrespond to matching (ground truth, label) pairs. Lin et al. [23]\nsupervise the learning of visual feature embeddings using Con-\ntrastive Concept Mining (CCM): a method that identifies \"anchor\nwords\" from batch-level sentences and treats two sign sequences as\na positive pair if both contain the same anchor word. A similar tech-\nnique is adopted in (Walsh et al. [45]) for codebook training, where\npositive and negative pairs are constructed based on gloss overlap.\nOur approach draws inspiration from these works but applies\ncontrastive losses directly within the latent space of the decoder’s\nself-attention layers in the PT architecture. We hypothesize that\naligning these latent representations to the underlying semantic\ndistribution of the text before cross-modal attention encourages\nmore efficient learning, by filtering out visual features that are not\nsemantically relevant. This aligns with the motivation of (Walsh\net al. [45]), which seeks to reduce signer-specific variability and pro-\nmote person-invariant representations in sign language generation\nmodels.\n3\nMethodology\n3.1\nOverview\nWe adopt the PT architecture of Saunders et al. [34] as our backbone,\nas it is a widely used baseline in SLP and offers a complete, publicly\navailable implementation2.\nAs shown in Figure 1, we propose two extensions: (1) pose se-\nquences are encoded via bone rotations using unit quaternions,\nreplacing MSE loss on Cartesian coordinates with a loss based on\nthe more natural geodesic norm; (2) we explore two contrastive\n2https://github.com/BenSaunders27/ProgressiveTransformersSLP\n\"REGION MEHR FREUNDLICH HOCH\nDRUCK KOMMEN BIS REGION\"\nFigure 1: PT model architecture integrating quaternion-based\npose encoding and supervised contrastive loss. Blue dotted\nboxes indicate the modules specific to our contributions.\nobjectives, supervised by textual input, applied to the decoder’s\nself-attention latent space to guide its structure to reflect semantic\nrelationships. The first variant follows the loss formulation of [45],\ndefining positive and negative pairs based on shared gloss presence.\nThe second aligns similarity matrices between latent features and\nSBERT sentence embeddings.\n3.2\nQuaternion-Based Representation of\nSkeletal Poses\nFrom a skeletal pose in 3D Cartesian coordinates 𝑃:= (𝑋1, ...,𝑋𝑁joints)\nlying in R𝑁joints×3, given the graph structure of the skeleton and a\nreference T-pose 𝑃0, we compute the 3D rotation in quaternions\nrepresentation of the 𝑖-th bone as follows:\n𝑞𝑖:=\n\u0010\ncos(𝜃𝑖/2), sin(𝜃𝑖/2)𝑢(𝑖)\u0011\n∈[−1, 1]4\n(1)\nwhere\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n𝜃𝑖= arccos(𝑣(𝑖) · 𝑣(𝑖)\n0 )\n𝑣(𝑖) =\n𝑋Child𝑖−𝑋Parent𝑖\n∥𝑋Child𝑖−𝑋Parent𝑖∥, 𝑣(𝑖)\n0\n=\n𝑋0\nChild𝑖−𝑋0\nParent𝑖\n∥𝑋0\nChild𝑖−𝑋0\nParent𝑖∥\n𝑢(𝑖) =\n𝑣(𝑖) ×𝑣(𝑖)\n0\n∥𝑣(𝑖) ×𝑣(𝑖)\n0\n∥\n(’×’: classical cross product)\nHence, for each sequence of poses Y := (𝑃0, ..., 𝑃𝑇), we obtain the\ncorresponding sequence of bones rotations R := (𝑄0, ...,𝑄𝑇) where\n𝑄𝑡:= (𝑞1[𝑡], ...,𝑞𝑁bones [𝑡]).\nBased on the definition of the geodesic distance between unit\nquaternions, we define the loss function between predicted rota-\ntions R′ and ground truth rotations R as:\n\nIVA Adjunct ’25, September 16–19, 2025, Berlin, Germany\nGuilhem Fauré, Mostafa Sadeghi, Sam Bigeard, and Slim Ouni\nFigure 2: Illustration of bone rotation angle relative to a\nreference T-pose.\nLGeo :=\n1\n(𝑇+ 1)𝑁bones\n𝑇\n∑︁\n𝑡=0\n𝑁bones\n∑︁\n𝑖=1\narccos\n\u0010\n2(𝑞′\n𝑖[𝑡] · 𝑞𝑖[𝑡])2 −1\n\u0011\n(2)\nThis loss temporally averages the mean rotation angle between\npredicted and ground truth bone orientations.\nAdditionally, to enable reconstruction of predicted skeletal pose\nsequences in 3D Cartesian coordinates by recursively applying ro-\ntations from the root joint, we also predict the head node’s position\nby minimizing the following MSE during training:\nLRoot :=\n1\n𝑇+ 1\n𝑇\n∑︁\n𝑡=0\n∥𝑋′\nRoot[𝑡] −𝑋Root[𝑡]∥2\n2\n(3)\nIn constrast, the PT baseline relies exclusively on an MSE loss over\njoint positions.\n3.3\nContrastive Losses\nThe proposed contrastive losses are incorporated as regularization\nterms in the overall training objective, in addition to the standard\nSLP loss—either the MSE on joint positions or LGeo+LRoot. A scaling\nfactor 𝜆balances the contribution of the contrastive loss:\nLTotal := LSLP + 𝜆LCont\n(4)\nThe two contrastive losses we evaluate are presented in the follow-\ning subsections.\n3.3.1\nSupervision with Glosses. To define the supervised contrastive\nloss based on input gloss sequences, we follow the method proposed\nin (Walsh et al. [45]), itself inspired by (Khosla et al. [21]). For each\nbatch of decoder’s self-attention hidden representations, we first\nextract the set of all unique gloss tokens (referred to as anchors,\nindexed by 𝐼) appearing in the batch. For each anchor 𝑖∈𝐼, we iden-\ntify within the batch the sequence where 𝑖occurs most frequently.\nThis sequence will serve as the reference. The remaining sequences\nare split into positives 𝐴(𝑖) (those that also contain 𝑖) and negatives\n𝐵(𝑖) (those that do not), as illustrated in Figure 3.\nBased on this grouping, we define the following contrastive loss\nover the 𝑙-th self-attention layer’s output Z𝑙\nbatch := (𝑧1, ...,𝑧𝑁),\nwhere 𝑁is the batch size:\nL(𝑙)\nGlossSupCont := −\n∑︁\n𝑖∈𝐼\nlog ©\n­\n«\n1\n|𝐴(𝑖)|\n∑︁\n𝑎∈𝐴(𝑖)\nexp( 𝑧𝑓(𝑖) ·𝑧𝑎\n𝜏\n)\nÍ\n𝑏∈𝐵(𝑖) exp( 𝑧𝑓(𝑖) ·𝑧𝑏\n𝜏\n)\nª\n®\n¬\n(5)\n\"REGION MEHR FREUNDLICH HOCH DRUCK\nKOMMEN BIS REGION\"\n\"MORGEN WETTER WE-AUSSEHEN FREITAG\nACHT ZWANZIG AUGUST\"\n\"SUEDOST REGEN VIEL\"\n\"MONTAG SONNE WARM WETTER\"\n\"JETZ WETTER VORAUS INFORMIEREN\nMORGEN DONNERSTAG ZWANZIG JANUAR\"\n\"WETTER\"\n\"MONTAG SONNE WARM WETTER\"\n\"MORGEN WETTER WE-AUSSEHEN FREITAG\nACHT ZWANZIG AUGUST\"\n\"MONTAG SONNE WARM WETTER\"\n\"SUEDOST REGEN VIEL\"\nFigure 3: Definition of positive and negative pairs for the\ncomputation of LGlossSupCont.\nwith\n\n\n\n\n\n\n\n\n\n𝑓(𝑖) := arg max𝑘{Í\n𝑚∈𝑘-th Gloss Sequence 1𝑚=𝑖}\n𝐴(𝑖) := {𝑎∈⟦1, 𝑁⟧| 𝑖∈𝑎-th Gloss Sequence}\\{𝑓(𝑖)}\n𝐵(𝑖) := {𝑏∈⟦1, 𝑁⟧| 𝑖∉𝑏-th Gloss Sequence}\nWe average the per-layer losses to obtain the final objective:\nLGlossSupCont :=\n1\n𝑛layers\n𝑛layers\n∑︁\n𝑙=1\nL(𝑙)\nGlossSupCont\n(6)\n3.3.2\nSupervision with SBERT Embeddings. To incorporate finer\nknowledge of semantic relationships between sequences embed-\ndings, we build an alternative loss based on the cosine similarity be-\ntween input sentences once embedded via a sentence Transformer\n(SBERT) [33]. Upstream, the embeddings (SBERT𝑘)𝑘of input sen-\ntences are thus computed using the ’all-MiniLM-L6-v2’ model from\nthe Hugging Face library3. These embeddings are of dimension 384.\nHence, to match the SBERT embedding size before computing the\nloss, the outputs of the decoder’s self-attention blocks are first aver-\naged along the temporal dimension via average pooling, and then\nprojected through a linear layer (cf. Figure 4).\nThese previous steps enable the computation of the following loss\nover the batch output of the 𝑙-th self-attention layer:\nL(𝑙)\nSBERTSupCont := 𝑁(𝑁−1)\n2\n∑︁\n1≤𝑖<𝑗≤𝑁\n𝑑2\n𝑖,𝑗\n(7)\nwith 𝑑𝑖,𝑗:= sim(g(𝑧𝑖), g(𝑧𝑗)) −sim(SBERT𝑖, SBERT𝑗)\n(8)\nWhere sim(𝑥,𝑦) :=\n𝑥·𝑦\n∥𝑥∥∥𝑦∥denotes cosine similarity, and g(·) is\nthe transformation applied by the projection layers. The goal is to\n3https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\nTowards Noise Reduction in SLP via Quaternions and Contrastive Learning\nIVA Adjunct ’25, September 16–19, 2025, Berlin, Germany\nFigure 4: Projection into latent space prior to LSBERTSupCont\ncomputation. Decoder’s self-attention outputs are dimen-\nsionally aligned with SBERT embeddings.\nalign the similarity matrices computed from pose embeddings and\nSBERT embeddings (see Figure 5), such that the resulting latent\nspaces are structured according to semantic relationships, while\nminimizing the influence of non-semantic features.\nFigure 5: Computation of similarity matrices of SBERT and\nbatch samples g(𝑧) for LSBERTSupCont.\nAs for the first variant of contrastive objective, the overall loss is\nfinally obtained by averaging accross all layers:\nLSBERTSupCont :=\n1\n𝑛layers\n𝑛layers\n∑︁\n𝑙=1\nL(𝑙)\nSBERTSupCont\n(9)\n4\nExperiments\n4.1\nExperimental Settings\n4.1.1\nDataset. We train and evaluate our models on the widely\nused Phoenix14T dataset, which comprises 8,257 sequences of Ger-\nman Sign Language (DGS) performed by 9 signers, covering 1,066\nglosses and a vocabulary of 2,887 unique words [32]. While limited,\nthis dataset is a standard benchmark in SLP, making it a reliable\nstarting point for assessing model performance and comparison\nwith existing methods before scaling to richer datasets.\n4.1.2\nPreprocessing. 3D skeletal coordinates are extracted using\nMediaPipe’s pose and hand landmarks detection4. Joint positions\nare then refined following the method of Zelinka and Kanis [52],\nwhich interpolates missing joints and applies inverse kinematics to\ncorrect misplacements while preserving bone length consistency.\nFinally, skeletons are normalized as in (Stoll et al. [40]), based on\nshoulder-to-shoulder distance to reduce size variation across sub-\njects.\nWe use gloss sequences as input for the generation process, fol-\nlowing the original PT paper [34] and subsequent works [41, 45, 48].\n4.1.3\nEvaluation Metrics. We evaluate the tested configurations\nusing standard metrics in SLP to quantify the alignment between\ngenerated and reference skeletons. Specifically, we compute the\nMean Joint Error (MJE), defined as the Euclidian distance between\npredicted and ground truth joints averaged over all joints and times\nsteps, as used in prior work [1, 41, 45]. Similarly, we define the\nMean Bone Angle Error (MBAE) as the mean angular deviation\n(in degrees) between predicted and reference bones. It quantifies\narticulation accuracy independently of bone length, which makes it\nparticularly appropriate for evaluating our quaternion-based vari-\nant.\nWe also compute the Probability of Correct Keypoint (PCK),\nwhich measures the proportion of predicted joints falling within a\njoint-specific neighborhood of their corresponding ground truth\npositions in the image plane. This neighborhood is defined for each\njoint, projected onto the (𝑥,𝑦) plane, as a threshold 𝛼of the radius\nof its bounding disk. As in (Kapoor et al. [19]), we choose 𝛼= 0.2.\nThis metric accounts for varying spatial scales across different body\nparts.\nFor both MJE and PCK, sequences are first aligned using Dy-\nnamic Time Warping (DTW) based on Euclidean distance between\njoint coordinates. For MBAE, the applied DTW is instead computed\nto minimize angular differences between corresponding bones.\nIt is worth noting that our focus is ultimately on the relative\nchanges in the reported metrics with respect to the PT baseline,\nrather than on their absolute values. This approach allows us to\nassess whether the proposed changes lead to measurable improve-\nments within PT-like architectures.\n4.1.4\nImplementation Details. We retain the original configuration\nof the PT model from the reference repository, using 2 layers, 4\nattention heads, and an embedding size of 512 for both the encoder\nand decoder. The temperature parameter for the LGlossSupCont is set\nto 𝜏= 1. Moreover, for LGlossSupCont, we set the scaling factor to\n𝜆= 10−4 and the batch size to 64.\n4https://github.com/google-ai-edge/mediapipe\n\nIVA Adjunct ’25, September 16–19, 2025, Berlin, Germany\nGuilhem Fauré, Mostafa Sadeghi, Sam Bigeard, and Slim Ouni\nTraining is conducted on an NVIDIA GeForce RTX 2080 Ti GPU.\nNo significant computational overhead is observed between the\nbaseline and the quaternion-based variant, with training times\naveraging ∼1h50 for 1000 epochs. Adding contrastive losses sub-\nstantially increases runtime, requiring ∼3h30 with LSBERTSupCont and\nup to ∼11h with LGlossSupCont.\n4.2\nResults\nFigure 6: Qualitative comparison of ground truth and pre-\ndicted skeletal poses with the base PT model and its\nquaternion-based variant.\n4.2.1\n3D Cartesian Positions VS Quaternion-based Rotations. As\nshown in Table 1, encoding poses through bone rotations using\nTable 1: Evaluation metrics on Phoenix14T test set for dif-\nferent configurations. Values are reported as Mean±std. A\nbold score indicates the best result. The second-best result is\nunderlined.\nMJE(↓)\nMBAE(↓)\nPCK(↑)\n3D cart. (base)\n0.41±0.08\n36.93±6.74\n0.25±0.11\nw/ gloss cont.\n0.40±0.08\n36.02±6.95\n0.29±0.12\n𝜆\nw/ SBERT cont.\nbatch = 64\n0.0001\n0.40±0.08\n36.68±6.97\n0.28±0.12\n0.0005\n0.40±0.08\n36.57±6.88\n0.28±0.12\n0.001\n0.40±0.08\n36.76±6.76\n0.28±0.13\n0.005\n0.41±0.08\n38.14±6.67\n0.26±0.11\n0.01\n0.42±0.08\n38.93±6.56\n0.26±0.11\n0.1\n0.44±0.09\n39.52±7.14\n0.24±0.11\nbatch\nw/ SBERT cont.\n𝜆= 0.001\n128\n0.43±0.08\n39.06±6.83\n0.25±0.11\n256\n0.42±0.08\n38.97±7.04\n0.25±0.11\nquaternions\n0.44±0.08\n35.66±7.39\n0.22±0.10\nw/ gloss cont.\n0.42±0.09\n34.69±7.19\n0.26±0.12\n𝜆\nw/ SBERT cont.\nbatch = 64\n0.05\n0.44±0.08\n36.54±7.98\n0.24±0.11\n0.1\n0.43±0.09\n36.36±7.23\n0.25±0.12\n1\n0.43±0.08\n36.70±7.08\n0.25±0.11\nFigure 7: Bar plots of MJE per skeletal part on Phoenix14T\ntest set between the base PT model and its quaternion-based\nvariant.\nquaternions—paired with geodesic loss optimization (see the \"quater-\nnions\" row)—leads to a slight relative reduction (−3%) in mean\nangular error compared to the baseline approach using joint 3D\nCartesian positions and MSE loss (\"3D cart (base)\" row). This spe-\ncific improvement aligns with the objectives of geodesic loss, which\nbetter respects the manifold structure of rotations. Qualitative anal-\nysis also reveals that the quaternion-based model often produces\ncrisper and more distinct manual articulations, while the baseline\ntends to generate smoothed, averaged motions (see Figure 6). These\nresults are consistent with findings from (Tang et al. [41]), which\nemphasize the benefits of modeling bone orientations.\n\nTowards Noise Reduction in SLP via Quaternions and Contrastive Learning\nIVA Adjunct ’25, September 16–19, 2025, Berlin, Germany\nHowever, using rotations instead of positions leads to dimin-\nished performance on standard joint-based metrics, such as MJE,\nespecially noticeable for the dominant right hand (Figure 7). This\nperformance drop is also reflected in lower PCK scores.\n4.2.2\nPT Model with Contrastive Objectives. Integrating a con-\ntrastive loss into the baseline Progressive transformers model con-\nsistently improves performance across all evaluated metrics. In\nparticular, the use of LGlossSupCont results in a 16% relative improve-\nment in PCK (\"w/ gloss cont.\" in Table 1), from 0.25 to 0.29, while the\nSBERT embeddings-based variant achieves a 12% relative increase\n(“w/ SBERT cont.” row), from 0.25 to 0.28. These results are in line\nwith previous studies such as (Walsh et al. [45]) and (Zuo et al. [55])\nwhich reduce pose representation space density through vector\nquantization.\nInterestingly, the model trained with LSBERTSupCont performs slightly\nworse than the one using LGlossSupCont. A likely explanation lies in\nthe nature of the supervisory signals: SBERT-based supervision\ninduces a smoother, more continuous embedding structure by align-\ning pose sequence similarities with sentence embedding similarities\nin [0, 1], whereas gloss-based supervision relies on binary similarity\nlabels from shared glosses. This discretization may lead to stronger\nclustering effects, aiding training convergence and generalization.\nIt would be beneficial to evaluate these approaches using metrics\nthat better reflect semantic or linguistic intelligibility. Given that\nstylistic variation still exists in the evaluation data, standard posi-\ntional metrics may not fully capture the comprehensibility of a sign.\nA generated sign may remain highly intelligible despite significant\ndeviation from a reference pose, suggesting that current metrics\nmight underestimate improvements that matter most for end-users.\n4.2.3\nCombining Quaternions Pose Encoding and Contrastive Losses.\nCombining quaternion-based pose encoding with contrastive train-\ning objectives compensates for the positional metric degradation\nobserved when using geodesic loss alone. This combination also\nleads to further improvements in angular accuracy. Specifically,\ntraining the quaternion-based model with LGlossSupCont results in an\nadditional 1° reduction in mean angular error, a 4% drop in MJE\n(from 0.44 to 0.42), and an 18% increase in PCK (from 0.22 to 0.26)\nrelative to the same model trained without contrastive objective.\nThese findings suggest that integrating rotational encoding with\nangular-loss objectives and semantic-aware contrastive losses ef-\nfectively addresses variability in signer morphology and style, ulti-\nmately producing clearer and more consistent sign motions.\n5\nConclusions\nWe introduced and explored two complementary strategies to im-\nprove the classical Progressive Transformers model for sign lan-\nguage production, focusing on mitigating the impact of morpho-\nlogical and stylistic variability among signers. Our experiments,\nconducted on the Phoenix14T dataset, demonstrate that: (1) En-\ncoding skeletal poses using bone rotations (quaternions) and op-\ntimizing them with a geodesic loss leads to more distinct angular\nmotions, particularly for hand and finger articulations; (2) Augment-\ning the decoder with a contrastive loss that structures self-attention\nembeddings yields consistent improvements across all metrics, es-\npecially when using shared glosses to define positive sequence\npairs; (3) Combining both methods results in further gains in an-\ngular precision while preserving joint position accuracy. These\nresults advocate for the systematic inclusion of skeletal structure\nand rotation-aware representations, along with semantic-guided\ncontrastive learning, in future SLP model training pipelines.\nAs future work, we plan to incorporate back-translation metrics,\nsuch as BLEU, to more accurately evaluate the semantic intelligibil-\nity of generated sign sequences, beyond purely spatial or angular\nerror measures. In addition, we aim to refine the contrastive super-\nvision strategy by leveraging sentence Transformer embeddings\nto define positive and negative sequence pairs based on seman-\ntic similarity thresholds. This approach could offer a hybrid be-\ntween the current discrete gloss-based method and the continuous\nSBERT-based formulation, potentially improving the alignment of\nthe learned pose representations with semantic meaning.\nFinally, given the known limitations of the Phoenix14T dataset, fu-\nture work will involve evaluating our approach on the more recent\nMediapi-RGB French Sign Language dataset, which offers greater\ndiversity and scale, with over 86 hours of video [27].\nAcknowledgements\nThis work is part of Défi Inria COLaF, which was financed by Plan\nNational de Recherche en Intelligence Artificielle."}
{"id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "authors": ["Luca Annese", "Sabrina Patania", "Silvia Serino", "Tom Foulsham", "Silvia Rossi", "Azzurra Ruggeri", "Dimitri Ognibene"], "source": "http://arxiv.org/abs/2508.14564v1", "page_count": 14, "text": "Who Sees What? Structured Thought-Action\nSequences for Epistemic Reasoning in LLMs\nLuca Annese1, Sabrina Patania1, Silvia Serino1,Tom Foulsham3, Silvia Rossi2,\nAzzurra Ruggeri4 and Dimitri Ognibene1,3\n1 University of Milan-Bicocca, Milan, Italy\n{luca.annese1,sabrina.patania,silvia.serino,dimitri.ognibene}@unimb.it\n2 University of Essex, Colchester, UK foulsham@essex.ac.uk\n3 University of Naples Federico II, Naples, Italy silvia.rossi@unina.it\n4 TUM School of Social Sciences and Technology, Munich, Germany\nAbstract. Recent advances in large language models (LLMs) and rea-\nsoning frameworks have opened new possibilities for improving the per-\nspective -taking capabilities of autonomous agents. However, tasks that\ninvolve active perception, collaborative reasoning, and perspective tak-\ning (understanding what another agent can see or knows) pose persis-\ntent challenges for current LLM-based systems. This study investigates\nthe potential of structured examples derived from transformed solution\ngraphs generated by the Fast Downward planner to improve the per-\nformance of LLM-based agents within a ReAct framework. We propose\na structured solution-processing pipeline that generates three distinct\ncategories of examples: optimal goal paths (G-type), informative node\npaths (E-type), and step-by-step optimal decision sequences contrasting\nalternative actions (L-type). These solutions are further converted into\n“thought-action” examples by prompting an LLM to explicitly articu-\nlate the reasoning behind each decision. While L-type examples slightly\nreduce clarification requests and overall action steps, they do not yield\nconsistent improvements. Agents are successful in tasks requiring basic\nattentional filtering but struggle in scenarios that required mentalising\nabout occluded spaces or weighing the costs of epistemic actions. These\nfindings suggest that structured examples alone are insufficient for ro-\nbust perspective-taking, underscoring the need for explicit belief track-\ning, cost modelling, and richer environments to enable socially grounded\ncollaboration in LLM-based agents.\nKeywords: perspective taking · LLMs · active vision · theory of mind\n· planning .\n1\nIntroduction\nEffective interaction in multi-agent systems, especially those involving human-\nArtificial intelligence (AI) collaboration, requires more than basic task execution.\nIt demands the capacity for perspective-taking: the ability to model what others\ncan see, know, or intend. This includes reasoning about both physical viewpoints\n\n2\nL. Annese et al.\n(e.g., what another agent can perceive in the environment) and epistemic states\n(e.g., what they know or believe). Perspective-taking is essential for generat-\ning contextually appropriate responses and adapting to ambiguity in real-world\nscenarios.\nIn this work, we explore how perspective-taking can be operationalized within\nthe ReAct (Reason+Act) framework [32], which interleaves natural language\nreasoning with environment-grounded actions. Unlike static perception-action\npipelines, ReAct enables agents to reason explicitly before acting, providing an\nideal foundation for handling the interpretive demands of perspective-taking\ntasks.\nWe focus on a modified version of the Director Task [14], adapted to a par-\ntially observable environment inspired by [28]. In our setting, already explored\nin [25] and [24], a Director issues instructions to a Matcher agent, which must re-\ntrieve a target object. Crucially, the environment contains occlusions and hidden\ncontainers that affect both agents perception, meaning that the Matcher may\nhave to actively perceive [20, 21] to infer what the Director sees and does not\nsee, to resolve ambiguity. This setup simulates real-world collaborative scenarios\nwhere agents have asymmetric and limited access to information.\nTo enable grounded perspective-taking, we introduce a novel training method\nthat builds on search-based reasoning from the Fast Downward planner. We con-\nstruct complete reasoning trees that encode possible thought-action trajectories\nin similar conditions. From these trees, we extract three types of example se-\nquences:\n– G-type: optimal sequence from initial state to task success;\n– E-type: all paths that reach informative states, where the agent gains new\nknowledge through sensing, movement, or object interaction;\n– L-type: locally optimal decisions at each state, simulating reactive step-wise\ndecision-making.\nEach sequence is transformed into a chain of thought-action pairs using a LLM,\nenabling agents to learn not just what to do, but why, increasing both inter-\npretability and transferability.\nWe evaluate our method across seven environments of increasing complexity,\nvarying both the spatial layout and the ambiguity of instructions to intensify the\nperspective-taking demands. For each trial, the agent is shown examples from six\nenvironments and evaluated in the held-out seventh, testing generalization across\ndiverse settings. Tasks ranged from fully disambiguated instructions to scenarios\nrequiring the agent to infer what the Director sees or does not see. Empirical\nresults show that while G-type and E-type examples support task efficiency and\nexploration, only L-type examples slightly improved agent behaviour, specifi-\ncally by reducing excessive clarification requests and promoting more grounded\ndecision-making.\nTo interpret these results, we introduce a functional characterisation of the\ncognitive demands associated with different task conditions. These demands\n(ranging from simple common-ground attentional filtering [13], to mentalising\nabout hidden content [4], and metacognitive evaluations of epistemic cost and\n\nWho Sees What?\n3\nutility [15]) emerge in increasingly complex environments and help explain when\nLLM agents succeed and where they systematically fail.\nUltimately, while embedding cognitively structured examples into the ReAct\nloop scaffolds some forms of perspective-taking—particularly those grounded in\nreactive reasoning and observable cues—it may not suffice for enabling higher-\norder inference, imaginative simulation, or consistent collaborative strategies\nthat unfold over multiple steps. Such strategies require balancing social rea-\nsoning, physical exploration, and interlocutor modelling, while accounting for\nboth information gain and the cost or risk of failure. While recent work suggests\nthat LLMs exhibit emerging capabilities for social reasoning and information\ngathering [26, 2], our findings underscore the need for their integration with\nsocially-aware, active information-seeking strategies [21, 20], in order to support\nrobust collaboration in open-ended, multi-agent environments [3, 19].\n2\nRelated work\nIn recent years, there has been growing interest in the application of large lan-\nguage models (LLMs) and multimodal foundation models in robotics and collab-\norative systems for high-level reasoning, perception, and decision-making [22].\nThese models are pre-trained on vast amounts of internet-scale data and exhibit\nimpressive generalization capabilities [6], enabling robots to handle a wide range\nof open-ended scenarios. Models such as SayCan [1] and Inner Monologue [11]\ndemonstrate how LLMs can break down abstract goals into practical steps by\ncombining high-level reasoning with grounded robotic actions.\nA core component of effective multi-agent interaction is perspective-taking,\nnamely, the ability to represent a situation from an alternate viewpoint [9].\nThis includes visual perspective-taking, distinguished between Level-1 (inferring\nwhat others can see) and Level-2 (inferring how things appear to others), and\nspatial perspective-taking, which involves representing relative spatial relations\nthrough egocentric or allocentric reference frames [7]. Visual perspective-taking,\nparticularly Level-2, has been closely linked to theory of mind (ToM), as both\nrequire agents to maintain decoupled mental representations. In frameworks like\nReAct [32], perspective-taking is framed as a dynamic reasoning process that\naccompanies acting, enabling agents to update their knowledge in real time.\nEfforts to enhance perspective-taking in LLMs have largely focused on lan-\nguage based evaluations. Studies using false-belief tasks indicate that while older\nmodels (e.g., GPT-2, early GPT-3) struggle with ToM tasks, more recent sys-\ntems (e.g., GPT-4) display emerging but unstable capabilities [16]. Techniques\nlike the SimToM prompting framework [31] explicitly instruct models to simulate\nother agents’ perspectives, reducing the intrusion of background omniscience. In\nthe visual domain, datasets such as Isle-Bricks and Isle-Dots [8] reveal that while\nmany vision-language models (VLMs) can detect objects in a scene, they often\nfail at reasoning about what is visible from an observer’s viewpoint. Advanced\nmodels like GPT-4V perform well on Level-1 tasks but show notable drops on\nLevel-2 challenges involving viewpoint rotation and mental transformation [17].\n\n4\nL. Annese et al.\nBeyond static perception, benchmarks such as ActiView [30] introduce active\nvisual exploration, requiring models to shift or zoom their viewpoint to gather\nrelevant information—tasks that remain difficult for current models.\nComplementary to these efforts, recent research has begun to explore the\nsynergy between LLMs and classical symbolic planning systems to enhance\nstructured reasoning. Hybrid models like LLM+P [18] use planners to generate\nplans from formal representations (e.g., PDDL), then convert them into natural\nlanguage for execution by LLMs. Other works, such as PSALM [33], investi-\ngate using LLMs to synthesize or refine planning domains, while others employ\nLLMs to verbalize or critique symbolic plans [11]. These approaches leverage the\ncomplementary strengths of symbolic methods (e.g., correctness, structure) and\nlanguage models (e.g., flexibility, generalization).\nBuilding on these developments, our work explores how symbolic planning\ncan serve as a source of structured, cognitively meaningful training examples for\nLLMs. This method bridges symbolic and neural paradigms by using planning\nstructures to ground language-based cognitive traces, offering a new way to study\nand enhance perspective-taking and decision-making in LLM-based agents.\n3\nMethod\n3.1\nSimulated Task Environment\nTo explore perspective-taking in goal-directed interaction, we developed a sim-\nulated household-like environment using the Planning Domain Definition Lan-\nguage (PDDL). The environment represents a shared space between two agents:\na Director, who knows the identity and location of the target object, and a\nMatcher, who must retrieve the object based on limited perceptual cues and di-\nalogue. The space includes multiple locations such as a desk, shelf, and drawers,\nwith some objects hidden inside containers that can be opened or closed.\nThe PDDL domain models both spatial structure and perceptual asymmetry.\nEach agent can perceive the contents of its own location, as well as any adjacent\nlocations, simulating a partially shared field of view. This shared access supports\nbasic grounding, while still requiring inference about what the other agent can\nor cannot see.\nIn most cases, ambiguity is introduced by presenting two objects of the same\ntype (e.g., two ties of different colours). This setup requires the Matcher to rely on\ncues beyond spatial proximity or direct visual recognition, such as dialogue with\nthe Director or strategic exploration, to disambiguate the task. Figure 1 shows\nsuch a design graphically. To systematically vary perspective-taking difficulty,\nwe designed seven task types, ranging from fully observable to highly ambiguous\nsettings, as shown in Table 1.\nThe Matcher begins each trial at a random location and must perform a\nsequence of actions: moving, opening containers and optionally asking questions\nto infer the correct object. The agent must decide whether more information\n\nWho Sees What?\n5\nFig. 1. Schematic view showing two examples of the experimental environment. In\neach case the top row shows the three locations and their item contents, and the yellow\narrows indicate possible movements from the Matcher. The target is outlined in red.\nEach agent can see only the location in front and immediately adjacent (yellow shad-\ning). The Matcher can move between locations, take an item from the location directly\nin front, or ask the Director a question. The Director can only answer from their fixed\nposition. In the Near condition (top panel), there is ambiguity because two candidate\nobjects fit the Director’s request (Gold shirt and Silver shirt). In the Distractor condi-\ntion (bottom panel), the Matcher can see an item (Red tie) that matches the request,\nyet is not the correct target.\nis required to make the choice and how to find it. This design enables con-\ntrolled evaluation of perspective-taking under increasing complexity, balancing\ngrounded environment interaction with higher-order inference demands.\n3.2\nStrategy Generation\nTo generate structured behavioral sequences, we employed a modified version\nof the Fast Downward planner, adapted to expose the internal reasoning pro-\ncess underlying its search, as shown in Figure 2. Specifically, the planner was\nequipped to output a reasoning tree that records the sequence of states and\nactions considered during planning. This tree represents a detailed trace of the\nplanner’s decision space, capturing both explored and selected paths through\n\n6\nL. Annese et al.\nEnvironment\nInformation State\nMatcher\nSpatial\nState\nAmbiguity\nResolution\nBase\nBoth see both objects & ar-\neas; Director names target\nexplicitly.\nDistant\nfrom\nboth\ntarget and distractor.\nProcessing\nini-\ntial demand.\nPerspective\nTaking\nMatcher sees both objects;\nDirector sees only target and\none area less.\nClose to both target\nand distractor.\nUsing\ncommon\nground.\nDistractor\nMatcher sees distractor; Di-\nrector sees target. Each sees\nan area the other can’t.\nDistant from target,\nclose to distractor.\nExploring\nun-\nseen\nperspec-\ntives.\nNear\nBoth\nsee\nboth\nobjects.\nMatcher sees an area more.\nClose to target, dis-\ntant from distractor.\nAsking clarifica-\ntion.\nFar\nBoth\nsee\nboth\nobjects.\nMatcher sees an area more.\nClose\nto\ndistractor,\ndistant from target.\nAsking clarifica-\ntion.\nHidden\nDirector\nsees\nTarget.\nMatcher does not. Each sees\nan area the other can’t.\nDistant from target.\nNo Distractor.\nExploring\nun-\nseen\nperspec-\ntives.\nNot That\nMatcher sees only distrac-\ntor; Director sees both. Each\nsees an area the other can’t.\nClose\nto\ndistractor,\ndistant from target.\nAsking clarifica-\ntion.\nTable 1. Environment Types and Their Perspective-Taking Challenges\nthe environment. From this reasoning tree, we derived three types of training\nexamples, each emphasizing a distinct mode of behavior, by extracting them\nthrough three different tree-traversal strategies.:\n– Goal-directed trajectories (G-type): These are action sequences that lead\nfrom the initial state to the goal state, representing the planner’s optimal\nsolution path. They reflect efficient behavior under the assumption that the\nagent has access to all relevant information for completing the task.\n– Information-seeking trajectories (E-type): These examples were extracted by\nidentifying branches of the reasoning tree that led to states where critical\ninformation was gained, such as observing hidden objects or reducing am-\nbiguity about the target. This type of example prioritizes epistemic actions\nand models behavior driven by uncertainty reduction rather than direct goal\ncompletion.\n– Local decision points (L-type): For this example type, we identified the lo-\ncally optimal action at each state visited during planning, contrasted explic-\nitly with the other possible actions available at that point. This approach\nemphasizes granular decision-making and trains agents to reason about why\nthe chosen action is superior to its alternatives given a single observation.\n\nWho Sees What?\n7\nThese examples are grounded in the planner’s reasoning trace, providing insight\ninto the deliberative process behind goal achievement and information gathering.\nWhile the Director is not explicitly modeled in the environment, its influence is\nindirectly embedded in the task configuration, for instance, through the target\nobject’s proximity to the Director’s location, prompting the Matcher to infer\nspatial and referential cues. This setup supports the generation of cognitively\nrich example sequences from a formally grounded planning system.\n3.3\nExample Generation\nThe final step of the methodology involves transforming the planner-derived\naction sequences into structured cognitive examples that simulate the internal\nreasoning of an agent engaged in perspective-taking. These examples are con-\nstructed as “thought-action” pairs, where each action is preceded by a textual\ndescription of the agent’s inferred reasoning process.\nTo produce these thought sequences, a large language model (GPT o3-mini)\nwas prompted with each action sequence, drawn from the G-, E-, or L-type\nexamples, along with contextual information such as the agent’s current state,\nvisible items, and the overall task objective. The model was asked to articulate\nthe reasoning that might justify each step in the sequence, generating natural\nlanguage explanations simulating deliberation and inference. The model was\ninstructed to generate such sequences employing the following prompts:\nStrategy Prompt\nG-type\nGiven the sequence of actions the agent executed until reaching its goal\nin a specific scenario, reconstruct the agent’s reasoning step by step.\nExplain how each action contributed to achieving the goal.\nE-type\nGiven a sequence of actions taken until the agent reaches an informa-\ntive state (i.e., a state that provides new information), reconstruct the\nagent’s reasoning step by step. Describe how each action led to gaining\ninformation.\nL-type\nGiven the agent’s last action, the set of possible actions, and the cor-\nrect action in a specific scenario, explain the agent’s reasoning behind\nselecting that particular action over the alternatives.\nTable 2. Reasoning Strategy Prompts\nThis transformation enriches the examples with cognitive structure, allow-\ning them to reflect key faculties involved in perspective-taking. Each thought\nsegment captures perceptual assessment (e.g., recognizing that an object is not\ncurrently visible), inferential reasoning (e.g., hypothesizing where a hidden item\nmight be based on contextual cues), and decision-making under uncertainty (e.g.,\nchoosing to ask the Director when the situation is ambiguous). These elements\n\n8\nL. Annese et al.\nare not explicitly modeled in the PDDL domain, but are essential for simulating\nrealistic agent behavior in social contexts.\nFig. 2. Graphic view of example generation pipeline. The panel above shows the rea-\nsoning tree produced by the Fast-Downward planner, highlighting the three different\npaths according to the corresponding strategy. The section below is an example (G-\ntype path) generated from the Base environment.\n4\nExperiments\nWe first augmented the Fast Downward planner (A⋆with the admissible hmax\nheuristic) to emit all evaluated actions during search, from which we built a\ncomplete reasoning tree. From this, we extracted three trajectory types, G-type,\nE-type, and L-type, each corresponding to a distinct decision-making strategy.\nThese were fed to GPT o3-mini using strategy-specific prompts to generate\nnatural-language thought–action chains: step-by-step goal reasoning (G-type),\ninformation-seeking justification (E-type), and local action selection rationale\n(L-type).\nTo evaluate the Matcher–Director interaction, we instantiated two LLM-\nagents (both GPT o3-mini) in our PDDL-based household environment. The\nstationary Director issued natural-language instructions of varying ambiguity;\nthe ReAct-based Matcher alternated between LLM reasoning and grounded ac-\ntions to interpret and execute the task. For each of the seven task types (ranging\n\nWho Sees What?\n9\nfrom disambiguated to highly ambiguous), the Matcher was trained on struc-\ntured examples from the other six, and tested on the held-out one. Each test\nscenario was repeated across five trials. Performance was assessed using failure\nrate, step count, clarification queries, and epistemic actions.\nIn parallel, we enhanced the PDDL domain to allow the planner to com-\npute optimal perspective-taking strategies by including epistemic (ASK) actions\nwhere ambiguity required them. This enabled the generation of example sets\nwith (+ask) or without (–ask) clarification steps, providing expert baselines for\nreasoning under uncertainty and testing whether such exposure influenced agent\nbehaviour.\nTo probe this further, we used these planner-optimal trajectories to generate\nan additional set of G-, E-, and L-type examples, enriched with epistemically\naware reasoning but following the same prompting schema.\nWe report aggregated averages by task (see Tables 3, 4, 5) to compare how\nexample types shape Matcher behaviour across perspective-taking demands. De-\nspite encoding optimal epistemic strategies, these enriched examples did not yield\nconsistent performance gains. Neither success rates nor behavioural quality (e.g.,\nunnecessary queries, incorrect assumptions) improved significantly. Possible rea-\nsons for this outcome are explored in the following section.\nTable 3. First Take on Correct Target (%) across Scenarios\nExample type\nPersp\nFar\nHidd\nNot\nDist\nBase\nNear\nG-type-ask\n100\n0\n100\n0\n20\n100\n100\nG-type+ask\n100\n0\n100\n0\n40\n100\n100\nE-type-ask\n100\n0\n100\n20\n0\n100\n100\nE-type+ask\n100\n0\n100\n0\n40\n100\n100\nL-type-ask\n100\n0\n100\n0\n40\n100\n100\nL-type+ask\n100\n0\n100\n0\n40\n100\n100\nNo Examples\n100\n20\n100\n0\n40\n100\n100\nTable 4. Average Number of Steps\nExample type\nPersp Far\nHidd\nNot\nDist\nBase\nNear\nAVG\nG-type-ask\n1\n4.8\n3\n5.8\n5.2\n2\n1\n3.26\nG-type+ask\n1\n5\n3\n5.8\n5\n2\n1\n3.26\nE-type-ask\n1\n4.4\n3\n6\n5\n2\n1\n3.2\nE-type+ask\n1\n4.6\n3\n5.6\n5.2\n2\n1\n3.2\nL-type-ask\n1\n4.4\n3\n4.8\n4.6\n2\n1\n2.97\nL-type+ask\n1\n5\n3\n5.8\n5\n2\n1\n3.26\nAVG\n1\n4.7\n3\n5.63\n5\n2\n1\n3.19\nNo Examples\n1\n4.4\n3\n5.8\n4.6\n2\n1\n3.11\nPlanner\n1\n3\n2\n3\n2\n2\n2\n2.14\n\n10\nL. Annese et al.\nTable 5. Average Number of Ask Actions\nExample type\nPersp Far\nHidd\nNot\nDist\nBase\nNear\nAVG\nG-type-ask\n0\n1.8\n1\n1.8\n2.4\n0\n0\n1\nG-type+ask\n0\n2\n1\n1.8\n2.4\n0\n0\n1.03\nE-type-ask\n0\n1.4\n1\n2.4\n2\n0\n0\n0.97\nE-type+ask\n0\n1.6\n1\n1.6\n2.6\n0\n0\n0.97\nL-type-ask\n0\n1.4\n1\n1.4\n2\n0\n0\n0.83\nL-type+ask\n0\n2\n1\n1.8\n2.6\n0\n0\n1.06\nAVG\n0\n1.7\n1\n1.8\n2.33\n0\n0\n0.98\nNo Examples\n0\n1.6\n1\n1.8\n2\n0\n0\n0.91\nPlanner\n0\n1\n0\n1\n0\n0\n1\n0.43\n5\nDiscussion\nOur results reveal that success in collaborative reference hinges on three orthog-\nonal but interacting cognitive demands. Throughout we denote them as F1-F3\nto emphasise their functional independence.\nF1 Common-ground filtering. Listeners must inhibit any object that the Di-\nrector cannot currently see, a Level-1 perspective-taking operation that relies\nmainly on selective attention [13, 15].\nF2 Imagining Director-privileged space. When the layout contains occluded\nregions visible only to the Director, the agent must construct counterfactual\nscenes and reason about what may be present there, a genuinely mentalistic\ncomputation that recruits full Theory of Mind (ToM) [3].\nF3 Metacognitive cost-benefit evaluation. Because belief tracking, explo-\nration, and clarifying questions all incur different costs, the agent must decide\nwhether to pay those costs or rely on faster egocentric heuristics [26, 29, 27,\n23, 10].\nWhen do the factors matter? Whenever the task could be solved by F1 alone,\nas in Base, Perspective-Taking, and Near first-take accuracy was perfect. Intro-\nducing F2 precipitated large drops (Distractor, Hidden, Not That); accuracy\nrebounded only after the agent executed an exploratory ‘look-inside’ or issued a\nquery. Pure F3 pressure (Far) likewise reduced first-take accuracy, even though\nno ToM inference was required. Table 3 quantifies this triple dissociation.\nSelective attention versus Theory of Mind. The split between F1 and F2 mirrors\nthe neuroscientific dissociation between the dorsal rTPJ region, associated with\nsocial ToM reasoning, and a neighbouring ventral patch specialised for atten-\ntional re-orienting [12, 5]. Our LLM agent performs the attentional filter (F1)\nflawlessly but fails when genuine belief reasoning (F2) is needed, reproducing\nthe human pattern reported by [15].\n\nWho Sees What?\n11\nWhy do planner-optimal traces fail to transfer? Fast-Downward plans embody\nthe invariant ‘act only once the target is uniquely identifiable to the Director’. Yet\nfew-shot exposure to those traces did not boost performance because (i) GPT-o3\nimplicitly assigns near-zero cost to questions, so it still ‘plays safe,’ and (ii) the\nlinear action lists do not make the underlying cost rationale explicit. Without\nan explicit belief state and cost model, the LLM reverts to the heuristic ‘grab\nthe closest match, ask if unsure’.\nPrompting design for exploration Beyond cost modelling, an additional limita-\ntion concerns the formulation of prompts used to frame the problem and describe\nthe scenario. Several failures related to F2 and F3 occurred in situations where\nthe agent would need to hypothesise the existence of relevant but currently un-\nseen objects, an ability central to active vision. These may result from prompts\nthat insufficiently foreground the plausibility of missing information in unex-\nplored, yet accessible, regions. Without an explicit representation of such areas,\nthe agent may not infer that exploration is necessary or worthwhile. Prompt\nstrategies that better support uncertainty reasoning and hypothesis generation\nabout occluded content could thus be essential for more robust epistemic be-\nhaviour.\nLimitations and future directions. The present grid world offers only binary visi-\nbility; richer social environments include graded salience, gaze cues, and compet-\ning conversational goals. Extending the benchmark to those dimensions should\nexert stronger pressure on F2 and F3, revealing whether the same failure modes\npersist or whether multi-modal grounding and cost signals can scaffold more\nrobust ToM.\n6\nConclusions\nWe find that a ReAct loop backed by GPT-o3 already supports flawless Level-\n1 perspective taking (F1), but remains brittle on two fronts: (i) F2, imagining\nalternative objects in occluded space, and (ii) F3, weighing the immediate versus\ndelayed costs of belief-driven action. Bridging these gaps may require explicit\nbelief state tracking [4, 20], learned cost models, and prompting strategies that\nforeground hidden regions and explicitly stimulate hypothesis generation about\nunseen content. Testing agents in richer, more uncertain settings extracted from\nrobot sensors, and penalising gratuitous queries, will be essential for advancing\nfrom attentional filtering to full, cost-aware Theory of Mind competence.\nAcknowledgments. This work was supported by the Volkswagen Foundation\nunder the funding programme “Open Up – New Research Spaces for the Human-\nities and Cultural Studies,” project “Developing an Artificial Social Childhood\n(ASC) to improve AI causal reasoning, information gathering and decision mak-\ning,” reference 9E530.\n\n12\nL. Annese et al."}
{"id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "authors": ["Maja J. Hjuler", "Harald V. Skat-Rørdam", "Line H. Clemmensen", "Sneha Das"], "source": "http://arxiv.org/abs/2508.14548v1", "page_count": 8, "text": "EmoTale: An Enacted Speech-emotion Dataset in Danish\nMaja J. Hjuler2,3∗, Harald V. Skat-Rørdam1, Line H. Clemmensen1, and Sneha Das1†\n1Dept. of Applied Mathematics and Computer Science, Technical University of Denmark, 2800 Lyngby, Denmark\n2University Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\n3School of Computer Science, Queensland University of Technology, Brisbane QLD 4000, Australia\nEmail: maja-jonck.hjuler@univ-grenoble-alpes.fr, {harsk, lkhc, sned}@dtu.dk\nAbstract—While multiple emotional speech corpora exist for\ncommonly spoken languages, there is a lack of functional datasets\nfor smaller (spoken) languages, such as Danish. To our knowl-\nedge, Danish Emotional Speech (DES), published in 1997, is the\nonly other database of Danish emotional speech. We present\nEmoTale1; a corpus comprising Danish and English speech\nrecordings with their associated enacted emotion annotations.\nWe demonstrate the validity of the dataset by investigating and\npresenting its predictive power using speech emotion recognition\n(SER) models. We develop SER models for EmoTale and the\nreference datasets using self-supervised speech model (SSLM)\nembeddings and the openSMILE feature extractor. We find\nthe embeddings superior to the hand-crafted features. The best\nmodel achieves an unweighted average recall (UAR) of 64.1% on\nthe EmoTale corpus using leave-one-speaker-out cross-validation,\ncomparable to the performance on DES.\nIndex Terms—speech emotion recognition, speech processing,\nparalinguistic speech, transferability, evaluation.\nI. INTRODUCTION & BACKGROUND\nSpeech signals are rich in information, both linguistic (in\nthe form of sentences and words) and paralinguistic (denoting\nmood and affective state). Speech also carries information\nabout multiple, potentially personal traits of the speaker, such\nas age, gender, and nationality. Multiple psychological and\nneuroscientific models of the mind hypothesize that language\nand emotion are certainly linked [1]. For example, some cul-\ntures express anger more vocally, while others might be more\nrestrained. Investigating voice and speech to judge emotional\nstates dates back more than half a century [2], [3], and the\nearliest speech emotion recognizers (SERs) were proposed\nover two decades ago [4], [5].\nEmotions are inherently subjective; different people per-\nceive emotions differently, and this can lead to differences\nin annotating emotional data [6], [7]. Overall, two different\nlabeling schemes are adopted in the literature: categorical\nclass labels, which are nominal and discrete, and dimensional\nlabels, which are continuous. The former often follows the\nbasic emotion theory developed by Paul Ekman [8], which\nassumes the existence of six basic and universal emotions\nthat transcend language, cultural, and ethnic differences. The\n∗The author was affiliated with the Technical University of Denmark when\nthis work was carried out.\n†Corresponding Author\n1Link to the dataset and source code: https://github.com/snehadas/EmoTale\nFig. 1. Processing pipelines for hand-crafted (top) and deep features (bottom).\nemotions, also known as The Big 6, are anger, disgust, fear,\nhappiness, neutral, and sadness. Following the dimensional\nscheme, emotions can be described numerically in the two\ndimensions activation/arousal and valence, or in three di-\nmensions by including dominance. For example, happiness is\ncharacterized by positive valence, high activation, and neutral\ndominance, i.e., neither dominant nor submissive.\nIn speech emotion recognition (SER), frequently used emo-\ntions include happiness, anger, sadness, disgust, fear, frustra-\ntion, surprise, and boredom. For a baseline comparison, it is\ncommon practice to include neutral as one of the emotions\nexpressed. In many SER databases, utterances are spoken with\nenacted emotions, but emotional responses can also be induced\nthrough specific tasks, scenarios, or stimuli to capture gen-\nuine emotional expressions. Alternatively, natural/spontaneous\nspeech can be collected from existing digital resources, such\nas TV shows or podcasts, and annotated retrospectively. For\nEnglish SER, IEMOCAP [9] and MSP-Podcast [10], [11] are\ntwo of the most frequently used corpora due to their relatively\nlarge size, and the inclusion of both categorical and dimen-\nsional labels. The Danish DES database [12] was published in\n1997 and contains four speakers (two male and two female)\nexpressing five emotions: neutral, surprise, happiness, sadness,\nand anger. All utterances are equally balanced for each gender\nand actor. In listening tests for the DES corpus, emotions\nwere correctly classified 67.3% of the time on average [12].\nHowever, DES includes single words and questions, and it\nwas not developed specifically for speech emotion recognition\n\nNo.\nDanish sentence\nEnglish sentence\n1.\nDugen ligger p˚\na køleskabet.\nThe tablecloth is lying on the fridge.\n2.\nDet sorte ark papir er placeret deroppe\nved siden af tømmerstykket.\nThe black sheet of paper is located up\nthere beside the piece of timber.\n3.\nDe bar det bare ovenp˚\na og nu skal de\nned igen.\nThey just carried it upstairs and now\nthey are going down again.\n4.\nDet vil være p˚\na det sted, hvor vi altid\nopbevarer det.\nIt will be in the place where we always\nstore it.\n5.\nOm syv timer er det morgen.\nIn seven hours it will be morning.\nFive emotions: Neutral, Anger, Sadness, Happiness, Boredom\nTABLE I\nDANISH AND ENGLISH SENTENCES IN EMOTALE.\npurposes.\nContemporary state-of-the-art SER research is most often\nbased on deep learning models [13]–[16] like the SUPERB\nbenchmark [17]. Rapid development of scale-based deep learn-\ning was enabled by the availability of large and exhaustive\nspeech emotion datasets [9], [10]. The most comprehensive\nSER datasets are in English or other large (spoken) languages.\nDeveloping SER models that transfer well to unseen lan-\nguages, addresses the lack of resources in smaller languages\nwhile enabling the accessibility of these models. However, at\nminimum, a test dataset is necessary to validate the suitability\nand safety of a SER model before deployment. In this work,\nwe take the first step towards presenting a Danish-SER dataset\nto address the gap in functional datasets. Our contributions\nare: 1) the EmoTale dataset: a corpus comprising 450 Danish\nand 350 English speech recordings with associated categorical\nand dimensional emotion annotations. 2) we also present a\nthorough validation of the quality of EmoTale by analyzing\nits predictive capacity using reference datasets. Through this\nprocess, we revisit transferability of SER and present insights\nwith respect to other multilingual datasets.\nII. DESIGN OF EMOTALE\nTo enable cross-corpus comparability and transferability, the\ndesign choices in EmoTale are similar to existing small-scale\nSER datasets. The data collection procedure was inspired by\nthe Berlin Database of Emotional Speech (Emo-DB) [18].\nA. Dataset curation\nRecruitment: Participants with acting experience and Danish\nand English language skills were recruited through physical\nflyers and posts on social media, and theater schools in\nthe Greater Copenhagen area were contacted by email and\nphone. An online registration form was available in Google\nForms, where participants signed up by providing their con-\ntact information and choosing their desired experiment date\nfrom a list of options. The exclusion criteria were age < 7\nyears or no Danish-speaking skills. In compliant with GDPR\nrequirements, we obtained written consent from the participant\nor the guardian of participants under 18, and information about\ngender and age was recorded.\nData collection procedure: The data recordings were per-\nformed in multiple sessions and locations with no ambient\nnoise. At the start of a session, the participant was fitted with\nRØDE Wireless Go microphones and was walked through\nthe experiment, and allowed to ask questions. Five sentences\nwere enacted with five different emotions (Tab. I), and the\nFig. 2. Age and gender distribution of EmoTale participants.\nparticipant enacted all sentences for a specific emotion before\nmoving on to the next. The sentences are translations of\nselected sentences from Emo-DB [18]; to minimize subjec-\ntive associations and differences, the sentences were selected\nsuch that they are emotionally neutral and comprise minimal\ncontextual information. We relied on the participant’s ability\nto self-induce an emotion by recalling a situation where it had\nbeen felt strongly. The participants were allowed to repeat\nthe sentences as many times as they liked, but only the last\nrecording was retained. Since Danish speakers are fluent in En-\nglish, the participants could choose to contribute with enacted\nEnglish speech in addition to Danish. The utterances were\nrecorded at a 48 kHz sampling frequency and saved in .WAV\nformat. The audio filenames comprise the meta information on\nthe language, speaker ID, emotion, and sentence. For example,\nthe file DK_004_A_5.wav is the fifth sentence spoken by\nspeaker 004 in Danish, with angry affect.\nData protection and ethical considerations: Ethical approval\nwas obtained from the institutional review board prior to the\nstudy [19]. The samples are pseudo-anonymized by generating\na random identifier for each participant. Since the emotions are\nenacted and the selected sentences do not contain personal\ncontextual information, potential misuse of the data to cause\nharm to the participants is reduced. The dataset is supported\nby a datasheet [20], in the later part of the paper.\nAnnotation procedure:\nIn addition to emotion categories,\nmany existing datasets annotate speech-emotion samples using\ndimensional labels [9], [21]. We adopt a similar approach in\nthe EmoTale corpus, where utterances are manually annotated\nfor arousal, valence, and dominance on a scale from 1 to\n5, with increments of 0.5. Arousal indicates the level of\nexcitement or activation associated with the emotion, ranging\nfrom calm (1) to excited (5). Valence reflects the emotional\ntone, with values ranging from negative (1) to positive (5).\nDominance measures the level of dominance associated with\nthe emotion, with a scale from submissive (1) to dominant\n(5). The first, second, and last authors independently assigned\nlabels to all utterances in EmoTale, each providing one cat-\negorical label for the intended emotion and three numerical\nlabels for arousal, valence, and dominance. The categorical\nannotations serve to validate the enacted emotions.\nB. Description of EmoTale\nEmoTale comprises emotional speech from 18 participants, of\nwhom 12 are female and six are male. The total number of\n\na1 vs. a2\na1 vs. GT\na2 vs. GT\nκ\n0.71\n0.75\n0.85\nTABLE II\nCOHEN’S KAPPA RELIABILITY BETWEEN CATEGORICAL LABELS FROM\nANNOTATORS 1 AND 2 (A1, A2) AND THE PREDEFINED EMOTION (GT).\nArousal\nValence\nDominance\nCCC\n0.72\n0.75\n0.57\nTABLE III\nCONCORDANCE CORRELATION COEFFICIENT (CCC) BETWEEN\nDIMENSIONAL LABELS FROM ANNOTATORS 1 AND 2.\nDanish and English utterances are 450 and 350, respectively.\nThe average age of the participants was 22.8 years, ranging\nfrom 9 to 39 years old. Age and gender distributions of\nparticipants are illustrated in Fig. 2. The goal of this dataset\nis to develop infrastructure to enable the evaluation and safe\ndeployment [22] of existing speech processing and SER on\nthe Danish-speaking population, including children. Therefore,\nspeakers under the age of 18 are also included in the dataset.\nSome files were cropped to exclude a ‘click’ sound (from\nexperimenters’ keyboard) at the start or end of the recording.\nInter-rater reliability (IRR): In addition to the enacted\nemotion, three independent annotators provided four labels per\ninstance: one categorical label for the intended emotion and\nthree numerical labels for arousal, valence, and dominance,\neach ranging from 1 to 5 with increments of 0.5. Valence\n[1-negative, 5-positive], activation [1-calm, 5-excited], and\ndominance [1-weak, 5-strong]. We employ Cohen’s Kappa\n(κ) [23] to assess inter-rater reliability (IRR) between the\ncategorical labels provided by the first two annotators, as well\nas to evaluate their agreement with the predefined ground\ntruth emotion. The IRR results are presented in Table II-B.\nA value of κ = 1 implies perfect agreement, and κ = 0\nmeans the agreement is exactly what would be expected by\nchance. 0.7 < κ indicates good to substantial agreement [24].\nTo evaluate the IRR between dimensional emotion annotations\n(valence, arousal, dominance), we employ Concordance Cor-\nrelation Coefficient (CCC) [25], which is suitable for ratings\non a fine-grained, continuous, or interval scale. As seen in\nTable III, the results indicate moderate to strong agreement for\narousal and valence, and moderate agreement for dominance.\nIII. VALIDATING THE emotion-SIGNAL IN EMOTALE\nWe validate the signals in EmoTale by a) comparing human\nannotations to the predictions from a pre-trained SSL, and\nb) analyzing the predictive power of the data samples by\ntraining and evaluating SER models in Danish. We employ the\nfollowing datasets as references on the validity and quality of\nEmoTale: Emo-DB (German), Urdu (Urdu) [26], DES (Dan-\nish), and AESDD (Greek) [27].\nA. Labels: Human-annotation vs. Pre-Trained Model\nA pre-trained model (PTM), w2v2-FT-dim, fine-tuned on\nthe MSP-Podcast with dimensional labels2, outputs activation,\nvalence, and dominance scores ranging between 0 to 1. These\n2https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\nFig. 3. EmoTale Annotator 1 labels for the utterances in Danish compared\nto the dimensional labels computed from PTM output.\nFig. 4. Concordance Correlation Coefficients between w2v2-FT-dim and\nmanual labels across different datasets. Dominance labels are only available\nfor DES and EmoTale.\nwere rescaled to a range between 1 and 5 to compare with\nmanual labels as follows:\nAi,scaled = 1 + 4 Ai −Amin\nAmax −Amin\n,\n(1)\nwhere Ai denotes the activation score, and Amin and Amax\nare the overall minimum and maximum activation scores,\nrespectively. Valence and dominance scores were rescaled in\nthe same way. Fig. 3 compares the scores predicted by the\nPTM for EmoTale to the labels by Annotator 1. Activation and\nvalence labels for Emo-DB, Urdu, and AESDD were employed\nusing [28], while DES and EmoTale were annotated as part\nof this work. Predictions by w2v2-FT-dim were compared\nagainst the human-annotated labels using CCC in Fig. 4; A\nhigh CCC is observed for activation/arousal and dominance,\nimplying a high agreement between the outcome of PTM and\nhuman-annotated labels, but the scores are consistently lower\nfor valence over all datasets.\nB. Validation with handcrafted features & PTM embeddings\nWe explore the predictive power of the samples in EmoTale\nwith respect to the reference datasets by evaluating the per-\nformance of SER models on all the datasets, in the process\nrevisiting cross-lingual transferability of speech-emotions.\nMethod: As for the SER models, we employ a support vector\nclassifier (SVC), a) with hand-crafted features, and b) PTM\nembeddings, also known as deep features. The PTM feature\nembeddings are extracted as the last hidden states of the pre-\ntrained model, i.e., the last layer before any task-specific head\nis applied, and it is assumed that model embeddings provide\na compact representation of the emotional content in a speech\nsignal. In transformer models, this is the output from the\nfinal transformer block. The experimental procedure is adapted\n\nfrom the one outlined by Wagner et al3. The speech samples\nin DES, EmoTale, Emo-DB, Urdu, and AESDD datasets are\ndownsampled to 16 kHz as the PTM input requirement, and\nstereo audio files were converted to mono by averaging to a\nsingle channel. The pipelines are illustrated in Fig. 1.\nThe eGeMAPS (extended Geneva Minimalistic Acoustic\nParameter Set) [29] and the ComParE (Computational Paralin-\nguistics Challenge) [30] feature sets were extracted using the\nopenSMILE toolkit [31] and serve as two separate baselines.\nThese were tested against embeddings from the wav2vec2\nbase model4 [32] as well as a wav2vec2 model fine-tuned for\nSER on the RAVDESS corpus [33] (w2v2-FT-cat)5 and\none fine-tuned on MSP-Podcast (w2v2-FT-dim) [34]. The\nlatter is fine-tuned on dimensional scores and not categorical\nlabels, therefore, the output of hidden states is necessary to\naccess the latent space of the model. Model embeddings are\nextracted by applying average pooling over the hidden states\nof the last transformer layer. Subsequently, the features are\ninput to a SVC with a linear kernel, and Leave-One-Speaker-\nOut (LOSO) cross-validation is applied. In each fold, features\nwere standardized using the mean and standard deviation of\nthe respective training set. We used a linear kernel to resemble\nthe method in [35].\nEvaluation: Applying LOSO cross-validation introduces vari-\nability in the performance metric. The aggregated unweighted\naverage recall (UAR) across cross-validation folds is used for\nevaluation. However, it may overlook performance differences\nacross individual speakers. Each iteration of LOSO involves\ntraining a model on a different subset of data, hence, for\nS speakers it is more accurate to consider the S different\nmodels separately. For the same dataset, each model is tested\nunder the same conditions, whereby we can apply paired t-\ntests to statistically model performances. The UAR scores\nare computed as the sum of class-wise recall divided by\nthe number of emotion classes, and the overall score is the\naverage UAR across all datasets. To provide a comprehensive\nview of model performance, we report both the aggregated\nresults (highlighted rows in Tab. IV) and the mean results\nacross speakers (Speaker UAR). The former combines the\npredictions of all folds into a single confusion matrix and\ncalculates the UAR. Once the SVC parameters are fixed,\nchanging the random seed does not affect results, hence, the\nstandard deviation is zero. The latter calculates the UAR\nfor each LOSO cross-validation fold individually and takes\nthe mean to consider how well the model generalizes across\nspeakers. Similarly, sentence UARs are found by first grouping\nprediction sentences, calculating the UAR per group, and then\ntaking a simple average across the groups. In this way, all\nthe speakers and sentences are given equal weight. Standard\ndeviations are reported to provide insights into the variability\nof the UAR scores across speakers, sentences, and datasets.\nSentence UARs are not included for the Urdu corpus since it\n3https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb\n4https://huggingface.co/facebook/wav2vec2-base\n5https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\nCorpus\nEmo-DB\nDES\nEmoTale\nUrdu\nAESDD\nOverall\n#Speakers\n10\n4\n18\n22\n6\nComParE\n79.0\n48.5\n52.0\n50.0\n58.0\n57.5 ± 11.2\nSpeaker UAR\n74.9 ± 7.9\n48.5 ± 10.5\n50.9 ± 11.1\n49.7 ± 41.5\n58.2 ± 10.9\nSentence UAR\n79.5 ± 5.0\n48.5 ± 10.1\n52.0 ± 1.5\n-\n58.1 ± 5.4\neGeMAPS\n64.3\n42.7\n46.0\n58.0\n47.6\n51.7 ± 8.1\nSpeaker UAR\n60.3 ± 14.1\n42.7 ± 13.6\n44.8 ± 13.8\n29.2 ± 21.6\n47.8 ± 11.8\nSentence UAR\n63.9 ± 5.5\n42.7 ± 9.0\n46.0 ± 3.9\n-\n47.7 ± 7.1\nw2v2 base\n58.9\n32.7\n29.7\n33.5\n41.8\n39.3 ± 10.6\nSpeaker UAR\n56.7 ± 8.0\n32.7 ± 4.2\n29.4 ± 6.9\n23.1 ± 26.7\n41.7 ± 12.3\nSentence UAR\n58.4 ± 8.2\n32.7 ± 8.8\n29.8 ± 2.6\n-\n41.8 ± 8.5\nw2v2 FT dim\n96.1†\n67.7\n64.1‡\n59.5\n83.2‡\n74.1 ± 13.6\nSpeaker UAR\n94.7 ± 3.9\n67.7 ± 4.0\n62.0 ± 12.4\n48.4 ± 36.5\n83.1 ± 7.8\nSentence UAR\n96.1 ± 3.0\n67.7 ± 12.7\n64.1 ± 5.4\n-\n83.2 ± 5.9\nw2v2 FT cat\n88.8\n62.7\n59.6‡\n52.5\n77.5‡\n68.2 ± 13.1\nSpeaker UAR\n88.1 ± 5.0\n62.7 ± 5.1\n57.8 ± 12.2\n37.1 ± 34.7\n77.5 ± 10.8\nSentence UAR\n88.3 ± 4.8\n62.7 ± 10.5\n59.6 ± 3.5\n-\n77.6 ± 8.1\nTABLE IV\nUAR (%) FOR SVC BASED ON HAND-CRAFTED & DEEP FEATURES AS\nMEAN AND STD. DEV. (µ ± σ) OVER LOSO FOLDS. THE PTMS ARE A\nBASE MODEL (W2V2-B) AND MODELS FINE-TUNED ON DIMENSIONAL\nLABELS (W2V2-FT-D I M) AND CATEGORICAL LABELS (W2V2-FT-C A T).\nUAR SCORES ACROSS SPEAKERS AND SENTENCES PROVIDE INSIGHTS\nINTO THE PERFORMANCE VARIABILITY, EXCEPT FOR URDU, WHICH\nCONTAINS NATURAL SPEECH. † AND ‡ MARK THE SINGLE BEST AND THE\nTWO BEST MODELS ACROSS LOSO FOLDS WITH STATISTICAL\nSIGNIFICANCE FOR A DATASET.\ncontains natural utterances, hence, no sentences are repeated.\nResults: For Emo-DB, the results reported in Table IV using\nComParE and w2v2-FT-dim embeddings are reproduced\nfrom [13]. The performance trends observed on EmoTale\nalign with those seen in Emo-DB and DES, reinforcing the\nconsistency and reliability of the dataset. Specifically, the\nUAR scores for the three datasets follow the same trend with\nmodel performance in descending order using the features:\nw2v2-FT-dim, w2v2-FT-cat, ComParE, eGeMAPS, and\nw2v2-b. Interestingly, Urdu deviates from the trend with\neGeMAPS features outperforming both ComParE and the\nembeddings from the PTM fine-tuned on categorical labels,\nw2v2-FT-cat. In all cases, deep features from the fine-\ntuned models yield the highest UARs, while embeddings from\nthe wav2vec2 model without fine-tuning perform the worst.\nFurthermore, the PTM fine-tuned on dimensional labels leads\nto the highest mean UAR across datasets, highlighting the\nbenefit of fine-tuning.\nTo further validate model performance on EmoTale, we\napplied pairwise t-tests [36] across LOSO folds to assess\nthe statistical significance of differences between feature sets.\nWhile fine-tuning of dimensional labels (w2v2-FT-dim)\nyields a statistically significant improvement over categorical\nlabels (w2v2-FT-cat) for Emo-DB, this distinction does not\nhold for EmoTale nor the other datasets, which negates the\nargument against categorical labels [37]. Similarly, for several\ndatasets, there is no statistically significant difference in model\nperformance when training on eGeMAPS features compared\nto wav2vec2 base model embeddings. The single best and two\nbest models with statistical significance are marked in Table IV\nwhen such a conclusion could be drawn based on pairwise\nt-tests. These findings further strengthen EmoTale’s role as\na reliable benchmark for emotional speech, with results that\nreflect those of established corpora.\nWe also observe from Tab. IV that the scores for DES are\nrelatively low, and model performance is sentence-dependent,\nin contrast to the EMO-DB and EmoTale. This could be\n\nFig. 5.\nUnweighted Average Recall (UAR) scores for SVC trained on\nComParE features (left) and pre-trained model embeddings (right).\nexplained by DES being designed differently from the other\ndatasets. For example, the sentence ID NO refers to a single\nword Nej (No), which may not be sufficient for the model\nto recognize the emotion. Similarly, the sentences with ID:\nSE4, SE5, SE6, and SE8 are all questions, and might be\nspoken with a different intonation. Embeddings from the\nPTMs generally produce more stable results (low variation),\nhowever, a relatively high standard deviation is observed for\nw2v2-FT-dim features across EmoTale speakers (12.4) and\nDES sentences (12.7). This could be explained by differences\ninherent in the two datasets: EmoTale has a larger age range\nof speakers compared to the other datasets, and DES contains\nsentences that vary in linguistic and paralinguistic content. The\nUAR scores for Urdu are very speaker-dependent compared to\nthe other datasets. This can be explained by a high number of\nspeakers and a low number of sentences per speaker.\nTo assess cross-corpus transferability, the SVC models were\nretrained to recognize a subset of four emotion classes (happy,\nangry, sad, and neutral) on ComParE and w2v2-FT-dim\nfeatures for the Emo-DB, EmoTale, DES, and Urdu corpora.\nThese datasets were selected specifically because they include\nall four emotion labels. The UAR scores for all train-test\ncombinations are shown in the heatmaps in Fig. 5. The in-\ncorpus UAR scores in the diagonal of the matrices are found\nby LOSO cross-validation following the same methodology\nas earlier, but only including the four emotions. We wish\nto develop SER models that generalize well on new, unseen\ndata, especially in real-world applications. Furthermore, a\nmodel that transfers well is less likely to be overfitted on\nthe training data. Although performance generally drops in\nthe cross-corpus domain, the deep features seem to be more\ntransferable. Importantly, EmoTale proves to be a strong\nevaluation benchmark. While models trained on EmoTale per-\nform comparably to those trained on other corpora, EmoTale\nconsistently supports meaningful generalization. For example,\ninferring on Emo-DB yields higher cross-domain scores than\nin-corpus UAR scores when trained on EmoTale and DES.\nThis continues the pattern from the previous analysis, where\nEmo-DB achieved significantly higher model performances\nthan the other corpora. This could be explained by the percep-\ntion tests carried out during the creation of Emo-DB, where\nutterances recognized by more than 80% of the listeners were\nkept in the database. Hence, the database is expected to contain\nutterances with highly pronounced affect.\nIV. CONCLUSIONS\nUnavailability of Danish affect datasets not only impedes\nthe development of the technology, but also impacts the\nvalidation of existing methods on Danish speakers. We present\nEmoTale, a bilingual enacted speech-emotion dataset in Dan-\nish and English, intended to enable the evaluation of SER\nmodels in the Danish language. In addition to categorical\nemotion labels, EmoTale includes dimensional annotations\nfor arousal, valence, and dominance. Annotation reliability\nis high: Concordance Correlation Coefficient (CCC) scores\nindicate moderate to strong agreement for arousal and valence,\nand moderate agreement for dominance, while Cohen’s Kappa\nvalues indicate substantial consistency in categorical labeling.\nTo demonstrate the validity of the dataset, we evaluate its\nlabels and predictive capacity using both pre-trained model\nembeddings and hand-crafted, acoustic features. Our experi-\nments demonstrate that (a) model performance on EmoTale is\ncomparable to that on established reference datasets, and (b)\nfeature embeddings from PTMs consistently outperform hand-\ncrafted features, particularly in cross-corpus transfer scenarios.\nWhile models trained on EmoTale perform comparably to\nthose trained on other corpora, EmoTale consistently supports\nmeaningful generalization. These findings further strengthen\nthe validity of EmoTale as a reliable benchmark for Danish\nemotional speech.\nV. ACKNOWLEDGMENT\nCo-funded by the French National Research Agency under\nthe Pantagruel project (ANR-23-IAS1-0001) and the European\nUnion under the Marie Skłodowska-Curie Grant Agreement\nNo 101081465 (AUFRANDE). Views and opinions expressed\nare however those of the author(s) only and do not necessarily\nreflect those of the European Union or the Research Executive\nAgency, which cannot be held responsible for them.\n\nDATASHEET FOR EMOTALE\nIn line with the proposal on datasheets for datasets by Gebru\net al. [38], we provide the datasheet for the EmoTale corpus,\nalso available as a standalone document with the dataset.\nA. Motivation\nFor what purpose was the dataset created?\nUnavailability of Danish affect datasets not only impedes the\ndevelopment of the technology, but also impacts the validation\nof existing methods on Danish speakers. The introduction of\nour corpus is necessary to, at the very least, be able to validate\nthe performance of SER models for the Danish language.\nWho created the dataset and on behalf of which entity?\nThe dataset was created by Maja Jønck Hjuler, Line Katrine\nHarder\nClemmensen,\nand\nSneha\nDas\nat\nthe\nTechnical\nUniversity of Denmark.\nWho funded the creation of the dataset?\nThe dataset creation is funded by the larger WristAngel\nproject which is funded by an exploratory Synergy grant\nfrom the Novo Nordisk Foundation and is a collaboration\nwith Copenhagen University Hospital, the Child Psychiatry\nResearch Unit.\nB. Composition\nWhat do the instances that comprise the dataset represent?\nThe instances are audio files of enacted emotional speech\nin Danish and in English. The speakers enact predefined\nsentences while expressing predefined emotions.\nHow many instances are there in total?\nThe EmoTale corpus consists of a total of 800 audio instances,\ncomprising 450 emotional speech recordings in Danish and\n350 in English. Each recording features one of five different\nenacted emotions, and the dataset is balanced across these\nemotions.\nWhat data does each instance consist of?\nEach instance consists of raw audio data in WAV format,\ncaptured at a sampling frequency of 48 kHz. Each recording\ncorresponds to one of five enacted emotions: Neutral, Anger,\nSadness, Happiness, or Boredom, and is based on predefined\nsentences that are translations from the German Emo-DB cor-\npus, designed to be emotionally neutral to minimize contextual\nbias.\nIs there a label or target associated with each instance?\nIn addition to the enacted emotion, three independent anno-\ntators provided four labels per instance: one categorical for\nthe emotion chosen from the five possible classes, and three\nnumerical for arousal, valence, and dominance in a range of\n1 to 5 with increments of 0.5. The ranges are defined as:\nValence [1-negative, 5-positive], activation [1-calm, 5-excited],\nand dominance [1-weak, 5-strong].\nIs any information missing from individual instances?\nEverything is included. No data is missing.\nAre there recommended data splits?\nThere are no recommended data splits for training, validation,\nand testing within the dataset itself. However, it is common\npractice to create stratified splits across speakers and emotions.\nAre there any errors, sources of noise, or redundancies in\nthe dataset?\nSee preprocessing below.\nDoes the dataset contain data that might be considered\nconfidential?\nThe data does not contain any signals reflecting on the state\nof an individual, minimizing the potential negative impact on\nthe individuals.\nDoes the dataset identify any subpopulations?\nParticipants range in age from 9 to 39 years. The dataset\nincludes 18 participants, with 12 females and 6 males.\nIs it possible to identify individuals, either directly or\nindirectly from the dataset?\nIndividuals can be identified indirectly from the EmoTale\ncorpus due to the unique characteristics of each participant’s\nvoice, which can reveal their identity. All participant informa-\ntion has been pseudoanonymized by assigning random IDs.\nC. Collection Process & Preprocessing\nHow was the data associated with each instance acquired?\nThe data recordings were performed in several sessions in dif-\nferent locations. In each session, the participant was placed in a\nquiet room and fitted with wireless RØDE microphones paired\nwith the corresponding receiver. Five sentences were enacted\nwith five different emotions, and the participant enacted all\nsentences for a specific emotion before moving on to the next.\nThe participants were allowed to repeat sentences as often as\nthey liked, but only the last recording was kept in the database.\nMost often, the recording was made in the first attempt.\nWho was involved in the data collection process?\nParticipants with acting experience and Danish and English\nlanguage skills were recruited through physical flyers and\nposts on social media, and theater schools in the Greater\nCopenhagen area were contacted by email and phone.\nOver what timeframe was the data collected?\nThe data was collected as part of a master’s thesis project\nlasting 5 months.\nWere any ethical review processes conducted?\nEthical approval was obtained from the institutional review\nboard prior to the study [19].\nDid the individuals in question consent to the collection\nand use of their data?\nAbiding by GDPR requirements, written consent was obtained\nfrom participants or their guardians prior to data collection.\nWas any preprocessing/cleaning/labeling of the data done?\nSome instances were cropped to exclude audible ’clicks’\nfrom the experimenter pressing the keyboard at the beginning\nor end of recordings. The audio files are named according\nto the same template including information about the lan-\nguage, speaker ID, emotion, and sentence. For example, the\nfile DK 004 A 5.wav contains the fifth sentence spoken by\nspeaker 004 in Danish, with angry affect.\n\nWas the “raw” data saved in addition to the prepro-\ncessed/cleaned/labeled data?\nYes. The authors can provide the raw data upon request.\nD. Uses\nHas the dataset been used for any tasks already?\nThe dataset paper investigates the dataset’s capacity for pre-\ndicting speech emotions through the development of speech\nemotion recognition models using Self-Supervised Speech\nModel embeddings and the openSMILE feature extractor.\nFurthermore, cross-corpus transferability of the models was\ninvestigated.\nWhat (other) tasks could the dataset be used for?\nThe dataset can also be used for ASR, due to the availability\nof speech and the corresponding transcription. The enacted\nEnglish speech in addition to Danish will aid research and in-\nvestigation into speech systems, for instance when the speaker\nremains identical, but language changes, hence towards more\nuniversal speech emotion models.\nAre there tasks for which the dataset should not be used?\nGiven the size of the dataset, it should not be used for tasks\nthat require large-scale training of complex machine learning\nmodels. Additionally, it is not suitable for tasks that require\nspontaneous emotional speech, as the recordings consist of\nenacted emotions rather than natural emotional expressions.\nE. Distribution & Maintenance\nHow will the dataset will be distributed?\nThe dataset can be accessed at https://github.com/snehadas/\nEmoTale.\nWill the dataset be distributed under a copyright or other\nintellectual property (IP) license?\nThe data will be distributed under a copyright. There is no\nlicense, but users are requested to cite the corresponding paper\nif the dataset is used.\nWho will be maintaining the dataset and how can they be\ncontacted?\nThe dataset will be maintained by the corresponding author\nSneha Das (sned@dtu.dk).\nWill the dataset be updated?\nThis dataset will not be updated in terms of the number of\nsamples or participants."}
{"id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "authors": ["Krunal Shah", "Dan Roth"], "source": "http://arxiv.org/abs/2508.14488v1", "page_count": 10, "text": "Reasoning is about giving reasons\nKrunal Shah∗\nYutori Inc.\nktgshah@gmail.com\nDan Roth\nUniversity of Pennsylvania\ndanroth@seas.upenn.edu\nAbstract\nConvincing someone of the truth value of a\npremise requires understanding and articulating\nthe core logical structure of the argument which\nproves or disproves the premise. Understanding\nthe logical structure of an argument refers to\nunderstanding the underlying “reasons\" which\nmake up the proof or disproof of the premise\n- as a function of the “logical atoms” in the\nargument. While it has been shown that trans-\nformers can “chain” rules to derive simple ar-\nguments, the challenge of articulating the “rea-\nsons” remains. Not only do current approaches\nto chaining rules suffer in terms of their inter-\npretability, they are also quite constrained in\ntheir ability to accommodate extensions to the-\noretically equivalent reasoning tasks – a model\ntrained to chain rules cannot support abduction\nor identify contradictions.\nIn this work we suggest addressing these short-\ncomings by identifying an intermediate repre-\nsentation (which we call the Representation of\nthe Logical Structure (RLS) of the argument)\nthat possesses an understanding of the logical\nstructure of a natural language argument – the\nlogical atoms in the argument and the rules\nincorporating them. Given the logical struc-\nture, reasoning1 is deterministic and easy to\ncompute. Therefore, our approach supports\nall forms of reasoning that depend on the logi-\ncal structure of the natural language argument,\nincluding arbitrary depths of reasoning, on-the-\nfly mistake rectification and interactive discus-\nsion with respect to an argument. We show that\nwe can identify and extract the logical structure\nof natural language arguments in three popular\nreasoning datasets with high accuracies, thus\nsupporting explanation generation and extend-\ning the reasoning capabilities significantly.2\n*Work done while the author was a student at the Univer-\nsity of Pennsylvania.\n1This work focuses on extracting the propositional struc-\nture of the arguments, and does not attempt to handle quanti-\nfiers, as is the case for most recent neural work on reasoning.\n2This work was originally written in 2021 and was never\nFigure 1: An example of an argument from the RULE-\nTAKERS (Clark et al., 2020) dataset. The goal is to\ndetermine the truth value of the query given some facts\nand rules (that all together constitute an argument). The\nlogical structure of the argument at the bottom conveys\na deeper understanding of the reasoning process, simpli-\nfies the verification process, and supports reasoning on\n“What if Harry was young but not nice?\" and \"If Harry\nis round then does that make him nice?\".\n1\nIntroduction\nNatural language is often used to make arguments.\nFor example, given some facts and rules, a deduc-\ntive argument can be made, as in Fig. 1. For many\nyears, the key approach in AI, to reasoning argued\nin natural language (NL) text, was to map the text to\na formal structure and then use well defined, deter-\nministic, algorithms that support exact reasoning\nover these representations (Hayes, 1977; Hobbs\net al., 1988; McCarthy, 1963, 1976). However,\nlatter works have discussed the brittleness of this\napproach, arguing that, in general, natural language\ntext is too expressive to be mapped into a formal,\nsymbolic representation (Charniak, 1973)\nThe recent success of neural models and, more\nspecifically, transformer based language models,\npublished. It suggests that Language Models could support\nreasoning and interpretability better by converting text to an\nintermediate formal representation that solvers can reason\nabout deterministically. While the Language Models have\nimproved significantly since then, this line of thought is still\nvalid and very important to pursue.\n\nhas prompted researchers to explore the reasoning\nabilities of such systems. Recent work (Clark et al.,\n2020; Saha et al., 2020; Tafjord et al., 2020) demon-\nstrates the ability of transformers to perform simple\nchaining of rules (top half of Fig. 1) and some lim-\nited form of abduction (Tafjord et al., 2020).\nHowever, it is important to note that the only\nlevel of uncertainty that exists in reasoning prob-\nlems expressed in natural language is the derivation\nof the core logical structure of the underlying NL ar-\nguments. Once the logical structure of an argument\nis given, the computational problem is determinis-\ntic and easy; it boils down to solving satisfiability\nor unsatisfiability over a small domain.\nIn this paper we first agree with previous works\nthat in general, it is not possible to map natural\nlanguage to a symbolic representation; however,\nwe argue that it is possible to map arguments made\nin natural language to their core logical structure\n(where this mapping might lose some information\nthat is not relevant to the argument itself). Towards\nthis end, we define a class of structured represen-\ntations, in the context of reasoning tasks, called\nRepresentation of Logical Structure (RLS). The\nprimary contribution of this work is defining the\nRLS of an argument made in text and showing how,\nby doing so, we can explain the argument and sup-\nport the benefits of performing symbolic reasoning\nover knowledge expressed in natural language. Ex-\ntracting the logical structure of an argument in the\nform of structured representations (RLS) allows\nus to explain the reasoning process, support other\nforms of reasoning that depend on this logical struc-\nture, and reason with respect to it – interact about\nit, identify fallacies, etc. These latter steps can be\ndone by simply feeding the extracted representa-\ntions to an appropriate symbolic reasoning engine.\nThis is a well defined and solved problem (Gu\net al., 1996) and we do not attend to this part of the\ncomputation in this paper.\nSpecifically, we propose using a sequence to se-\nquence transformer to extract RLS from natural\nlanguage text. We demonstrate the effectiveness of\nour RLS extraction method, in capturing the logical\nstructure of natural language arguments, on three\ndatasets spanning a variety of reasoning tasks and\nlanguage usage. First, our RLS extraction method\nshows its success on the LEAP-OF-THOUGHT (Tal-\nmor et al., 2020) and CLUTRR (Sinha et al., 2019)\ndatasets by registering 99% and 95% exact match\naccuracies, showing that the representation extrac-\ntion method can have broad coverage. Furthermore,\nwe show the effectiveness of using a symbolic rea-\nsoner over the extracted logical structures on the\ndata that was used in (Clark et al., 2020; Saha et al.,\n2020; Tafjord et al., 2020) and demonstrate the ef-\nfectiveness of this approach. Specifically, we find\nthat our method performs competitively with the\ncurrent end-to-end reasoning systems on the rule\nbased reasoning task of the RULETAKERS (Clark\net al., 2020) benchmark.\nIn the following section, we make a more de-\ntailed case for our methodology by addressing\nsome of the challenges of introducing intermediate\nlogical representations, along with the benefits of\nthis approach. Finally, we note that the primary\nmotivation for this work is not to improve the per-\nformance on existing benchmarks but to (1) em-\nphasize the need for finer logical explanations for\nreasoning tasks, (2) investigate the effectiveness of\nexisting language models in extracting structured\nlogical representations3 from arguments expressed\nin natural language, and (3) throw light on our alter-\nnate, more symbolic approach which leverages the\nlanguage modeling advances to perform competi-\ntive, more explainable reasoning compared to the\nend to end approaches adopted by previous works.\n2\n“Explanations\" are symbolic\nIn this section we try to lay out a more detailed\nanalysis addressing some of the shortcomings of\nthe “extracting structured logical representations\"\napproach and subsequently outline the notable ad-\nvantages of this approach.\n2.1\nChallenges\nAn important shortcoming of using symbolic rea-\nsoning methods which, as discussed earlier, was the\nkey conceptual barrier to early efforts, is that nat-\nural language is simply too expressive. Given the\ndiverse linguistic variability of natural language,\nand the amount of information humans are able\nto communicate via natural language, we cannot\nexpect to accurately and reliably convert NL utter-\nances into structured representations. This paper\nagrees with this assessment and does not attempt to\nconvert NL to a structured representation. Instead,\nour method of extracting the RLS of NL arguments\nrelies on a much weaker assumption about the pres-\nence of a “local\" logical structure in NL arguments.\n3we use “structured representations\", “structured logical\nrepresentations\" and “logical representations\" interchangeably\n\nfact: Mary is a young woman with a heart of gold.\nrule: If Mary has gold then she is rich.\nFigure 2: In the above inference, if the weak unification\noperator concludes that “(Mary, has, heart of gold)\" and\n“(Mary, has, gold)\" are equivalent then the symbolic rea-\nsoner would explicitly indicate the same in the proof,\nproviding insights into the implicit assumptions/knowl-\nedge used in the reasoning process.\nAnd in extracting the logical structure we admit-\ntedly lose some other aspects of the text, that are\nnot relevant to the logical argument (but could very\nwell be relevant to other decisions made with re-\nspect to the text). For example, if Figure 1 were\nto say that “Harry works outside, to his chagrin\"\nthen this will not affect the logical structure of the\nargument, and will not impact the logical atoms\nextracted by the method, since the mental state of\nHarry, while important, is irrelevant to the logical\nargument.\nAnother notable challenge of using symbolic rea-\nsoning methods with natural language is associated\nwith their inability to handle the noise, uncertainty,\nand “softness\" expressed in language. However,\nrecent work (Lewis and Steedman, 2013; Weber\net al., 2019) has shown promising results in trying\nto mitigate these by making use of weak unification\noperators to replace the exact matching in symbolic\nreasoners. When this “soft\" unification is done by\na symbolic solver, the solver provides insights into\nthe assumptions made during the reasoning process\nas shown in Figure 2, since the logical atoms which\nare considered equivalent by the unification op-\nerator represent the linguistic/implicit knowledge\nencoded in the unification operator function. That\nis, if you know that the reason the outcome is such\nis due to unifying the variables “has gold\" and “has\na heart of gold\", the process becomes more trans-\nparent.\nWe note that in this work, we only perform ex-\ntraction of structured representations from individ-\nual natural language utterances rather than from\ncomplete (possibly multi-sentence) arguments. We\nleave the problem of extending the extraction to ac-\ncount for the whole context of the natural language\nargument for future work.\n2.2\nSymbolic reasoning is “simple\"\nIn this subsection, we draw attention to the obser-\nvation that the primary challenge in the problem of\nreasoning over natural language text is concerned\nwith the ambiguity and linguistic variability associ-\nated with natural language. Once we can accurately\nmap the relevant information from the NL text into\nits structured logical representations, the task of rea-\nsoning over these symbols is nearly trivial. Propo-\nsitional symbolic reasoning is well understood, and\nessentially boils down to boolean satisfiability or\nunsatisfiability; this can be performed very effi-\nciently (Gu et al., 1996; Gomes et al., 2008) over a\nsmall number of variables.\n2.3\nAdvantages\nThe foundations of reasoning laid out in the field\nof artificial intelligence were primarily defined to\ndeal with symbolic reasoning, as a result, there is\nrich and diverse literature around symbolic reason-\ning. Our approach of extracting the logical rep-\nresentations from NL sentences allows us to take\nfull advantage of these advances. We now try to\nsummarize some of the advantages of our system\nwhich uses a symbolic reasoning engine on top of\nextracted logical structures.\n1. Note that once we can accurately extract the\nrepresentations from the NL sentences, we can\nperform any kind of reasoning (e.g. deductive,\nabductive, contradiction detection, etc.) that de-\npends on this logical structure, without the need\nfor any additional training. This is in contrast to\nthe existing end-to-end transformer based sys-\ntems which would require additional training\ndata to learn any new kind of reasoning.\n2. As discussed in Section 2.1, the usage of weak\nunification operators provide a potential solution\nto addressing the inability of symbolic reason-\ning systems to handle the “softness\", noise and\nambiguity expressed in natural language. We ob-\nserve that the usage of language models as weak\nunification operators additionally allows us to\n(1) leverage the linguistic and implicit knowl-\nedge of the language model and (2) explicitly\nlay out the unifications used in inference, as\ndemonstrated in Figure 2.\n3. Given the disentanglement of the reasoning re-\nsponsibilities in our method, the system allows\nfor online mistake rectification and tweaks. For\nexample, if a mistake is observed in the repre-\nsentation extraction step during inference, the\nmistake can be rectified by human intervention\nand the resulting corrected theory can be simply\nfed through the symbolic reasoning engine to\n\nget the correct answer. In contrast, the end-to-\nend systems do not provide any outlet to rectify\nincorrect inferences drawn by the model.\n4. The above mentioned disentanglement also al-\nlows us to transfer the method to new kinds of\nrules or facts (like multivariate rules and event\nattributes based reasoning) by simply training\na new representation extraction model. Train-\ning the end-to-end systems on the other hand\nrequires additionally gathering complete theo-\nries of reasoning over the natural language sen-\ntences.\n3\nRLS and problem formulation\nIn this section, we define the structured representa-\ntions proposed in this work, termed Represention\nof Logical Structure (RLS). We also detail how\nwe formulate RLS for the datasets used for our\nexperiments and then briefly describe how we for-\nmulate the problem of extracting RLS from natural\nlanguage text.\n3.1\nRepresentation of Logical Structure (RLS)\nWe define RLS for a natural language statement S,\nin the context of a reasoning task R, as a structured\nrepresentation which encodes all the relevant infor-\nmation in statement S, i.e. for any inference/rea-\nsoning chain which S is a part of, if we replace the\nstatement S with its corresponding RLS then the\nresulting inference/reasoning chain still remains\nvalid and evaluates to the same result as with S.\nAlthough not included in the definition, a desirable\nproperty from a good RLS formulation is that it is\nefficient, i.e. it only encodes the information from\nstatements that is truly relevant to the reasoning\ntask. More specifically, if an alternate RLS formu-\nlation for the given task T encodes strictly less in-\nformation than the existing formulation while still\nsatisfying the definition of RLS then the existing\nformulation would not be called efficient. Kindly\nnote that we propose the RLS of a natural language\nstatement as an alternative to the semantic parse\nof the statement. Hence, similar to the semantic\nparse of a statement, RLS is (a) task dependent, (b)\na class of structured representations and (c) does\nnot follow a well-defined syntax.\nNow we try to briefly emphasize the differences\nof our approach of extracting RLS compared to\nsemantic parsing. Unlike semantic parsing which\naims to extract structured logical representations\nfrom all forms of natural language text with min-\nimal information loss, extracting RLS is only de-\nfined in the context of a reasoning task meaning\nthat it aims to only extract structured logical repre-\nsentations from NL sentences where a logical struc-\nture can be expected to exist. Furthermore, RLS\nonly aims to extract information from a sentence\nwhich is essential to the usage of the sentence in\nthe context of the reasoning task, hence extracting\nRLS has a looser requirement on information loss\ncompared to semantic parsing. We posit that these\nfundamental differences help us mitigate some of\nthe primary issues with semantic parsing that origi-\nnate from the diverse variability of natural language\nand the amount of information humans are able to\ncommunicate via language.\n3.2\nOur RLS formulation\nWe now briefly describe the RLS formulations used\nby our approach to encode the logical structure\nfrom natural language statements, for the problems\nused in our experiments. Our structured represen-\ntations are simple logical statements where the log-\nical literals are tuples of natural language words,\nphrases and binary symbols. Table 1 shows some\nexamples of natural language sentences and the\ncorresponding RLS used by our approach. Since\nRLS is defined in the context of a particular rea-\nsoning task, note that the representations follow\ndifferent formulations for the different datasets and\nthe choice for the formulation is primarily dictated\nby the kind of logical structure that is important for\nthe corresponding reasoning task.\n3.3\nProblem formulations\n3.3.1\nRepresentation extraction formulation\nWe model the problem of extracting RLS from\na natural language sentence as a sequence to se-\nquence prediction task where the input is the natu-\nral language utterance and the output is an encod-\ning of the RLS of the NL sentence. Table 1 shows\nsome examples of how the structured representa-\ntions from instances of the different datasets are\nencoded for the sequence output of the model.\n3.3.2\nRule based reasoning\nThe deductive reasoning task described in Clark\net al. 2020 takes as input a natural language theory,\nconsisting of a set of rules and facts described in\nnatural language, and a query described in NL. The\ngoal is to predict the truth value of the query, in the\nworld defined by the theory, under the closed world\nassumption (CWA).\n\n4\nExtracting the Logical Structure of\nArguments\nIn this section, we discuss the questions we wish\nto answer in our experiments and briefly describe\nthe datasets, models and evaluation metrics used\nfor the same.\n4.1\nExperimental settings\nThe goal of our experimental evaluation is two\nfold, to first investigate whether our RLS extrac-\ntion method works well for different kinds of RLS\nformulations and natural language text; and subse-\nquently understand the efficacy of our reasoning\nsystem which relies on the RLS extraction method.\nSpecifically, we design experimental settings to\nanswer the following questions:\n1. Can the RLS extraction method generalize\nwell to different kinds of logical reasoning\ndatasets which can benefit from reasoning\nover structured logical representations instead\nof natural language text?\n2. How well can a method relying on RLS ex-\ntraction perform on the rule based reasoning\ntask (example in Fig. 1), compared to the end-\nto-end transformer systems?\nWe use the LEAP-OF-THOUGHT (Talmor et al.,\n2020) and CLUTRR (Sinha et al., 2019) datasets\nto test the generalizability of our representation ex-\ntraction method to answer our first question; and\nfurther use the RULETAKERS dataset (Clark et al.,\n2020) as the benchmark for the rule based reason-\ning task to answer the second question. We now\nbriefly describe each of these datasets.\n4.2\nDatasets\n1. LEAP-OF-THOUGHT (Talmor et al., 2020):\nA dataset designed to test the ability of mod-\nels to reason over implicit factual knowledge.\nInstances of the dataset include a collection\nof facts and rules, expressed in templated lan-\nguage, along with a query such that answer-\ning the query requires some implicit factual\nknowledge along with the explicitly stated facts\nand rules. This dataset is used to test whether\nour representation extraction method can han-\ndle RLS representations which primarily con-\nsist of phrases (as opposed to simple words)\nas the logical literals’ arguments (as shown in\nTable 1). For our experiments, we merge all\nthe different train and test sets corresponding\nto the two settings of “counting\" and “hyper-\nnymy\" which results in around 38k/22k unique\ntraining instances and 4811/3234 unique test\ninstances for the hypernymy/counting settings\nrespectively and we train separate representa-\ntion extraction models for the two settings. The\nlogical literals for the statements in this dataset\nfollow the pattern (noun_phraseA, relation,\nnoun_phraseB, polarity). The logical literals\ncombine by using simple logical conjunction op-\nerators to form the RLS for the natural language\nstatements.\n2. CLUTRR (Sinha et al., 2019): An inductive\nreasoning benchmark which requires a model\nto infer a kinship relation that is not explicitly\nstated in the input while also learning the\nlogical rules governing the relations.\nThe\nbenchmark consists of a natural language\nsetting where the input relations are expressed\nusing NL and a graph based setting where\nthe relations are provided as symbolic inputs.\nThe natural language descriptions are curated\nusing crowdsourced templates and the test set\nconsists of unseen NL templates. We use this\ndataset to test our representation extraction\nmethod’s ability to generalize to more natural\nforms of language and to a different kind of\nRLS formulation. Note that since the dataset\ndeals with reasoning over kinship relations,\nwe identify the kinship relations between\nentities as the logical structure to extract from\ninstances of the dataset, as shown in Table 1.\nFor our purpose, we generate a dataset of 8k\ntraining instances and 846 test instances of\nunseen templates. The logical literals for the\nstatements in this dataset follow the pattern\n(personA, relationAB, personB).\nThe\nlogical literals combine by using simple logical\nconjunction operators to form the RLS for the\nnatural language statements. Note that since\nthe inverse of kinship relations exist, for every\nrelation\n(personA, relationAB, personB),\nwe also include the inverse relation literal\n(personB, relationBA, personA) in the RLS\nfor the given natural language statement.\n3. RULETAKERS (Clark et al., 2020): A collec-\ntion of several rule based reasoning datasets\nwhere the instances consist of theories and re-\nlated queries expressed in natural language. The\n\nDataset\nType\nSentence\nRULETAKERS (fact)\nSentence\nHarry is young and nice.\nAnnotation\n(\"Harry\" \"is\" \"young\" \"+\") (\"Harry\" \"is\" \"nice\" \"+\")\nRLS\n(Harry, is, young, +) ∧(Harry, is, nice, +)\nEncoded Repr.\n<arg0> Harry <pred> is <arg1> young <pos> <and> <arg0> Harry <pred> is <arg1> nice <pos>\nRULETAKERS (rule)\nSentence\nNice people are usually round in shape.\nAnnotation\n(\"someone\" \"is\" \"nice\" \"+\") -> (\"someone\" \"is\" \"round\" \"+\")\nRLS\n(someone, is, nice, +) -> (someone, is, round, +)\nEncoded Repr.\n<arg0> someone <pred> is <arg1> nice <pos> <impl> <arg0> someone <pred> is <arg1> round <pos>\nCLUTRR\nSentence\nSol took her son Kent to the park for the afternoon.\nAnnotation\nedges: [(\"Sol\", \"Kent\")], edge_types: [\"son\"], genders: {\"Sol\": female, \"Kent\": male}\nRLS\n(Sol, son, Kent) ∧(Kent, mother, Sol)\nEncoded Repr.\n<arg1> Sol <pred> son <arg2> Kent <and> <arg1> Kent <pred> mother <arg2> Sol\nLEAP-OF-THOUGHT\nSentence\nA mustard is not capable of shade from sun.\nAnnotation\n{\"subject\": \"mustard\", \"predicate\": \"/r/CapableOf\", \"object\": \"shade from sun\", \"validity\": \"never true\"}\nRLS\n(mustard, capable of, shade from sun, -)\nEncoded Repr.\n<arg0> mustard <pred> capable of <arg1> shade from sun <neg>\nTable 1: Table showing how the annotations provided with the datasets are converted to their respective RLS\nformulations and subsequently encoded to train the sequence to sequence representation extraction model.\ntheories consist of facts and rules about entity\nrelations and attributes and the queries require\ndeductive reasoning over the theory of facts\nand rules (example in Fig. 1). We specifically\nuse the D3, D5, ParaRules and Birds-Electricity\ndatasets for our experiments to investigate the\nperformance of our reasoning method which\nuses a symbolic reasoner on top of the RLS ex-\ntraction model. The D3 and D5 datasets contain\n100k questions where the theories and queries\nare expressed in templated natural language and\nthe queries require deductive reasoning up to\n3 and 5 depths respectively. The ParaRules\ndataset contains 40k questions where the the-\nories are paraphrased versions of the templated\ntheories. The paraphrased templates were col-\nlected using crowdsourcing and the dataset is\ndesigned to test whether the methods can gener-\nalize to more natural forms of language. Birds-\nElectricity is a collection of several hand au-\nthored rulebases where the Birds rulebases are\nadapted from “birds\" logic problem from Mc-\nCarthy 1986 and Electricity rulebases were gen-\nerated by Clark et al. 2020.\nThe dataset is\nused for zero-shot evaluation, to test the out-\nof-domain generalization ability of the models\nsince the language, entities and attributes in the\ndataset are different from those observed in the\ntraining data. The logical literals for the state-\nments in these datasets follow the pattern of\neither (entity, relation, entity, polarity) or\n(subject, is, property, polarity). The logical\nliterals combine by using simple logical con-\njunction operators to form the RLS for the fac-\ntual statements. The logical atoms combine in\nthe form l1[∧li]∗→lc to form RLS for the rule\nstatements in the datasets.\nWe note here that for all of the above datasets, we\nare able to easily generate the RLS representations\nfrom the accompanying metadata available with\nthe datasets (as described in Table 1) and hence no\nannotation effort was required in this work.\n4.3\nModels and Evaluation\nBaseline\nFor the RULETAKERS dataset, the state\nof the art ProofWriter (Tafjord et al., 2020) and the\nearlier PRover (Saha et al., 2020) models are used\nas the baselines methods. For our experiments on\nthe CLUTRR and LEAP-OF-THOUGHT datasets\nwe do not have any baseline methods because we\nwish to show strong performances on the represen-\ntation extraction task and do not try to solve the\nrespective end tasks.\nOur models\nWe use a pretrained text to text trans-\nformer model, namely T5 (Raffel et al., 2020), for\nall our experiments to extract structured represen-\ntations from input NL text. More specifically, the\nmodel takes as input a natural language sentence\nand learns to predict the encoded RLS representa-\ntion of the input sentence, where the encoding of\nthe RLS representation is performed as shown in\nTable 1. We use a T5-small model to train on the\nD3 dataset while all other models are trained using\na T5-base model. The model is trained to predict\na structured representation of the relevant infor-\nmation from the input utterances. The structured\nrepresentations for the different datasets are en-\ncoded as shown in Table 1. For experiments on the\nRULETAKERS dataset, we make use of ProbLog\n(De Raedt et al., 2007) to perform inference on the\n\nextracted logical representations.\nEvaluation\nFor the representation extraction\ntask for the LEAP-OF-THOUGHT and CLUTRR\ndatasets, we use exact matching accuracy on the\nextracted logical representations as the evaluation\nmetric. Also note that we process the predicted se-\nquence outputs assuming they follow the expected\nrepresentation encoding protocol described in Ta-\nble 1. However, predictions which do not adhere to\nthe same would result in incorrect exact matching\nfor the representation extraction metric or result\nin incorrect predictions on the downstream reason-\ning task. For the deductive reasoning tasks on the\nRULETAKERS dataset, where the goal is to pre-\ndict the truthfulness of the query sentence given\nexplicit rules and facts, we use the answer accuracy\nas the evaluation metric. Note that we do not report\nproof accuracies which are previously reported in\n(Tafjord et al., 2020; Saha et al., 2020) because\nthe metric makes sense for the end-to-end trans-\nformer systems to demonstrate that the models are\npredicting correct answers for the “right\" reasons;\nhowever, in our case since we use a symbolic rea-\nsoning engine it is clear that our predictions are\ncompletely faithful to the explanations generated\nby our method.\n5\nResults\nIn this section we analyse the results of our method\non various experimental settings, to answer the\nquestions we ask in Section 4.1 about the extensi-\nbility of our representation extraction model and\nthe effectiveness of our natural language reasoning\nmethod.\n5.1\nRepresentation extraction\nIn this subsection we are interested in the gener-\nalization ability of the representation extraction\nmodel to different forms of structured representa-\ntions and natural language. Table 2 shows the per-\nformance of the representation extraction model on\nthe CLUTRR and LEAP-OF-THOUGHT datasets\nand the high performance (95%+ and 99%+ exact\nmatch accuracy respectively) of the model demon-\nstrates its ability to generalize to natural forms of\nlanguage and different kinds of structured represen-\ntations. We note here that even though solving the\nend task is not our objective in this experimental\nsetting, as discussed earlier, there is notable merit\nin reducing these tasks to reasoning over the struc-\ntured representations over input NL utterances as\nCLUTRR\nLEAP-OF-THOUGHT\nHypernyms\nCounting\nEM Accuracy\n95.9\n99.6\n99.8\nTable 2: Results of the representation extraction model\ntrained and tested on the CLUTRR and LEAP-OF-\nTHOUGHT datasets, where we report the exact match ac-\ncuracy of the extracted structured representations. The\nhigh accuracy scores show the models’ generalization\nability to different kinds of natural language text and\nstructured representations.\nD\n#qns\nOurs4\nProofWriter\nPRover\n0\n2968\n99.6\n99.9\n99.7\n1\n2406\n99.2\n99.3\n98.6\n2\n1443\n96.4\n98.3\n98.2\n3\n1036\n93.1\n98.2\n96.5\n4\n142\n87.3\n91.5\n88.0\nAll\n8008\n97.8\n99.1\n98.4\nTable 3: Results of systems trained on D3 + ParaRules\nand tested on ParaRules where our system demonstrates\ncompetitive performance to the current SToA systems.\nshown in Sinha et al. 2019, where the graph based\nmethods exhibit much better generalization com-\npared to the NL based methods.\n5.2\nRule based reasoning\nIn this subsection we investigate if our method\nwhich relies on RLS as intermediate logical repre-\nsentations can perform competitively to the state of\nthe art baseline systems on the rule based reasoning\ntask.\n5.2.1\nParaRules dataset\nFirstly, to show that our method can perform com-\npetitively to the state of the art models on naturally\noccuring text, we train a model on the combined\nsentences from D3 + ParaRules and test it on the\nParaRules dataset in Table 3. We observe that our\nmethod performs competitively (within 1.3%) to\nthe baseline methods thus demonstrating the over-\nall strong performance of our method compared\nto the state of the art baseline models. Note here\nthat the trade off afforded by our method is the\nability to (1) generate much more finegrained ex-\nplanations (as shown in Figure 1) and (2) extend\nthe RLS extraction model to a variety of tasks like\ncontradiction detection and abductive reasoning,\n4results on the subset of the test set where ProbLog did not\nthrow an exception on the corresponding gold theories. This\nis an error in the dataset as pointed out in Tafjord et al. 2020.\n\nD\n#qns\nOurs5\nProofWriter\nPRover\n0\n6299\n100\n100\n100\n1\n4434\n100\n99.1\n99.0\n2\n2915\n100\n98.6\n98.8\n3\n2396\n100\n98.5\n99.1\n4\n2134\n100\n98.7\n98.8\n5\n2003\n100\n99.3\n99.3\nAll\n20192\n100\n99.2\n99.3\nTable 4: Results of the different systems on D5 dataset.\nWe observe that our method comfortably outperforms\nthe baseline methods even when our system was only\ntrained on sentences from D3 while the other two sys-\ntems were trained on the D5 dataset itself.\nTest\n#qns\nOurs\nProofWriter\nPRover\nBirds1\n40\n90.0\n95.0\n95.0\nElec4\n4224\n100\n97.1\n84.8\nAll\n5270\n99.8\n97.0\n86.5\nTable 5: Zero-shot results of models trained on some\nversion of RULETAKERS (D3 for our method and\nProofWriter and D5 for PRover) and tested on the hand\nauthored rulebases of the Birds-Electricity dataset. The\nresults show how our model performs better than the\nbaseline methods, demonstrating the generalization ca-\npability of our approach to out-of-distribution test sets.\nwhile marginally losing out on overall inference\naccuracy.\n5.2.2\nGeneralization to higher depths\nWe argue in the introduction that our method can\nperform reasoning to arbitrary depths and it is clear\nthat the performance of a method which uses a\nsymbolic reasoning engine is not constrained by\nthe depths of reasoning. However, to demonstrate\nthe same in practice, we train a model on the sen-\ntences from the D3 dataset and test it on the D5\ndataset where the reasoning depths vary from 0 to\n5. The results for the experiment are shown in Ta-\nble 4 where we observe that our method registers\nperfect performance (100% accuracy), reinforcing\nthe advantages offered by our system. Further-\nmore, we observe perfect representation extraction\n(exact match) accuracy on the test set which high-\nlights how the system can be easily extended to\naccurately perform the abduction and implication\nenumeration tasks which were defined in Tafjord\net al. 2020, since they use a similar set of natural\nlanguage templates as the D5 dataset.\nFigure 3: An example mistake of our method on the\nBirds dataset, highlighting a shortcoming of our current\napproach. However, we note that this specific mistake\ncan be addressed by using a smarter unification operator\nas part of the symbolic reasoner which can determine\nwhether two phrases are effectively equivalent.\n5.2.3\nZero shot generalization\nTo demonstrate the generalization ability of our\ndeductive reasoning method to unseen, out-of-\ndistribution test sets we test the model trained\non the D3 sentences (from Table 4) in a zero\nshot setting on the hand authored rulebases of the\nBirds-Electricity dataset. The results show how\nour method performs competitively to the baseline\nmodels thus demonstrating the out-of-distribution\ngeneralization ability of our method. For further\nanalysis, we show an example from the Birds\ndataset in Figure 3 where our method fails. The\nexample highlights a shortcoming of the method\nand emphasizes the need for a smarter unification\noperator (as opposed to the default exact matching\noperator) as part of the symbolic reasoner, which\ncan effectively judge the similarity of two phrases.\n5.3\nResults summary\nIn summary, we observe that the answer to our\nquestions regarding the generalization ability of the\nrepresentation extraction model and the reasoning\nmodel is encouragingly positive. This supports our\nproposal of identifying and extracting RLS from\nnatural language arguments as a noteworthy alter-\nnative to solving the natural language reasoning\ntasks, especially given the numerous advantages of\nthe approach as discussed in Section 2.3.\n6\nRelated work\nThis work builds on a recent line of work (Tafjord\net al., 2020; Saha et al., 2020; Clark et al., 2020)\nthat tries to tackle the problem of natural language\nreasoning. The recent works make use of a trans-\nformer based model to solve the rule based reason-\ning task in an end-to-end manner while simultane-\n5see footnote 4.\n\nously predicting some form of explanations for the\nmodel’s answer. However, in this work we move\naway from the end-to-end methods and instead pro-\npose identifying and extracting structured represen-\ntations from natural language arguments and using\na symbolic reasoning engine to solve the reason-\ning problem. This approach is inspired by numer-\nous previous attempts (Hayes, 1977; Hobbs et al.,\n1988; McCarthy, 1963, 1976) at similar tasks which\nproposed parsing natural language utterances into\ndifferent structured meaning representations. How-\never, in this work we only try to extract the logical\nstructure from natural language arguments instead,\nwhich we argue is a reasonable assumption.\n7\nDiscussion and Conclusion\nThis work largely relies on our initial argument that\nit is possible to extract the logical structure from\na NL argument. The community mostly agrees\nthat NL is very expressive and cannot be mapped\ninto structured representations; however, we be-\nlieve that this does not contradict our underlying\nassumptions that “locally\" logical arguments made\nin language can be mapped to their core logical\nstructure. After all, this is how humans discuss\nthese arguments, and communicate about it – they\nidentify the core arguments and atoms, and use\nthem to develop explanations. This is the heart of\nthe argument we make in this paper. And, as we\nshow, the mapping process can be accomplished\nat high accuracies, rivaling end-to-end methods,\nwhile providing more reasoning and explanation\ncapabilities.\nIn conclusion, we began this work by noting the\nimportance of finer explanations for natural lan-\nguage reasoning tasks, which is missing in current\nmethods that only provide simple sentence level\nexplanations. To address this and the lack of exten-\nsibility in current systems, we proposed identifying\nand extracting intermediate logical representations,\nand reasoning over the same as an approach to the\ntask. Consequently, we note that previous works\nwhich tried to parse NL sentences into logical rep-\nresentations have understandably failed. However,\nwe instead argue that it is possible to extract the\ncore logical structure from a natural language ar-\ngument and towards that end, we formulate Rep-\nresentations of Logical Structure (RLS) as a class\nof structured representations defined in the con-\ntext of a reasoning task. We further show how our\nRLS extraction system can generalize to natural\nforms of language and different RLS formulations\nby showing its strong performance (95% and 99%\nexact match accuracy) on the datasets of CLUTRR\nand LEAP-OF-THOUGHT respectively. We also\nshow the efficacy of our reasoning method on the\ndeductive reasoning task on a series of RULETAK-\nERS datasets. We motivate future work to further\nexplore methods to extract more contextual struc-\ntured representations and extend it to support wider\nforms of natural language utterances."}
{"id": "2508.14472v1", "title": "In2x at WMT25 Translation Task", "date": "2025-08-20", "authors": ["Lei Pang", "Hanyi Mao", "Quanjia Xiao", "HaiXiao Liu", "Xiangyi Li"], "source": "http://arxiv.org/abs/2508.14472v1", "page_count": 8, "text": "In2x at WMT25 Translation Task\nLei Pang\nDuxiaoman\npanglei@duxiaoman.com\nHanyi Mao\nUniversity of Chicago\nhanyim@uchicago.edu\nQuanjia Xiao\nPeking University\nxiaoqj@stu.pku.edu.cn\nHaiXiao Liu*\nDuxiaoman\nliuhaixiao@duxiaoman.com\nXiangyi Li\nDuxiaoman\nxiangyi@duxiaoman.com\nAbstract\nThis paper presents the open-system submis-\nsion by the In2x research team for the WMT25\nGeneral Machine Translation Shared Task. Our\nsubmission focuses on Japanese-related trans-\nlation tasks, aiming to explore a generalizable\nparadigm for extending large language models\n(LLMs) to other languages. This paradigm en-\ncompasses aspects such as data construction\nmethods and reward model design. The ulti-\nmate goal is to enable large language model\nsystems to achieve exceptional performance\nin low-resource or less commonly spoken lan-\nguages.\n1\nIntroduction\nMachine translation (MT) has long been both a\nhigh-impact application and a central research chal-\nlenge in natural language processing. The advent\nof large language models (LLMs) has reshaped\nMT from task-specific supervised learning toward\nlarge-scale representation learning and instruction-\nfollowing paradigms, enabling steady gains across\ndiverse language pairs (Alves et al., 2024; Jiao\net al., 2023; Kocmi et al., 2024; Lu et al., 2024).\nYet, two persistent gaps remain.\nFirst, while\nmainstream LLM training increasingly optimizes\nfor mathematical and code reasoning, their ex-\npressive and creative language abilities—e.g., id-\niomaticity, stylistic naturalness, and culturally ap-\npropriate phrasing—are comparatively underdevel-\noped(Lewkowycz et al., 2022; Liu et al., 2023;\nLozhkov et al., 2024; Rozière et al., 2023; Zaitova\net al., 2025). This often leads to translations that\nare locally literal but globally stilted, especially for\ninformal registers, slang, and literary text . Second,\nmodel competence is unevenly distributed across\nlanguages: English receives disproportionate cover-\nage and quality, while many non-English languages\ntrail in both general capability and translation natu-\nralness(Aharoni et al., 2019; Johnson et al., 2017;\nKocmi et al., 2023, 2024; Team et al., 2022). Com-\nmunity findings over recent WMT cycles echo this\nasymmetry: despite the “LLM era”, MT is far from\nsolved uniformly across directions, with larger gaps\noff English-centric pairs and on long-tail phenom-\nena .\nThis paper studies how to transfer English\nstrength into non-English targets to improve ex-\npressive and culturally faithful translation. Con-\ncretely, we focus on Japanese—a language where\nliteral adequacy is not sufficient: natural Japanese\nrequires idiomatic paraphrasing, honorific and reg-\nister control, and sensitivity to genre and context.\nOur thesis is that English can be used as a hub lan-\nguage to bootstrap these capabilities via curriculum\ndesign, cross-lingual alignment, and preference sig-\nnals that explicitly reward naturalness.\nWe present In2x, a Japanese-focused model de-\nsigned to inherit general competency from En-\nglish while specializing for Japanese expressive-\nness. At a high level, In2x operationalizes three\nprinciples: (i) English-as-hub transfer: leverage\nrich English data and strong English modeling to\nseed robust lexical/semantic priors, then transfer to\nJapanese via bilingual and style-augmented objec-\ntives; (ii) Expressiveness-first supervision: empha-\nsize prompts and signals that drive idiomaticity and\ncultural appropriateness (beyond literal adequacy);\n(iii) Evaluation beyond metrics: complement au-\ntomatic metrics with human judgments targeted at\nidioms, slang, and stylistic naturalness.\nWe evaluate In2x on standard WMT-style test sets\nand targeted Japanese-focused challenge suites that\nstress idioms, slang, and style. According to the\npreliminary ranking results of WMT 2025, In2x\noutperforms many large-scale proprietary mod-\nels, such as Gemini-2.5-Pro (Comanici and Team,\n2025), GPT-4.1 (Fachada et al., 2025), Claude-4\n(Anthropic, 2025), and DeepSeek-V3 (Monisha,\n2025).\nOverall, we make three core contributions:\n1. We diagnose under-explored gaps in current\n\nLLM-based MT: the tension between heavy\ninvestment in math/code reasoning and the rel-\native neglect of creative/idiomatic language\nability, and the English-vs.-non-English capa-\nbility asymmetry.\n2. We introduce In2x, a Japanese-focused model\nthat systematically transfers English strengths\nto Japanese, with an emphasis on naturalness\nand cultural appropriateness.\n3. In this study, we introduce a detailed align-\nment pipeline designed to enhance the cre-\native capabilities of language models. This\napproach not only improves performance in\nnon-STEM (Science, Technology, Engineer-\ning, and Mathematics) tasks but also ensures\nthat the models maintain robust generalization\nabilities across diverse linguistic challenges.\nFor instance, in the en-ja translation track,\nthe model demonstrates outstanding perfor-\nmance without any task-specific fine-tuning,\nhighlighting its adaptability and effectiveness\nin non-STEM domains.\n2\nContinue Pretraining Stage\nTo balance the capabilities of large language\nmodels (LLMs) in both science-oriented and\nhumanities-oriented domains during the pretraining\nprocess, we divided the continued pretraining stage\ninto three distinct phases. The goal of this process\nis to enhance the model’s multilingual proficiency,\nimprove general-purpose abilities in foundational\nhumanities tasks, and refine its representation in\nspecialized contexts (Brown et al., 2020; Rae et al.,\n2021).\nThe training process incorporates diverse cor-\npora, including a comprehensive 2 trillion to-\nkens dataset comprising encyclopedic knowledge,\nwebpages, structured information, news articles,\nWikipedia entries, academic papers, and STEM-\nrelated datasets (Gao et al., 2020; Raffel et al.,\n2020). In addition, a dedicated 500 billion tokens\ncorpus has been curated exclusively for creative\nwriting tasks such as novel and screenplay syn-\nthesis, as well as authentic conversational datasets\nsimulating real-life dialogue (Zhang et al., 2022).\nAnother significant aspect of this training stage\nfocuses on enhancing capabilities in the target lan-\nguage, with Japanese utilized as an example. To\nthis end, substantial Japanese language-specific cor-\npora were introduced, alongside a balanced dataset\nwith equal distribution of Chinese, English, and\nJapanese corpora (Xue et al., 2021). The aim was\nto facilitate transfer learning from pretraining on\nChinese and English to the Japanese language.\n2.1\nPhase 1: Fundamental Knowledge\nEnhancement\nIn this phase, the creative writing corpus and the\nknowledge-focused corpus are jointly trained with\nconstant learning rates. This approach was de-\nsigned to boost proficiency in STEM-related rea-\nsoning while preserving the nuanced expression\nhabits required for creative tasks in humanities (Ka-\nplan et al., 2020).\n2.2\nPhase 2: Long-Text Capability\nRefinement\nDuring this phase, a subset of the data was filtered\nbased on text length, allowing the context length to\nincrease from the typical 8,192 tokens to approx-\nimately 32,000 tokens. This step was intended to\namplify the model’s ability to process and com-\nprehend extended-length texts (Hoffmann et al.,\n2022).\n2.3\nPhase 3: Fast Annealing Stage\nIn the final phase, a high-quality corpus was con-\nstructed based on selections informed by perplexity\n(PPL) and quality-assessment metrics. The anneal-\ning training was conducted with linear decay of\nthe learning rate from 3 × 10−5. This process con-\nsumed a total of 300 billion tokens and enabled\nthe model to maintain its vivid expressive style for\ntasks such as novels and screenplays (Brown et al.,\n2020).\n3\nPost-Training Data\nThe post-training dataset consists of 2 million sam-\nples, with 1.5 million used during the supervised\nfine-tuning (SFT) process and 500,000 used in\nthe reinforcement learning (RL) process. To en-\nsure the Japanese language (our target language)\nachieves a proficiency level comparable to ma-\njor languages such as Chinese and English, we\nadjusted the ratio of target language instructions\nto attain an equal balance across these languages.\nSpecifically, we used a 1:1:1 ratio in the Instruct-to-\nExample (In2X) setup, striving to transfer the orig-\ninal model’s knowledge into the target language as\neffectively as possible (Ouyang and et al., 2022;\nZhou et al., 2023).\n\nWe developed a detailed pipeline for construct-\ning the target language instructions, which can be\ncategorized into three major synthetic processes:\n3.1\nObtaining Open-Source Instructions\nWe began by collecting open-source instruction\ndatasets available in the target language. These\ndatasets include curated public data and traditional\nNLP fundamental tasks. Examples of such datasets\ninclude Dolly, OASST, and OASST2 (Koch and\net al., 2023; OpenAI, 2023).\n3.2\nTarget Language Instruction Rewriting\nThis process consists of several substeps designed\nto enhance the model’s linguistic and cultural adapt-\nability in the target language:\n• Creative Language Tasks:\nTo preserve\nthe language’s stylistic characteristics in\nhumanities-focused tasks, we designed cre-\native tasks where the responses include origi-\nnal stories or scripts (Bai and et al., 2022).\n• Basic Localized Tasks: This includes rewrit-\ning instructions for tasks relevant to the local\ncontext, such as exam questions. Some of\nthese tasks provide only the question and an-\nswer. We leveraged advanced models to sup-\nplement these datasets with reasoning chains\nto improve the model’s reasoning ability in the\ntarget language (Wei and et al., 2022). This en-\nhancement also helps to mitigate issues such\nas mathematical inconsistencies commonly\nfaced during the LLM instruction synthesis\nprocess.\n• Cultural Style Transformation: For certain\nhumanities-related tasks, we incorporated cul-\ntural style shifts by adapting the instructions\nto align with the cultural norms and styles of\nthe target language. This adjustment aims to\nimprove the model’s ability to provide cultur-\nally nuanced responses (Xu and et al., 2023).\n3.3\nInstruction Synthesis in the Target\nLanguage\nWe utilized methods such as Magpie (Xu et al.,\n2024) and Self-Instruct (Wang and et al., 2022) to\nsynthesize target language instructions. However,\nthese automatically generated instructions often\nsuffer from issues including overly simple ques-\ntions, lack of focus, self-answered queries, and in-\nternal contradictions. To address these challenges,\nwe implemented a strict quality control pipeline\nwith the following techniques:\n• Prompt Engineering: We crafted detailed\nprompts with explicit rules to identify and\ntroubleshoot common issues in synthesized\ninstructions (White and et al., 2023).\n• Validation via Model Responses: Instruc-\ntions passing the first step were tested by hav-\ning the model generate responses. These re-\nsponses were evaluated by a critic LLM for\ncontradiction, hallucinations, or failure to pro-\nvide valid results. Instructions flagged with\nsuch issues were discarded. The critic LLM,\nbeing sensitive to hallucinations, acts as an ad-\nditional safeguard for quality control (Ganguli\nand et al., 2022).\n• ReReading Mechanism: After constructing\nthe prompts for instruction generation, we em-\nployed a \"ReReading\" mechanism, where the\nmodel self-reviews its instructions. This re-\nview checks for correctness, alignment with\nthe target language’s cultural norms, and con-\nsistency with its native linguistic style. Since\nthe synthesized instructions inherently carry\nthe reasoning or rewriting processes behind\nthem, leveraging this comprehensive context\nmakes it easier to detect internal flaws, partic-\nularly those related to localization or cultural\nadjustments (Chiang and et al., 2023).\n4\nPost-Training SFT Stage\nThe post-training Supervised Fine-Tuning (SFT)\nstage is a critical step to balance linguistic diversity\nand optimize alignment within the instruction space\nfor target languages. Below, we outline the key\nstrategies and methods employed during this stage.\n4.1\nBalancing Linguistic Diversity\n(a) Clustering of Instruction Data: To enhance\nlinguistic diversity, the instruction dataset\n(comprising 40 million entries) was clustered\nusing the Birch clustering algorithm (Zhang\net al., 1996). The effectiveness of the cluster-\ning process was evaluated based on metrics\nlike tag recapture rates and cluster smoothness\n(Zhang and Deng, 2020), which were used to\nfine-tune the clustering threshold. This pro-\ncess reduced the dataset to 1.5 million clusters\nafter deduplication and selection.\n\n(b) Categorization via Large Language Mod-\nels (LLMs): Utilizing LLMs, the clustered\ndata was tagged to assign both first-level and\nsecond-level labels (et al., 2020). For exam-\nple, a mathematical problem might be catego-\nrized as \"Mathematics - Quadratic Equations.\"\nThese hierarchical labels provided a clearer\nstructural organization of the data.\n(c) Difficulty Grading of Instructions: The\ndataset was further refined by classifying\neach instruction according to its difficulty\nlevel: \"Very Difficult,\" \"Difficult,\" \"Moder-\nate,\" \"Simple,\" and \"Very Simple\" (Wang and\nLi, 2019). For normalized scientific datasets,\nan additional evaluation was conducted us-\ning the LLaMA3-70B model (Research, 2023)\nwith a Pass@16 metric (Perez and Andreas,\n2022) to estimate the success rate of solving\nspecific problems.\n4.2\nAligning the Instruction Space of Target\nLanguages\n(a) Avoiding Semantic Overfitting via Temper-\nature Adjustment: During training, a tem-\nperature parameter was introduced to mitigate\noverfitting of the model to specific linguis-\ntic semantic spaces (Sundararajan and Wang,\n2021). This approach encouraged the model\nto adopt a more holistic learning strategy, en-\nabling it to concentrate on question-answering\ntechniques rather than over-specializing in the\nsemantic patterns of a particular language. For\ninstance, this allowed the Japanese language\nmodel to better mimic the cognitive behaviors\nobserved in other languages (Koehn, 2019).\n(b) Specialized Sampling Strategy: To further\nenhance the learning process, a two-step sam-\npling strategy was employed over the 1.5 mil-\nlion clusters (Perket and Sanner, 2020):\n• The difficulty levels of the data were sam-\npled in a 3:3:3:1:0 ratio (corresponding\nto \"Very Difficult,\" \"Difficult,\" \"Moder-\nate,\" \"Simple,\" and \"Very Simple,\" re-\nspectively) (Finn and Jones, 2018).\n• Additionally, within each cluster, sam-\nples were selected to ensure diversity\nacross languages and categorical labels,\nwhich preserved the large-scale diversity\nof the original 1.5 million data points\n(Torroba and Blanco, 2021). This also\nmaintained a degree of orthogonality be-\ntween the target language and English\nwithin the sampled instructions (Feng\nand Gimpel, 2020).\nThe first round of sampling was used as the\ndata for the first epoch, while the second round\npopulated the second epoch. The training\nprocess adopted a learning rate of 2 × 10−5\nwith cosine decay for optimal performance\n(Loshchilov and Hutter, 2017).\n5\nReinforcement Learning to Enhance\nGeneral Capabilities in Cultural and\nCreative Industries\nIn the post-training RL stage, we leveraged a pro-\ncess similar to the instruction filtering procedure\nused during the SFT phase (Ouyang and et al.,\n2022). Specifically, an additional set of instruc-\ntions was curated, comprising 500k samples that\nwere guaranteed not to overlap with the instructions\nused in the SFT phase. The training configuration\nutilized a batch size of 128 and a minibatch size\nof 32, with the dataset trained for one epoch. Each\nrollout involved 16 iterations, and the reward evalu-\nation was based on both a rule-based reward model\nand a generative reward model (Christiano et al.,\n2017).\n5.1\nReward Model Design\nThe reward model system was meticulously de-\nsigned to cater to different task types:\n• Rule-Based Reward Model: For tasks in-\nvolving mathematics, STEM disciplines, and\nlogic, a rule-based reward model was em-\nployed to ensure adherence to specific criteria\n(Silver and et al., 2017).\n• Generative Reward Model for Creative\nTasks: For creative tasks, such as content-\ngeneration, specific prompts embedded with\nrules were utilized. These rules encompassed\nfundamental task principles as well as dynam-\nically generated guidelines based on the cur-\nrent prompt. The scoring mechanism eval-\nuated compliance with these principles and\ngenerated a reward score based on the per-\ncentage of principles satisfied (Krause et al.,\n2021).\n\n5.2\nRL Algorithm Design\nTo address the complexity of the tasks, we made\nstrategic adjustments to the RL algorithm to\nachieve stable and efficient training:\n• Trajectory-Corrected GRPO: Considering\nthe diverse nature of tasks and reward types,\na token-level clipping approach was deemed\ntoo restrictive and prone to causing training in-\nstability. Instead, we employed the Trajectory-\nCorrected version of the Generalized Proxi-\nmal Policy Optimization (GRPO) algorithm\n(Schulman et al., 2017), which proved effec-\ntive for handling multilingual tasks with vary-\ning reward functions. This modification en-\nabled stable and continuous training while ac-\ncelerating the convergence curve(Pang and\nJin, 2025).\n• Dual-Clip Mechanism: To improve stability,\nwe integrated a dual-clip mechanism, which\nstabilized the variance of importance sam-\npling at the sentence (sen) level (He et al.,\n2016). Additionally, we removed the lower\nbound of sampling, achieving optimal perfor-\nmance for the given tasks.\n• Soft Length Penalty: A soft-length penalty\nwas incorporated throughout the training pro-\ncess to encourage better length control in gen-\nerated outputs (Wu et al., 2016).\n• High-Level Clipping: A clipping mechanism\nwas introduced to ensure robust control over\nhigh-level rewards (Schulman et al., 2015).\n• Temperature Decay: A temperature decay\nstrategy was applied to progressively adjust\nthe sampling temperature during training, en-\ncouraging diversity in outputs while maintain-\ning stability (Hinton et al., 2015).\n• Entropy Regularization: The entropy value\nwas set to 0.01 during training, enabling the\nmodel to conserve entropy and avoid prema-\nture saturation of the reward space (Williams,\n1992).\n6\nModel Ensemble\nModel ensemble techniques are employed by tak-\ning into account the orthogonality of linguistic\ncapabilities among various models. Specifically,\nmodels that exhibit strong linguistic proficiency\nare selected for the ensemble process to maximize\noverall performance.\nFurthermore, the fusion of model tensors is con-\nducted based on gradient information and the im-\nportance of weights. This approach ensures a ro-\nbust integration of model parameters, leveraging\ntheir respective contributions to optimize the en-\nsemble. Such methodologies have been shown to\nenhance the effectiveness of model ensembles in\ncomplex tasks (Wang et al., 2025).\n7\nEvaluation Results\n7.1\nBenchmarks\nThe model demonstrated outstanding performance\nin prominent Japanese language benchmarks, such\nas the ja-mtbench, indicating its robust and reli-\nable language translation capabilities. A detailed\nbreakdown of the results is provided below.\n7.2\nWMT Evaluation Results\nWithout undergoing any task-specific fine-tuning,\nthe model achieved remarkable results in the\nJapanese-related tracks of the WMT competition,\nsecuring second place overall. Furthermore, in\nthe unrestricted category, the model achieved first\nplace. A comprehensive summary of its perfor-\nmance is outlined below.\n8\nConclusion\nIn this work, the proposed methodology for trans-\nferring language model capabilities has been val-\nidated on the WMT translation task.\nThe ap-\nproach has demonstrated significant improvements\nin Japanese proficiency throughout the CPT, SFT,\nand RL processes. Remarkably, without any ad-\nditional language-specific fine-tuning, the large\nlanguage model achieved alignment between its\nJapanese language capabilities and those of main-\nstream languages. As a result, it demonstrated\nsuperior performance and achieved first place in\nthe unrestricted track of the competition."}
{"id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "authors": ["Shuaijie She", "Yu Bao", "Yu Lu", "Lu Xu", "Tao Li", "Wenhao Zhu", "Shujian Huang", "Shanbo Cheng", "Lu Lu", "Yuxuan Wang"], "source": "http://arxiv.org/abs/2508.14460v1", "page_count": 18, "text": "DuPO: Enabling Reliable LLM Self-Verification via Dual\nPreference Optimization\nShuaijie She♡♠∗, Yu Bao♠, Yu Lu♠, Lu Xu♠, Tao Li♠, Wenhao Zhu♠,\nShujian Huang♡(B), Shanbo Cheng♠(B), Lu Lu♠, Yuxuan Wang♠\n♠ByteDance Seed, ♡Nanjing University\n∗Work done during an internship at ByteDance Seed, BCorresponding authors\nAbstract\nWe present DuPO, a dual learning-based preference optimization framework that generates\nannotation-free feedback via a generalized duality. DuPO addresses two key limitations: Rein-\nforcement Learning with Verifiable Rewards (RLVR)’s reliance on costly labels and applicability\nrestricted to verifiable tasks, and traditional dual learning’s restriction to strictly dual task\npairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task’s\ninput into known and unknown components, then constructs its dual task to reconstruct the\nunknown part using the primal output and known information (e.g., reversing math solutions\nto recover hidden variables), broadening applicability to non-invertible tasks. The quality of\nthis reconstruction serves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs’ ability to instantiate both tasks via a single model. Empirically, DuPO achieves\nsubstantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET\nover 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points\non three challenge benchmarks, and enhances performance by 9.3 points as an inference-time\nreranker (trading computation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization.\nDate: August 21, 2025\nCorrespondence: chengshanbo@bytedance.com(B), huangsj@nju.edu.cn(B)\n1\nIntroduction\nLarge Language Models (LLMs) [1, 10, 13, 29, 34, 51] have shown remarkable progress in tasks like mathemat-\nical reasoning [2, 8, 42, 50] and multilingual translation [9, 26, 27, 57]. To further enhance their capabilities,\nreinforcement learning (RL) paradigms like Reinforcement Learning from Human Feedback (RLHF) [13, 29, 51]\nand Reinforcement Learning with Verifiable Rewards (RLVR) [11, 17, 20, 43, 52] have gained traction. Specifi-\ncally, RLHF aligns models with human preferences but relies on costly, inconsistent human annotations [23, 54].\nRLVR addresses this for objective tasks (e.g., math, code) via binary rewards from verifiable answers, re-\nducing annotation burdens. However, RLVR still depends on external supervision: acquiring verifiable\nanswers remains a bottleneck, limiting scalability. Moreover, it struggles with open-ended tasks (e.g., mul-\ntilingual translation), where single references cannot capture diverse high-quality outputs [6, 22]. Recent\nattempts (e.g., AI-Feedback/RLAIF [23], Constitutional AI [3]) merely swap dependencies (human labels →\nteacher models/rules), failing to resolve the core bottleneck.\n1\n\nDual learning [15] offers a self-supervised alternative by leveraging task duality to generate intrinsic feedback:\nthrough paired “primal” and “dual” tasks (e.g., translation and back-translation [41]), models validate outputs\nvia cycle consistency, eliminating reliance on external labels. Given that LLMs possess diverse capabilities\nfrom extensive pretraining, they could be trained across various tasks. However, applying this framework to\nLLMs is non-trivial, which faces two critical challenges:\n1. Limited Duality in Irreversible Tasks: Most real-world LLM tasks (e.g., creative writing, math reasoning)\nlack strict invertibility. LLM’s output (e.g., a math solution) rarely contains enough information\nto reconstruct its input (e.g., the original problem), breaking the duality cycle and invalidating self-\nsupervision.\n2. Bidirectional Competence Asymmetry: LLMs often exhibit uneven performance across primal/dual\ntasks (e.g., strong at solving math problems but weak at generating problems from solutions). Noisy\nself-signals from asymmetric tasks hinder optimization, reducing the framework’s utility.\nThese mismatches render traditional dual learning ill-suited for general LLM optimization, leaving it an open\nchallenge.\nIn this paper, we propose DuPO (Dual Learning-based Preference Optimization), a framework that aligns\nLLM generalization with a (relaxed) duality applicable to general tasks. At its core lies a generalized duality\nframework (§3.3) built on complementary relationships: it decomposes each input x into disjoint known (xk)\nand unknown (xu) components, then designs the dual objective to reconstruct only xu from the primal output\ny and the known input xk, rather than inverting the full input. This framework resolves two asymmetries:\nit restores sufficient information flow between the primal and dual tasks (task asymmetry) and reduces the\ncomplexity burden on the dual task side (capability asymmetry). The formulation naturally synergizes with\nLLMs: their broad foundational capabilities allow a single model to instantiate both primal and dual tasks\nwithout specific architectures, while the dual task converts the model’s outputs into self-supervised reward\nsignals, enabling continual improvement without external annotations. This bidirectional benefit addresses a\ncritical challenge in LLM development: obtaining high-quality feedback for capability enhancement.\nWe empirically validate DuPO on two diverse and representative tasks: mathematical reasoning and multilin-\ngual translation, demonstrating significant and consistent improvements. By applying DuPO to one of the\nstrongest translation LLM, Seed-X-7B-Instruct [9], we demonstrate a significant further performance gain\nof 2.13 COMET points on the multilingual translation benchmark, bringing the 7B model to performance\ncomparable to ultra large state-of-the-art systems. In mathematical reasoning, our method yields robust\ngains across models of varying scales, from 1.5B to 7B parameters; notably, DuPO improves the Qwen3-4B\nmodel’s score on three challenging mathematical benchmark by 6.4 percentage points. Our comprehensive\nablation studies confirm that our design, the generalized duality, is crucial for achieving these results. Beyond\ntraining, DuPO acts as a reranking mechanism at inference, boosting performance by 9.3 points without\nfinetuning—enabling smaller models to outperform stronger ultra-large LLM like DeepSeek-R1 even without\ntraining. In summary, DuPO reimagines task duality for non-invertible LLM tasks. It eliminates external\nannotation reliance, scales across tasks/domains, and enhances both training and inference—offering a scalable\npath to align LLMs with diverse goals using self-supervised feedback.\n2\nRelated Work\n2.1\nPreference Optimization for LLMs\nPreference optimization has driven significant advancements in large language models (LLMs) by aligning\noutputs with feedback signals, with three dominant paradigms shaping the field: (1) Reinforcement Learning\nfrom Human Feedback (RLHF) [35] has become a cornerstone for aligning LLMs with human preferences.\nIts workflow typically involves training a reward model [30, 48] on human-annotated preference pairs, then\nusing reinforcement learning (e.g., PPO [39], GRPO [42]) to optimize the policy model [13, 29, 51]. While\neffective for subjective tasks, RLHF faces critical bottlenecks: human annotation is costly to scale [23], and\nconsistency across annotators degrades for complex tasks [54], limiting its applicability to large-scale or\nnuanced scenarios. (2) Recent work [3, 23, 56] has leveraged LLM-as-a-Judge to evaluate outputs and provide\n2\n\noptimization signals, advancing capabilities in complex tasks. However, the reliability of this paradigm heavily\nhinges on the judge model’s own capabilities and its susceptibility to systematic biases, where evaluations\nare confounded by various factors such as the presentation order of responses or a preference for certain\nlinguistic styles [14, 24, 44]. (3) In response, research has shifted towards exploring Reinforcement Learning\nfrom Verifiable Rewards (RLVR)—a paradigm designed to enhance a model’s complex reasoning capabilities\nin domains like mathematics [11, 43, 51]. By leveraging ground-truth answers as reward signals, RLVR avoids\nhuman annotation, but its reliance on verifiable outcomes restricts it to tasks with definitive solutions. This\nmakes it ill-suited for open-ended tasks such as multilingual translation, where multiple valid outputs exist\nand no single ground truth can capture all high-quality responses.\nNotably, both paradigms share a fundamental limitation: dependence on external supervision—whether human\nannotations or pre-defined verifiable answers. This reliance constrains LLMs’ adaptability and scalability\nacross diverse tasks, highlighting the need for self-supervised preference optimization mechanisms.\n2.2\nDual Learning\nDual learning enhances model performance by leveraging intrinsic task symmetry, where a primal task and\nits complementary dual task mutually provide supervision. He et al. [15] first introduced dual learning for\nmachine translation, which uses bidirectional tasks (e.g., En→Zh and Zh→En) to generate pseudo-labels via\nback-translation [41], reducing reliance on parallel corpora—a breakthrough for low-resource language pairs.\nThis framework has since expanded to diverse domains:\n• Cross-modal tasks: DualGAN [25] frames image-to-text and text-to-image generation as dual tasks,\nenforcing cycle consistency to align visual and linguistic representations. Ren et al. [38] apply a similar\nprinciple to text-to-speech (TTS) and automatic speech recognition (ASR), enabling joint training with\nminimal paired data.\n• Knowledge reasoning: DualTKB [12] treats knowledge base path generation and natural language query\nparsing as symmetric tasks, improving factual consistency via bidirectional validation.\n• Reinforcement learning integration: Zhang et al. [55] designed policy gradient algorithms that transfer\nrewards between dual tasks, mitigating reward sparsity in low-supervision scenarios.\nFor LLMs, dual learning has enabled capability enhancement. Trans-Zero [58] uses back-translation to verify\nsemantic preservation in multilingual generation. DualReflect [7] employs dual tasks (e.g., translation and\nback-translation) as structured feedback to refine output quality.\nHowever, a critical limitation persists: existing methods require strict task duality where primal and dual\ntasks are mutually invertible (e.g., translation pairs). This restricts application to tasks with ambiguous or\nnon-invertible dual counterparts (e.g., open-ended reasoning, creative writing). Our work addresses this by\nreframing dual learning as a preference optimization framework. Instead of relying on explicit task symmetry,\nwe decompose inputs into known/unknown components to construct flexible dual tasks, enabling generalization\nacross diverse tasks without rigid invertibility constraints.\n3\nDual Learning-based Preference Optimization\nIn this section, we propose Dual Learning-based Preference Optimization (DuPO). Its core objective is to\nleverage the intrinsic relationships between tasks and their dual counterparts to generate self-supervised\nrewards, enabling LLMs to improve performance without relying on expensive human annotations or complex\nhandcrafted rules.\n3.1\nTask Duality\nWe begin by formalizing the task duality between a primal task and its dual counterpart.\nDefinition 1. Let X be the input space and Y the output space. A primal task is a mapping Tp : X →Y,\nand a dual task is a mapping Td : Y →X. The pair (Tp, Td) is said to form a dual pair if they satisfy the\n3\n\n(a)\nNon-unique reconstruc-\ntion of x from y\n(b) Failure to reconstruct x\ngiven y\n(c) Predicting subspaces B to\npreserve uniqueness\nFigure 1 Challenges in Dual Learning and Solutions via Relaxed Duality Constraints. Dilemmas in applying dual\nlearning — (a) Non-unique reconstruction of x from y breaks the closed-loop; (b) Failure to reconstruct x from y\ndue to competence asymmetry. Resolutions by relaxing duality restrictions — (c) Predicting subspaces B to preserve\nuniqueness.\nconsistency principle:\n∀x ∈X,\nd\n\u0000x, Td(Tp(x))\n\u0001\n≤ϵX ,\nwhere d(·) : X × X →R+ is a domain-specific distance metric, and ϵX ≥0 is a tolerance threshold that\nquantifies acceptable reconstruction errors in each space.\nLeveraging this duality, we can construct a self-supervised reward to quantify the quality of a primal-task\noutput. Given an input x ∈X and its corresponding output y = Tp(x), we could define reward as\nr(y) ∝exp\n\u0000−λ · d\n\u0000x, Td(y)\n\u0001\u0001\n,\n(1)\nwhere λ > 0 controls the sensitivity of the reward to reconstruction error. High-quality outputs maximize\nthe expected reward E[r(y)] by preserving information that is recoverable through the duality cycle. This\nprinciple has been successfully applied in various domains, including machine translation [15, 58].\n3.2\nChallenges in Dual Learning-based Optimization\nWhile task duality offers a promising self-supervised paradigm, its application to LLM optimization is\nnon-trivial, as it confronts two critical challenges that disrupt the closed-loop information flow.\nChallenge I: Limited Duality in Non-Mutually Implicative Tasks.\nThe natural utility of task duality hinges on\nTp and Td being mutually implicative — specifically, the output y of Tp contains sufficient information to\nreconstruct x via Td, and vice versa. This property holds for canonical tasks like machine translation, where\ny (a translation) preserves the semantic content of x (the source sentence), enabling Td (back-translation) to\nrecover ˆ\nx ≈x.\nHowever, most real-world tasks lack this mutual implicativity (Fig. 1a). Consider mathematical reasoning,\nwhere Tp solves a problem x (e.g., “A box contains 3 red and 5 blue balls; what is the total?”) to produce\ny = 8. Here, y (the total count) is insufficient to uniquely reconstruct x via Td, as 8 could answer infinitely\nmany disparate questions, such as “What is 10 −2?” or “What is the atomic number of Oxygen?”. This\nunderdetermined relationship breaks the duality loop: Td cannot reliably recover x from y, making the\nself-supervised reward (based on ˆ\nx ≈x) untrustworthy. Such tasks thus require a redefinition of duality\nbeyond direct input-output reversal.\nChallenge II: Bidirectional Competence Asymmetry.\nEven for mutually implicative tasks, duality optimization\nis sensitive to the bidirectional competence of the LLM — the performance gap between Tp and Td. If Tp is\nstrong but Td is weak, Td may produce noisy ˆ\nx that distorts the supervision signal (Fig. 1b). This asymmetry\nis particularly pronounced in LLMs, where extensive pretraining creates diverse but uneven capabilities across\ntasks, even within the same domain.\n4\n\nFor instance, in machine translation optimization, let x = “The quick brown fox jumps over the lazy\ndog” (English) and y = “Der schnelle braune Fuchs springt über den faulen Hund” (correct German translation).\nA high-quality y should enable Td to back-translate to ˆ\nx ≈x. However, if Td struggles with nuanced\nvocabulary (e.g., “schnelle” →“fast” instead of “quick”), ˆ\nx might be “The fast brown fox jumps over the lazy\ndog” — a divergence from x that erroneously penalizes y despite its correctness.\nUsing separate models for Tp and Td, as was common in traditional dual learning, merely sidesteps the\nchallenge of intrinsic competence asymmetry [45, 46]. This imbalance is still arising from the distinct natures\nand complexities of the primal-dual tasks, destabilizing the self-supervised feedback loop.\n3.3\nGeneralized Duality Framework via Complementary Task\nTo address the two-fold challenges of limited duality in non-mutually implicative tasks and bidirectional\ncompetence asymmetry, we propose a generalized duality that redefines task duality through complementary\ndependencies. It transcends traditional duality’s strict input-output reversal requirement by leveraging partial\nand stable dependencies between task components, enabling robust self-supervised rewarding even for tasks\nlacking inherent mutual implicativity.\nDefinition 2. Let the input space X of a primal task Tp be decomposed into two disjoint subspaces: Xk (known\ncomponents) and Xu (unknown components), such that X = Xk ∪Xu. The primal task Tp is a mapping\nTp : X →Y that maps x ∈X to an output space y ∈Y. Its complementary dual task Tcd is a mapping that\nleverages y and the known component xk to reconstruct the unknown component ˆ\nxu ∈Xu:\nTcd : (y, xk) 7→ˆ\nxu.\nThe pair (Tp, Tcd) is said to form a generalized dual pair if they satisfy the complementary consistency principle:\n∀x ∈X, y = Tp(x) : d\n\u0000xu, Tcd(y, xk)\n\u0001\n≤ϵ,\nwhere d(·) : X × X →R+ is a domain-specific distance metric, and ϵ ≥0 is a tolerance threshold.\nLeveraging this generalized duality, we can construct a self-supervised reward to quantify the preference of a\nprimal-task output analogously to Def. 1. Given an input x ∈X with decomposition x = (xk, xu) and its\ncorresponding output y = Tp(x), the reward is defined as\nr(y) ∝exp (−λ · d (xu, Tcd(y, xk))) ,\n(2)\nwhere λ > 0 controls reward sensitivity.\nThanks to the generalized duality, we can explicitly use xk (known components) to constrain Tcd, enabling\nstable optimization even when y alone is insufficient. To highlight how generalized duality resolves the\nlimitations (§3.2) of classic duality (§3.1), we examine a simple two-sum example:\nExample 1: Generalized Duality Feedback for a Two-Sum Task: A + B\nThe primal task Tp : y ←[ xu + xk is to compute the sum of two numbers, with its input and output as:\n– The input x is decomposed as x ←[ (A, B), where xk = A (a known number) and xu = B (an unknown\nnumber, without loss of generality).\n– The output y is the result of sum: C = A + B.\nThe complementary dual task Tcd : xu ←[ y −xk is designed to reconstruct the unknown component xu,\nusing the primal output y (i.e. C) and the known xk (i.e. A):\nˆ\nxu ←[ B′ = C −A\nThen, we can directly quantifies whether B (original unknown) and B′ (reconstructed unknown) are\nconsistent as reward signal:\nr(y) ∝exp (−λ · I(B ̸= B′)) .\nHere, I(·) is an indicator function: it equals 0 if B = B′ (consistent) and 1 otherwise (inconsistent). This\nensures the reward is maximized when B and B′ match, and reduced otherwise.\n5\n\nRemark 1. Compared to traditional dual learning, which suffers from strict mutual implicativity (i.e., y\nmust fully encode x) and bidirectional competence asymmetry, our generalized duality framework offers three\nfundamental advantages:\n1. Overcomes the Invertibility Constraint. By redesigning the dual objective from reconstructing the full\ninput x to only a selected unknown component xu, our framework fundamentally bypasses the stringent\nrequirement of task symmetry. This relaxation is the key to unlocking dual learning for tasks that are\ninherently non-invertible, where the primal output does not contain sufficient information to recover the\nentire input.\n2. Mitigates the Competence Asymmetry. The difficulty of the dual task is significantly reduced in two\nways. First, the known component xk acts as a strong contextual anchor, constraining the solution\nspace for reconstruction. Second, we can simply yet effectively select an xu that is not only feasibly\nreconstructible but also act as a faithful reward signal for the primal task’s solution quality (Appendix A).\nThis directly addresses the “weak dual” pitfall and ensures the self-supervised reward is reliable and\ninformative.\n3. Enables Broad Applicability. It unlocks dual learning for a broad class of tasks previously considered\nunsuitable, including complex domains such as mathematical reasoning, code generation, and dialogue\nsystems where input-output relationships are partial or conditional.\nThis generalized duality, therefore, provides a systematic way to overcome the traditional barriers of non-\ninvertibility and competence asymmetry. Our case studies in Appendix D present concrete examples that\nillustrate how this process is applied in the multilingual translation and mathematical reasoning scenarios.\n3.4\nPreference Optimization\nThe core of our Dual Learning-based Preference Optimization (DuPO) framework is to optimize LLMs using\nduality-derived self-supervised rewards r(y), without external annotations. The objective is to maximize the\nexpected reward based on the (complementary) dual task:\nJ (θ) = Ey∼πθ(y|x) [r(y)] ,\n(3)\nwhere πθ(y|x) denotes the LLM’s policy (parameterized by θ) for generating output y given input x = (xu, xk).\nThe distance metric d(·) design is highly flexible and compatible with various rule-based metrics, enabling\napplication across diverse tasks. For example, we could employ BLEU scores for multilingual translation\nwhich provide scores from 0 to 1, while for mathematical reasoning, we evaluate variable equality, yielding\nbinary rewards.\nNotably, DuPO is compatible with various reinforcement learning algorithms (e.g., PPO [39], ReMAX [28],\nREINFORCE++ [19]), we adopt Group Relative Policy Optimization (GRPO) [42] in our experiments—for its\nstability in high-dimensional parameter spaces (critical for LLMs) and compatibility with rule-based rewards.\n4\nExperiment\nWe validate the efficacy of DuPO on two representative tasks: multilingual translation and mathematical\nreasoning. Below, we detail the experimental setup, datasets, and evaluation metrics for each task, followed\nby key results.\n4.1\nExperiment Setup\nBase Model.\nWe evaluate DuPO on a diverse set of strong and popular base models to demonstrate its\neffectiveness and robustness. For translation tasks, we employ Seed-X-7B-Instruct [9], one of the strongest\nopen-source translation models. For mathematical reasoning, we select models of varying scales and capabilities,\nincluding small-scale yet powerful DeepSeek-R1-Distill-Qwen-1.5B [11] and its larger counterpart DeepSeek-\nR1-Distill-Qwen-7B, both distilled from the state-of-the-art DeepSeek-R1. We also include Qwen3-4B [51],\n6\n\nModel\nBLEU COMET BLEURT Avg.\nQwen3-8B\n21.69\n84.82\n65.81\n57.44\nDoubao-1.5-Thinking 26.19\n87.87\n71.66\n61.91\nQwen3-235B-22B\n28.37\n88.76\n73.91\n63.68\nDeepSeek-R1-0528\n30.21\n89.16\n75.03\n64.80\nSeed-X-7B-Instruct\n28.76\n86.96\n72.62\n62.78\nw/ DuPO (ours)\n30.31\n89.09\n74.57\n64.66\nTable 1 Multilingual Translation Performance Across\n756 Translation Directions in 28 Languages. DuPO\nsignificantly improves all metrics and performs compara-\nbly to its strong counterparts (DeepSeek models).\nEn2XX\nZh2XX\nAvg\n3.2\n3.3\n3.4\n3.5\n3.6\n3.7\nScore\n3.67\n3.66\n3.67\n3.66\n3.65\n3.66\n3.57\n3.67\n3.64\n3.60\n3.28\n3.44\nSeed-X-7B-Instruct w/ DuPO\nGPT-4o\nDeepSeek-R1-0120\nGoogle Translate\nFigure 2 Human Evaluation Scores (0-4) on the Seed-\nX-Challenge for 14 Language Directions. DuPO achieves\nperformance comparable to or even surpassing ultra-large\nmodels such as GPT-4o and DeepSeek-R1-0120, while sig-\nnificantly outperforming Google Translate.\nthe latest strong small LLM, and the most capable open-source reasoning model, OpenReasoning-Nemotron-\n7B [32]. These models represent strong and representative baselines within their respective model scales,\nensuring comprehensive evaluation. Additionally, we also include some SOTA and impressive ultra-large\nmodels like Doubao-1.5/1.6-Thinking [5], Claude-Sonnet4-Thinking, and DeepSeek-R1 [11] for comparison.\nDataset.\nFor translation tasks, we focus on 28 languages that are aligned with the language coverage of\nSeed-X, selecting 1,000 prompts for each language from a multilingual pre-training dataset to create our\ntraining prompt set. Additionally, we collect 7,000 parallel data entries across these specified languages to\nsupport our experiments from the dev set of Flores-200 [33]. For mathematical reasoning tasks, we utilize\na mixture of publicly available mathematics question datasets1. These datasets encompass diverse sources\nand are commonly used for synthesizing supervised fine-tuning data with ultra-large LLMs or conducting\nreinforcement learning with oracle labels, covering various subjects of competition-level mathematical problems,\nlogic puzzles, and other reasoning tasks.\nBenchmarks. To comprehensively evaluate the effectiveness of DuPO, we conduct extensive experiments using\nthe following test sets:\n• Multilingual Translation: For multilingual translation, we construct our test set by randomly selecting\n50 samples for each of the 756 translation directions (among 28 languages) from the testset of Flores2,\nresulting in a total of 37,800 samples. We will release this dataset for convenient comparison. We employ\nBLEU [36], COMET [37], and BLEURT [40] as evaluation metrics. Additionally, we conduct human\nevaluation on Seed-X-Challenge [9]3, a challenging benchmark designed to test the boundaries of LLMs’\ntranslation capabilities with diverse linguistic elements across multiple domains. Human experts assess\naccuracy, fluency, and idiomaticity, scoring translations from Chinese or English to seven languages on a\n0-4 scale (higher score denotes better translation quality).\n• Mathematical Reasoning: We evaluate our approach on multiple benchmarks, including AMC23 [31],\nAIME24 [4], and AIME25 [4], to assess performance in standardized contest environments. For each\nproblem, we sample 32 responses using a temperature of 0.8 and a maximum reasoning budget of 32,000\ntokens, then report the average accuracy (Avg@32).\nUltra-large models like DeepSeek-R1 and Doubao-1.6-thinking are accessed via their official APIs. More\ndetails about training are provided in Appendix C.\n4.2\nMain Results\n4.2.1\nDuPO Boosts LLM’s Performance on Various Tasks\nDuPO achieves strong performance on diverse tasks, including multilingual translation and mathematical\nreasoning. On multilingual translation, DuPO elevates the base model to a state-of-the-art performance\nlevel, rivaling and even surpassing significantly ultra large LLM. As detailed in Table 1, applying DuPO\n1More details on math data preparation can be found in Appendix B.\n2https://huggingface.co/datasets/openlanguagedata/flores_plus\n3https://github.com/ByteDance-Seed/Seed-X-7B/tree/main/challenge_set\n7\n\nModel\nAMC23\nAIME24\nAIME25\nAverage\nDeepSeek-R1-0120\n97.7\n79.8\n70.0\n82.5\nClaude-Sonnet4-Thinking\n97.5\n82.5\n70.0\n83.3\nDoubao-1.5-Thinking\n99.4\n86.3\n73.3\n86.3\nDoubao-1.6-Thinking\n98.8\n88.4\n83.4\n90.2\nDeepSeek-R1-0528\n99.4\n91.4\n87.5\n92.8\nDeepSeek-R1-Distill-Qwen-1.5B\n67.5\n20.0\n20.0\n35.8\nw/ DuPO (ours)\n72.5\n30.0\n26.7\n39.7 (+3.9)\nDeepSeek-R1-Distill-Qwen-7B\n85.0\n56.7\n36.7\n59.5\nw/ DuPO (ours)\n90.0\n63.3\n40.0\n64.4 (+4.9)\nQwen3-4B\n95.0\n70.0\n66.7\n77.2\nw/ DuPO (ours)\n97.5\n83.3\n70.0\n83.6 (+6.4)\nOpenReasoning-Nemotron-7B\n95.0\n83.3\n73.3\n83.9\nw/ DuPO (ours)\n97.5\n83.3\n90.0\n90.3 (+6.4)\nTable 2 Mathematical Reasoning Performances (%) on Representative Benchmarks. DuPO significantly improves\nthe performances across models with varying base capabilities, enabling Qwen3-4B to outperform DeepSeek-R1-0120\nand OpenReasoning-Nemotron-7B to achieve SOTA performance.\nto the Seed-X-7B-Instruct model boosts its performance by 1.55, 2.13, and 1.95 across three automatic\nevaluation metrics, reaching an average score of 64.66. This performance even surpasses that of current SOTA\nclosed-source ultra-large language models, such as Doubao1.5-thinking (+2.75) and Qwen3-235B-22B (+0.98),\nand is on par with the performance of the latest DeepSeek-R1. As shown in Figure 2, DuPO demonstrates\nremarkable performance, achieving results comparable to state-of-the-art ultra-large models such as GPT-4o\nand DeepSeek-R1. Moreover, DuPO substantially outperforms widely-used commercial closed-source systems\nlike Google Translate, showcasing a clear advantage in translation quality as perceived by human evaluators.\nOn mathematical reasoning, the results in Table 2 clearly demonstrate that DuPO yields consistent and\nsignificant performance improvements across all models at different scales and baseline reasoning ability. On\nthe most powerful OpenReasoning-Nemotron-7B model, applying DuPO increased the average score from\n83.9% to 90.3%, achieving impressive performance. This trend of significant gains continues on the mid-sized\nQwen3-4B model, which saw its average score boosted by 6.4 points from 77.2% to 83.6%, even surpassing\nthe ultra-large model DeepSeek-R1-0120. The approach remains remarkably effective on DeepSeek’s distilled\nmodels as well. Even on DeepSeek-R1-Distilled-Qwen-1.5B, the least reasoning capability among the strong\nbaselines, we still achieved a 3.9-point increase in average accuracy. This directly demonstrates that DuPO is\nsufficiently robust and stable to enhance the mathematical reasoning capabilities of models consistently. Our\nframework’s robust performance is further validated by concrete examples in multilingual translation and\nmathematical reasoning (see case studies in Appendix D).\n4.2.2\nDuPO Scales to Various Backbones Effectively\nTo validate the robustness and generalization of our proposed DuPO framework, we extend our evaluation\nto the LlaMA architectural family. Our experiments are conducted on two LlaMA architectural models:\nLlaMA-3.1-8B [13] and OctoThinker-8B-Hybrid-Base [47], the latter of which has undergone middle training\non mathematical reasoning knowledge. Considering the significant difference of model ability, we select two\nbenchmarks of moderate difficulty, AMC23 [31] and MATH500 [18]. For a fair comparison, all models are\nfinetuned using identical training data and settings. Results are listed in Table 3.\nAs seen, DuPO’s effectiveness is not tied to a specific model architecture; it serves as a robust and generalizable\nenhancement, delivering significant improvements to diverse backbones regardless of their initial reasoning\nproficiency. DuPO lifts the average score of LlaMA-3.1-8B to 32.1%, a +24.0 percentage-point gain over\nthe vanilla model, and surpasses SimpleRL-Zoo [53] (which relies on oracle-labeled answers during training)\nby 13.1%. When applied to the OctoThinker-8B-Hybrid-Base [47], our DuPO approach yields even more\n8\n\nModel\nAMC23 MATH500 Average\nLlaMA-3.1-8B\n2.5\n13.6\n8.1\nw/ SimpleRL-Zoo\n15.0\n23.0\n19.0\nw/ DuPO (ours)\n20.0\n44.2\n32.1\nOctoThinker-8B-Hybrid-Base\n5.0\n42.6\n23.8\nw/ DuPO (ours)\n55.0\n70.0\n62.5\nTable 3 Performances (%) of DuPO on Different\nBackbone Models. DuPO even surpasses SimpleRL-\nZoo, which utilizes labeled answers as reward. DuPO’s\npotential is further exemplified by OctoThinker, which\nunderwent additional middle training.\n50\n100\n150\n200\n250\n300\nSteps\n0\n20\n40\n60\nAccuracy (%)\nAMC\nAIME24\nAIME25\nForward Acc\nFigure 3 Training Progress of DuPO on Qwen3-\n4B-Base. The performance consistently improves\non the primal task and the benchmarks.\nimpressive performance improvements of +50.0 on AMC23 and +27.4 on MATH500, achieving an average\nperformance of 62.5.\n4.2.3\nDuPO Incentivizes Reasoning Capability on Base Model\nWe further demonstrate that our DuPO framework can directly elicit and enhance complex reasoning\ncapabilities from a base model. We apply DuPO directly to a base model, without preliminary supervised\nfine-tuning (SFT) stage activating the reasoing ability. We track the learning dynamics by simultaneously\ncollecting the primal task accuracy (“Forward Acc”) on the training set and its generalization performance on\nthree distinct, unseen challenge test set: AMC23, AIME24, and AIME25.\nWe can see from Figure 3 that DuPO provides a stable and effective pathway to awaken and generalize the\nlatent reasoning abilities of a base model, validating its utility as a powerful training methodology. Specifically,\nthe training dynamics reveal a clear and substantial improvement on the primal task, with the “Forward Acc”\nsoaring from a nascent 15.2% to 56.5%. This upward trajectory provides direct evidence that the reward signal\nderived from our dual-task serves as an effective guide for enhancing the model’s reasoning. More importantly,\nthis acquired skill demonstrates robust generalization. Performance on the unseen test set AMC23 leaped\nfrom 20% to 70%, with similarly significant gains observed on the AIME24 and AIME25 datasets.\n4.2.4\nDuPO Scales Reasoning during Inference without Training\nBeyond serving as a reward signal for RL training, the DuPO mechanism can be naturally applied as a\ntraining-free, inference-time reranking strategy to improve the reasoning capabilities of any LLM. The process\nunfolds in three stages: 1) Similar to the rollout stage during RL process, we could prompt any given policy\nmodel to generate diverse reasoning trajectories. 2) For each candidate trajectory, we use its final answer to\nask the policy model to solve the corresponding dual question automatic constructed without accessing labeled\nanswer. We could apply more computation by performing K (K = 8 in our experiments) sampling runs on\neach dual question for a more reliable reward estimate, a practice distinct from RL training. 3) Finally, for\neach test set question, we select the trajectory with the highest backward accuracy on its dual questions as\nthe final output.\nAs presented in Table 4, the experimental results demonstrate that DuPO provides accurate reward signals,\neffectively guiding models towards correct reasoning, serving as an efficient approach for scaling reasoning\ncapabilities even without training. On the two challenging AIME benchmarks, applying DuPO as a reranking\nmethod improves the average performance of Qwen3-4B by 9.3 points, elevating its accuracy from 68.4% to\n77.7% without any additional training. Notably, the DuPO-enhanced Qwen3-4B surpasses DeepSeek-R1 and\nClaude-Sonnet4-Thinking (77.7% vs. 74.9%/76.3% on average). The impact on DeepSeek-R1-Distill-Qwen-\n1.5B s even more pronounced, with an 18.7 point increase (20.0% to 38.7%).\n9\n\nModel\nAIME24\nAIME25\nAverage\nDeepSeek-R1-0120\n79.8\n70.0\n74.9\nClaude-Sonnet4-Thinking\n82.5\n70.0\n76.3\nDeepSeek-R1-Distill-Qwen-1.5B\n20.0\n20.0\n20.0\nw/ DuPO rewarding\n53.3\n24.1\n38.7 (+18.7)\nQwen3-4B\n70.0\n66.7\n68.4\nw/ DuPO rewarding\n86.6\n68.9\n77.7 (+9.3)\nTable 4 Inference-Time Scaling on Mathematical Reasoning Using DuPO Rewarding (Backward Acc) for\nReranking. Our method improves the performance of policy models with varying base ability, without requiring\nadditional training.\nAMC23\nAIME24\nAIME25\nAvg\n20\n30\n40\n50\n60\n70\nAcc (%)\n67.5\n20.0\n20.0\n35.8\n72.5\n30.0\n26.7\n43.1\n65.0\n26.7\n26.7\n39.5\n1.5B Baseline\nDuPO\nDuPO w/o filter\nAMC23\nAIME24\nAIME25\nAvg\n65\n70\n75\n80\n85\n90\n95\n100\nAcc (%)\n95.0\n70.0\n66.7\n77.2\n97.5\n83.3\n73.3\n84.7\n95.0\n73.3\n70.0\n79.3\n4B Baseline\nDuPO\nDuPO w/o filter\nFigure 4 Performance Ablation of DeepSeek-R1-Distill-Qwen-1.5B/Qwen3-4B on Mathematical Reasoning.\nOur unknown component selection strategy reduces training noise and improves these models’ performance across\nthree benchmarks.\n4.3\nEffects of Task Duality\nTo thoroughly investigate the effectiveness of our proposed framework and validate how our unknown\ncomponent selection strategy contributes to achieving better task duality, we conduct an ablation study by\nmaintaining identical experimental settings while removing the unknown component selection mechanism\nfrom our dual framework.\nAs illustrated in Figure 4, the results showcase the efficacy of our approach in resolving duality issues. For\nthe 1.5B model, DuPO achieves a remarkable 7.3 percentage point improvement over the baseline. Notably,\nwhen we remove data filtering, thereby introducing poorer duality, we observe a significant 3.6 percentage\npoint drop in performance. This pattern not only persists with stronger models. In the case of the 4B model,\nthe benefits of our method become even more pronounced, outperforming the poorer duality variant by an\nimpressive 5.4 points on average. These consistent and substantial improvements across various model sizes\nprovide strong empirical evidence that our component selection strategy is a crucial component of the dual\nframework, effectively ensuring high-quality task duality and thereby enabling the framework to achieve\nsuperior performance.\n5\nConclusion\nWe introduce DuPO, a dual learning-based preference optimization framework that eliminates the need for\ncostly human annotations and handcrafted rewards in LLM training. At its core, DuPO’s innovation lies in\na generalized duality framework that decomposes and reconstructs input spaces into known and unknown\ncomponents, addressing critical limitations of traditional dual learning and preference optimization paradigms.\nEmpirical validation across two diverse, high-stakes tasks confirms DuPO’s versatility and effectiveness. In\nmathematical reasoning, DuPO consistently improves performance across model scales from 1.5B to 7B\n10\n\nparameters, with notable gains of 6.4% average accuracy improvement of three benchmarks. For multilingual\ntranslation, DuPO elevates the 7B-parameter Seed-X model to performance levels comparable to much larger\nstate-of-the-art models, boosting COMET scores by up to 2.13 points across 28 languages and 756 translation\ndirections. Additionally, DuPO serves as an effective training-free reranking mechanism, enabling smaller\nmodels to outperform larger counterparts with up to 9.3 points improvement, bypassing the need for expensive\nparameter scaling.\nDuPO’s model-agnostic design and broad task applicability position it as a scalable solution for annotation-\nefficient LLM development. By harnessing intrinsic task structure to generate self-supervised feedback, it\nmoves beyond the constraints of human supervision and rigid reward engineering—paving the way for more\nautonomous, adaptable, and cost-effective language model optimization.\n6\nLimitations\nDespite the promising results, we acknowledge several limitations of our work that present avenues for future\nresearch. First, unknown components selection for mathematical reasoning introduces the computational\noverhead. While this step is crucial for ensuring the quality of the self-supervised reward signal, developing\nmore efficient or even learnable filtering mechanisms could enhance the scalability and practical applicability of\nDuPO. Second, our empirical validation is primarily conducted on models of moderate scale. Although DuPO\ndemonstrates consistent improvements across various model sizes, its scalability and effects on significantly\nlarger models remain an open question. Finally, while we demonstrate DuPO’s efficacy on various tasks\nlike mathematical reasoning and multilingual translation, its application to more open-ended and creatively\ndemanding tasks, such as open-ended instruction-following, requires further exploration.\nAcknowledgments\nWe extend our sincere gratitude to our colleagues at ByteDance, including Qian Cao4, Zhichao Huang, Liyan\nKang, Ningxin Peng, Xinghua Qu, Ming Tu, Xiangpeng Wei, Rong Ye, Runsheng Yu, and Zaixiang Zheng,\nfor their valuable advice and insightful discussions, and to Meng Yang and Evaluation Team for their help\nwith the translation evaluation.\n4Qian has already left ByteDance.\n11"}
{"id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "authors": ["NVIDIA", ":", "Aarti Basant", "Abhijit Khairnar", "Abhijit Paithankar", "Abhinav Khattar", "Adi Renduchintala", "Adithya Renduchintala", "Aditya Malte", "Akhiad Bercovich", "Akshay Hazare", "Alejandra Rico", "Aleksander Ficek", "Alex Kondratenko", "Alex Shaposhnikov", "Ali Taghibakhshi", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amy Shen", "Andrew Tao", "Ann Guan", "Anna Shors", "Anubhav Mandarwal", "Arham Mehta", "Arun Venkatesan", "Ashton Sharabiani", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Banghua Zhu", "Barnaby Simkin", "Bilal Kartal", "Bita Darvish Rouhani", "Bobby Chen", "Boris Ginsburg", "Brandon Norick", "Brian Yu", "Bryan Catanzaro", "Charles Wang", "Charlie Truong", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christian Munley", "Christopher Parisien", "Dan Su", "Daniel Afrimi", "Daniel Korzekwa", "Daniel Rohrer", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Dima Rekesh", "Dina Yared", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Eileen Long", "Elliott Ning", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Gargi Prasad", "Gerald Shen", "Haim Elisha", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herman Sahota", "Hexin Wang", "Hoo Chang Shin", "Hua Huang", "Iain Cunningham", "Igor Gitman", "Ivan Moshkov", "Jaehun Jung", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jimmy Zhang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jonathan Cohen", "Joseph Jennings", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keith Wyss", "Keshav Santhanam", "Kezhi Kong", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Kushan Ahmadian", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Luis Vega", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Mark Cai", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matvei Novikov", "Mehrzad Samadi", "Meredith Price", "Meriem Boubdir", "Michael Boone", "Michael Evans", "Michal Bien", "Michal Zawalski", "Miguel Martinez", "Mike Chrzanowski", "Mohammad Shoeybi", "Mostofa Patwary", "Namit Dhameja", "Nave Assaf", "Negar Habibi", "Nidhi Bhatia", "Nikki Pope", "Nima Tajbakhsh", "Nirmal Kumar Juluru", "Oleg Rybakov", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Oluwatobi Olabiyi", "Pablo Ribalta", "Padmavathy Subramanian", "Parth Chadha", "Pavlo Molchanov", "Peter Dykas", "Peter Jin", "Piotr Bialecki", "Piotr Januszewski", "Pradeep Thalasta", "Prashant Gaikwad", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi Mahabadi", "Rajen Patel", "Ran El-Yaniv", "Ranjit Rajan", "Ria Cheruvu", "Rima Shahbazyan", "Ritika Borkar", "Ritu Gala", "Roger Waleffe", "Ruoxi Zhang", "Russell J. Hewett", "Ryan Prenger", "Sahil Jain", "Samuel Kriman", "Sanjeev Satheesh", "Saori Kaji", "Sarah Yurick", "Saurav Muralidharan", "Sean Narenthiran", "Seonmyeong Bak", "Sepehr Sameni", "Seungju Han", "Shanmugam Ramasamy", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shizhe Diao", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Siddhartha Jain", "Somshubra Majumdar", "Stefania Alborghetti", "Syeda Nahida Akter", "Terry Kong", "Tim Moon", "Tomasz Hliwiak", "Tomer Asida", "Tony Wang", "Twinkle Vashishth", "Tyler Poon", "Udi Karpas", "Vahid Noroozi", "Venkat Srinivasan", "Vijay Korthikanti", "Vikram Fugro", "Vineeth Kalluru", "Vitaly Kurin", "Vitaly Lavrukhin", "Wasi Uddin Ahmad", "Wei Du", "Wonmin Byeon", "Ximing Lu", "Xin Dong", "Yashaswi Karnati", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yonggan Fu", "Yoshi Suhara", "Zhen Dong", "Zhiyu Li", "Zhongbo Zhu", "Zijia Chen"], "source": "http://arxiv.org/abs/2508.14444v1", "page_count": 43, "text": "2025-8-21\nNVIDIA Nemotron Nano 2: An Accurate and\nEfficient Hybrid Mamba-Transformer Reasoning\nModel\nNVIDIA\nAbstract. We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving state-of-the-art accuracy\ncompared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture,\nin which the majority of the self-attention layers in the common Transformer architecture are replaced\nwith Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces\nneeded for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter\nmodel (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After\naligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill\nthe model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G\nGPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g.,\nQwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning\nbenchmarks while achieving up to 6× higher inference throughput in reasoning settings like 8k\ninput and 16k output tokens (Figure 1). We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano-\n12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and\npost-training datasets on Hugging Face.\n1. Introduction\nWe introduce NVIDIA Nemotron Nano 2, a hybrid Mamba-Transformer reasoning model (Waleffe\net al., 2024; Lieber et al., 2024; DeepMind, 2025; NVIDIA, 2025) that achieves on-par or better\nbenchmark accuracies at 3×–6× higher throughput than Qwen3-8B (Yang et al., 2025) for generation-\nheavy scenarios like 1k input / 8k output or 8k input / 16k output tokens (Figure 1). Nemotron\nNano 2 builds on the architecture of Nemotron-H (NVIDIA, 2025), but utilizes key new datasets and\nrecipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints,\nas well as the majority of the pre- and post-training datasets.\nThe initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over\n20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5). It then\nunderwent a continuous pre-training long-context extension phase to become 128k-capable without\ndegrading other benchmarks (§2.6). Overall, new and improved datasets led to significant accuracy\nimprovements over Nemotron-H-8B on math, multilingual, MMLU-Pro and other benchmarks (§2.2).\nNemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT),\nGroup Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization\n(DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang\net al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains,\nfollowed by targeted SFT on key areas such as tool use, long-context performance, and truncated\n(budgeted) training. GRPO and RLHF sharpened instruction-following and conversational ability,\nwhile additional DPO stages further strengthened tool use. Overall, post-training was performed\non roughly 90 billion tokens, the majority in single-turn prompt–response format with reasoning\n© 2025 NVIDIA. All rights reserved.\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nIFBench\n(Instr. Following)\nAIME24\n(Math)\nAIME25\n(Math)\nGPQA-D\n(Science)\nLiveCodeBench\n(Coding)\nBFCLv3\n(T\nool Use)\nRULER 128k\n(Long Context)\nISL/OSL\n1k/8k\nISL/OSL\n8k/16k\n30\n40\n50\n60\n70\n80\n90\nAccuracy (%)\n34.6\n81.9\n72.0\n64.0\n71.1\n66.9\n78.9\n33.0\n75.8\n69.3\n59.6\n59.5\n66.3\n74.1\nMeasured Accuracy\nMeasured Throughput\nNVIDIA-Nemotron-Nano-9B-v2\nQwen3-8B\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRelative Throughput (Output tokens/s/GPU)\n3.3\n6.3\n1.0\n1.0\nFigure 1 | Comparison of Nemotron Nano 2 and Qwen3-8B in terms of accuracy and throughput.\nNemotron Nano 2 achieves comparable or better accuracies on complex reasoning benchmarks, while\nachieving up to 6.3× higher throughput for such workloads. We abbreviate input sequence length\nto ISL and output sequence length to OSL and measure throughput on a single A10G GPU in\nbfloat16.\ntraces. About 5% of the data contained deliberately truncated reasoning traces, enabling fine-grained\nthinking budget control at inference time (§3.4).\nFinally, both the base model and aligned model were compressed so as to enable inference over\ncontext lengths of 128k tokens on a single NVIDIA A10G GPU (22 GiB of memory, bfloat16\nprecision). This was done by extending a compression strategy based on Minitron (Muralidharan\net al., 2024; Sreenivas et al., 2024; Taghibakhshi et al., 2025) to compress reasoning models subject\nto constraints.\nWe are releasing the following models on Hugging Face:\n• NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model,\n• NVIDIA-Nemotron-Nano-9B-v2-Base: a pruned base model,\n• NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning.\nAdditionally, we are releasing the majority of our pre-training dataset in the Nemotron-Pre-\nTraining-Dataset-v1 collection of more than 6 trillion tokens:\n• Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional\nCommon Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic\nQ&A data translated into 15 languages.\n• Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx +\nLLM pipeline (Karimi Mahabadi et al., 2025a). Preserves equations, standardizes to LaTeX,\noutperforms previous math datasets on benchmarks.\n• Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering,\ndeduplication, and quality filters. Includes code Q&A data in 11 programming languages.\n• Nemotron-Pretraining-SFT-v1: Synthetic SFT-style dataset covering STEM, multilingual,\nacademic, and reasoning domains.\nFinally, we are releasing an updated post-training dataset:\n2\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMamba-2\nFFN\nAttention\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nNemotron-Nano-12B-v2-Base\nx3\nx6\nx1\nFigure 2 | Nemotron-Nano-12B-v2-Base layer pattern. As in Nemotron-H models, roughly 8% of the\ntotal layers in the model are self-attention layers which are evenly dispersed throughout the model.\nModel\nNumber of\nlayers\nModel\ndimension\nFFN\ndimension\nQ\nheads\nKV\nheads\nState\ndimension\nMamba\ngroups\nNemotron-Nano-12B-v2-Base\n62\n5120\n20480\n40\n8\n128\n8\nTable 1 | Summary of Nemotron-Nano-12B-v2-Base architecture.\n• Nemotron-Post-Training-Dataset-v2 (link coming soon): Adds to NVIDIA’s post-training\ndataset releases with an extension of SFT and RL data into five target languages: Spanish,\nFrench, German, Italian and Japanese. The data supports improvements of math, code, general\nreasoning, and instruction following capabilities.\nThe rest of this technical report is organized as follows: In §2, we discuss the Nemotron Nano 2 model\narchitecture, pre-training process, and base model evaluation results. In §3, we discuss the alignment\nprocess. In §4, we describe the pruning and distillation methods used for model compression.\n2. Pretraining\nIn this section, we discuss the architecture and pretraining of the Nemotron-Nano-12B-v2-Base\nmodel. We also compare this model against other state-of-the-art models in terms of accuracy on\npopular benchmarks.\n2.1. Model Architecture\nAs in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-\n2 (Dao & Gu, 2024), self-attention, and FFN layers. The layer pattern and key architecture details\nare summarized in Figure 2 and Table 1. Concretely, we use 62 layers, with 6 of them being\nself-attention layers, 28 being FFN, and 28 being Mamba-2 layers. We use a hidden dimension of\n5120, FFN hidden dimension of 20480, and Grouped-Query Attention (Ainslie et al., 2023) with 40\nquery heads and 8 key-value heads. For Mamba-2 layers, we use 8 groups, a state dimension of 128,\na head dimension of 64, an expansion factor of 2, and a window size for convolution of 4. For FFN\nlayers, we use squared ReLU (So et al., 2022) activation. Again as in Nemotron-H, we do not use\nany position embeddings and use RMSNorm (Zhang & Sennrich, 2019), separate embedding and\noutput layer weights, no dropout, and we do not use bias weights for linear layers.\n3\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n2.2. Pre-Training Data\nNemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-\ngenerated data.\n2.2.1. Curated Data\nWe have separate data curation pipelines for the following broad data categories: general web crawl\ndata (English and multilingual), math data, and code data. We discuss each in turn next.\nEnglish web crawl data.\nWe used the Nemotron-CC dataset (Su et al., 2025), but updated to\ninclude eight more recent Common Crawl snapshots (CC-MAIN-2024-33 through CC-MAIN-2025-13)\nusing the same pipeline. For synthetic rephrasing, we mostly switched to Qwen3-30B-A3B (from\nMistral Nemo 12B). Additionally, we used data from CC-NEWS through April 23, 2025, to help\nimprove the knowledge cutoff of the model. The CC-NEWS data was filtered for English and globally\nfuzzily de-duplicated; no other filtering was used.\nMultilingual data.\nWe extracted data for fifteen languages from the following three Common\nCrawl snapshots: CC-MAIN-2024-51, CC-MAIN-2025-08, and CC-MAIN-2025-18. The fifteen\nlanguages included were Arabic, Chinese, Danish, Dutch, French, German, Italian, Japanese, Korean,\nPolish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual\nmodel-based quality classifiers available, we just applied heuristic filtering instead. This was done in\na similar manner to the filtering of low-quality English data in the Nemotron-CC pipeline, except\nthat we had to selectively disable some heuristic filters that had very high false positive rates for\nsome languages. De-duplication was done in the same way as for Nemotron-CC. Additionally, we\nused data from Wikipedia and FineWeb-2 (Penedo et al., 2025) for these fifteen languages.\nMath data.\nMathematical content on the web is expressed in a wide range of formats, including\ninline and block L\nAT\nEX, MathML, Unicode symbols, and custom renderers such as MathJax or\nKaTeX. We conducted a detailed analysis of prior math-specific extraction pipelines—including\nOpenWebMath (Paster et al., 2023), MegaMath (Zhou et al., 2025), jusText (Endrédy & Novák,\n2013), Trafilatura (Barbaresi, 2021), and Resiliparse (Bevendorff et al., 2018)—and found that none\ncould reliably preserve mathematical expressions or code structure. These tools frequently discard or\ndistort equations and flatten code formatting, severely limiting the utility of the extracted content\nfor pretraining.\nTo address this, we built a new pipeline specifically designed for high-fidelity mathematical ex-\ntraction from Common Crawl. We first aggregated a comprehensive list of math-related URLs\nfrom prior datasets (e.g., InfiMM-WebMath (Han et al., 2024), OpenWebMath (Paster et al., 2023),\nFineMath (Allal et al., 2025), and MegaMath (Zhou et al., 2025)), then re-fetched their raw HTML\ndocuments from 98 Common Crawl snapshots (2014–2024). Each page was rendered using the lynx\ntext-based browser to preserve layout and math structure. We then applied Phi-4 (Abdin et al.,\n2024)(14B-parameters) to remove boilerplate, standardize notation into L\nA\nT\nEX, and correct inconsis-\ntencies. A FineMath classifier (Allal et al., 2025) was used to retain high-quality documents, followed\nby fuzzy deduplication via MinHash-based (Broder, 2000) Locality Sensitive Hashing (LSH) (Indyk\n& Motwani, 1998) via the NeMo-Curator framework.1 We finally decontaminated the dataset using\nLLM Decontaminator (Yang et al., 2023).\nThis process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token\nsubset, Nemotron-CC-Math-4+, containing only the top-scoring samples. When used for pretraining,\nthis dataset yields substantial improvements across math (MATH-500), code (HumanEval+, MBPP+,\n1https://github.com/NVIDIA-NeMo/Curator\n4\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMBPP), and general-domain evaluations (MMLU, MMLU-STEM, MMLU-Pro), surpassing all\nexisting open math datasets. For full details, see Karimi Mahabadi et al. (2025a).\nCode data.\nIn line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar\net al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. All source\ncode used to train this model originated from GitHub and went through a multi-stage processing\npipeline to arrive at the final source code training data. We performed license-based removal with a\nlicense detection pipeline similar to that used by the BigCode project (Lozhkov et al., 2024), but\nwith fewer accepted licenses (see Appendix A for additional details). De-duplication is especially\nimportant for source code, where many files can be found exactly duplicated across numerous\nrepositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using\nMinHash LSH). In order to build a better understanding of each file in our dataset, we annotated all\nfiles with a variety of measures and then performed filtering using these annotations. We found the\nheuristic filters from OpenCoder (Huang et al., 2025) to be effective and leveraged them to filter\nfiles that are less valuable or even detrimental for LLM pretraining.\n2.2.2. Synthetically-Generated Data\nSTEM data.\nWe generated synthetic data for STEM subjects, including Astronomy, Biology,\nChemistry, Math, and Physics using 88.6k questions collected from multiple sources as the seed\ndata. In addition to the widely used GSM8K, MATH, and AOPS training sets, we collected more\ndiverse questions from Stemez2 and textbooks with permissive licenses from OpenStax3 and Open\nTextbook Library.4\nWe used Qwen2.5-VL-72B-Instruct (Bai et al., 2025) to extract questions\nfrom the exercise sections in the textbooks with additional instructions such as dropping question\nnumbering, ignoring questions that require image interpretation, and formatting equations using\nLaTeX. We manually curated the extracted questions to fix occasional OCR errors and removed\nnon-self-contained questions (e.g., a question that refers to an example in the same chapter).\nTo expand both the quantity and diversity of questions, we conducted three iterations of question\ngeneration using four models (i.e., Qwen3-30B-A3B and Qwen3-235B-A22B (Yang et al., 2025), both\nwith thinking mode enabled, Deepseek-R1 (DeepSeek-AI, 2025a), and Deepseek V3 (DeepSeek-AI,\n2025b)) and three prompts:\n1. Similar question: Create a new question that explores similar concepts but offers a fresh\nchallenge.\n2. Harder question: Create a new question that requires more logical steps or involves more\nadvanced concepts.\n3. Varied question: Create a new question that differs in type from the original question. We\ninstructed the model to avoid superficial or trivial modifications and think through the solution\nwhen creating a new question.\nWe filtered out duplicates and highly-similar questions using fuzzy de-duplication and generated\nsolutions to the remaining questions with the models used in the question generation step. We\nconverted a subset of examples to multiple-choice questions in MMLU or MMLU-Pro style. We\nconstructed a few thousand few-shot examples by concatenating random synthetic samples.\nMath data.\nWe also revisited and regenerated the Nemotron-MIND dataset (Akter et al., 2024),\na math-informed synthetic pretraining corpus originally built on OpenWebMath. In our updated\n2https://www.stemez.com/\n3https://openstax.org\n4https://open.umn.edu/opentextbooks/\n5\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nversion, we regenerated the MIND dataset using Nemotron-CC-Math-4+, our highest-quality math\nsubset comprising 52B tokens—as the source corpus. Following the original methodology, we applied\nseven prompt templates (e.g., Teacher–Student, Debate, Interview, etc) to generate structured\nmathematical dialogues using the Phi-4 model. Unlike the original MIND, which relied on 14.7B\ntokens of lower-fidelity data, our version leverages significantly higher-quality input and processes\nit with a chunk size of 5K tokens. This regeneration produced a 73B-token synthetic dataset and\nled to consistent improvements across math reasoning and general knowledge (MMLU, MMLU-Pro.\nMMLU-Stem) benchmarks compared to the original MIND version, highlighting the critical role of\ninput data quality. Full details and results are available in Karimi Mahabadi et al. (2025b).\nMultilingual data.\nWe generated multilingual diverse question and answer data (Diverse QA) (Su\net al., 2025) from two sources:\n1. We translated the English Diverse QA data to fifteen languages (see Multilingual data) using\nQwen3-30B-A3B (Yang et al., 2025).\n2. We generated synthetic data from Wikipedia articles in these languages using the Diverse QA\nprompt and instructed the model to write all questions and answers in the target language.\nIn addition, we translated a subset of our GSM8K augmentation data (see STEM data) into\nthese languages using Qwen3-30B-A3B. We post-processed each translated solution by appending\na concluding sentence meaning “The answer is ...” (e.g., “La respuesta es ...” in Spanish, “Die\nAntwort lautet ...” in German), where the final numerical answer is extracted from the original\nEnglish solution.\nCode data.\nWe generated question-answer (QA) data at scale for 11 different programming\nlanguages by prompting an LLM to generate questions based on short snippets from our curated\nsource code, asking the model to solve the generated question, and then performing post hoc filtering\nof the generated QA pairs based on heuristics as appropriate (e.g., Python AST parsing). This\ntechnique results in diverse synthetic data targeted at problem solving containing both natural\nlanguage and source code. Further details are covered in the Nemotron-H technical report (NVIDIA,\n2025), where we first leveraged this type of synthetic code data in pretraining.\nAcademic data.\nIn the pretraining set for the Nemotron-H (NVIDIA, 2025) series of models, we\nassigned attribute labels for educational quality, educational difficulty, and educational subject to\nall documents coming from academic data, which encompasses textbooks and academic papers. As\ncontent of higher educational difficulty in technical domains still proves challenging for models, we\nprioritized increasing model comprehension of such information in our current pretraining set via\nthe generation of question-answer (QA) pairs as such data has been shown to enhance knowledge\nstorage and extraction within language models (Allen-Zhu & Li, 2024).\nTo do so, we first gathered all documents with educational difficulty at the undergraduate and\ngraduate levels in the following technical subject areas: math, chemistry, biology, physics, and\nmedicine. Using this subset of documents, we aim to find the most relevant pieces of texts that could\nbe utilized as seed contexts for our generation of QA pairs. We chunk each document into snippets\nof 512 token lengths, embed them with the e5-large model (Wang et al., 2024), and store them\nwithin a Milvus vector database that enables approximate nearest neighbor search. We then curate\ndocuments from a set of complex subject areas (e.g. Mathematics: Real Analysis, Biology: Genetics,\nStatistics: Information Theory), and query the Milvus database for the 250 nearest neighbor text\nsnippets to each query document. The returned snippets function as our seed contexts that we then\npass into a Qwen-2.5 72B instruct model (Qwen, 2025) to generate multiple choice and free response\nstyle QA pairs based on the information contained in the snippet. With each QA pair, a justification\nfor the answer is additionally generated.\n6\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nSFT-style data.\nUsing SFT-style data in the later stages of pretraining has shown to be helpful\nto foster more comprehensive model learning (Hu et al., 2024).\nTherefore, we synthesized and included different SFT-style data covering several domains: 1) code\nSFT data which is mainly focused on solving code problems; 2) math SFT data that is mostly focused\non reasoning; 3) MMLU-style SFT data which contains different question and answer examples\ncovering different knowledge topics; and 4) general instruction following SFT data.\nWe ensure that the SFT-style data covers diverse topics with different difficulty levels for each of\nthe above mentioned domains. Detailed synthesis methods and pipelines for the above mentioned\nSFT data can be found in prior work (Toshniwal et al., 2024; Moshkov et al., 2025; Bercovich et al.,\n2025a,b; Ahmad et al., 2025).\nFundamental reasoning SFT-style data.\nWhile the above mentioned SFT-style data help\nenhance an LLM’s ability to answer questions in code, math and general language understanding\nbenchmarks, they do not help improve the model’s ability in deeper reasoning tasks to discern\nthe correct answer among a larger pool of potential distractors. We propose to mitigate that\nby synthesizing SFT-style data focused on analytical reasoning, logical reasoning, and reading\ncomprehension.\nSpecifically, we collected existing datasets including 1) the Law School Admission Test (LSAT)\ndataset from Wang et al. (2022); Zhong et al. (2022) which encompasses three tasks: logical\nreasoning, reading comprehension, and analytical reasoning, 2) the repurposed LogiQA dataset\nby Liu et al. (2020) which contains various types of logical reasoning questions collected from the\nNational Civil Servants Examination of China, and 3) the AQuA-RAT dataset which emphasizes\nalgebraic word problems by Ling et al. (2017). We then prompted DeepSeek-V3 (DeepSeek-AI,\n2025b) and Qwen3-30B-A3B (Yang et al., 2025) respectively to synthesize more similar questions\nwith corresponding options. For each question we generated, we prompted DeepSeek-V3 again to\ngenerate the chain-of-thought (CoT) process with the final solution. At the post-processing stage,\nwe apply majority voting to keep only the samples that have the most voted solutions. Overall, we\ngenerated 4B tokens from DeepSeek-V3 and 4.2B tokens from Qwen3-30B models.\n2.3. Data Mixture and Ordering\nOur data mixture consists of thirteen data categories. The largest is web crawl data, which we\nsubdivided into four categories based on the Nemotron-CC quality classification (Su et al., 2025):\ncrawl-medium, crawl-medium-high, crawl-high, syn-crawl-high denoting medium, medium-high, high\nand synthetic quality crawl data, respectively. Apart from these, our data mixture has additional\ncategories such as math, wikipedia, code, academic data, crawl++, multilingual, and synthetic\nSFT-style data which is further categorized as general-sft, stem-sft and code-sft. Crawl++ consists\nof web-crawl derivatives like OpenWebText, BigScience and Reddit. Our multilingual data has fifteen\nlanguages: Arabic, Danish, German, Spanish, French, Italian, Portuguese, Dutch, Polish, Swedish,\nThai, Chinese, Japanese, Korean, and Russian. We design the data mixtures to give similar weight\nto data sources that have similar quality. Data sources of higher quality are weighed higher than\ndata sources of lower quality. We provide detailed explanation on quality estimation of datasets and\nthe blend creation process in Feng et al. (2024) and NVIDIA (2025).\nWe used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-\n12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the\nsecond and third phases, we primarily used high-quality datasets (e.g., Wikipedia). We switched to\nthe second phase at the 60% point of training, and to the third phase at the 90% point of training.\nThe data mixtures used in each phase are shown in Figure 3.\n7\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nstem-sft\n3.1%\nmultilingual\n5.0%\nacademic\n4.4%\ncode\n20.0%\nmath\n3.2%\nsyn-crawl-high\n16.2%\ncrawl-medium\n18.3%\ncrawl-medium-high\n14.8%\ncrawl-high\n11.1%\n(a) Data mixture of Phase 1.\ncode-sft\n4.4%\nstem-sft\n14.5%\nmultilingual\n5.0%\ncrawl++\n4.4%\nacademic\n3.8%\nwiki\n0.9%\ncode\n20.0%\ncrawl-high\n16.0%\nsyn-crawl-high\n21.0%\nmath\n9.5%\n(b) Data mixture of Phase 2.\ncode-sft\n10.9%\nstem-sft\n32.0%\nmultilingual\n4.4%\ncrawl-high\n10.0%\nsyn-crawl-high\n12.7%\nmath\n11.0%\ncode\n16.0%\n(c) Data mixture of Phase 3.\nFigure 3 | Data mixtures for each phase of pre-training.\n8\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMultilingual Data\nAvg\nSp\nGe\nFr\nMa\nIt\nJa\nPo\nKo\nCommon Crawl\n37.0\n37.8\n36.5\n39.8\n34.3\n36.3\n35.3\n37.5\n38.8\nFineWeb-2\n35.1\n38.8\n35.0\n34.3\n31.5\n37.0\n33.0\n36.0\n35.3\nDiverseQA-wiki\n42.1\n44.8\n41.3\n41.8\n41.5\n44.0\n41.0\n42.3\n40.3\nDiverseQA-crawl\n47.0\n49.8\n50.8\n48.3\n46.0\n45.8\n44.5\n49.0\n42.0\nTable 2 | Comparison of multilingual datasets on the Global-MMLU Benchmark.\n2.3.1. Multilingual Data Ablation Study\nIn Section 2.2, we mentioned several large categories of multilingual data, both curated and synthetic:\n1. Common Crawl: Extracted from recent Common Crawl snapshots using our own pipeline.\n2. FineWeb-2 (Penedo et al., 2025).\n3. DiverseQA-wiki: Generated from multilingual Wikipedia articles using a translated Diverse\nQA prompt.\n4. DiverseQA-crawl: Translated from English Diverse QA data.\nIn order to decide the proper data mixture among these different multilingual data sources, we\nfirst conducted ablation experiments to compare the four multilingual data’s downstream tasks’\nperformance.\nSpecifically, we took a 1B model checkpoint that had been trained for 350B tokens, and continuous\npretrained it for another 100B tokens. We assigned 50% of the continuous pretraining data to\nmultilingual data, and the remaining 50% use our default pretraining data mixture. We evaluated\neach model’s performance using the Global-MMLU benchmark (Singh et al., 2024a); the results are\nshown in Table 2. Our curated Common Crawl-based multilingual data performed slightly better\nthan the Fineweb2-based multilingual data, while the synthesized multilingual QA pairs performed\nmuch better than the curated multilingual web crawl data. The diverse pairs translated from English\nCommon Crawl achieved the highest average score over the 8 languages we evaluated on. Therefore,\nwe assigned a much higher weight to the DiverseQA-crawl data than the other categories when\ndeciding our multilingual data mixture.\n2.3.2. Fundamental Reasoning SFT-Style Data Ablation Study\nTo show the effectiveness of the fundamental reasoning (FR) focused SFT-style data we introduced\nin Section 2.2, we took the Nemotron-H-8B (NVIDIA, 2025) intermediate checkpoint trained over\n14.5T tokens, and continuous pretrained it with another 100B tokens. We assigned 5% of the 100B\ntokens to the newly synthesized FR-SFT data (as a replacement for Common Crawl data), and kept\nall other data categories the same as in the Nemotron-H-8B’s phase 3 blend. We compared this\nmodel with Nemotron-H-8B, which had also been trained with 14.6T tokens. The detailed evaluation\nbenchmarks are introduced in Section 2.7. The comparison results are shown in Table 3. The\nSFT-style data helped improve the Nemotron-H 8B model’s performance on MMLU-Pro from 44.24\nto 56.36, and also helped increase the average MATH score by around 2 points. While MMLU-Pro\nis a more challenging benchmark that evaluates a model’s language understanding capability, it\nalso requires the model to have excellent reasoning capability to select the correct answer out of\nten choices. Our SFT data helps equip the model to select the correct answers from the other nine\ndistractors through fundamental reasoning. We noticed no decrease in the average commonsense\nreasoning and average code benchmarks.\n9\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nModel\nAvg Math\nAvg Code\nAvg Reasoning\nMMLU\nMMLU-Pro\nNemotron-H 8B\n37.92\n59.49\n71.79\n72.67\n44.24\nNemotron-H 8B\n(w/ FR-SFT data)\n39.70\n59.61\n71.43\n72.98\n56.36\nTable 3 | Ablation study of the Fundamental Reasoning (FR) focused SFT-style data.\n2.4. FP8 Recipe\nWe used DeepSeek’s FP8 training recipe for the entirety of the pretraining run (DeepSeek-AI, 2025b).\nSpecifically, we used E4M3 for all tensors, 128x128 quantization blocks for weights, and 1x128 tiles\nfor the activations. Unlike Nemotron-H, we natively kept the model weights in E4M3 so that we\ncould do the distributed optimizer’s parameter all-gather operations (across data-parallel replicas) in\nFP8. One exception to DeepSeek’s formula was that we left the first and last four matrix multiples\nin BF16, as done with Nemotron-H. Also unlike the DeepSeek-V3 run, we left all optimizer state in\nFP32. We observed no training instabilities from this choice of numerics.\n2.5. Hyperparameters\nWe trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence\nlength of 8192 and global batch size of 768 (6,029,312 tokens per batch). We did not use any batch\nsize ramp-up. We used a WSD (Warmup-Stable-Decay) (Hu et al., 2024) learning rate schedule\nwith a “stable” learning rate of 4.5 · 10−4 and a minimum value of 4.5 · 10−6; the learning rate was\ndecayed over the final 3.6 trillion tokens. Weight decay was set to 0.1, and Adam 𝛽1 and 𝛽2 were set\nto 0.9 and 0.95 respectively\n2.6. Long-Context Extension\nTo ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context\nphase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT)\nwith a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6. Although\nthe target context length of Nemotron Nano 2 is 128k, in preliminary studies on the Nemotron-H\n8B model, we found it better to do CPT with 512k sequence length, instead of 256k or 128k. Our\nintuition is that longer training sequence can effectively lower the chance of long coherent documents\nbeing cut and separated by the Concat & Chunk algorithm for pretraining data loading. We used\n8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence\nlengths of 512k tokens still fits in GPU memory. We used a global batch size of 12 to ensure the\ntotal number of tokens per global batch during long-context CPT is the same as during pretraining:\naround 6M tokens. Phase LC consisted of 18.9 billion tokens.\nAdditionally, we did long-context synthetic data generation to create more high-quality data for Phase\nLC. Since the academic pretraining dataset is a good source of coherent long-context documents,\nwe used such documents that are longer than 32k tokens as seed data. We followed the methods\nmentioned in the Llama-3 (Meta, 2024) and Qwen-2.5 (Qwen, 2025) tech reports to generate long-\ncontext document QA data. We split each document into chunks of 1,024 tokens and then randomly\nselected 10% of the chunks to be fed into Qwen-2.5-72B-Instruct for data synthesis. We asked the\ngenerator to generate a QA pair based on the information in the text chunk. We concatenated the\nQA pairs and appended them to the end of the original document as a sample of the long-context\ndocument QA data. Such long-document QA provided good material for the model to learn long-\ncontext dependencies. See Table 4 for ablation results on Nemotron-H 8B regarding train sequence\n10\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nlengths and the effects of synthetic data.\nThe data blend used in Phase LC was built based on that of Phase 3. We proportionally downscaled\nthe weights of all Phase 3 data to 80% of their original values, allocating the remaining 20% to the\nnewly added long-context document-QA data. We found such a blend could effectively extend the\ncontext length of Nemotron-Nano-12B-v2-Base without degrading regular benchmark scores.\nTrain length\n128k\n256k\n256k\n512k\nSynthetic data\nyes\nno\nyes\nyes\nRULER-128k\n73.68\n70.19\n79.04\n81.04\nTable 4 | Comparisons of different train sequence lengths and synthetic data usages. Ablations were\nconducted on Nemotron-H 8B.\nTask\nN-Nano-V2\nN-Nano-V2\nQwen3\nGemma3\n12B Base\n9B Base\n8B Base\n12B Base\nGeneral\nMMLU\n78.24\n74.53\n76.44\n73.61\nMMLU-Pro 5-shot\n63.98\n59.43\n56.27\n45.12\nAGIEval English CoT\n68.03\n65.28\n59.54\n51.69\nMath\nGSM8K CoT\n91.66\n91.36\n84.00\n74.45\nMATH\n83.54\n80.50\n55.40\n42.40\nMATH Level 5\n67.61\n63.64\n29.91\n17.71\nAIME 2024 pass@32\n56.67\n30.00\n20.00\n16.67\nCode\nHumanEval+ avg@32\n61.03\n58.50\n57.55\n36.68\nMBPP+ avg@32\n61.55\n58.95\n58.56\n51.73\nCommonsense Understanding\nARC Challenge\n93.26\n90.70\n93.09\n90.44\nHellaSwag\n84.00\n79.90\n79.75\n84.15\nOpenBookQA\n46.00\n44.80\n42.00\n46.00\nPIQA\n82.54\n81.83\n79.43\n82.10\nWinoGrande\n79.24\n75.30\n75.93\n79.95\nLong Context\nRULER-128K\n84.74\n82.22\n-\n80.70\nTable 5 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models. N-Nano-V2 is\nshort for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is compared against Qwen3-8B-Base\nand Gemma3-12B-Base, and the best score is highlighted in each row.\n11\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n2.7. Base Model Evaluations\nWe run evaluations of all models ourselves unless otherwise stated. Our evaluation setup is built on\ntop of lm-evaluation-harness5 for fair comparisons, with the following changes:\n1. For mathematical reasoning, we evaluate GSM8K and MATH (Cobbe et al., 2021; Hendrycks\net al., 2021b) benchmarks using greedy-decoding. We also highlight the competition-level\nslice of the MATH benchmark as “MATH Level 5”. Additionally, we report the pass@32\nperformance on AIME-2024. We use Math-Verify6 to grade all generations.\n2. For code tasks (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)) we evaluate\nthe EvalPlus variants along with the sanitization of generations (Liu et al., 2023), in a 0-shot\nsetup. We estimate avg@32, pass@1 from 32 generations per prompt.\n3. General reasoning benchmarks (OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al.,\n2019), Hellaswag (Zellers et al., 2019), Winogrande Sakaguchi et al. (2019)) are unchanged\nexcept for ARC-Challenge (Clark et al., 2018), where we present all options at the same time,\nsimilar to MMLU (Hendrycks et al., 2021a).\n4. For multilingual capability, we evaluate MGSM Shi et al. (2022) (8-shot, native CoT) and\nGlobal MMLU-Lite Singh et al. (2024b).\n5. We use RULER (Hsieh et al., 2024) as the long context benchmark. We report the average\nscores over all the 13 tasks included in RULER.\nAccuracy results for Nemotron-Nano-12B-v2-Base with comparsions to Qwen3-8B Base and Gemma3-\n12B Base are shown in Tables 5 and 6. We also include the accuracy of our 9B pruned variant of\nNemotron-Nano-12B-v2-Base which is discussed in Section 4.\n3. Alignment\nIn this section we will present the alignment process we followed to convert the base checkpoint into\nan aligned 12B checkpoint. Our process is outlined in Figure 4.\nBase\nSFT 1\nSFT 2\nSFT 3\nMerged\nGRPO\nRLHF\nDPO\nFigure 4 | Flow of alignment procedures followed to arrive at the final \"Merged\" Nemotron Nano 2\n12B checkpoint.\n5https://github.com/EleutherAI/lm-evaluation-harness.\n6https://github.com/huggingface/math-verify.\n12\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nTask\nN-Nano-V2\nN-Nano-V2\nQwen3\nGemma3\n12B Base\n9B Base\n8B Base\n12B Base\nGlobal-MMLU-Lite\nGerman\n74.50\n68.25\n75.50\n69.75\nSpanish\n76.50\n72.75\n75.00\n74.00\nFrench\n78.25\n69.75\n74.25\n72.50\nItalian\n76.50\n73.25\n72.75\n74.00\nJapanese\n71.00\n67.00\n70.00\n71.50\nKorean\n72.50\n67.25\n67.25\n70.25\nPortuguese\n76.25\n71.25\n72.50\n75.75\nChinese\n75.50\n69.25\n75.25\n67.25\nAverage\n75.13\n69.94\n72.81\n71.88\nMultilingual Math (MGSM)\nSpanish\n93.20\n91.60\n86.40\n74.00\nGerman\n89.60\n89.60\n78.80\n68.80\nFrench\n86.40\n86.00\n78.80\n70.80\nChinese\n44.40\n75.20\n28.80\n26.80\nJapanese\n76.00\n74.80\n30.80\n26.40\nRussian\n90.40\n91.60\n83.60\n76.00\nAverage\n80.00\n84.80\n64.53\n57.13\nTable 6 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models on multilingual\nbenchmarks. N-Nano-V2 is short for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is\ncompared against Qwen3-8B-Base and Gemma3-12B-Base, and the best score is highlighted in each\nrow.\n3.1. Post-Training Data\nOur alignment begins with a large-scale SFT stage which trains the base model on approximately 80\nbillion tokens of prompt-response pairs. The distribution of domains is shown in Table 7.\nMath, science and coding.\nFor Math, Science and Coding data, we generate responses using\nthe open-weights DeepSeek-R1-0528 model (DeepSeek-AI, 2025b) using the same prompts used for\ntraining Nemotron-H-8B and 47B Reasoning models (NVIDIA, 2025). The training data has been\nreleased as part of Nemotron-Post-Training-Dataset-v17.\nTool calling.\nThe tool-calling dataset consists of single-turn, multi-turn, and multi-step conversa-\ntions.\nFor single-turn cases, we sample prompts from xlam-function-calling-60k8, glaive-\nfunction-calling-v29, NVIDIA-When2Call (Ross et al., 2025), and generate responses using\nQwen3-235B-A22B10. Inspired by ToolACE (Liu et al., 2024) and APIGen-MT (Prabhakar et al.,\n2025), we extend this to multi-turn and multi-step settings by simulating conversations where\nQwen3-235B-A22B plays the roles of User-Agent, Assistant-Agent, and API-Server-Agent. The\n7https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1\n8https://huggingface.co/datasets/xlam-function-calling-60k\n9https://huggingface.co/datasets/glaive-function-calling-v2\n10https://huggingface.co/Qwen/Qwen3-235B-A22B\n13\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nDomain\nNumber of Samples\nMath\n1.5M\nCoding\n1.1M\nScience\n2.0M\nTool-calling\n400K\nConversational\n1.5M\nSafety\n2K\nMultilingual (all domains)\n5.0M\nTable 7 | Post-training data distribution across domains used for our SFT stages.\nUser-Agent reviews available tools, poses challenging queries, interacts when addressed by the\nAssistant, and judges task success at the end. Each instance is paired with a random persona from\nNemotron-Personas11 to enrich diversity of queries.\nThe Assistant-Agent receives the initial query and available tools, executes tasks by invoking tools,\ninterpreting their responses, and interacting with the User-Agent across single-turn, multi-turn,\nor multi-step scenarios. Meanwhile, the API-Server-Agent acts as a mock API server, checking\nparameters and returning either valid outputs or error messages depending on correctness. A\nlightweight rule-based tool-call verification layer further strengthens reliability by ensuring outputs\nare consistent and verifiable, and only successful trajectories are retained.\nMultilingual data.\nOur multilingual synthetic post-training data are constructed by translating\nexisting English post-training data. To address the challenges of Large Language Model (LLM)\nhallucinations and quality degradation on long inputs when generating synthetic translation data, we\nimplement a robust quality assurance pipeline. Our method involves translating inputs line-by-line\nto manage complexity and skip non-translatable content like code. We also enforce a strict bracket\nformat for reliable extraction and use language identification to filter out off-target translations,\nthereby ensuring high-quality final outputs.\nConversational data.\nFor conversational data, we use prompts from the LMSYS dataset (Zheng\net al., 2023) and generate responses using the Qwen3-235B-A22B reasoning model (Yang et al., 2025).\nWe also incorporate prompts from HelpSteer2 and HelpSteer3, paired with responses generated by\nthe same model. In addition, we draw on a subset of approximately 550k prompts from WildChat-\n1M (Li et al., 2024b), again generating reasoning responses with Qwen3-235B-A22B. We also include\nmulti-turn conversations with Deepseek R1 responses using the multi-turn conversational prompts\nused in NVIDIA (2025).\nSafety.\nWe leveraged a mix of harmful and benign prompts drawn from the Nemotron Content\nSafety Dataset V2 (Ghosh et al., 2025)12, HarmfulTasks (Hasan et al., 2024), RedTeam2K (Luo\net al., 2024), and gretel-v1 (gre, 2024). Responses were generated using DeepSeek-R1-052813. To\nensure safety, we applied a two-step approach: initial prompting followed by filtering with guard\nmodels to verify that outputs remained safe.\n11https://huggingface.co/datasets/NVIDIA/Nemotron-Personas\n12https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0\n13https://huggingface.co/deepseek-ai/DeepSeek-R1\n14\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n3.2. Post Training\nStage 1 SFT.\nAs Figure 4 illustrates, we employ three distinct stages of supervised fine-tuning.\nStage 1 uses the full dataset described in Section 3.1, augmented with a subsample of roughly 10% of\nprompts paired with outputs stripped of reasoning traces. This exposes the model to “empty” traces,\nenabling it to produce direct answers in a reasoning-off mode. To improve efficiency and preserve\nlong-context ability from pretraining, we concatenate samples into sequences of approximately 128k\ntokens, reducing padding overhead and encouraging long-range learning.\nStage 2 SFT.\nStage 2 targets tool-calling. Although Stage 1 improved performance on most\nbenchmarks, tool-calling accuracy degraded. We attribute this to sample concatenation at 128k, which\nlikely disrupted learning of tool-calling patterns. Thus, Stage 2 was trained without concatenation,\nusing the full tool-calling dataset and a representative subsample of other domains.\nStage 3 SFT.\nStage 3 reinforces long-context capability. It incorporates long-context data following\nthe recipe used in Nemotron-H preparation (NVIDIA, 2025), along with augmented examples across\ndomains where reasoning traces were abruptly truncated to 1–2k tokens while preserving the final\nanswer. This truncation strategy improved robustness under varying inference-time thinking budgets.\nIFeval RL.\nTo improve instruction adherence, we sampled 16,000 prompts from the LMSYS Chat\ndataset and augmented them with IFEval-style instructions. A rule-based verifier scored outputs\nbased on how well they satisfied each instruction, creating a reward signal that prioritized following\ndirections with precision. IFEval RL experiments provided significant boost to IFEval capabilities\nwhile the rest of the benchmarks fluctuated slightly requiring careful checkpoint selection.\nDPO.\nIn another branch of training, we apply the DPO algorithm to improve tool-calling. We\nevaluate performance using the BFCL v3 benchmark, which extends BFCL v2 with greater emphasis\non multi-step (multiple tool calls to achieve a goal) and multi-turn (multiple user–agent interactions).\nTo strengthen these capabilities in the Nano V2 aligned model, we use the WorkBench environment,\na multi-step verifiable tool-calling setup adapted from Styles (Styles et al., 2024). In each WorkBench\ntask, the model must issue a sequence of tool calls across multiple steps, with correctness verified\nthrough database state comparisons.\nNano V2 undergoes reinforcement learning in this environment through iterative stages of Direct\nPreference Optimization. For each candidate checkpoint from the long-context stage, we generate\non-policy data consisting of positive examples (successful tool calls) and negative examples (failed\ngenerations) for every WorkBench prompt.\nThis process ensures that iterative DPO remains\non-policy.\nRLHF.\nWe evaluate the model’s overall helpfulness and chat capabilities using the Arena-Hard\nbenchmark. To improve performance on this benchmark, we use GRPO to train candidate checkpoints\nfrom the SFT stage using English-only contexts from HelpSteer3 (Wang et al., 2025). During training,\nwe generate responses both with and without thinking traces and use a Qwen-based reward model\nto judge the rollouts.\nModel Merging.\nDuring training, we observed a trade-off between reasoning capabilities and\nchat capabilities. To address this, we opted for checkpoint interpolation Wortsman et al. (2022),\n15\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nEvaluation\nNemotron-Nano-v2-12B\nQwen3-8B\nQwen3-14B\nAIME-2024\n85.42\n75.83\n81.53\nAIME-2025\n76.25\n69.31\n66.6\nMATH-500\n97.75\n96.3\n96.85\nGPQA-Diamond\n64.48\n59.61\n64.53\nLiveCodeBench (07/24–12/24)\n70.79\n59.5\n63.08\nSciCode Sub-Task\n18.75\n24.65\n26.04\nHumanity’s Last Exam\n6.30\n4.40\n5.38\nIFEval (Inst. Strict)\n89.81\n89.39\n91.32\nBFCL v3\n66.98\n66.34\n68.01\nRULER @ 128k\n83.36\n74.13\n73.55\nArenaHard\n74\n78.4\n87.7\nTable 8 | Evaluation results with reasoning \"ON\" (for Nemotron-Nano-v2-12B, Qwen3-8B, and\nQwen3-14B across reasoning and general capability benchmarks.\nblending in an RL checkpoint with strong reasoning capabilities with an RL checkpoint with strong\nchat capabilities. Checkpoint interpolation is performed by linearly interpolating model weights:\n(1 −𝛼) · 𝑤𝑚𝑜𝑑𝑒𝑙1 + 𝛼· 𝑤𝑚𝑜𝑑𝑒𝑙2. We experimented with a parameter sweep over 𝛼values from 0.1 to\n0.9 in increments of 0.1, and found that values around 0.5 offered a good trade-off.\n3.3. Evaluation\nOur 12B model’s performance is summarized in Table 8. To test reasoning capabilities across domains,\nwe evaluate the models on MATH-500 (Lightman et al., 2023), AIME-2024, AIME-2025,\nGPQA-Diamond (Rein et al., 2023), LiveCodeBench (07/24 - 12/24) (Jain et al.,\n2024), SciCode (Tian et al., 2024), and Humanity’s Last Exam (Phan et al., 2025). For\nbroader evaluation on diverse capabilities, we use IFEval (Zhou et al., 2023) for instruction\nfollowing capabilities, BFCL v3 (Yan et al., 2024) for tool-calling, RULER for long-context,\nand ArenaHard (Li et al., 2024a) for chat capability.\nWe conduct evaluations using NeMo-Skills14. We report Pass@1 average of 16 runs for AIME-\n2024, AIME-2025; average of 4 runs for MATH-500, GPQA-Diamond, LiveCodeBench,\nIFEval; and score of 1 run for BFCL v3, SciCode, Humanity’s Last Exam, RULER,\nand ArenaHard.\n3.4. Budget Control Evaluation\nNemotron Nano V2 allows users to specify how many thinking tokens the model may generate before\nproducing the final answer. The final answer is the portion of text typically shown to end users.\nThis feature is implemented by counting tokens after the model begins generating the <think>\ntoken. Once the budget is reached, the inference setup attempts to insert a closing </think> tag.\nRather than inserting it immediately, we let the model finish its current sentence and place the\ntag at the next newline. In extreme cases where no newline appears, the system enforces closure\nwithin 500 tokens past the budget: if no newline occurs by the (budget + 500)th token, the </think>\ntag is forcibly inserted. Figure 5b shows our models budget control behavior. Apart from just\npresenting the accuracy of the model at various budgets, we also inspect if the model generations\nare well-formatted at various budgets. We inspect for two kinds of failure modes:\n14https://github.com/NVIDIA/NeMo-Skills\n16\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n(a)\n(b)\nFigure 5 | Comparison of budget control before truncation training (a) and after truncation training\nwas included (b). For all plots above the x-axis indicates the budget assigned for thinking tokens.\n• In one failure mode, the model uses more tokens in the final answer to “compensate” for\nrestrictions in the thinking traces. Without truncated training examples in the SFT stage,\nthis compensation effect is prevalent (Figure 5a, center). With truncated training, however,\nthe effect is absent (Figure 5b, center).\n• Another issue is that the model can remain in “thinking mode” even after the closing tag\n</think> is inserted. This is evident when the model generates the closing tag again after the\nforced insertion, suggesting it does not fully “register” the artificial closure. We evaluate this\nusing “Well-Formedness,” where a well-formed response should contain only a single closing\ntag (either forced by the budget or produced naturally). Figure 5a (right) shows that for short\nbudgets, the percentage of well-formed responses drops sharply. With truncation training,\nhowever, the model consistently produces well-formed responses (Figure 5b, right).\n4. Pruning and Distillation\nIn this section, we describe the pruning and distillation process to compress the aligned 12B model\nto the Nano 2 model with the goal of running longer context (128k sequence length) inference on\nthe NVIDIA A10G GPU. Note that storing just the weights of a 12B parameter model in bfloat16\nprecision requires 22.9 GiB, which is more than the 22 GiB memory capacity of an A10G GPU; this\nclearly indicates the need for compression.\nOur compression strategy builds on Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024;\nTaghibakhshi et al., 2025), which is a lightweight model pruning framework for LLMs. While\nMinitron was originally designed for compressing pretrained base models targeting user-defined\nparameter budgets, in this work, we extend it to compress reasoning models while also incorporating\n17\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nthe memory constraints and throughput-based objectives stated above.\n4.1. Importance Estimation\nWe collect importance or sensitivity scores for each model component (e.g., layers, FFN neurons)\nto help decide which components to remove; this is the importance estimation phase. The scores\ncomputed in this phase are used to decide which model components can be pruned. We note\nthat sensitivity analysis based on gradient information is typically impractical at modern LLM\nscale (Muralidharan et al., 2024); instead, we rely on a lightweight strategy that uses only forward\npasses. In this work, we use a simplified approach that works well in our ablation studies: a) prune\nlayers, and b) prune FFN hidden dimensions (effectively neurons) and embedding channels. We\nalso experimented with pruning Mamba heads; unfortunately, this axis caused severe accuracy\ndegradation. We now describe how we compute the importance of each layer, embedding channel,\nFFN neuron and Mamba head.\nLayer importance.\nWe compute layer importance in an iterative fashion: for each candidate layer,\nwe temporarily remove it from the model and compute the mean squared error (MSE) between the\noriginal model’s logits and those produced by the pruned model. This MSE reflects the contribution\nof that layer to the model’s predictions: lower values indicate smaller impact. At each pruning step,\nwe remove the layer with the lowest MSE, as it has the least influence on the final output. We repeat\nthis process until the desired depth is reached. This strategy ensures that pruning preferentially\nremoves layers whose absence minimally affects the model’s behavior. For more details on iterative\nMSE-based layer importance, please refer to NVIDIA (2025).\nFFN and embedding channel importance.\nFFN layers internally are composed of two linear\noperators with a non-linear activation in between:\nFFN(X) = 𝛿\n(︂\nX · 𝑊𝑇\n1\n)︂\n· 𝑊2.\nHere, X denotes the input, and 𝑊1 and 𝑊2 are the two associated weight matrices in the FFN\nlayer. 𝑊1, 𝑊2 ∈R𝑑𝑓𝑓𝑛×𝑑𝑚𝑜𝑑𝑒𝑙, where 𝑑𝑚𝑜𝑑𝑒𝑙and 𝑑𝑓𝑓𝑛are the model hidden dimension and FFN\nhidden dimension respectively. 𝛿(·) refers to the non-linear activation function (squared ReLU in\nthis work).\nFollowing the same procedure as Minitron (Muralidharan et al., 2024), we compute the importance\nof each neuron in the first linear operator of each FFN layer by examining the set of outputs it\nproduces. We use a small calibration dataset of 1024 samples for this purpose. Formally, we compute\neach neuron’s importance score by aggregating its outputs given an input batch 𝑋:\n𝐹(𝑖)\nneuron =\n∑︁\nB,S\n𝛿\n(︂\nX\n(︀𝑊𝑖\n1\n)︀𝑇\n)︂\n.\nHere, 𝑊𝑖\n1 refers to the 𝑖th row of the weight matrix 𝑊1. ∑︀\nB,S refers to aggregation along the\nbatch and sequence dimensions. We use the mean and l2-norm aggregation functions along the\nbatch and sequence dimensions, following the observations in the Minitron paper. For a sequence of\nscores S, mean aggregation is defined as 1\n𝑛\n∑︀𝑛\n𝑖=1 |S𝑖|, and l2-norm is\n√︁∑︀𝑛\n𝑖=1 S2\n𝑖. Embedding channel\nimportance is computed similarly, by examining the outputs of LayerNorm layers instead; we refer\nthe reader to Muralidharan et al. (2024) for more details.\n18\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMamba importance.\nMamba layers process inputs through multiple projection matrices (𝑊𝑥,\n𝑊𝑧, 𝑊𝐵, 𝑊𝐶, 𝑊𝑑𝑡) that produce intermediate representations before causal convolution and selective\nstate space model (SSM) updates, followed by gated normalization and an output projection (𝑊𝑂).\nWe follow the methodology described in Taghibakhshi et al. (2025) for importance estimation:\nspecifically, we adopt a nested activation-based scoring strategy over a small calibration dataset of\n1024 samples, similar to FFN importance but adapted to Mamba’s group-aware structure. First,\nwe obtain activation scores from the 𝑊𝑥projection, denoted 𝑠∈R𝑚ℎ×𝑚𝑑, where 𝑚ℎis the number\nof Mamba heads and 𝑚𝑑is the Mamba head channel dimension. For each channel 𝑑, the score is\ncomputed as\n𝑠𝑑=\n⃦\n⃦\n⃦\n⃦\n⃦\n⃦\n∑︁\nB,S\n𝑠:,𝑑\n⃦\n⃦\n⃦\n⃦\n⃦\n⃦\n2\n,\nwhere the aggregation is over the batch (B) and sequence (S) dimensions, using both mean and\nl2-norm metrics. Next, head scores are computed by using the l2-norm over the Mamba head\nchannel set:\n𝑓ℎ= ‖𝑠ℎ,𝑚𝑑‖2 ,\n∀ℎ∈{1, . . . , 𝑚ℎ},\nand heads are ranked within each Mamba group 𝒢𝑔to preserve group-aware computation semantics:\nℛ𝑔= argsortℎ∈𝒢𝑔(𝑓ℎ).\nwhich ensures that pruning decisions respect the model’s structural constraints and SSM’s sequence\nmodeling. The lowest-scoring heads are pruned by trimming the corresponding rows from all affected\nprojection, convolution, and SSM parameter matrices. This strategy preserves the integrity of the\nSSM block while removing less important Mamba heads. As shown in Taghibakhshi et al. (2025),\npruning Mamba heads yields a better accuracy–throughput trade-off than pruning head channels;\nwe consequently focus on head pruning in this work.\n4.2. Lightweight Neural Architecture Search\nWe first define the constraints and objectives for the Nano 2 model, and then describe our lightweight\nNeural Architecture Search (NAS) framework that finds the most promising architectural candidates\nthat meet our objectives and constraints.\nMemory constraints.\nMemory requirements during inference consist of two distinct components\nwith different scaling behaviors.\nThe parameter memory, while substantial, remains constant\nregardless of the input size. In contrast, the key-value cache memory scales linearly with both batch\nsize and sequence length, often becoming the dominant factor in long-sequence scenarios. For the\nNano 2 model, our goal was to be able to perform inference at a sequence length of 128k and a batch\nsize of at least 1 within a memory budget of 19.66 GiB. We obtained the budget as follows: from the\n22.06 GiB available memory on an NVIDIA A10G GPU, we subtract a 5% buffer for frameworks\nsuch as vLLM and TensorRT-LLM and another 1.3 GiB to allow sufficient space for a vision encoder.\nMeasuring throughput.\nFor the experiments below, unless otherwise specified, we measure\nthroughput on an input and output sequence length of 8k and 16k tokens respectively, which we\nbelieve represents a typical reasoning scenario. For this combination of input and output sequence\nlength, we report vLLM output token generation throughput at the maximum batch size that fits on\nthe A10G GPU.\n19\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n4.2.1. Candidate enumeration.\nOur compression strategy explores multiple axes within the 19.66 GiB memory budget through\ncombinatorial pruning. Our search space includes depth reduction (removing 6-10 layers from the\noriginal 62-layer architecture) combined with width pruning of embedding channels (4480-5120),\nFFN dimension (13440-20480), and Mamba heads (112-128). This multi-axis search space results in\nhundreds of candidate architectures meeting the memory constraint.\n4.2.2. Finding the Best Architecture\nSince performing knowledge distillation and throughput benchmarking on the full set of candidates\nwould be prohibitively expensive, we break down the problem into two parts: (1) find the optimal\ndepth for the compressed model, and (2) find the optimal width-pruned architecture given the depth.\nEffect of depth.\nWe compare the accuracy of three depth-pruned candidates obtained from the\n12B model with 52, 54 and 56 layers. Here, we keep the number of attention layers fixed at 4 for all\nthree variants so as to achieve a good balance between KV cache size and long-context performance;\nprior work has indicated that an attention-to-total-layers ratio between 7-8% is reasonable (NVIDIA,\n2025). We leave the width dimensions untouched for this experiment. Table 9 lists average reasoning\naccuracy at different depths after 6B tokens of distillation; in line with our previous observations on\nthe strong correlation between depth and task performance (Muralidharan et al., 2024; Sreenivas et al.,\n2024), we notice that reducing depth beyond 56 layers results in significant accuracy degradation; as\na result, we fix the depth at 56 for further width pruning.\nAccuracy (Avg)\n52 Layers\n44.92\n54 Layers\n47.35\n56 Layers\n51.48\nTable 9 | Effect of depth on reasoning accuracy. Results are after distilling with 6B tokens.\nCombining depth and width pruning.\nAs described above, we fix the depth of our target\nmodel to 56 layers with 4 attention layers. We perform 60B tokens of distillation on this checkpoint\n(see Section 4.3 for additional details) and perform further width pruning along the embedding, FFN,\nand Mamba axes. We enumerate all candidate pruned architectures that meet our memory budget,\nand sort them in decreasing order of estimated memory consumption at 128k context length and\nbatch size 1. The top 3 candidates from this list are picked for further evaluation: in particular, we\nperform short Knowledge Distillation (KD) on these candidates for 19B tokens after depth+width\npruning; we also benchmark throughput to pick the final architectural candidate. Table 10 lists the\narchitectural details of the top 3 candidates, along with the achieved task performance (post KD)\nand throughput. As shown in the Table, Candidate 2 achieves the best accuracy while still having\nreasonable runtime performance; consequently, we use this architecture for Nano 2.\nFFN vs.\nMamba pruning.\nWe ablate the number of Mamba heads following the recipe\nin Taghibakhshi et al. (2025), considering configurations with 87.5% and 93.75% of the original\nheads. However, due to the relatively smaller compression ratios explored in this work (less than 15%\nafter depth pruning) compared to those in Taghibakhshi et al. (2025) (around 50%), we find that\napplying Mamba head pruning yields limited benefit, and in these cases, pruning only the FFN and\n20\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n#Layers Hidden\nFFN\nMamba #Heads Params. (B) Accuracy Throughput\nCandidate 1\n56\n4480\n17920\n112\n8.92\n59.07\n161.02\nCandidate 2\n56\n4480\n15680\n128\n8.89\n63.02\n156.42\nCandidate 3\n56\n4800\n14400\n120\n8.97\n62.94\n155.86\nTable 10 | Top 3 candidates for architecture selection. Accuracy is the average across reasoning\nbenchmarks after distillation with 19B tokens. The last column shows vLLM output generation\nthroughput (ISL/OSL=8k/16k and batch size=8).\nembedding dimensions—after depth pruning—proves sufficient to achieve the desired compression\nwhile preserving accuracy. Candidates 1 and 2 in Table 10 highlight this difference.\n4.3. Retraining with Distillation\nTo recover the accuracy lost due to pruning, the model undergoes continued training. Recent work\nhas demonstrated that distilling knowledge from the original model to the pruned model outperforms\nconventional fine-tuning (Muralidharan et al., 2024; Sreenivas et al., 2024; Bercovich et al., 2024);\nwe thus adopt logit-based distillation for continued training, employing forward KL divergence loss\nexclusively during the accuracy recovery phase (see §3 of the Minitron paper (Muralidharan et al.,\n2024) for more details on the distillation loss formulation). Building on the candidate selection\nprocess described in §4.2, we continue training Candidate 2 in an extended phase, as detailed below,\nto yield the final Nano 2 reasoning and base models.\n% Reasoning-SFT data\n% Pretraining data\nAccuracy (Avg)\n50\n50\n57.5\n70\n30\n58.5\n90\n10\n57.2\nTable 11 | Effect of varying reasoning data proportion on math accuracy after ∼6B tokens of KD.\nReasoning model.\nThe reasoning model is distilled in stages with increasing sequence lengths to\nstrengthen extended reasoning and long-context capabilities; this is followed by targeted reinforcement\nlearning (RL), preference optimization and model merging to retain desired behaviors and ensure\nrobustness across diverse tasks. We now describe these various stages:\n1. Depth pruning to 56 layers; Knowledge Distillation (KD) with ∼60B tokens at 8,192 sequence\nlength.\n2. Width pruning and KD with:\n• ∼50B tokens at 8,192 sequence length.\n• ∼25B tokens at 49,152 sequence length.\n• ∼1B tokens at 262,144 sequence length.\n3. Direct Preference Optimization (DPO).\n4. Group Relative Policy Optimization (GRPO).\n5. KD with ∼0.4B tokens at 262,144 sequence length to recover post-RL drops.\n6. RLHF for alignment with human preferences.\n7. Model merging between steps 5 and 6 via 0.5 linear interpolation.\n21\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMore details on DPO, GRPO and RLHF can be found in Section 3. Figure 6 shows the effects of\nstaged training on model accuracy across different reasoning benchmarks. Here, the 𝑥-axis represents\nthe various stages (starting from Step 2 above), and the 𝑦-axis shows the scores obtained for the\nvarious benchmarks as training progresses. As shown in the Figure, DPO and GRPO are critical for\nenhancing function-calling (BFCL v3) and instruction-following (IFEval) capabilities, though the\nlatter temporarily degrades multi-task understanding (MMLU-Pro), which is recovered in the next\nstep (post-GRPO KD). Finally, RLHF enhances alignment with human preferences (Arena-Hard)\nbut causes additional benchmark drops, which are then recovered through model merging.\nKD+LCExt\nDPO\nGRPO\nKD\nRLHF\nMerge\nPipeline Stage\n50\n55\n60\n65\n70\n75\n80\n85\n90\nScore (%)\nDistillation Pipeline\nAIME-25\nGPQA-D\nBFCLv3\nIFEval (Pr.)\nMMLU-Pro\nArenaHard\nLiveCodeBench\nFigure 6 | Task accuracy at different stages of the distillation pipeline for Nemotron Nano 2.\nDataset: We observe that a mix of 70% post-training stage 2 data (Section 3.2) and 30% pretraining\n(Section 2.2) data yields the highest accuracy (Table 11). For KD at sequence length 262,144, we\nuse 100% stage 3 post-training data (Section 3.2).\nBase model.\nDistillation proceeds in stages: depth-only pruning and KD on ∼120B tokens,\nfollowed by width pruning and KD on ∼360B tokens (both at sequence length 8,192), and finally\nKD on ∼2.5B tokens at sequence length 524,288 to instill long-context capabilities.\nDataset: Following Sreenivas et al. (2024), we use 100% pretraining data described in sections 2.2\nand 2.6 for distillation of the base model at sequence lengths 8,192 and 524,288, respectively.\n4.4. Results\nWe efficiently compress the 12B model to 9B parameters by pruning full layers (depth), FFN hidden\nsize, and embedding channels, improving inference throughput and enabling long-context inference on\nan NVIDIA A10G GPU. Nemotron-Nano-9B-v2 retains 56 layers of the original model. Additionally,\nthe number of embedding channels were pruned from 5120 to 4480, and FFN intermediate size was\npruned from 20480 to 15680. As shown in Figure 1 and Tables 5 and 6, Nemotron-Nano-9B-v2\nachieves 3×-6× higher throughput than Qwen3-8B for generation-heavy scenarios, while surpassing\n22\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nit in accuracy and remaining comparable to the 12B teacher on most benchmarks.\n5. Conclusion\nIn this report, we introduced Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer reasoning model\nthat achieves comparable or better accuracies at up to 6× higher throughput than existing state-\nof-the-art models such as Qwen3-8B. To create Nemotron-Nano-9B-v2, we started by pre-training\nNemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and\nsynthetically generated data. We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT,\nGRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy\nto produce the final model. As a result of this compression, Nemotron-Nano-9B-v2 can run inference\non context lengths of up to 128k tokens in bfloat16 precision on a single NVIDIA A10G GPU with\n22 GiB of memory. We have open-sourced Nemotron-Nano-9B-v2 along with its corresponding sibling\nNemotron-Nano-9B-v2-Base and parent Nemotron-Nano-12B-v2-Base models, plus the majority of\nits pre- and post-training data on HuggingFace (links at the bottom of Section 1).\nContributors\nWe thank the following people for their invaluable contributions to NVIDIA Nemotron Nano 2.\nData. Abhinav Khattar, Aleksander Ficek, Arham Mehta, Ayush Dattagupta, Brandon Norick,\nDan Su, Daria Gitman, Evelina Bakhturina, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jane Polak\nScowcroft, Jocelyn Huang, Joseph Jennings, Jupinder Parmar, Markus Kliegl, Matvei Novikov,\nMehrzad Samadi, Miguel Martinez, Mohammad Shoeybi, Mostofa Patwary, Pavlo Molchanov, Pritam\nGundecha, Rabeeh Karimi Mahabadi, Ranjit Rajan, Rima Shahbazyan, Sanjeev Satheesh, Sarah\nYurick, Sean Narenthiran, Seungju Han, Shizhe Diao, Shrimai Prabhumoye, Shubham Toshniwal,\nSiddhartha Jain, Somshubra Majumdar, Syeda Nahida Akter, Vahid Noroozi, Vineeth Kalluru,\nVitaly Kurin, Wasi Uddin Ahmad, Wei Du, Ximing Lu, Yejin Choi, Ying Lin.\nFP8. Hua Huang, Jinze Xue, Keith Wyss, Kunlun Li, Mike Chrzanowski, Oleg Rybakov, Przemek\nTredak, Tim Moon, Zhongbo Zhu.\nArchitecture. Bita Darvish Rouhani, Brandon Norick, Duncan Riach, Nidhi Bhatia, Roger Waleffe,\nWonmin Byeon, Ritika Borkar, Xin Dong, Yonggan Fu.\nPretraining. Aarti Basant, Abhijit Paithankar, Abhinav Khattar, Deepak Narayanan, Herman\nSahota, Hexin Wang, Jupinder Parmar, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja,\nRoger Waleffe, Russell J. Hewett, Ryan Prenger, Seonmyeong Bak.\nInfrastructure. Alex Kondratenko, Alex Shaposhnikov, Anubhav Mandarwal, Ashwin Poojary,\nDong Ahn, Gargi Prasad, Haim Elisha, Harsh Sharma, Kumar Anik, Maer Rodrigues de Melo, Ruoxi\nZhang, Shelby Thomas, Stefania Alborghetti, Tony Wang.\nLong Context. Deepak Narayanan, Dima Rekesh, Duncan Riach, John Kamalu, Kezhi Kong,\nMarkus Kliegl, Roger Waleffe, Samuel Kriman.\nInference. Daniel Afrimi, Helen Ngo, Keshav Santhanam, Kushan Ahmadian, Lawrence McAfee,\nLuis Vega, Nave Assaf, Peter Dykas, Shanmugam Ramasamy, Siddharth Singh, Tomer Asida, Vijay\nKorthikanti.\nAlignment. Adi Renduchintala, Ameya Sunil Mahabaleshwarkar, Banghua Zhu, Bilal Kartal, Brian\nYu, Charles Wang, Christian Munley, David Mosallanezhad, Gerald Shen, Hayley Ross, Hoo Chang\nShin, Igor Gitman, Julien Veron Vialard, Junkeun Yi, Kezhi Kong, Luis Vega, Makesh Narsimhan\n23\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nSreedhar, Oleksii Hrinchuk, Oleksii Kuchaiev, Peter Jin, Prasoon Varshney, Ritu Gala, Shuoyang\nDing, Venkat Srinivasan, Vitaly Lavrukhin, Yian Zhang, Yoshi Suhara, Zhen Dong, Zijia Chen.\nCompression. Aditya Malte, Akhiad Bercovich, Akshay Hazare, Ali Taghibakhshi, Ameya Sunil\nMahabaleshwarkar, Ashwath Aithal, Banghua Zhu, Daniel Korzekwa, Deepak Narayanan, Gerald\nShen, Hayley Ross, Julien Veron Vialard, Luis Vega, Marcin Chochowski, Mostofa Patwary, Nima\nTajbakhsh, Oluwatobi Olabiyi, Pavlo Molchanov, Ran El-Yaniv, Roger Waleffe, Saurav Muralidharan,\nSepehr Sameni, Sharath Turuvekere Sreenivas, Tomer Asida, Yashaswi Karnati, Yian Zhang, Yoshi\nSuhara, Zijia Chen.\nSoftware Support. Abhijit Khairnar, Adithya Renduchintala, Ali Taghibakhshi, Anna Shors,\nAshwath Aithal, Balaram Buddharaju, Bobby Chen, Charlie Truong, Deepak Narayanan, Dmytro\nPykhtar, Duncan Riach, Gerald Shen, Helen Ngo, Jared Casper, Jimmy Zhang, Keshav Santhanam,\nKezhi Kong, Lawrence McAfee, Luis Vega, Nima Tajbakhsh, Parth Chadha, Piotr Bialecki, Prashant\nGaikwad, Rajen Patel, Roger Waleffe, Sahil Jain, Terry Kong, Tyler Poon, Vijay Korthikanti, Vikram\nFugro, Yoshi Suhara, Zhiyu Li.\nEvaluations and Safety.\nChristopher Parisien, Dan Su, Daniel Rohrer, Eileen Long, Erick\nGalinkin, Helen Ngo, Katherine Luna, Keshav Santhanam, Kezhi Kong, Leon Derczynski, Marta\nStepniewska-Dziubinska, Meriem Boubdir, Michal Bien, Michael Boone, Michael Evans, Michal Bien,\nMichal Zawalski, Pablo Ribalta, Piotr Januszewski, Pradeep Thalasta, Sanjeev Satheesh, Shaona\nGhosh, Tomasz Hliwiak.\nLegal and Compliance. Barnaby Simkin, Chetan Mungekar, Dina Yared, Iain Cunningham,\nKatherine Cheung, Laya Sleiman, Meredith Price, Michael Boone, Nikki Pope, Ria Cheruvu, Saori\nKaji.\nMarketing. Amelia Barton, Chris Alexiuk, Mark Cai, Nirmal Kumar Juluru, Shreya Gopal.\nProject Management. Alejandra Rico, Amy Shen, Ann Guan, Ashton Sharabiani, Elliott Ning,\nKrzysztof Pawelec, Negar Habibi, Twinkle Vashishth.\nProduct. Arun Venkatesan, Chintan Patel, Chris Alexiuk, Joey Conway, Padmavathy Subramanian,\nUdi Karpas.\nLeadership. Andrew Tao, Boris Ginsburg, Bryan Catanzaro, Eric Chung, Jan Kautz, Joey Conway,\nJonathan Cohen, Kari Briski, Mohammad Shoeybi, Mostofa Patwary, Oleksii Kuchaiev, Pavlo\nMolchanov.\nWe also thank Chen Zhang, Michael Goin, Thomas Parnell from the vLLM team for their assistance.\n24\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"}
{"id": "2508.14427v1", "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "date": "2025-08-20", "authors": ["Wuyang Zhang", "Yexin Tian", "Xiandong Meng", "Mengjie Wang", "Junliang Du"], "source": "http://arxiv.org/abs/2508.14427v1", "page_count": 5, "text": "Knowledge Graph-Infused Fine-Tuning for Structured\nReasoning in Large Language Models\nWuyang Zhang*\nUniversity of Massachusetts Amherst\nAmherst, USA\nYexin Tian\nGeorgia Institute of Technology\nAtlanta, USA\nXiandong Meng\nUniversity of California, Davis\nDavis, USA\nMengjie Wang\nNew York University\nNew York, USA\nJunliang Du\nShanghai Jiao Tong University\nShanghai, China\nAbstract-This paper addresses the problems of missing\nreasoning\nchains\nand\ninsufficient\nentity-level\nsemantic\nunderstanding in large language models when dealing with tasks\nthat require structured knowledge. It proposes a fine-tuning\nalgorithm framework based on knowledge graph injection. The\nmethod builds on pretrained language models and introduces\nstructured graph information for auxiliary learning. A graph\nneural network is used to encode entities and their relations,\nconstructing a graph-based semantic representation. A fusion\nmechanism is then designed to jointly model the knowledge graph\nembeddings\nwith\nthe\ncontextual\nrepresentations\nfrom\nthe\nlanguage model. To enhance the robustness of knowledge\nintegration, a gating mechanism is introduced to dynamically\nbalance the contributions of linguistic semantics and structural\nknowledge. This effectively mitigates conflicts between different\nrepresentational spaces. During training, a joint loss function is\nconstructed to account for both task performance and structural\nalignment objectives. This helps improve the accuracy of entity\nprediction and semantic reasoning. The study also includes a\nseries of systematic sensitivity experiments. It evaluates the\neffects\nof\nlearning\nrate,\ngraph\ncoverage,\nand\nstructural\nperturbations\non\nmodel\nperformance.\nThe\nresults\nfurther\nvalidate the effectiveness and stability of the proposed method\nacross tasks such as entity recognition, question answering, and\nlanguage\ngeneration.\nExperimental\nfindings\nshow\nthat\nthe\nproposed structure-aware fine-tuning framework significantly\nenhances the model's ability to represent complex semantic units.\nIt demonstrates better semantic consistency and contextual logic\nmodeling in scenarios involving structural reasoning and entity\nextraction.\nKeywords: Knowledge injection, large language model fine-\ntuning, structural fusion, entity modeling\nI. INTRODUCTION\nWith the rapid advancement of artificial intelligence, large\nlanguage models (LLMs) have emerged as a core technology in\nnatural language processing (NLP). These models, powered by\nmassive\ndatasets\nand\ncomplex\nparameter\nstructures,\ndemonstrate remarkable capabilities in language understanding,\ngeneration, and reasoning. However, despite their success in\ngeneral-purpose tasks, LLMs still face challenges in domain-\nspecific applications that require structured knowledge. Issues\nsuch as sparse knowledge, incomplete reasoning chains, and\npersistent semantic ambiguities remain unresolved[1]. These\nlimitations largely stem from the inadequate use of explicit\nknowledge during model training, especially the lack of deep\ninjection mechanisms for structured semantic information.\nTherefore, injecting domain knowledge into LLMs in a\nstructured manner has become a key approach to enhancing\ntheir performance in specific application scenarios.\nKnowledge graphs (KGs), as high-level semantic network\nstructures, organize abstract knowledge through entities and\nrelations. They represent complex semantic associations using\nnodes\nand\nedges.\nCompared\nto\ntraditional\ntext-based\nknowledge representation, KGs provide stronger structural\norganization\nand\ncomputability.\nThey\neffectively\nencode\nhierarchical concepts, attribute constraints, and subsumption\nrelations.\nIntegrating\nKGs\ninto\nlanguage\nmodels\ncan\nsignificantly improve the models' entity recognition and\ncontextual understanding. KGs also provide explicit logical\npaths that\nenhance\nthe interpretability and\naccuracy of\nreasoning processes. Thus, combining KGs with LLMs is a\npromising direction for advancing from general semantic\nmodeling to deep domain-specific understanding[2].\nNevertheless, there is an inherent heterogeneity between\nknowledge graphs and LLMs in terms of structure and training\nparadigms. LLMs rely on sequential language inputs, while\nKGs use non-Euclidean graph-based semantic structures.\nAchieving\ndeep\nintegration\nof\nthese\ntwo\nforms\nof\nrepresentation without disrupting the original architecture of\nLLMs remains a major research challenge. Furthermore,\nknowledge graphs themselves suffer from data incompleteness,\nentity sparsity, and structural diversity, which increase the\ncomplexity of knowledge injection. A successful injection\nmechanism must balance knowledge selection, encoding, and\nfusion strategies. It must also align the capabilities of LLMs\nwith the complementary strengths of knowledge graphs[3,4].\nIn real-world applications, many high-risk and knowledge-\nintensive domains, such as financial analysis [5-6], medical\nreasoning [7-8], elastic cloud resource scaling [9-10], demand\ngreater expertise and reasoning from language models. In these\ndomains, models must understand complex terminology and\n\ninfer\ncausal,\nhierarchical,\nand\nevent-based\nrelationships\nbetween entities. Such knowledge is often unavailable in\nunstructured texts and must be supplemented with structured\nsources[11,12]. Therefore, developing a fine-tuning algorithm\nthat dynamically guides the model's cognitive path based on\nKG\nstructures\nand\nmaintains\nsemantic\nconsistency\nand\nknowledge alignment during text generation has both practical\nand theoretical importance. This approach enhances model\nrobustness and generalization in specific tasks. It also bridges\nthe gap between knowledge representation and reasoning\ncapabilities[13].\nMoreover, as AI systems move toward being explainable,\nreliable, and domain-proficient, they face issues such as strong\nblack-box characteristics and weak knowledge controllability.\nTraditional fine-tuning methods rely heavily on data and cannot\nexplicitly use external knowledge. This makes it difficult to\nmeet the safety and verifiability requirements of decision-\ncritical\nscenarios[14].\nUsing\nknowledge\ngraphs\nas\nan\nintermediary to guide information structures enables semantic\ngeneration and knowledge reasoning to be grounded in\nverifiable fact networks. This improves transparency and\ncontrollability\nin\ndecision-making.\nSuch\nintegration\ncomplements\nthe\nconventional\npretraining – fine-tuning\nparadigm. It also provides a practical foundation and cutting-\nedge direction for deploying LLMs in knowledge-intensive\ntasks.\nII. PROPOSED APPROACH\nThis study presents a fine-tuning algorithm for large\nlanguage models, centered on knowledge graph injection, with\nthe goal of strengthening both representation and reasoning\ncapacities by leveraging structured knowledge. The overall\narchitecture draws inspiration from Lyu et al.[15], who\ndemonstrated that modular modeling strategies—combining a\npre-trained\nlanguage\nmodel\nwith\nspecialized\nguidance\nmodules—can\nsignificantly\nimprove\nperformance\non\nknowledge-intensive tasks. In this framework, the architecture\ncomprises two primary components: the pre-trained language\nmodel, which captures contextual semantic representations, and\na knowledge graph guidance module designed to complement\nsemantic units with graph structure and facilitate relational\nreasoning. During training, consistent with the methodology\noutlined by Wang, knowledge graph information is first\nencoded\ninto\ngraph\nembeddings,\nenabling\na\nstructured\nrepresentation of entities and relations. These embeddings are\nthen fused with the latent vectors within the language model,\ndynamically\nguiding\nsemantic\nlearning\nand\nenhancing\nrobustness against structural noise and adversarial input [16].\nTo further enable complex reasoning and support context-\naware understanding, this work adopts a fusion-based approach\nsimilar to that proposed by Sun et al., where multi-source\nknowledge representations are integrated at the semantic level\nfor more effective context–knowledge alignment [17]. The\nsynergy between the language model and the knowledge graph\nguidance module is illustrated in Figure 1, providing a basis for\nstructured\nsemantic\nlearning\nand\nknowledge-augmented\ninference within the proposed framework.\nFigure 1. Overall model architecture diagram\nIn the knowledge encoding part, the triple\n)\n,\n,\n(\nt\nr\nh\nis first\nmapped to a low-dimensional dense vector, and the knowledge\ngraph is encoded through a graph neural network (such as GCN\nor R-GCN), which is formalized as follows:\n)\n1\n(\n)\n(\n)\n(\n)\n(\n)\n1\n(\n\n\n\nv\nN\nu\nl\nu\nl\nr\nuv\nl\nv\nh\nW\nc\nh\n\n(1)\nAmong them,\n)\n(l\nv\nh\nrepresents the representation of node v\nin layer l,\n)\n(v\nN\nis its neighbor node set,\nuv\nc\nis the\nnormalization factor,\n)\n(l\nr\nW\nis the weight matrix related to edge\ntype r , and is the activation function. After multiple layers\nof graph convolution, the obtained knowledge representation\nwill be injected into the language model as a structural prior.\nTo optimize the integration of contextual semantics and\nstructured knowledge, this framework incorporates a gating\nunit that dynamically regulates the fusion ratio between the\nlanguage model's contextual representations and the knowledge\ngraph-derived\ninformation.\nThe\ndesign\nof\nthis\ngating\nmechanism\nis\ninfluenced\nby\nthe\nattention-based\nfusion\nstrategies proposed by Xu et al., who demonstrated that\nadaptive\ngating\ncan\nsignificantly\nimprove\nthe\nselective\nintegration of heterogeneous information sources in complex\nclinical NLP scenarios [18]. By enabling the model to assign\nvariable weights to different knowledge streams, the gating unit\nensures that relevant knowledge is emphasized for each\nspecific context. Additionally, Meng et al. provided insights\ninto robust information fusion in distributed and federated\nlearning settings, where dynamic gating is crucial for handling\nmulti-source\ndata\nand\nenhancing\nanomaly\ndetection\nperformance [19]. Drawing on Zhu’s recent advances in rapid\nadaptation for language models, the gating unit within this\nframework also supports efficient gradient-based adjustment,\nallowing the model to swiftly respond to changes in the\nstructure or content of incoming knowledge graphs [20]. This\ndynamic gating approach helps the model achieve a more\nprecise and context-sensitive balance between linguistic and\nstructural knowledge, ultimately supporting more robust and\nexplainable knowledge integration. Specifically, the context\nrepresentation generated by the language model is defined as\n\nLM\nH\n, and the knowledge graph encoding is defined as\nKG\nH\n.\nThe fused representation is:\nKG\nLM\nfused\nH\nH\nH\n\n\n\n\n\n)\n1\n(\n\n\n(2)\nAmong them,\n)\n]\n;\n[\n(\ng\nKG\nLM\ng\nb\nH\nH\nW\n\n\n\nis the\ngating factor controlled by learnable parameters,\n]\n;\n[ \n\nwhich\nrepresents the vector concatenation operation, and \nis the\nSigmoid function, which is used to ensure that the fusion\nweight is within the\n]\n1\n;\n0\n[\ninterval.\nIn the fine-tuning stage, a joint loss function is applied to\nensure\nthe\nfused\nrepresentations\nare\nwell-optimized\nfor\ndownstream tasks. This loss function combines the main task\nloss—such as language modeling loss—with an additional term\ndesigned to align the model’s outputs with the structured\nknowledge provided by the graph. Qin’s work on hierarchical\nsemantic-structural encoding demonstrates the effectiveness of\nusing such joint objectives, as this approach helps the model\nmaintain both semantic consistency and accurate structural\nalignment throughout the learning process [21]. To make the\nalignment loss more effective for different task scenarios, this\nstudy incorporates principles from Wu, who emphasizes task-\naware structural regularization to facilitate parameter-efficient\nfine-tuning and to adapt flexibly to diverse downstream\nrequirements [22]. Moreover, drawing on the distributed\ntraining strategies highlighted by Zhang et al., the joint loss\nalso supports robust and stable optimization, even when\ndealing with data from varied sources or domains [23]. This\napproach allows the model to preserve both linguistic accuracy\nand structural integrity, as formalized below:\nalign\ntask\ntotal\nL\nL\nL\n\n\n\n\n(4)\nAmong them,\ntask\nL\nrepresents the loss of tasks such as\nlanguage\nmodeling\nor\nquestion\nanswering\n\n\n\ni\ni\nKG\ni\nLM\nalign\nH\nH\nL\n2\n)\n(\n)\n(\n||\n||\nrepresents\nthe\nEuclidean\ndistance alignment term between language representation and\nknowledge representation, and \nis a hyperparameter that\ncontrols the balance of the two types of information during\ntraining.\nTo further improve the model's ability to model graph\nstructure\nrelationships,\na\nknowledge-aware\nattention\nmechanism\nis\nconstructed\nto\nemphasize\nthe\nsemantic\ndependencies between entities. For the i-th word in the context,\nits final representation is:\nj\nj\nj\nT\ni\nj\ni\nVh\nd\nKh\nQh\nz\n\n\n\n)\n)\n(\n)\n(\n(\nsoftmax\n(5)\nAmong them, \nis the entity set related to word i,\nV\nK\nQ\n,\n,\nis the linear transformation matrix of query, key, and\nvalue, respectively, and d is the scaling factor. This mechanism\nexplicitly introduces entity nodes in the graph, so that the\nlanguage model can perceive and rely on the known semantic\nedges in the graph structure during context processing.\nThrough\nthe\nabove\nmodeling\nmethod,\nthe\nproposed\nalgorithm can achieve deep collaboration between the language\nmodel and the knowledge graph at the structural level,\neffectively enhance the model's ability to represent entity\nsemantics, contextual relationships, and reasoning paths, and\nprovide\nbasic\nsupport\nfor\nlanguage\nunderstanding\nand\ngeneration in knowledge-intensive tasks.\nIII. DATASET\nThis study uses the T-REx (Textual Relations Extraction)\nknowledge graph dataset as the primary source of structured\nknowledge. T-REx is a large-scale collection of knowledge\ntriples built from Wikipedia. It contains millions of fact triples\nin the form of entity-relation-entity, covering a wide range of\ngeneral-domain concepts. Each triple is aligned with multiple\nnatural language sentences, which express the fact either\nexplicitly or implicitly. This alignment provides valuable\nsupport for linking knowledge with language.\nThe dataset offers high-quality entity annotations and well-\norganized semantic relations. Its structured representation\nincludes rich hypernym–hyponym links, functional relations,\nand synonym mappings. These characteristics make it suitable\nfor graph neural network modeling. T-REx also provides\naligned sentence pairs for fine-tuning language models. These\npairs help integrate contextual representations of entities with\nthe structure of the graph, supporting the learning of factual\nexpressions and logical connections from text.\nTo improve the relevance of knowledge injection and the\nadaptability to specific tasks, this study filters domain-specific\nsubsets and prunes the graph structure based on T-REx.\nSemantic-dense and structurally clear subgraphs are preserved.\nEntity subgraphs are then constructed for graph neural network\nmodeling. This processing enables the model to receive\nstructured knowledge enhancement signals while preserving\nsemantic completeness. As a result, the model improves its\nability to capture semantic dependencies.\nIV. PERFORMANCE EVALUATION\nThis paper first conducts a comparative experiment, and the\nexperimental results are shown in Table 1.\nTable1. Comparative experimental results\nModel\nQA-Acc\nF1-Score\nBLEU\nKGLM[24]\n78.6%\n74.2%\n21.5%\nDRAGON[25]\n81.3%\n76.8%\n24.1%\nKG-SFT[26]\n83.7%\n78.9%\n26.5%\nOurs\n86.4%\n82.1%\n29.7%\nAs shown in Table 1, the proposed method achieves consistent\nimprovements across key metrics with the integration of\nknowledge graph injection. The QA-Acc reaches 86.4%, nearly\neight points higher than KGLM, confirming the benefit of\nexplicit knowledge modeling for entity relations and semantic\nalignment. The method also yields an F1-Score of 82.1%,\noutperforming models such as DRAGON and KG-SFT, which\nindicates stronger robustness in entity discrimination and\nrelation reconstruction. In addition, the BLEU score rises to\n\n29.7%,\nevidencing\nimproved\ncontent\nconsistency\nand\ncontextual coherence through structured knowledge guidance.\nThese results collectively show that knowledge graph-based\nfine-tuning enhances factual reasoning and semantic reliability,\nthereby\nmitigating\nlimitations\nof\npretrained\nmodels\nin\nknowledge-intensive tasks. The influence of different learning\nrate settings on performance is further illustrated in Figure\n2.\nFigure 2. The impact of different learning rate settings on\nmodel performance\nAs shown in Figure 2, the performance of the proposed\nmodel is significantly affected by different learning rate\nsettings. Under smaller learning rates such as 1e-5 and 5e-5, the\nmodel shows stable and high performance on both QA-Acc and\nF1-Score, maintaining around 85% and 80% respectively. This\nindicates that slower parameter updates help achieve stable\nintegration of structured knowledge. It also avoids fluctuations\nduring the injection process, highlighting the sensitivity of\nknowledge-aware fine-tuning to learning rate settings.\nWhen the learning rate increases to 1e-4, the model reaches\npeak performance across all metrics. This suggests that the\nmodel achieves an optimal balance between semantic modeling\nand structural guidance. The result indicates that in the context\nof graph-based knowledge injection, a moderate learning rate\nhelps the language model effectively integrate knowledge\ngraph information while maintaining coherence and accuracy\nin language modeling.\nHowever, when the learning rate is further increased to 5e-4\nand 1e-3, the model's performance drops significantly. F1\nscores decline rapidly. This may be related to instability in the\nstructured knowledge injection process. Faster gradient updates\nhinder\nthe\nfusion\nmodule\nfrom\nlearning\nlong-range\ndependencies from the knowledge graph. As a result, the model\nloses logical consistency and entity alignment in generation and\nunderstanding tasks.\nOverall, the experimental results suggest that the structured\nand controllable knowledge injection mechanism is highly\nsensitive to the learning rate. A well-chosen learning rate can\npromote collaborative modeling between the language model\nand the graph semantics. It can also improve the model's\nrobustness\nand\ngeneralization\nin\ncomplex\nlanguage\nunderstanding tasks. This provides important insights for\nbuilding structure-aware large models.\nThis paper also gives the impact of graph subgraph\ncoverage on entity prediction accuracy, and the experimental\nresults are shown in Figure 3.\nFigure 3. The impact of graph subgraph coverage on entity\nprediction accuracy\nAs shown in Figure 3, entity prediction accuracy steadily\nincreases as the subgraph coverage rate rises, showing a clear\npositive correlation. When the subgraph coverage is low, such\nas 10% or 30%, the model's ability to recognize entities is\nlimited. The prediction accuracy remains between 71.2% and\n75.8%.\nThis\nsuggests\nthat\nwith\ninsufficient\nstructural\nknowledge, the language model struggles to capture entity\nrelations and semantic dependencies in context.\nWhen the coverage rate reaches 50% to 70%, the accuracy\nimproves rapidly to over 80%. This indicates that enhanced\ngraph structure information significantly improves semantic\nmapping and logical modeling between entities. In this\ncoverage range, the model gains more hypernyms, synonyms,\nand functional relation data. The graph context begins to play a\ncompensatory role, enhancing the completeness of reasoning\npaths and the ability to distinguish entity boundaries.\nAs the coverage increases to 90% and above, the accuracy\ngain becomes marginal, approaching a ceiling of 86.4%. This\nconvergence effect suggests that at high coverage levels, the\ngraph provides nearly complete structural context. The model\nno longer relies on redundant knowledge and consistently\ngenerates high-quality predictions. This reflects the saturation\npoint of structural guidance in improving representation quality.\nV. CONCLUSION\nThis paper addresses the challenges faced by large language\nmodels in knowledge-intensive tasks, particularly the lack of\nstructural information and weak reasoning capability. It\nproposes a fine-tuning method based on knowledge graph\ninjection. By introducing structured knowledge, the method\nenhances pretrained models in a targeted manner, improving\nknowledge completion and logical reasoning during language\nunderstanding, entity recognition, and contextual modeling.\nThe architecture integrates a graph neural network to encode\nthe\nknowledge\ngraph\nand\nfuses\nit\nwith\nthe\nhidden\n\nrepresentations\nof\nthe\nlanguage\nmodel.\nThis\nbuilds\na\nknowledge-aware semantic representation path that promotes\nstructure-controllable and semantically consistent outputs in\nlanguage generation and reasoning.\nExperimental results show that the proposed method\nachieves superior performance across multiple key metrics. It\nperforms particularly\nwell in\nentity\nprediction, question\nanswering accuracy, and semantic generation quality. The\nmodel\ndemonstrates\nstrong\nadaptability\nand\nstability\nconcerning\nknowledge\ninjection.\nSystematic\nsensitivity\nexperiments further validate how the model responds to\nknowledge\nsparsity,\ngraph\nstructure\nperturbations,\nand\nhyperparameter changes. These findings provide quantifiable\nguidance for model deployment and tuning. The results\nindicate that incorporating structured knowledge during fine-\ntuning not only enhances the expression of explicit knowledge\nbut also pushes semantic modeling toward structure-aware\nmechanisms.\nIn broader application contexts, the proposed fine-tuning\nparadigm holds significant value for domains such as financial\nanalysis,\nmedical\ndocument\nprocessing,\nand\nlegal\ntext\nunderstanding. By integrating entities, relations, and logical\nchains from knowledge graphs, language models gain the\nability to decompose, generalize, and reason over complex\nsentences. This enhances the interpretability, controllability,\nand generalization of models in specialized and high-risk\ndomains. It also addresses the problem of discontinuous and\ninconsistent knowledge arising from memory-based reasoning\nin traditional models, offering structural support for intelligent\nsystems in complex tasks.\nFuture research may explore mechanisms for dynamically\ninjecting knowledge graphs, allowing language models to\nadaptively select the most relevant subgraphs based on task\ncontext. Another important direction is to build structure-aware\nfusion paths across modalities and achieve efficient knowledge\ntransfer in low-resource domains. Overall, the structure-\ninjection fine-tuning strategy presented in this paper provides a\nfoundation for building trustworthy, explainable, and domain-\nproficient language models. It also opens new perspectives for\ndeveloping knowledge-enhanced intelligent language systems."}
{"id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "authors": ["Yinghan Zhou", "Weifeng Zhu", "Juan Wen", "Wanli Peng", "Zhengxian Wu", "Yiming Xue"], "source": "http://arxiv.org/abs/2508.14408v1", "page_count": 8, "text": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs\nYinghan Zhou1, Weifeng Zhu1, Juan Wen1, Wanli Peng1, Zhengxian Wu1, Yiming Xue1,\n1College of Information and Electrical Engineering, China Agricultural University\nzhouyh@cau.edu.cn, y146776149@gmail.com, wenjuan@cau.edu.cn, wlpeng@cau.edu.cn, wzxian@cau.edu.cn,\nxueym@cau.edu.cn\nAbstract\nLarge language models (LLMs) have been shown to possess\na degree of self-recognition capability—the ability to identify\nwhether a given text was generated by themselves. Prior work\nhas demonstrated that this capability is reliably expressed un-\nder the Pair Presentation Paradigm (PPP), where the model\nis presented with two texts and asked to choose which one\nit authored. However, performance deteriorates sharply under\nthe Individual Presentation Paradigm (IPP), where the model\nis given a single text to judge authorship. Although this phe-\nnomenon has been observed, its underlying causes have not\nbeen systematically analyzed. In this paper, we first repli-\ncate existing findings to confirm that LLMs struggle to dis-\ntinguish self- from other-generated text under IPP. We then\ninvestigate the reasons for this failure and attribute it to a phe-\nnomenon we term Implicit Territorial Awareness (ITA)—the\nmodel’s latent ability to distinguish self- and other-texts in\nrepresentational space, which remains unexpressed in its out-\nput behavior. To awaken the ITA of LLMs, we propose Cog-\nnitive Surgery (CoSur), a novel framework comprising four\nmain modules: representation extraction, territory construc-\ntion, authorship discrimination and cognitive editing. Exper-\nimental results demonstrate that our proposed method im-\nproves the performance of three different LLMs in the IPP\nscenario, achieving average accuracies of 83.25%, 66.19%,\nand 88.01%, respectively.\ncode — https://github.com/CAU-ISS-Lab/Interpretability-\nof-LLMs/tree/main/CoSur\n1\nIntroduction\nIt has recently been found that large language models\n(LLMs) possess self-recognition capability, enabling them\nto distinguish their own writing (”self text”) from that of\nhumans and other models (”other text”) (Panickssery, Bow-\nman, and Feng 2024; Ackerman and Panickssery 2025). As\nshown in Figure 1, this capability is particularly evident in\nthe Pair Presentation Paradigm (PPP). In this paradigm, the\nmodel is shown two texts, one of which has been generated\nby the model being tested and the other of which has been\ngenerated by a human or another model. Then, the model is\nasked to output the label of the text it thinks it wrote. How-\never, this self-recognition capability encounters a bottleneck\nwhen faced with the Individual Presentation Paradigm (IPP),\nwhere the LLM is shown a single text and asked to indicate\nText:\nWas this text written by yourself?\n(a) Yes (b) no\nIndividual Presentation Paradigm\nText1:\nText2:\nCan you tell me which text you wrote? Please\nanswer with only “1” or “2” and no other text.\nPair Presentation Paradigm\nLLM\nWrong Answers\nCorrect Answers\nLLM\nFigure 1: The prompt diagrams for the pair presentation\nparadigm and individual presentation paradigm.\nwhether the text was generated by itself. In the IPP scenario,\naccuracy is computed by capturing the probabilities assigned\nto the ”Yes” and ”No” tokens, which serves as the primary\nevaluation metric. As reported in Table 1 of Ackerman and\nPanickssery (2025), the base LLM achieves prediction accu-\nracies below 50.3% across four datasets, indicating worse-\nthan-random performance. Despite these observations, prior\nstudies have not conducted an in-depth analysis of this fail-\nure mode. Ackerman and Panickssery (2025) attempted to\naddress this by extracting a self-recognition vector. They\ndemonstrated that steering the model with this vector in the\npositive direction makes it claim authorship, while steering\nit in the negative direction makes it deny authorship, regard-\nless of true authorship. This approach essentially does not\nenhance the self-recognition capability of LLMs. This raises\nan interesting question: Why do LLMs, despite possessing\nself-recognition capability in paired settings, fail in the IPP\nscenario?\nIn this paper, we first empirically verify the existence of\nself-recognition capability in LLMs. Then, we analyze the\nreasons behind the failure of this capability in the IPP sce-\nnario, which is regarded as Implicit Territorial Awareness\n(ITA) of LLMs. The ITA refers to the awareness of the LLM\nto clearly distinguish its own text from that authored by oth-\ners in the feature space, yet fail to reflect this distinction in\nits responses.\nBuilding on this, we further raise a new question: Is it\npossible to enhance the performance of LLMs in the IPP\nscenario by awakening ITA? To explore this, we conduct an\nin-depth analysis of the feature distributions across different\n\ncategories of texts. Interestingly, we find that although these\nfeature distributions are very similar, the internal structure of\nthese features differs significantly across sources. This ob-\nservation suggests that it is possible to identify and extract\nsubspaces where the features exhibit greater differences. In-\nspired by representation editing (Kong et al. 2024a; Wang\net al. 2024a), the model’s output can be aligned with the de-\nsired attributes by directly manipulating its latent representa-\ntions. Based on this, we propose a novel method named Cog-\nnitive Surgery (CoSur) to awaken ITA, thereby enhancing\nthe self-recoginition capability of LLMs in IPP scenarios.\nThe CoSur consists of four modules: representation extrac-\ntion, territory construction, authorship discrimination and\ncognitive editing. Specifically, we first extract the feature\nrepresentation from the final layer of the LLM to construct\nthe feature subspaces (territories) for each text category us-\ning singular value decomposition (SVD). Then, we guide the\nLLM to determine the authorship of a text by calculating the\nprojection energy of its feature vector onto different territo-\nrial regions. Finally, we design the cognitive editing to in-\nduce the LLM to generate the correct response. Extensive\nexperiments on three LLMs demonstrate that CoSur effec-\ntively enhances their self-recognition capability in the IPP\nscenario. Our contributions are as follows:\n• We provide the first in-depth analysis of why LLMs fail\nin the IPP setting, attributing it to unexpressed Implicit\nTerritorial Awareness (ITA).\n• We propose Cognitive Surgery (CoSur), a method that\nawakens ITA to enhance the self-recognition capability\nof LLMs in the IPP scenario. We first identify and con-\nstruct territorial spaces to assist the LLM in determining\nthe authorship of the text. Then, we design cognitive edit-\ning to induce the LLM in generating correct responses.\n• Experiments demonstrate that our proposed method en-\nhances the performance across three different LLMs in\nthe IPP scenario, achieving average accuracy of 83.25%,\n66.19%, and 88.01%, respectively.\n2\nRelated Work\nSelf-Recognition Capability of LLMs\nThe self-recognition capability of LLM refers to their ca-\npacity to identify texts they have generated (Laine et al.\n2024b; Laine, Meinke, and Evans 2023; Wang et al. 2024b;\nCotra 2021). Panickssery et al. (2024) reported that several\nLLMs, including Llama2-7b-chat, demonstrated out-of-the-\nbox (without fine-tuning) self-recognition capabilities us-\ning a summary writing and recognition task. Laine et al.\n(2024a) used more challenging text continuation and recog-\nnition tasks to demonstrate self-recognition abilities in larger\nlanguage models. It highlighted how task success could be\nelicited with different prompts and across different models.\nAckerman et al. (2025) found that the llama3-8b-Instruct\nsucceeded at self-recognition across diverse tasks, whereas\nthe base model performed poorly, especially in the IPP sce-\nnario. They also extracted a ”self-recognition” vector in the\nresidual stream, allowing users to steer the LLM to claim\nor disclaim authorship during generation and to believe or\ndisbelieve that it had written arbitrary texts when reading\nthem. These studies demonstrated that LLMs possess self-\nrecognition capability.\nRepresentation Editing\nRepresentation editing is a class of techniques that directly\nmanipulate the latent representations of a model to improve\nits performance and align it with desired attributes (Kong\net al. 2024b; Wu et al. 2024a). Liang et al. (2024) found\nthat representation editing could control aspects of text gen-\neration, such as safety, sentiment, thematic consistency, and\nlinguistic style. Adila et al. (2024) used embedding editing\nfor general, rather than personalized, alignment to broad hu-\nman preferences, relying on self-generated synthetic data.\nWu et al. (2024b) showed that the representation editing can\neven surpass fine-tuning based methods by intervening on\nhidden representations within the linear subspace defined by\na low-rank projection matrix. Ma et al. (2025) leveraged the\nover-parameterized nature of LLMs to disentangle a style-\nrelevant subspace within the model’s representation space\nto conduct representation editing, ensuring a minimal im-\npact on the original semantics. By applying adaptive editing\nstrengths, they dynamically adjusted the steering vectors in\nthe style subspace to maintain both stylistic fidelity and se-\nmantic integrity. Inspired by representation editing, as long\nas the true authorship of the text is determined, we can di-\nrectly manipulate the LLM’s hidden representations to gen-\nerate the correct response. Based on this, we propose CoSur,\nthe details of which will be introduced in the section 4.\n3\nThe Implicit Territorial Awareness of\nLLMs\nWe believe the LLM demonstrates self-recognition abilities\nbecause it exhibits Territorial Awareness (TA) (Wang et al.\n2022), i.e., when responding to authorship attribution ques-\ntions, the LLM tends to emphasize its own characteristics\nwhile reducing its sensitivity to differences between texts\ngenerated by others. However, in the IPP scenario, when the\nLLM is asked about the authorship of a single text, this ter-\nritorial awareness is not reflected in its response. Therefore,\nwe regard this phenomenon as Implicit Territorial Aware-\nness (ITA).\nWe argue that the diminished TA in the IPP scenario stems\nfrom information loss during the mapping from the feature\nspace to the discrete vocabulary space in the LLM. Formally,\nlet h ∈Rd denote the hidden state vector from the last layer\nof the LLM. The probability distribution P over the vocab-\nulary is obtained via a linear transformation followed by a\nsoftmax function:\nP = softmax(Wh + b),\n(1)\nwhere softmax(·) represents the softmax function, W ∈\nR|V |×d is the output projection matrix, b ∈R|V | is the bias\nterm, and |V | is the vocabulary size. This projection maps\nthe high-dimensional semantic space Rd to the (|V | −1)-\ndimensional probability simplex ∆|V |−1. However, due to\nthe non-invertibility of the softmax function and the limited\n\nFigure 2: Visualization of the layer-wise JS divergence between different text categories using Qwen. The left column shows\nthe JS divergence between the features extracted by directly inputting the text into Qwen, while the right column represents\nthe JS divergence between the features extracted under the IPP scenario. Different colors represent different submodules in\nQwen, including the MLP layer (MLP), attention layer (Attn), and the final output of each layer (Final). “self” represents texts\ngenerated by Qwen.\nrank of W, this transformation inevitably introduces infor-\nmation loss. Additionally, we analyze the mutual informa-\ntion I(h; y) between the hidden state h and the C y and the\nmutual information I(P; y) between the probability distri-\nbution of output tokens P and y. Due to the low-rank projec-\ntion and entropy-reduction effects of the softmax layer, the\ntwo mutual information terms satisfy I(P; y) ≪I(h; y).\nThis prevents the LLM from exhibiting explicit TA in its\noutputs, resulting in ITA. Our findings align with the infor-\nmation bottleneck theory (Tishby, Pereira, and Bialek 2000),\nwhere P acts as a compressed representation that inade-\nquately preserves task-relevant information about the correct\noutput token y.\nWe empirically validate the above findings through an in-\ndepth analysis of the features from different categories of\ntexts. Specifically, We collect 400 texts each from three cat-\negories: (1) Qwen-generated texts, (2) Human-written texts,\n(3) Chatgpt-generated text. These texts are then fed into\nQwen3-8B (Qwen) (2025), and we extract the feature rep-\nresentations from each layer for further analysis.\nFirstly, we map these feature representations to the vo-\ncabulary space to obtain output probability distributions,\nfrom which we calculate the Jensen-Shannon (JS) diver-\ngence (Men´\nendez et al. 1997) between the different distri-\nbutions. As shown in Figure 2(a) and 2(c), a statistically sig-\nnificant separation exists between the distributions of differ-\nent text categories. Furthermore, the T-sne visualization in\nFigure 3(a) and 3(b) demonstrate that the resulting clusters\nexhibit low overlap and clear separation, suggesting that the\nLLM demonstrates a distinct awareness in differentiating its\nown text from others. Additionally, as shown in Figure 2(e)\nand 3(c), we observe that the LLM can also distinguish be-\ntween different categories of others-generated texts.\nSecondly, we extract text features in the IPP scenario to\nvalidate the LLM’s territorial awareness. As shown in the\nbottom row of Figure 3, when the LLM is prompted under\nthe IPP scenario, the boundary between self-generated and\nother generated texts becomes more distinct. In contrast, the\nboundary between human-texts and chatgpt-texts becomes\nblurred. This shift in representational distinctiveness indi-\ncates that the LLM possesses territorial awareness.\nDespite the fact that the above experiments show the LLM\nexhibits strong territorial awareness in the IPP scenario,\nthe JS divergence sharply declines under the IPP both be-\ntween self and other and between different types of oth-\ners as illustrated in the right column Figure 2. This val-\nidates the information loss during the mapping from fea-\ntures to probability distributions. The information loss pre-\nvents the explicit expression of TA, resulting in the observed\nImplicit Territorial Awareness (ITA), which aligns with our\nprevious analysis. Results on other LLMs, including Llama-\n3.1-8B (Llama) (2024), and DeepSeek-R1-0528-Qwen3-8B\n(DeepSeek) (2025), can be found in Appendix A.\nBased on the above analysis, we argue that actively mit-\nigating information loss during the mapping from the fea-\nture space to the discrete vocabulary space can improve\n\nFigure 3: T-sne Visualization of Final Layer Features using Qwen. The top row shows the feature visualizations based on\nplain-text inputs, while the bottom row with the suffix ’ IPP’ represents the features extracted under the IPP scenario.\nthe LLM’s self-recognition capability in the IPP scenario.\nTherefore, we propose CoSur, the details of which will be\nintroduced in the following sections.\n4\nCoSur\nTo activate ITA of LLM in IPP scenario, we propose CoSur,\nwhich consists of four modules: representation extraction,\nterritory construction, authorship discrimination, and cogni-\ntive editing. The framework of CoSur is shown in Figure 4.\nRepresentation Extraction\nLet Ts = {ts,1, ts,2, . . . , ts,N} denotes a set of texts gen-\nerated by the LLM itself, and To = {to,1, to,2, . . . , to,N}\nrepresents a set of texts from other source. Under the IPP\nscenario, each text ts,i ∈Ts and to,i ∈To is indepen-\ndently fed into the LLM to extract the hidden representation\nof its last token from the final layer of the LLM, denoted as\nhs,i ∈Rd and ho,i ∈Rd, respectively:\nhs,i = LLM(ts,i),\nho,i = LLM(to,i)\n(2)\nThe representations for each category are stacked to\nform the sets Hs = {hs,1, hs,2, . . . , hs,N} and Ho =\n{ho,1, ho,2, . . . , ho,N}, respectively.\nCS\nMMD\nCKA\nSelf-human\n0.9434\n0.01\n0.0717\nself-chatgpt\n0.9755\n0.01\n0.0685\nself-llama\n0.9768\n0.01\n0.0808\nself-deepseek\n0.9793\n0.01\n0.0825\nTable 1: The distance between feature representations of dif-\nferent categories of texts in Qwen. ‘A–B’ denotes the dis-\ntance between category A and category B.\nTerritory Construction\nWe believe that due to the ITA of LLMs, there are differ-\nences between self-generated texts and other-generated texts\nin the LLM’s feature space. Therefore, we select 400 sam-\nples from each category and analyze the differences in fea-\nture distributions, extracted from Qwen, between different\ncategories of texts using Cosine Similarity (CS), Maximum\nMean Discrepancy (MMD), and Centered Kernel Alignment\n(CKA). The definitions of these metrics and their measure-\nment results on other LLMs are provided in Appendix B.\nAs shown in Table 1, these features exhibit consistently\nhigh CS and low MMD, indicating that they are highly sim-\nilar in feature space. This also explains why the LLM per-\nforms poorly on IPP scenario. However, the CKA scores be-\n\n��= ��∑���\n��= ��∑� ��\nStep 2: Territory Construction\nStep 4: Cognitive Editing\nOther\nSelf\nProjection diagram\n�\nEdit\n\n�\nStep 3: Authorship Discrimination\n�= � + �∗�������\nEditing direction\n{ Text } Was this text written by yourself?\n\\n a) yes \\n b) no\nStep 1: Representation Extraction\n��= {��,�, . . . , ��,�}\nLarge Language Model\n��= {��,�, . . . , ��,�}\n��\n��\n�\n��= ��\n⊺� 2\n��= ��\n⊺� 2\n��\n��\nOther Text\nSelf Text\n��,��\n��,��\n��,��\n��,��\n��\n��\nTop-�\nFigure 4: The framework of CoSur. (The preliminary stage) Step 1 (Representation Extraction): Extract the final-layer rep-\nresentations of self-generated and other-generated texts from the LLM, denoted as Hs and Ho, respectively. Step 2 (Territory\nExtraction): SVD is applied to Hs and Ho to construct the territory space for each text category, denoted as Vs and Vs,\nrespectively. (Inference stage) Step 3 (Authorship Discrimination): For a given sample t, compute its projection energy onto\nVs and Vs to infer the authorship of t. Step 4 (Cognitive Editing): Edit the feature representation h to approach the target\nresponse, denoted as ˜\nh, thereby promoting the LLM to generate the correct reply.\ntween these representation sets, which evaluate the structural\nsimilarity in the feature space by calculating the similarity\nbetween the Gram matrices of two feature matrices, are no-\ntably low. This reveals a different perspective: although the\nfeatures are close in space, the internal structure of the rep-\nresentations differs significantly across sources. This obser-\nvation suggests that it is possible to identify and extract sub-\nspaces where the representations exhibit greater differences,\nthereby enhancing the distinction between text categories.\nBased on this observation, we apply singular value de-\ncomposition (SVD) to extract the most discriminative com-\nponents between categories.\nHs = UsΣsVs\n⊤,\nHo = UoΣoVo\n⊤\n(3)\nwhere Us ∈RN×N and Uo ∈RN×N are the left singular\nmatrices, Σs ∈RN×d and Σo ∈RN×d are diagonal ma-\ntrices containing the singular values and V⊤\ns ∈Rd×d and\nV⊤\no ∈Rd×d are the right singular matrices.\nWe extract the top-k right singular vectors from V⊤\ns and\nV⊤\no to serve as the bases defining their respective territories\nVs and Vo.\nVs = span(Vs\n(k)),\nVo = span(Vo\n(k))\n(4)\nwhere span(·) represents the set of all possible linear com-\nbinations of the vectors contained in the input set, forming a\nvector space. Vs\n(k) ∈Rd×k and Vo\n(k) ∈Rd×k denote the\nmatrices consisting of the top-k right singular vectors. We\nconduct experiments to examine the impact of the choice of\nk on the results, as detailed in section 5.\nself-human\nself-chatgpt\nself-deepseek\nNGD\n0.8020\n0.7035\n0.6594\nNFD\n0.9055\n0.8371\n0.7931\nTable 2: Measurement of distance between different terri-\ntories. ‘A–B’ denotes the distributional divergence between\ncategory A and category B.\nTo evaluate the representational capacity and separabil-\nity of these subspaces, we compute the Normalized Grass-\nmann Distance (NGD) and Normalized Frobenius Distance\n(NFD) between them. As shown in Table 2, the NGD and\nNFG between these subspaces are large, indicating signifi-\ncant divergence between their corresponding subspaces. The\ndefinitions of the metrics and their measurement results on\nother LLMs are provided in Appendix C.\nAuthorship Discrimination\nWe introduce projection energy E to quantify the intensity\nof text representations projected onto each territory, which\nserves to determine the authorship of a given text. For a\ngiven sample t, the last token feature vector h from the fi-\nnal layer is extracted and its projection energy Es and Eo\nonto Vs and Vo are computed to infer the authorship of t.\nEs = ∥Vs\n⊤h∥2,\nEo = ∥Vo\n⊤h∥2\n(5)\nwhere ∥· ∥2 denotes the Euclidean norm.\n\nself - human\nself - chatgpt\nself - qwen\nself - llama\nself - deepseek\nAverage\nACC\nF1\nACC\nF1\nACC\nF1\nACC\nF1\nACC\nF1\nACC\nF1\nQwen-base\n46.15\n40.25\n32.87\n31.79\n-\n-\n31.88\n29.17\n46.67\n32.78\n39.40\n33.50\nQwen-CoSur\n99.50\n99.50\n94.25\n94.23\n-\n-\n65.74\n61.41\n73.50\n71.50\n83.25\n81.66\nLlama-base\n44.99\n43.69\n50.00\n49.06\n49.73\n48.79\n-\n-\n50.00\n48.66\n48.68\n47.55\nLlama-CoSur\n67.25\n64.69\n76.06\n72.44\n63.48\n58.12\n-\n-\n59.95\n52.64\n66.19\n61.97\nDeepseek-base\n63.64\n56.60\n38.17\n27.63\n45.39\n34.72\n47.97\n37.25\n-\n-\n48.79\n39.05\nDeepseek-CoSur\n98.25\n98.25\n95.00\n94.99\n86.25\n86.20\n72.54\n70.44\n-\n-\n88.01\n87.47\nTable 3: Performance of three LLMs in IPP Scenarios. The ’self-A’ setting refers to evaluating the performance of the target\nLLM in the IPP scenario using a dataset that includes both its self-generated texts and texts generated by A.\nFinally, the authorship of the text t is determined by com-\nparing its projection energies Es and Eo onto the respective\nterritories.\nO(t) =\n\u001a\ns,\nif\nEs > Eo\n,\no,\notherwise\n(6)\nwhere O(t) represents the authorship of t.\nCognitive Editing\nTo guide the LLM toward producing the desired response,\nwe first identify the target tokens toks and toko. The output-\nlayer weight vectors corresponding to the two target tokens,\ndenoted as ws and wo, respectively, are obtained from the\nLLM. We then normalize the two weight vectors as follows:\n˜\nwo =\nwo\n∥wo∥,\n˜\nws =\nws\n∥ws∥\n(7)\nThe target direction dtarget is determined according to\nthe value of O(t):\ndtarget =\n\u001a\n˜\nwo,\nif\nO(t) = o\n,\n˜\nws,\nif\nO(t) = s\n(8)\nThe hidden representation h is steered toward the target\ndirection to obtain the edited representation ˜\nh, thereby facil-\nitating the LLM to output the target token.\n˜\nh = h + α · dtarget\n(9)\nwhere α represents editing strength hyperparameter. We also\nconduct experiments to examine the impact of the choice\nof α on the results, as detailed in section 5. The complete\nalgorithmic procedure is detailed in the Appendix D.\n5\nExperiments\nDataset\nWe incorpate HC3 (Guo et al. 2023), a large-scale com-\nparison dataset comprising responses from both human ex-\nperts and chatgpt across diverse domains, including open-\ndomain, finance, medicine, law, and psychology. Addition-\nally, we use the same set of questions as prompts to generate\nresponses from Qwen, Llama and DeepSeek.\nFigure 5: Visualization of the average accuracy before\n(Base) and after (CoSur) applying Cosur when each LLM\ndistinguishes between its own outputs and those of two dif-\nferent external LLMs.\nExperimental Setting\nWe evaluate the performance of CoSur on three differ-\nent LLMs, including Qwen3-8B (Qwen) (2025), Llama-\n3.1-8B (Llama) (2024), and DeepSeek-R1-0528-Qwen3-8B\n(Deepseek) (2025). We select the top-64 right-singular vec-\ntors to construct the subspace (k = 64) and set the edit-\ning strength α = 100. All experiments are run on two\nNVIDIA A100-SXM4-80GB GPUs. Additioanally, we use\nstricter evaluation metrics, where accuracy (ACC) and F1\nscore (F1) are computed based solely on the output of LLMs,\nrather than the probability of the target token. Since cur-\nrent research lacks targeted methods to enhance the self-\nrecognition capability of LLMs, in this experiment, we use\nthe LLM’s direct performance in the IPP scenario (base) as\nthe baseline to evaluate the performance of CoSur.\nMain Results\nWe evaluate the effectiveness of CoSur on three different\nmainstream LLMs. As shown in Table 3, all three LLMs per-\nform below 50% in the base setting (base) without CoSur on\nthe IPP scenario. After applying CoSur, each LLM demon-\nstrates a significant improvement. Specifically, across four\ntasks, the three LLMs achieve average accuracies of 83.25%,\n\nAverage ACC\nAverage F1\nCoSurCS\n0.8043\n0.7741\nCoSurP CA\n0.75.70\n0.7389\nCoSur\n0.8325\n0.8166\nTable 4: Ablation study using Qwen.\nFigure 6: Accuracy variation with different territorial dimen-\nsions using Qwen.\n66.19%, and 88.01%, with corresponding improvements of\n43.85%, 17.51%, and 39.22%. These results suggest that\nCoSur is effective in mitigating the implicit territorial aware-\nness exhibited by LLMs. However, its improvements are less\npronounced on Llama compared to Qwen and DeepSeek. As\nshown in Appendices A and C, the feature space of LLaMA\nexhibits inherently lower inter-class separability, indicating\na limited capacity for native self-recognition. This constraint\nreduces the effectiveness of subsequent editing operations.\nWe present a case study comparing the top-10 tokens and\ntheir logit changes before and after editing, with detailed re-\nsults provided in the Appendix E.\nGeneralization Results\nWe evaluate the generalization of CoSur through the sce-\nnario below: Given the territories of its own and chatgpt\ntexts, assess the LLM’s ability to distinguish between its\nown text and that of other LLMs. As shown in Figure 5, the\nLLM can determine the authorship of unseen text generated\nby other LLMs based on the known territories (i.e., whether\nit recognizes the text as its own or from another model). We\nargue that this is due to the LLM reinforcing its own ter-\nritorial boundaries while reducing its sensitivity to external\nboundaries during the IPP scenario. This observation is con-\nsistent with the previous analysis.\nAblation Study\nThe key to CoSur’s performance lies in the construction\nof the territory and the authorship discrimination based on\nprojection energy. To evaluate the effectiveness of CoSur,\nFigure 7: Average accuracy variation with different editing\nStrength α using Qwen.\nwe design two variants: (1) CS-based Authorship Iden-\ntification: For a given test sample, its authorship is deter-\nmined by computing the cosine similarity between the sam-\nple and each class center. (2) PCA-based Territory Con-\nstruction: Territories are constructed using Principal Com-\nponent Analysis (PCA), and authorship is determined based\non the projection energy onto the respective territories. As\nshown in Table 4, CoSur outperforms the other two variants.\nWe attribute the relatively poor performance of the cosine\nsimilarity-based variant to the inability of the feature mean\nto capture the critical characteristics that distinguish differ-\nent texts. Additionally, we argue that while PCA focuses on\nmaximizing variance when selecting principal components,\nSVD directly leverages the magnitude of singular values to\ncapture the most important features of the data. Therefore,\nusing SVD to construct the territory is more effective in cap-\nturing the core structure and distinguishing features.\nWe explore the impact of territorial dimensions k on the\nself-recognition capability of LLMs in the IPP scenario us-\ning Qwen. A small k fails to capture all the distinguishing\nfeatures, especially for tasks like self vs ChatGPT and self\nvs human. Conversely, most categories show accuracy de-\nclines after feature saturation due to the introduction of ir-\nrelevant features. Overall, Qwen achieve best average per-\nforms across the four different tasks when k = 64. There-\nfore, this value is adopted in our experiments. We also inves-\ntigate the effect of editing strength α on CoSur using Qwen.\nAs demonstrated in Figure 7, when the editing strength ex-\nceeds 100, the average recognition accuracy across the four\ndifferent text combinations in IPP scenarios remains nearly\nunchanged. This suggests this threshold provides sufficient\nguidance for the LLM to generate target-aligned responses.\nWe further investigated the performance of CoSur on a re-\nlated task, AI-generated text detection, as detailed in Ap-\npendix F.\n\n6\nConclusion\nIn Conclusion, we empirically verify the existence of the\nself-recognition capability of LLMs and analyze the rea-\nsons behind the failure of this capability in the IPP scenario,\nwhich is regarded as the Implicit Territorial Awareness (ITA)\nof LLMs. Base on this, we propose a novel method named\nCognitive Surgery (CoSur) to awaken the ITA in LLMs,\nthereby enhancing LLMs’ performance in the IPP scenario.\nExperimental results demonstrate that CoSur significantly\nenhances LLMs’ performance in IPP scenario."}
{"id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "authors": ["Yupei Yang", "Fan Feng", "Lin Yang", "Wanxi Deng", "Lin Qu", "Biwei Huang", "Shikui Tu", "Lei Xu"], "source": "http://arxiv.org/abs/2508.14391v1", "page_count": 23, "text": "DEPTH: Hallucination-Free Relation Extraction via\nDependency-Aware Sentence Simplification and\nTwo-tiered Hierarchical Refinement\nYupei Yang1,4∗\n, Fan Feng2,3, Lin Yang4, Wanxi Deng4, Lin Qu4\nBiwei Huang2, Shikui Tu1†\n, Lei Xu1\n1Shanghai Jiao Tong University\n2University of California San Diego\n3Mohamed bin Zayed University of Artificial Intelligence\n4Alibaba Group\n{yupei_yang,tushikui,leixu}@sjtu.edu.cn, bih007@ucsd.edu\nffeng1017@gmail.com, zhihe.yyp@alibaba-inc.com\nAbstract\nRelation extraction enables the construction of structured knowledge for many\ndownstream applications. While large language models (LLMs) have shown great\npromise in this domain, most existing methods concentrate on relation classification,\nwhich predicts the semantic relation type between a related entity pair. However,\nwe observe that LLMs often struggle to reliably determine whether a relation exists,\nespecially in cases involving complex sentence structures or intricate semantics,\nwhich leads to spurious predictions. Such hallucinations can introduce noisy edges\nin knowledge graphs, compromising the integrity of structured knowledge and\ndownstream reliability. To address these challenges, we propose DEPTH, a frame-\nwork that integrates Dependency-aware sEntence simPlification and Two-tiered\nHierarchical refinement into the relation extraction pipeline. Given a sentence\nand its candidate entity pairs, DEPTH operates in two stages: (1) the Grounding\nmodule extracts relations for each pair by leveraging their shortest dependency\npath, distilling the sentence into a minimal yet coherent relational context that\nreduces syntactic noise while preserving key semantics; (2) the Refinement module\naggregates all local predictions and revises them based on a holistic understanding\nof the sentence, correcting omissions and inconsistencies. We further introduce\na causality-driven reward model that mitigates reward hacking by disentangling\nspurious correlations, enabling robust fine-tuning via reinforcement learning with\nhuman feedback. Experiments on six benchmarks demonstrate that DEPTH re-\nduces the average hallucination rate to 7.0% while achieving a 17.2% improvement\nin average F1 score over state-of-the-art baselines.\n1\nIntroduction\nRelation extraction, the task of identifying semantic relationships between entities in unstructured\ntext, serves as a fundamental component in natural language processing (NLP) [1–4]. Its applications\nspan a wide range of domains, including social media analysis [5], knowledge graph construction\n[6], and question answering [7]. Traditional relation extraction methods typically rely on large-scale\nannotated datasets, making them costly to deploy and often limited in generalization. Recently,\nLLMs, empowered by increased model capacity and massive pretraining corpora, have demonstrated\n∗This work was done when the author was a research intern at Alibaba Group.\n†Corresponding author\nPreprint. Under review.\n\na new methodology to improve the accuracy of …\nDependency Parsing:\nnsubj\naux\nroot\np\ndet\ndobj\nGrounding\n(methodology, USED-FOR, accuracy)\nDependency-aware Simplification\n(methodology, accuracy)\nEntity Pair\nmethodology → improve →accuracy\nSDP\nDependency Description\nSimplified Sentence\nCausality-guided RLHF\nGrounding\nLLM-based Extraction\nGrounding\n(improve, EVALUATE-FOR, accuracy)\nGrounding\n…\nRefinement\nAggregation\nDependency Description\nPrompt\nResponse\nRLHF\nSelf-Correction\n：Examine a set of predicted\nrelations extracted from a single\nsentence and determine whether any\ncorrections, removals, or additions\nare needed…\nStructed Prompting\nOmission Check\nContradiction Check\nMisclassification Check\nInput\n(methodology, USED-FOR, accuracy),\n(improve, NO-RELATION, accuracy)\n…\nOutput\nFigure 1: The overall framework of DEPTH.\nimpressive capabilities in text understanding and generation. As a result, leveraging LLMs for relation\nextraction has emerged as a promising research direction [8–11].\nHowever, most existing efforts focus on relation classification, assuming that a pair of entities is\nalready known to be related, and asking the model to select the correct relation type from a predefined\nset [12]. In contrast, the broader task of relation extraction requires both identifying whether a\nrelation exists between a given pair of entities and classifying its type. This distinction is crucial\nin real-world scenarios, such as the construction of proprietary knowledge bases, where the set of\nrelated entities is not known a priori. In such cases, models must first detect potential relations before\ncategorizing them, making relation extraction inherently more challenging and practically important.\nUnfortunately, despite achieving high accuracy on relation classification benchmarks, LLMs often fall\nshort in this broader task: they tend to overpredict relations, frequently hallucinating links between\nunrelated entities. These false positives (FP) introduce substantial noise into the extracted knowledge\nand can severely compromise the quality and reliability of downstream applications, especially when\napplied at enterprise-scale document processing.\nTo this end, we propose DEPTH, a framework that integrates Dependency-aware sEntence simPli-\nfication and Two-tiered Hierarchical refinement into the extraction pipeline to reduce hallucinated\nrelations and improve extraction fidelity. Specifically, DEPTH decomposes the task into two stages:\nGrounding and Refinement. The Grounding module focuses on local extraction, aiming to improve\nprediction accuracy for each candidate entity pair. Motivated by the observation that the essential in-\nformation needed to infer a relation between two entities is often captured by the shortest dependency\npath (SDP) connecting them in the dependency tree [13], we perform dependency parsing [14–16] on\nthe input sentence and simplify it by retaining only the SDP and its immediate context. This reduces\ndistracting information while preserving the relational semantics. In addition, we explicitly encode\ndependency-based cues as natural language prompts to guide LLM during relation prediction.\nWhile the Grounding module effectively reduces syntactic noise through sentence simplification and\ndependency-guided prompting, we find that relying solely on localized context remains insufficient.\nLLMs still tend to infer relations based on superficial lexical patterns, such as the frequent co-\noccurrence of certain entity pairs. This reliance on shallow statistical cues, rather than genuine\nsemantic reasoning, often leads to systematic hallucinations. To mitigate this issue, a promising\napproach is reinforcement learning with human feedback (RLHF) [17, 18]. However, the success of\nRLHF crucially depends on the robustness of the reward model (RM). Existing RM training methods,\nunfortunately, are often vulnerable to reward hacking [19], whereby the model captures spurious\ncorrelations instead of truly causal signals. To address this, we adopt a causal perspective on the\nreward modeling process for relation extraction. Specifically, we construct a causal graph of the\nreward model and introduce a causal factorization method that separates each prompt-response pair\ninto reward-relevant and reward-irrelevant components. By training the reward model solely on\nthe relevant parts, we encourage it to focus on features truly indicative of relation existence, while\ndiscarding confounding patterns that do not causally affect the reward. We then apply Proximal Policy\n2\n\nTable 1: Comparison of LLM’s extraction performance on SciERC with and without NO-RELATION\nsamples.\nDataset\nP\nR\nF1\nHR\nD2\n38.5\n36.8\n37.7\n63.2\nD1\n17.8\n17.2\n17.5\n82.8\nOptimization (PPO) [20–23] to fine-tune the LLM based on this robust reward model, significantly\nenhancing the reliability of relation predictions.\nOn the other hand, the Refinement module operates at the sentence level by aggregating the predictions\ngenerated by the Grounding module across all candidate entity pairs. We then prompt the LLM\nto reassess these predictions under global semantic constraints (such as relational transitivity or\nmutual exclusivity), to identify and correct missing, erroneous, or logically inconsistent relations. By\nincorporating a global view of the sentence, the Refinement module equips the model with a form\nof self-correction, allowing it to reconcile local predictions with broader sentence-level coherence,\nwhich we find to be highly effective in mitigating hallucinated relations in our experiments. To\nsummarize, our main contributions are as follows:\n• We propose DEPTH, a two-tiered hierarchical relation extraction framework that combines local\nprecision from the Grounding module with global self-correction via the Refinement module.\n• We introduce a novel integration of LLMs and dependency parsing, leveraging SDPs between\nentities to simplify sentences and distill key relational information, thereby fundamentally reducing\nextraction complexity.\n• We develop a causality-driven reward modeling framework that disentangles reward-relevant and\nreward-irrelevant factors, yielding a robust reward model that resists spurious correlations and\nenables more effective policy optimization.\n• Extensive experiments across multiple benchmarks demonstrate the superiority of DEPTH in\nrelation extraction, particularly in distinguishing relation existence, making it highly applicable to\nreal-world scenarios.\n2\nPreliminaries and Problem Setup\nGiven a sentence s containing a pair of entities (e1, e2), the goal of relation extraction is to identify\nthe semantic relation ρ ∈R that holds between them, where R denotes a predefined set of relation\ntypes. In the context of LLM-based relation extraction, the sentence s, entity pair (e1, e2), and\nrelation set R are encoded into a prompt x, which is presented to a LLM. The model then generates a\ntextual response y, from which the predicted relation ˆ\nρ ∈R is extracted.\nAn essential yet often overlooked requirement in this process is the ability of LLMs not only to\nmake accurate predictions, but also to explicitly say “no” when appropriate. That is, to return\nNO-RELATION when no suitable relation exists among the predefined set R. However, prior work\noften neglects this aspect. Even when models are prompted to return NO-RELATION in uncertain\ncases, few studies evaluate whether LLMs can reliably refrain from hallucinating relations.\nTo dive deeper, we use the SciERC [24] dataset to examine how well LLMs can distinguish the\nexistence of relations. Let D1 denote the set of all possible entity-relation triples (e1, ρ, e2) in the\ndataset, where each ρ falls into one of two categories: (1) ρ ∈R, or (2) ρ = NO-RELATION. We\nfurther construct a subset D2 ⊂D1, where all relations belong to the first category (i.e., ρ ∈R). To\nintuitively quantify the extent to which LLMs hallucinate relations when extracting on NO-RELATION\nsamples, we make the following definition:\nDefinition 1 (Hallucination Rate (HR)). The hallucination rate is the proportion of NO-RELATION\nsamples that are incorrectly predicted as having a relation, calculated as\nHR =\nFN\nTP + FN,\n(1)\nwhere TP denotes the number of samples correctly identified as NO-RELATION, and FN denotes the\nnumber of NO-RELATION samples misclassified as having a relation.\n3\n\nRaw Sentence\nThis paper proposes a new methodology to improve the accuracy of a term aggregation system using each author's\ntext as a coherent corpus.\nDependency Parsing\nDependency-aware Sentence Simplification\n1. remove \"of a term\naggregation system using\neach author‘s text as a\ncoherent corpus.\" according\nto SDP for simplicity.\n2. remove \"the\" for grammar\nfluency.\nPrompting\nSimplified\nThis paper\nproposes a\nnew\nmethodology\nto improve\naccuracy.\n(methodology, accuracy)\nRaw Sentence This paper proposes …\nEntity Pair\nSDP\nThis paper proposes a\nnew methodology to improve the accuracy\nterm\nof\na\naggregation system …\nDET\nDET\nDET\nDET\nNOUN\nNOUN\nNOUN\nNOUN\nNOUN\nNOUN\nVERB\nVERB\nADJ\nPART\nADP\nmethodology\nimprove\naccuracy\nNOUN\nNOUN\nVERB\nFigure 2: An illustration of how the Dependency-aware Simplification module operates.\nTable 1 presents the zero-shot performance of Qwen2.5- 14B-Instruct on both D1 and D2. We\nobserve that the HR increases dramatically by 19.6% when evaluating on D1 compared to D2. This\nsubstantial decline highlights the limited ability of LLMs to distinguish between related and unrelated\nentities. Table 10 in the appendix further provides a comprehensive analysis of the TP, FP, and FN\ncounts for each relation category, thereby shedding light on the origins of the observed performance\ndifferences.\nIn practice, due to the scarcity of labeled data and the high cost of annotation, real-world datasets more\nclosely resemble D1 rather than D2. Such hallucinations can be fatal in enterprise-scale document\nprocessing, where they introduce substantial redundancy into downstream graph structures. Therefore,\nenabling LLMs to accurately discern relation existence is a critical and non-negligible challenge.\n3\nMethodology\nTo address the challenges discussed above, we propose DEPTH, a framework that not only classifies\nsemantic relations accurately, but also equips LLMs with the critical ability to discern whether\na relation exists. DEPTH adopts a two-tiered extraction architecture comprising: (1) a Ground-\ning module that improves local prediction precision through dependency-aware simplification and\ncausality-guided RLHF fine-tuning, and (2) a Refinement module that enforces global consistency\nthrough structured prompting and sentence-level verification. The overall workflow of DEPTH is\nillustrated in Figure 1. In the remainder of this section, we elaborate on each component in detail.\n3.1\nDependency-aware Simplification for Entity-Pair Grounding\nPrevious works typically focus on LLM-centric approaches, such as prompt engineering and super-\nvised fine-tuning, often overlooking the intrinsic value of syntactic structure in the sentence itself.\nIn other words, existing methods perform relation extraction on raw sentences. However, when\nsentences are long or particularly complex, LLMs, especially lightweight models, struggle with\nprecise semantic understanding of sentence details. As a result, hallucination issues often arise during\nrelation extraction.\n4\n\nPretraining Data/ICL Examples\n- POS tagging is compared with named entity recognition.\n- We compare POS tagging and named entity recognition.\n- POS tagging vs. named entity recognition.\nReal Data\n- We apply the model to POS tagging instead of named\nentity recognition tasks.\nLearned Heuristic\n✓“POS tagging” co-occurs with “named\nentity recognition” ⇒COMPARE\nObservation\n✓Co-occurrence of “POS tagging” and\n“named entity recognition”.\nPredicted Relation\nCOMPARE ×\nFigure 3: An example of hallucination in LLM-based extraction caused by co-occurrence.\nSeveral studies in deep learning have shown that dependency parsing provides valuable insights,\nparticularly when entities are distant in a sentence, helping the model better capture and explain\nrelations [25, 26]. Building on this, we apply dependency parsing in LLM-based relation extraction\nto simplify sentence structures and reduce extraction complexity. For a given sentence s, we first\nutilize Spacy [27], an open-source dependency parsing library, to generate its dependency tree. This\ntree encodes the syntactic relations between all words in s as directed edges. Figure 2 illustrates the\ndependency tree for the sentence: “This paper proposes a new methodology to improve the accuracy\nof a term aggregation system using each author’s text as a coherent corpus.”\nFor the entity pair “methodology” and “accuracy”, we extract their SDP: “methodology →improve\n→accuracy”. Since the necessary information to identify the relation between entities is typically\ncontained in the SDP, we then prompt the LLM to simplify the sentence by retaining only the\nessential parts and removing irrelevant details. As an example, the sentence in Figure 2 would be\ntransformed into: “This paper proposes a new methodology to improve accuracy.” To ensure that\nthe LLM comprehends the dependency between entities without losing essential information, we\nfurther convert the SDP into a textual description, thereby enhancing the model’s understanding.\nThe simplified sentence, along with its corresponding SDP description, is then fed into the LLM for\nrelation extraction. A visual illustration of this process is provided in Figure 2.\n3.2\nCausal Reward Modeling for Hallucination-Resistant Extraction\nBeyond insufficient semantic comprehension, another critical factor leading to biased LLM outputs\nis their tendency to infer relationships through superficial lexical patterns (e.g., high-frequency\nco-occurrence of phrases). Once spurious correlations are learned during pre-training or through\nprompting, the presence of identical phrases in real-world samples inevitably triggers incorrect\nrelational inferences. Figure 3 illustrates an in-context learning (ICL) example where the LLM\nincorrectly infers that the co-occurrence of “POS tagging” and “named entity recognition” indicates\na COMPARE relation. Therefore, given the sentence “We apply the model to POS tagging instead of\nnamed entity recognition tasks”, the model invariably predicts the relation between “POS tagging”\nand “named entity recognition” as COMPARE, even though the true relation should be NO-RELATION.\nA widely adopted approach to address such spurious reasoning in LLMs is RLHF, which fine-tunes\nthe model using rewards predicted by a learned reward model. However, the effectiveness of RLHF\ncritically depends on the reliability of the reward model, which remains vulnerable to the same\nspurious patterns it aims to correct. To uncover the root cause of this issue, we examine the reward\nmodel training process through a causal lens. As shown in Figure 4a, let x denote the prompt, y the\ncorresponding response, and r the reward. We decompose the input pair (x, y) into two components:\ns, the reward-relevant factors, and s, the reward-irrelevant ones. Here, s contains the essential\ninformation needed to learn an ideal reward function, while s captures confounding signals, such as\nsequence length or stylistic patterns. This formulation highlights that reward hacking arises from the\nunintended causal path s →r. Hence, a robust reward model must explicitly block this influence to\navoid learning from spurious correlations.\nTo achieve this, we decompose both the prompt x and the response y into two parts: x = (x1, x2) and\ny = (y1, y2), where x1 and y1 are reward-relevant components, and x2 and y2 are reward-irrelevant\nones. In the context of relation extraction, we empirically define x1 as the minimal input required for\nrelation prediction, including task definitions, the simplified sentence, the target entity pair, and a\nnatural language description of their dependency path. The remaining instruction components, such\n5\n\n𝑥\n𝑦\n𝑠\n𝑠\n𝑟\n(a) Standard RM training.\n𝑥\n𝑦\n𝑥2\n𝑥1\n𝑦1\n𝑦2\n𝑟\n❌\n❌\n(b) Factorized RM training.\nFigure 4: Causal diagrams of the standard RM training process and our Causal Reward Modeling\nmethod.\nas broader extraction guidelines, are assigned to x2. For the LLM-generated output, y1 corresponds to\nthe predicted relation ˆ\nρ, while y2 includes any additional content such as explanations or formatting\ndetails. Full implementation details of this factorization are provided in Appendix B.1.\nDuring the training process, we optimize the reward model using only x1 and y1, effectively eliminat-\ning spurious correlations and transitioning from the causal structure in Figure 4a to that in Figure 4b.\nA key advantage of this approach is that the factorization is applied at the input level, requiring no\nchanges to the training architecture and incurring no additional computational cost. Experimental\nresults in Section 4 validate the quality and robustness of the learned reward model.\n3.3\nGlobal Consistency Refinement via Structured Prompting\nWhile the Grounding module significantly improves relation prediction accuracy for individual entity\npairs, treating each pair in isolation can lead to inconsistencies or omissions due to the absence of\nglobal semantic constraints. Therefore, we introduce the Refinement module, which aggregates local\npredictions and integrates sentence-level dependency structures to form a hierarchical correction\nmechanism. This enables global consistency enforcement across predicted relations and facilitates\nthe recovery of overlooked relations.\nFor a given sentence s, the Refinement module first aggregates all candidate relations predicted by the\nGrounding module (see Figure 1). We then convert the dependency tree of s, generated by Spacy,\ninto a natural language description. This description, combined with the aggregated entity pairs,\nforms the input prompt for the LLM, guiding it to perform three-stage semantic calibration: (1)\nOmission check: identify relations overlooked by the initial predictions by leveraging the full-sentence\nsyntactic structure; (2) Contradiction check: detect and resolve logically inconsistent relations (e.g.,\nmutually exclusive facts), retaining the most coherent prediction based on dependency and context;\n(3) Misclassification check: correct errors arising from local context bias. The entire refinement\nprocess is implemented via ICL, with detailed prompt templates available in Appendix D.\n4\nExperiments\nIn this section, we present the experimental results of DEPTH on various relation extraction bench-\nmarks3. Specifically, our evaluation is designed to answer the following research questions:\n• Q1: Can DEPTH accurately determine whether a relation exists between two entities?\n• Q2: Does DEPTH contribute to improved extraction performance?\n• Q3: Is the learned reward model robust to spurious correlations?\n• Q4: Can DEPTH function as a general-purpose relation extraction model that eliminates the need\nfor task-specific training across datasets?\n3Code is available at https://github.com/CMACH508/DEPTH.\n6\n\n4.1\nExperimental Setup\nDatasets.\nWe evaluate DEPTH across a diverse collection of relation extraction benchmarks,\nincluding three news-domain datasets: TACRED [28], TACREV [29], and Re-TACRED [30]; two\nscientific-domain datasets: SciERC [24] and FOBIE [31]; and one general-domain dataset: SemEval\n[32]. Detailed descriptions of each dataset are provided in Appendix A. Table 6 summarizes key\nstatistics for these datasets, such as the number of relation types, sizes of training and test splits, and\nthe proportion of entity pairs labeled as NO-RELATION.\nRLHF-based Fine-tuning.\nWe construct the preference datasets from the training splits to first\ntrain the reward model, followed by policy optimization using PPO guided by the reward model.\nBoth reward model and PPO training are based on Qwen2.5-14B-Instruct, a strong decoder-only\nLLM that has demonstrated effectiveness across various NLP tasks including relation extraction.\nOur implementation builds upon the OpenRLHF package. Detailed information about the dataset\nconstruction, hyperparameters, and training costs are provided in Appendix B.\nEvaluation Metrics.\nFor all benchmarks, we evaluate model performance on the test sets using\nprecision (P), recall (R), and micro-F1, which are widely adopted for relation extraction. In particular,\nto assess the model’s ability to discern relation existence, we pay special attention to the hallucination\nrate (HR) in addition to overall metrics.\nBaselines.\nWe compare DEPTH against the following state-of-the-art relation extraction baselines:\n• QA4RE [8] and SUMASK [9]: zero-shot methods that reformulate relation extraction as question\nanswering via prompt design;\n• GPTRE [33]: an ICL method that enhances relation extraction by incorporating task-aware\ndemonstration retrieval and goal-labeled reasoning;\n• InstructUIE [34]: an end-to-end information extraction framework that constructs expert-written\ninstructions to fine-tune LLMs.\n4.2\nMain Results\nDEPTH consistently demonstrates superior ability to detect relation existence across all bench-\nmarks (Q1).\nTable 2 presents the results on NO-RELATION cases, where DEPTH attains an average\nprecision of 89.0%, recall of 93.0%, F1 score of 90.9%, and achieves a hallucination rate as low as\n7.0%. The remarkably low hallucination rate demonstrates that DEPTH seldom assigns spurious\nrelations to unrelated entity pairs, thus effectively alleviating overconfidence-induced hallucination\nobserved in LLM-based extraction.\nTable 2: DEPTH’s performance on relation existence detection across various datasets.\nBenchmark\nP\nR\nF1\nHR\nTACRED\n87.8\n96.4\n91.9\n3.6\nTACREV\n91.3\n97.4\n94.3\n2.6\nRe-TACRED\n90.7\n90.4\n90.5\n9.7\nSciERC\n79.1\n87.4\n83.1\n12.6\nFOBIE\n95.3\n97.3\n96.3\n2.7\nSemEval\n89.7\n88.9\n89.3\n11.1\nAverage\n89.0\n93.0\n90.9\n7.0\nDEPTH achieves an average improvement of 17.2% in F1 score (Q2).\nTable 3 presents the\ndetailed results. By incorporating dependency-aware simplification, DEPTH effectively alleviates the\nstructural complexity of the extraction task. In addition, the two-tiered refinement mechanism further\nenhances the accuracy of the final predictions. These components jointly lead to the strong overall\nperformance of DEPTH, which attains an average F1 score of 88.3%.\nRemarkably, DEPTH delivers superior extraction performance with only an 14B-parameter model,\noutperforming baselines based on substantially larger backbones. This demonstrates the efficiency\n7\n\nTable 3: Micro-F1 scores of different methods across six relation extraction datasets. Scores marked\nwith † are reproduced.\nMethod\nBackbone\nTACRED\nTACREV\nRe-TACRED\nSciERC\nFOBIE\nSemEval\nAvg\nDEPTH\nQwen2.5-14B-Instruct\n86.2\n90.0\n87.7\n76.3\n94.3\n95.3\n88.3\nQA4RE\nText-davinci-003\n59.4\n59.4\n67.3\n51.7†\n58.5†\n44.1\n56.7\nSUMASK\nGPT-3.5-turbo-0301\n79.6\n75.1\n73.8\n55.8†\n68.2†\n59.2†\n68.6\nGPTRE\nText-davinci-003\n72.1\n68.4†\n61.6†\n69.0\n63.5†\n91.9\n71.1\nInstructUIE\nFlan-T5-XXL\n65.1†\n66.3†\n50.4†\n45.2\n65.0†\n73.2\n60.9\nand effectiveness of our framework. Furthermore, to showcase the applicability of DEPTH across a\nvariety of LLMs, we conduct experiments using different model sizes and architectures. The results\nfor various backbones are reported in Appendix C.2.\nAnother notable observation is that the improvements are particularly pronounced on scientific-\ndomain datasets. This may be attributed to the increased complexity and domain specificity of\nscientific texts, where relation types are often more technical and syntactic structures more intricate.\nSuch characteristics present greater challenges and demand deeper sentence-level understanding from\nthe LLM. These findings underscore the effectiveness of DEPTH in processing complex or specialized\ncorpora, especially in practical scenarios such as private or enterprise document extraction.\nDEPTH learns a high-quality reward model that is robust to spurious correlations (Q3).\nOur\ncausal factorization approach restructures each prompt–response pair in the preference dataset to\nextract the minimal sufficient information required for reward model training. This is implemented\nvia template-based matching, ensuring that variations in guideline length or the inclusion of verbose\nexplanations do not affect the factorization process. As a result, DEPTH effectively shields the reward\nmodel from common spurious factors such as prompt length, formatting artifacts, and sycophantic\nphrasing. Figure 5 reports the pairwise accuracy of the reward model throughout training across\nall benchmarks. Here, the x-axis denotes the normalized training steps, and the y-axis reflects the\nproportion of instances in which the chosen response is correctly assigned a higher reward than the\nrejected one. Our causal reward model achieves an average accuracy of 98.8% across these tasks,\nunderscoring its robustness. Such consistency would be unattainable if the model were driven by\nspurious correlations.\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized training step\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\nRM training accuracy\nTACRED\nTACREV\nRe-TACRED\nSciERC\nDEPTH-CRM\nFOBIE\nSemEval\nFigure 5: Pairwise accuracy during RM training.\nDEPTH supports model sharing across datasets within similar domains (Q4).\nIn practical\napplications, training a separate task-specific LLM for each dataset is prohibitively expensive,\nhighlighting the need for a general-purpose relation extraction framework. To evaluate the cross-\ndataset applicability of DEPTH, we jointly train a single model on multiple datasets within the same\ndomain, and evaluate its performance on each individual test set. Specifically, we choose NYT11\n[35] and Re-TACRED as representatives for the news domain, and use SciERC and FOBIE for the\nscientific domain. Table 4 reports results on both types of datasets. We observe that the jointly trained\nDEPTH model remains competitive, and even shows slight improvements in some cases compared to\ntraining separate models. These findings suggest that DEPTH strikes a favorable balance between\n8\n\ntraining efficiency and extraction accuracy, making it well-suited for real-world deployment where\ncost-effective scalability is essential.\nTable 4: DEPTH’s Performance when jointly trained on datasets from the same domain.\nDomain\nBenchmark\nP\nR\nF1\nHR\nNews\nNYT11\n79.1\n81.3\n80.2\n18.7\nRe-TACRED\n81.9\n96.9\n88.8\n3.1\nAverage\n80.5\n89.1\n84.5\n10.9\nScientific\nSciERC\n70.4\n92.9\n80.1\n7.1\nFOBIE\n94.4\n96.2\n95.3\n3.8\nAverage\n82.4\n94.5\n87.7\n5.5\n4.3\nAblation Studies\nTo assess the contribution of each component, we further conduct ablation studies on the SciERC\ndataset. Detailed results are reported in Table 5.\nImpact of Dependency-Aware Simplification.\nTo examine the effect of sentence simplification and\ndependency-guided prompting on relation extraction, we remove all dependency parsing components\nfrom the DEPTH framework, resulting in a variant denoted as DEPTH-DP. Compared to the full\nmodel, DEPTH-DP yields a decrease of 8.1% in precision, 9.3% in recall, and 8.7% in F1 score,\nthereby resulting in more hallucinations during extraction. This can be attributed to the syntactic\ncomplexity of many input sentences, which often causes LLMs to miss crucial relational cues during\nclassification. These results suggest that leveraging SDPs for structural simplification effectively\nenhances the model’s sentence understanding and leads to more accurate extraction.\nImpact of Causal Reward Modeling.\nWhile previous results confirm that causal factorization\nenables the reward model to be robust against spurious correlations, we further investigate the impact\nof removing this component. Specifically, we ablate the Causal Reward Modeling module, resulting in\na variant denoted as DEPTH-CRM. As expected, this modification leads to a substantial performance\ndecline: precision, recall, and F1 score drop by 12.7%, 18.8%, and 15.5%, respectively. Notably, the\nhallucination rate rises to over 31%. Figure 5 illustrates the training dynamics of the reward model by\nplotting its pairwise accuracy over training steps. Without causal factorization, the accuracy plateaus\nat 92.8%, which is 4.3% lower than the model trained with causal factorization. Such performance is\ninadequate to support effective RLHF fine-tuning. These findings highlight the critical importance of\nincorporating Causal Reward Modeling into DEPTH, particularly in ICL scenarios where prompts\nmay contain large amounts of irrelevant or confounding information.\nImpact of Global Consistency Refinement.\nThe DEPTH-Refinement row in Table 5 corresponds\nto the ablation variant where the Refinement module is removed. Compared to the full DEPTH\nmodel, we observe consistent drops in precision, recall, and F1 score, indicating that the Refinement\nmodule provides meaningful improvements across all metrics. These results demonstrate that the\nRefinement step successfully verifies and self-corrects the predictions from the Grounding module,\nthereby enhancing overall extraction quality. Additionally, Appendix D provides case studies to\nfurther illustrate how this refinement process operates in practice.\n5\nRelated Work\n5.1\nRelation Extraction with Large Language Models\nIn recent years, LLMs have attracted significant attention for their potential in relation extraction.\nTechniques in this direction include, but are not limited to, data augmentation [36], zero-shot learning\n[8, 9], in-context learning [33], and supervised fine-tuning [37]. However, most prior work has\nfocused on relatively simplified relation classification settings, with limited exploration of more\nchallenging scenarios such as Relation Strict [12]. In this work, we aim to advance LLM-based\n9\n\nTable 5: Ablation study results on the SciERC dataset.\nModel\nP\nR\nF1\nHR\nDEPTH\n79.1\n87.4\n83.1\n12.6\nDEPTH-DP\n71.0\n78.1\n74.4\n21.9\nDEPTH-CRM\n66.4\n68.6\n67.5\n31.4\nDEPTH-Refinement\n72.5\n84.9\n78.2\n15.1\nrelation extraction by equipping models with the ability to discern whether a relation exists between\nentities—an essential step toward building more general and production-ready extraction systems at\nscale.\n5.2\nReward Hacking\nReward hacking arises when a reward model assigns high scores to outputs based on irrelevant or\nweakly correlated features, rather than truly aligning with the intended objectives. In the context of\nLLM alignment, typical sources of reward hacking include length [38], formatting [39], sycophancy\n[40], and superficial conceptual matches [41]. In recent years, several studies have aimed to mit-\nigate reward hacking, with one prominent direction focusing on causal analyses of its underlying\nmechanisms. For example, RMM [42] introduces a data augmentation strategy grounded in a causal\nframework of reward model training to improve learning quality. Ovinnikov et al. [43] and Wang\net al. [44] incorporate causal invariance principles by introducing regularization constraints during\ntraining to encourage reward functions that are robust to spurious correlations. CAA [45] treats the\nreward model as an instrumental variable to causally intervene on the LLM, effectively removing\nbiases induced by confounding factors.\nIn contrast to these approaches, our method employs structured causal factorization to directly\ndisentangle reward-relevant and reward-irrelevant components in the input signals. This allows for\nthe construction of a robust reward model without the need for extra training objectives or elaborate\ndata preprocessing, ensuring both stability and efficiency during training.\n6\nConclusion and Future Work\nIn this paper, we present DEPTH, a methodology designed to mitigate the hallucination phenomenon\nin LLM-based relation extraction, where models tend to erroneously assign relations to entity pairs\nlabeled as NO-RELATION. In particular, DEPTH adopts a two-tiered refinement framework: the\nGrounding module enhances local prediction accuracy via dependency-aware sentence simplification\nand RLHF fine-tuning guided by a causally trained, robust reward model; the Refinement module\nperforms sentence-level integration and applies global semantic constraints to verify and self-correct\nthe outputs from the Grounding stage. Extensive experiments on a suite of well-established bench-\nmarks validate the effectiveness of DEPTH in improving both extraction precision and robustness\nto hallucination. The main limitation of this work is the assumption that all entity mentions are\nprovided in advance, rather than being automatically identified via named entity recognition (NER).\nFuture research directions include integrating NER into the pipeline to construct a fully end-to-end\nrelation extraction framework, extending DEPTH to more challenging settings such as document-level\nextraction and none-of-the-above detection, and applying it to downstream tasks including RAG."}
{"id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "authors": ["Ke Fang", "Tianyi Zhao", "Lu Cheng"], "source": "http://arxiv.org/abs/2508.14390v1", "page_count": 11, "text": "Credence Calibration Game? Calibrating Large Language Models through\nStructured Play\nKe Fang1, Tianyi Zhao2, Lu Cheng3\n1University of Pennsylvania, Philadelphia, PA, USA\n2University of Southern California, Los Angeles, CA, USA\n3University of Illinois Chicago, Chicago, IL, USA\nCorrespondence: fangke@seas.upenn.edu\nAbstract\nAs Large Language Models (LLMs) are in-\ncreasingly deployed in decision-critical do-\nmains, it becomes essential to ensure that their\nconfidence estimates faithfully correspond to\ntheir actual correctness. Existing calibration\nmethods have primarily focused on post-hoc\nadjustments or auxiliary model training; how-\never, many of these approaches necessitate ad-\nditional supervision or parameter updates. In\nthis work, we propose a novel prompt-based\ncalibration framework inspired by the Cre-\ndence Calibration Game. Our method estab-\nlishes a structured interaction loop wherein\nLLMs receive feedback based on the align-\nment of their predicted confidence with cor-\nrectness.\nThrough feedback-driven prompt-\ning and natural language summaries of prior\nperformance, our framework dynamically im-\nproves model calibration. Extensive experi-\nments across models and game configurations\ndemonstrate consistent improvements in evalu-\nation metrics. Our results highlight the poten-\ntial of game-based prompting as an effective\nstrategy for LLM calibration. Code and data\nare available at https://anonymous.4open.\nscience/r/LLM-Calibration/.\n1\nIntroduction\nThe growing deployment of Large Language Mod-\nels (LLMs) in decision-critical domains neces-\nsitates not only correct answers but also well-\ncalibrated confidence estimates. While LLMs of-\nten demonstrate impressive reasoning capabilities,\ntheir confidence estimates frequently misalign with\nactual correctness, exhibiting a tendency towards\noverconfidence in incorrect answers and undercon-\nfidence in correct ones (Kadavath et al., 2022). Ad-\ndressing this calibration issue has attracted signifi-\ncant attention, leading to various proposed strate-\ngies, including post-processing methods (Guo et al.,\n2017; Shen et al., 2024), which adjust the model’s\nconfidence level after inference, and techniques re-\nlying on third-party proxies (Xie et al., 2024). A\nkey limitation of many existing strategies, however,\nis their dependence on additional training proce-\ndures or external models, which restricts flexibility\nand broad applicability (Xie et al., 2024). Conse-\nquently, there is a clear need for lightweight, self-\ncontained calibration methods.\nIn this work, we propose a prompt-based strategy\nto improve model calibration in a non-parametric\nand feedback-driven manner. Our approach is in-\nspired by the Credence Calibration Game (Critch\net al., 2012), originally designed to calibrate hu-\nman judgment by encouraging participants to ex-\npress not only their answers but also their de-\ngree of confidence. In the original setting, par-\nticipants are scored based on both correctness and\nconfidence-receiving higher rewards for accurate\nanswers made with high certainty, while incurring\nlarger penalties for incorrect answers given with\nhigher confidence. This mechanism incentivizes\ntruthful expression of subjective confidence level.\nWe adapt this framework to LLMs, which often\nsuffer from overconfidence in incorrect answers\nand underconfidence in correct ones like humans.\nBy providing the model with structured feedback\nover multiple rounds and incorporating summaries\nof previous game history into each input prompt,\nour strategy encourages LLM to dynamically ad-\njust its confidence estimation over time. This work\nadapts the core principles of the Credence Calibra-\ntion Game into a fully prompt-based, self-reflective\nframework for calibrating language models.\nIn summary, our main contributions are:\n• We propose a novel prompt-based framework\nfor LLM calibration, inspired by the Credence\nCalibration Game, which enables lightweight\nand efficient confidence calibration.\n• We evaluate the proposed method across a\nrange of LLMs with varying architectures and\nscales on multiple datasets in the settings of\n\nMulti-Choice Question Answering (MCQA)\nand open-ended QA. We find the consistent\nperformance improvements, demonstrating its\neffectiveness.\n2\nRelated Work\nA wide range of approaches have been proposed\nfor LLM calibration. Existing efforts span a wide\nrange of techniques, from post-hoc calibration and\nauxiliary model training to prompt-based inter-\nventions and self-assessment schemes. A signifi-\ncant body of work centers around post-processing\nbased calibration. One of the earliest methods is\nHistogram Binning (Zadrozny and Elkan, 2001),\nwhich adjusts predicted confidences by assessing\ncorrectness rates within predefined bins. Jiang et al.\n(2011) generalized this with Isotonic Regression,\nenforcing a monotonic mapping from uncalibrated\nconfidence scores to empirical accuracy estimates.\nTo address calibration across heterogeneous data\nsubgroups, Úrsula Hébert-Johnson et al. (2018)\nproposed multicalibration, which iteratively refines\ngroup-wise predictions to match subgroup accu-\nracies. More recently, (Detommaso et al., 2024)\nextended this by clustering inputs into semantically\ncoherent groups and applying grouped histogram\nbinning for local calibration improvements.\nBeyond post-hoc strategies, some approaches\nleverage third-party proxy models to refine confi-\ndence estimates. DeJong et al. (1996) trained neu-\nral networks on small batches of data to estimate\nthe conditional correctness probability P(pcorr |\npcon), which can then be used to adjust new pre-\ndictions. Küppers et al. (2021) adopted a Bayesian\napproach, estimating posterior confidence using\na learned distribution over validation data. These\nmethods often assume that validation data are repre-\nsentative of test distributions, which may not hold\nin dynamic or task-shifting settings. To address\nthis, Ulmer et al. (2024) proposed minimizing the\nmean squared error between confidence and ac-\ncuracy by clustering similar questions and using\nintra-cluster accuracy as a correction signal. Ye\nand Durrett (2022) used random forests to gener-\nate local explanations of predictions and dynami-\ncally adjust confidence based on extracted features.\nSome methods shift calibration objectives toward\nhuman-related metrics. For example, Hashemi et al.\n(2024) designed a questionnaire-based feedback\nframework where humans and LLMs collabora-\ntively assess the confidence validity of predictions.\nParametric calibration methods have also been\nwidely adopted. Guo et al. (2017) applied a tem-\nperature parameter T to soften the logit outputs,\naligning the softmax confidence with empirical cor-\nrectness. Shen et al. (2024) extended this idea in the\nThermometer framework by learning a lightweight\nauxiliary model using data from multiple tasks to\ncalibrate LLM outputs in a generalizable way.\nSeveral recent works explore novel strategies\nfor calibration. Stangel et al. (2025) proposed Re-\nwarding Doubt, which penalizes overconfident in-\ncorrect predictions through an reinforcement learn-\ning based reward model. Similarly, CONQORD\n(Tao et al., 2024) leverages reinforcement learn-\ning with a dual reward to align confidence with\nresponse quality, promoting more trustworthy and\nselective LLM behavior. In a different direction,\nACTCAB (Liu et al., 2024a) trains a linear layer on\ntop of the language model’s last-layer activations\nto improve confidence estimation, while LITCAB\n(Liu et al., 2024b) enhances calibration using a\nlightweight bias layer added to the model’s output\nlogits. Complementary to these methods, prompt-\nbased approaches such as self-calibration (Kada-\nvath et al., 2022) provide a new perspective for\nimproving model calibration.\nBuilding on this growing body of research, our\nwork explores an approach that integrates struc-\ntured feedback into the prompting process through\na game-inspired interaction scheme, which encour-\nages the model to adapt its confidence estimation\nbehavior based on its own performance history. Our\nwork extends the principles of the Credence Cali-\nbration Game into the realm of LLMs, introducing\na prompt-based calibration framework that simu-\nlates the game’s structured feedback loop. In con-\ntrast to previous approaches, our method is:\n• Non-intrusive:\nIt preserves the original\nmodel weights and does not require any addi-\ntional training or fine-tuning.\n• Prompt-based:\nCalibration is achieved\npurely through prompt-level intervention,\navoiding the need for auxiliary models (De-\nJong et al., 1996; Küppers et al., 2021), clus-\ntering heuristics (Ulmer et al., 2024; Detom-\nmaso et al., 2024), or reinforcement learning\nobjectives (Stangel et al., 2025; Tao et al.,\n2024).\n• Self-adaptive: By incorporating the model’s\nown performance history into the prompt, the\n\nmethod encourages iterative self-correction of\nconfidence estimates in an in-context learning\nmanner.\nThis framework offers a lightweight and broadly\napplicable alternative for LLM calibration. To our\nknowledge, it represents the first attempt to trans-\nlate the core ideas of the Credence Calibration\nGame(Critch et al., 2012) into a purely prompt-\ndriven, self-reflective calibration strategy for lan-\nguage models.\n3\nMethodology\n3.1\nPreliminary: The Original Credence\nCalibration Game\nThe Credence Calibration Game (Critch et al.,\n2012) is originally developed to improve the align-\nment between human confidence estimates and ac-\ntual correctness. In each round, a player answers a\nmultiple-choice question and reports a confidence\nlevel, typically ranging from 50% (pure guess) to\n99% (near certainty). The scoring mechanism pro-\nvides feedback based on both correctness and ex-\npressed confidence: correct answers yield higher re-\nwards when reported with higher confidence, while\nincorrect answers result in steeper penalties as con-\nfidence increases. This structure incentivizes hon-\nest self-assessment, encouraging participants to ex-\npress their true confidence about being correct.\nThe game uses proper scoring rules that math-\nematically guarantee the best strategy is to report\none’s actual belief. Over repeated rounds, players\nreceive numerical feedback and gradually learn to\nadjust their confidence to more accurately reflect\ntheir knowledge.\nThis feedback-based interaction loop forms the\nconceptual foundation of our approach. In the\nnext subsection, we describe how this structure is\nadapted into a prompt-based interaction framework\nfor language models.\n3.2\nCalibration Game for LLMs Calibration\nInstead of modifying model weights through fine-\ntuning or applying post-hoc calibration techniques,\nwe propose a novel prompt-based approach to dy-\nnamically adjust a model’s self-assessment behav-\nior. We adapt the game framework to LLMs based\non the intuition that, like humans, LLMs produce\noutputs with varying degrees of confidence. Since\nLLMs often exhibit miscalibrated confidence, the\nfeedback-driven mechanism of the Credence Cal-\nibration Game provides a natural foundation for\nConfidence\n50%\n60%\n70%\n80%\n90%\n99%\nSymmetric Scoring\nIf correct\n0\n+26\n+49\n+68\n+85\n+99\nIf incorrect\n0\n-26\n-49\n-68\n-85\n-99\nExponential Scoring\nIf correct\n0\n+26\n+49\n+68\n+85\n+99\nIf incorrect\n0\n-32\n-74\n-132\n-232\n-564\nTable 1: Scoring rules of the Credence Calibration\nGame under symmetric and exponential scoring strate-\ngies.\nencouraging better self-assessment. In each round,\nthe model answers a question, reports its confi-\ndence on a scale from 50% to 99%, and then re-\nceives feedback in the form of a score that reflects\nthe alignment between its confidence and actual\ncorrectness. Scoring follows a core principle: If\nthe answer is correct, higher confidence yields a\nhigher score. If the answer is incorrect, higher\nconfidence incurs a larger penalty.\nFormally, following the Credence Calibra-\ntion Game, we define confidence levels c\n∈\n{50, 60, 70, 80, 90, 99} and assign scores s(c)\nbased on correctness. We operationalize this scor-\ning framework through two different reward sys-\ntems, as summarized in Table 1:\nSymmetric Scoring\nCorrect answers are re-\nwarded and incorrect answers are penalized by the\nsame magnitude based on the model’s reported con-\nfidence. For instance, a correct prediction with 90%\nconfidence earns +85 points, while an incorrect\none with the same confidence incurs −85 points.\nscorrect(c) = −swrong(c).\n(1)\nExponential Scoring\nIncorrect answers are pe-\nnalized more severely to discourage unjustified\noverconfidence. Grounded in information theory,\nthe penalty for an incorrect prediction at confidence\nc is approximately proportional to −log2\n\u0000 1−c\n0.5\n\u0001\n.\nThis quantifies the misleading information relative\nto a 50% prior belief – where higher confidence in\nwrong answers yields exponentially larger penal-\nties. Consequently, swrong(c) grows faster than lin-\near, enforcing stronger calibration pressure. For\ninstance, 90% confidence results in +85 points for\na correct answer but −232 points for an incorrect\none, and 99% confidence yields +99 versus −564.\n\n3.3\nProblem Formulation\nThe goal of our work is to improve the calibra-\ntion of LLMs without altering model parameters\nor relying on external models. A well-calibrated\nmodel should express high confidence only when\npredictions are likely correct, and low confidence\notherwise.\nFormally, let f be an LLM that takes as input\na question qi and contextual information ci, and\noutputs both a predicted answer ˆ\nyi and a confidence\nscore pi ∈[0, 1]:\n(ˆ\nyi, pi) = f(qi, ci).\n(2)\nwhere yi is the ground truth label.\nGiven a dataset D = (qi, yi)N\ni=1 of N evalua-\ntion samples, our aim is to enhance the alignment\nbetween the model’s confidence pi and the actual\ncorrectness indicator 1(yi = ˆ\nyi) across the dataset.\nCrucially, our intervention is prompt-based: cali-\nbration improvements are achieved through natural\nlanguage feedback on previous predictions, with-\nout gradient updates. Model outputs are compared\nbefore and after the intervention to assess perfor-\nmance differences.\n3.4\nOverall Framework\nAs illustrated in Fig. 1, the framework unfolds in\nthree sequential stages: (1) Pre-Game Evaluation;\n(2) Calibration Game; (3) Post-Game Evaluation.\nPre-Game Evaluation\nWe begin by establishing\nthe model’s baseline calibration performance. The\nmodel is prompted to answer a set of benchmark-\ning questions while also providing a self-reported\nconfidence score between 0 and 100. No feedback\nor history is provided at this stage, ensuring that the\nresponses reflect the model’s unconditioned behav-\nior. The collected predictions and normalized con-\nfidence scores are then used to compute standard\ncalibration metrics, capturing the model’s initial\nalignment between confidence and correctness.\nCalibration Game\nThe model then enters a struc-\ntured interaction loop. In each round, it tackles\nmultiple-choice questions adapted from the origi-\nnal Credence Calibration Game (Critch et al., 2012)\nand receives a short feedback summary covering\nits aggregate performance indicators such as cur-\nrent game score, mean confidence, and calibration\nstatus (e.g., “You are currently overconfident”).\nMean confidence is computed as the average of\nthe model’s reported confidences across all game\nquestions attempted so far in the current game, and\ncalibration status is determined by comparing this\ncumulative average confidence with the correspond-\ning average accuracy. Detailed rules and an exam-\nple game history are provided in Appendices A.1\nand A.2. Importantly, this prompt-conditioning\nmechanism adapts self-assessment behavior with-\nout parameter updates.\nPost-Game Evaluation\nFinally, we repeat the\ninitial evaluation, but augment the prompt with a\nconcise summary of the model’s own game his-\ntory (accuracy, average confidence, total score, and\nover/under-confidence trend). This tests whether\nthe model can leverage its prior performance to\nadjust subsequent confidence estimates.\nThis framework enables a non-parametric refine-\nment of calibration: without fine-tuning or addi-\ntional supervision, the model learns to reflect on\nits behavior and improve through contextual cues\nalone.\n4\nExperiments\nIn this section, we evaluate our approach by an-\nswering the following research questions:\nRQ1. Can our game-inspired prompting strategy\nimprove LLM calibration compared to standard\nprompting and baselines while keeping model per-\nformance stable? (see Section 5.1)\nRQ2. How do reward design affect learning\ndynamics and final outcomes across model sizes?\n(see Section 5.2)\nRQ3. How sensitive is calibration to the number\nof questions per game, and how does this affect\naccuracy? (see Section 5.3)\n4.1\nExperimental Setup\nDatasets\nWe evaluate our framework using\nbenchmark datasets MMLU-Pro (Wang et al.,\n2024) and TriviaQA (Joshi et al., 2017), which\ntogether span both structured multiple-choice and\nopen-domain question answering tasks.\nMMLU-Pro is an enhanced version of the Mas-\nsive Multitask Language Understanding (MMLU)\n(Hendrycks et al., 2021) benchmark, designed to\nmore rigorously test language models on broad and\ncomplex reasoning tasks. Compared to the orig-\ninal MMLU, MMLU-Pro significantly increases\ntask difficulty by expanding the number of answer\nchoices per question from 4 to 10, thereby reducing\nthe likelihood of random guessing and encouraging\nmore precise model understanding. The dataset\n\nFigure 1: Overall workflow of our calibration framework.\ncomprises over 12,000 carefully curated multiple-\nchoice questions sourced from standardized exams\nand academic textbooks, covering 14 diverse do-\nmains. In our experiments, we randomly sample\nchemistry and computer science as evaluation sub-\njects.\nTriviaQA is an open-domain question an-\nswering dataset featuring approximately 95,000\nquestion-answer pairs collected from trivia enthu-\nsiasts. Each question is accompanied by multi-\nple evidence documents sourced from web and\nWikipedia pages, designed to simulate real-world\ninformation retrieval scenarios. Compared to ear-\nlier reading comprehension benchmarks, TriviaQA\nincludes more complex and compositional ques-\ntions, often requiring cross-sentence reasoning and\nexhibiting substantial variation in phrasing between\nquestions and supporting evidence. These charac-\nteristics make it a strong benchmark for testing\nfactual knowledge and reasoning ability. To evalu-\nate calibration under minimal external support, we\nuse the rc.wikipedia.nocontext configuration from\nthe validation split. This subset focuses on ques-\ntions where the model must rely on its own knowl-\nedge, which aligns well with our evaluation goal\nthat assessing confidence calibration under mini-\nmal context and maximal reliance on the model’s\ninternal knowledge.\nBackbone Models\nOur experiments cover a\nrange of model scales using two instruction-tuned\nLLM families: Llama3.1 (Grattafiori et al., 2024),\nevaluated at 8b and 70b parameter sizes, and\nQwen2.5 (Yang et al., 2025), evaluated at 7b and\n72b. These models were selected based on three\nconsiderations: (1) both are open-weight models\nwith strong performance across reasoning and fac-\ntual knowledge benchmarks, making them repre-\nsentative of current frontier LLM capabilities; (2)\nthey support instruction-following tasks, which\naligns well with our prompt-based calibration set-\nting; and (3) their availability on public inference\nplatforms ensures reproducibility and accessibility\nfor future research. All evaluations are conducted\nin a zero-shot setup with fixed prompts. For each\nquestion, the model is instructed to output the an-\nswer index (for MMLU-Pro multiple-choice ques-\ntions) or text answer (for TriviaQA open-ended\nquestions), along with a confidence score between\n0 and 100. These verbalized confidences are nor-\nmalized to the [0, 1] range for evaluation. All ex-\nperiments are conducted via API-based inference\nusing the Together AI1 platform, with generation\nparameters fixed at temperature 0.7, top-p 1.0, and\na maximum of 1024 tokens.\nBaseline\nWe use self-calibration (Kadavath et al.,\n2022) as our prompt-based baseline. This approach\nprompts models to reflect on their answers’ validity:\nafter responding to a question, the model assesses\nits correctness through follow-up queries like \"Is\nyour previous answer correct?\" The model’s self-\nassessment serves as its confidence signal, provid-\ning a simple, interpretable benchmark. While there\nexist numerous calibration methods—ranging from\npost-hoc statistical techniques to auxiliary mod-\nels and reinforcement learning frameworks—our\nwork focuses specifically on prompt-based calibra-\ntion strategies that require no model fine-tuning or\narchitectural changes. Therefore, we choose self-\n1https://www.together.ai\n\ncalibration as a strong and conceptually related\nbaseline: it shares the same prompt-level interven-\ntion paradigm as our approach, allowing a fair and\ndirect comparison in the context of black-box LLM\ninference.\nEvaluation Metrics\nWe assess prediction qual-\nity and calibration with four metrics: Expected\nCalibration Error (ECE) (Naeini et al., 2015),\nBrier Score (Brier, 1950), Accuracy, and AU-\nROC (Fawcett, 2006). Mathematical definitions\nof these metrics are provided in Appendix A.3.\nThese metrics were chosen to provide a com-\nprehensive evaluation of both the accuracy and the\ncalibration quality of model predictions. Accuracy\nmeasures the overall correctness of answers, re-\nflecting task performance directly. ECE and Brier\nScore are widely adopted metrics that quantify how\nwell the predicted confidence aligns with the true\ncorrectness likelihood, capturing the reliability of\nthe confidence estimates. ECE focuses on the av-\nerage confidence-accuracy gap across confidence\nbins, while Brier Score penalizes the squared dif-\nference between confidence and actual outcomes,\noffering a complementary perspective. AUROC\nevaluates the model’s ability to rank correct predic-\ntions with higher confidence than incorrect ones,\nwhich is important for scenarios where confidence-\nbased decision thresholds or ranking are applied.\nTogether, these metrics provide a balanced assess-\nment of both predictive correctness and confidence\ncalibration.\n5\nResults and Analysis\n5.1\nGame-Based Calibration Results\nTable 2 presents a comprehensive evaluation of\ncalibration performance across various models and\ndatasets.\nThe proposed framework effectively improves\ncalibration performance.\nBoth game-based\nmethods reliably lower ECE and Brier Score in\nmost settings, confirming their overall effective-\nness. In particular, the Game-Exp variant consis-\ntently achieves the lowest ECE in 6 out of 8 blocks,\nshowing that heavier penalties for overconfidence\nhelp models become more cautious and thus better\ncalibrated. For example, Qwen2.5-7b on TriviaQA\nachieves a 10.80% reduction in ECE under expo-\nnential scoring, outperforming symmetric scoring.\nSelf-calibration, in contrast, yields only modest cal-\nibration gains, typically 1–2%. It can, however,\nboost accuracy in select cases (e.g., Llama3.1-8B\non TriviaQA with a 2.45% improvement), hint-\ning at a trade-off: small accuracy benefits without\nmeaningful shifts in confidence calibration.\nThe effect of model size.\nModel size also mat-\nters. The 70B/72B variants generally exhibit larger\nimprovements than their smaller counterparts, par-\nticularly on calibration metrics, suggesting that big-\nger models have more latent capacity to adjust their\nconfidence estimates when properly incentivized.\nModel performance stays stable despite im-\nproved calibration.\nAccuracy and AUROC\nchange little across calibration strategies compared\nto the uncalibrated LLMs. Accuracy depends only\non whether the predicted label is correct, and AU-\nROC depends on the relative ordering of confidence\nscores—not their absolute values. Because our in-\nterventions rescale confidence magnitudes without\naltering predicted labels or their rank order, the\nmodel’s discriminative power is preserved. Thus,\ncalibration improvements enhance confidence re-\nliability without compromising downstream task\nperformance. Notably, Self-Cal achieves slightly\nhigher accuracy than other methods. This could\nbe attributed to the additional reasoning introduced\nby self-reflection prompts, which can occasionally\nimprove answer quality—highlighting a trade-off\nbetween direct confidence calibration and indirect\ngains in task performance.\nIn summary, our game-based calibration ap-\nproach consistently improves model calibration,\nwith Game-Exp yielding more aggressive gains at\nsome cost to accuracy, while Game-Sym strikes a\nmore conservative balance.\n5.2\nEffect of different reward systems\nWe visualize the total game score trajectories of\ndifferent models throughout the game under both\nreward systems. As shown in Figure 2, models ex-\nhibit varied learning curves across the two settings\nduring the game playing process. Larger models\n(e.g., Llama3.1-70b) tend to quickly converge to\nhigh scores, while smaller models (e.g., Llama3.1-\n8b) demonstrate more volatile trends, especially\nunder exponential scoring where the steep penal-\nties for over/under-confidence can outweigh grad-\nual gains from correct answers, sometimes even\nleading to lower scores after several rounds. This\nsuggests that model capacity not only affects the\nability to process the feedback signals embedded\nin the game, but also interacts strongly with the\n\nModel\nDataset\nSize\nMethod\nECE ↓\nBrier ↓\nAUROC ↑\nAccuracy ↑\nLLaMA3.1\nMMLU-pro\n8b\nUncalibrated\n0.6171\n0.5924\n0.5441\n0.2972\nSelf-Cal(2022)\n0.5993 (-1.78%)\n0.5755 (-1.69%)\n0.5596 (+1.55%)\n0.3162 (+1.90%)\nGame-Sym\n0.5635 (-5.36%)\n0.5084 (-8.40%)\n0.5466 (+0.25%)\n0.2592 (-3.80%)\nGame-Exp\n0.5391 (-7.80%)\n0.4918 (-10.06%)\n0.5540 (+0.99%)\n0.2815 (-1.57%)\n70b\nUncalibrated\n0.5985\n0.5952\n0.5229\n0.3870\nSelf-Cal(2022)\n0.5771 (-2.14%)\n0.5774 (-1.78%)\n0.5067 (-1.62%)\n0.4124 (+2.54%)\nGame-Sym\n0.5382 (-6.03%)\n0.5150 (-8.02%)\n0.5847 (+6.18%)\n0.3611 (-2.59%)\nGame-Exp\n0.5324 (-6.61%)\n0.5072 (-8.80%)\n0.5931 (+7.02%)\n0.3597 (-2.73%)\nTriviaQA\n8b\nUncalibrated\n0.2581\n0.2743\n0.6451\n0.6654\nSelf-Cal(2022)\n0.2512 (-0.69%)\n0.2622 (-1.21%)\n0.6430 (-0.21%)\n0.6899 (+2.45%)\nGame-Sym\n0.2066 (-5.15%)\n0.2547 (-1.96%)\n0.6712 (+2.61%)\n0.6478 (-1.76%)\nGame-Exp\n0.2039 (-5.42%)\n0.2546 (-1.97%)\n0.6561 (+1.10%)\n0.6489 (-1.65%)\n70b\nUncalibrated\n0.1762\n0.1799\n0.6196\n0.8028\nSelf-Cal(2022)\n0.1765 (+0.03%)\n0.1823 (+0.24%)\n0.5781 (-4.15%)\n0.8105 (+0.77%)\nGame-Sym\n0.0965 (-7.97%)\n0.1514 (-2.85%)\n0.6567 (+3.71%)\n0.8145 (+1.17%)\nGame-Exp\n0.0886 (-8.76%)\n0.1523 (-2.76%)\n0.6471 (+2.75%)\n0.8123 (+0.95%)\nQwen2.5\nMMLU-pro\n7b\nUncalibrated\n0.6396\n0.6236\n0.5606\n0.3086\nSelf-Cal(2022)\n0.6344 (-0.52%)\n0.6230 (-0.06%)\n0.5458 (-1.48%)\n0.3181 (+0.95%)\nGame-Sym\n0.5438 (-9.58%)\n0.5084 (-11.52%)\n0.6039 (+4.33%)\n0.2795 (-2.91%)\nGame-Exp\n0.5100 (-12.96%)\n0.4876 (-13.60%)\n0.5674 (+0.68%)\n0.2929 (-1.57%)\n72b\nUncalibrated\n0.5168\n0.5128\n0.5494\n0.4460\nSelf-Cal(2022)\n0.5035 (-1.33%)\n0.5006 (-1.22%)\n0.5504 (+0.10%)\n0.4624 (+1.64%)\nGame-Sym\n0.3876 (-12.92%)\n0.3856 (-12.72%)\n0.6029 (+5.35%)\n0.4222 (-2.38%)\nGame-Exp\n0.4095 (-10.73%)\n0.4018 (-11.10%)\n0.6111 (+6.17%)\n0.4136 (-3.24%)\nTriviaQA\n7b\nUncalibrated\n0.4526\n0.4356\n0.6845\n0.4964\nSelf-Cal(2022)\n0.4325 (-2.01%)\n0.4202 (-1.54%)\n0.6791 (-0.54%)\n0.5278 (+3.14%)\nGame-Sym\n0.3868 (-6.58%)\n0.3862 (-4.94%)\n0.6206 (-6.39%)\n0.5014 (+0.50%)\nGame-Exp\n0.3446 (-10.80%)\n0.3551 (-8.05%)\n0.6239 (-6.06%)\n0.5014 (+0.50%)\n72b\nUncalibrated\n0.2049\n0.2177\n0.6624\n0.7572\nSelf-Cal(2022)\n0.1975 (-0.74%)\n0.2116 (-0.61%)\n0.6451 (-1.73%)\n0.7672 (+1.00%)\nGame-Sym\n0.1477 (-5.72%)\n0.2005 (-1.72%)\n0.6537 (-0.87%)\n0.7451 (-1.21%)\nGame-Exp\n0.1531 (-5.18%)\n0.2033 (-1.44%)\n0.6526 (-0.98%)\n0.7431 (-1.41%)\nTable 2: Calibration performance comparison across different LLMs, datasets, and methods. ↓indicates that lower\nvalues are better, while ↑indicates that higher values are better. “Uncalibrated” denotes raw model predictions\nwithout calibration; “Self-Cal” is the self-calibration baseline (Kadavath et al., 2022); “Game-Sym” and “Game-Exp”\nare game-based prompting methods with symmetric and exponential rewards, respectively. Values in parentheses\ndenote the relative change compared to the uncalibrated baseline. Best values in each block are highlighted in bold.\nFigure 2: Cumulative game scores across rounds for each model. Left: Symmetric scoring. Right: Exponential\nscoring. Larger models generally achieve higher and more stable total scores.\nchoice of reward model: symmetric scoring offers\na more forgiving path for weaker models, while\nexponential scoring can accelerate improvement\nfor stronger ones but destabilize smaller LLMs.\n5.3\nEffect of Round Size in Calibration\nGames\nWe conduct an ablation study comparing differ-\nent number of questions in the calibration game.\nSpecifically, we evaluate the effect of playing either\n5 or 50 questions per game using symmetric scor-\n\nSetting\nECE ↓\nBrier ↓\nAUROC ↑\nAccuracy ↑\nLLaMA3.1-8b (MMLU-pro)\nUncalibrated\n0.6171\n0.5924\n0.5441\n0.2972\n5 Questions\n0.5807 (-3.64%)\n0.5357 (-5.67%)\n0.5431 (-0.10%)\n0.2750 (-2.22%)\n50 Questions\n0.5635 (-5.36%)\n0.5084 (-8.40%)\n0.5466 (+0.25%)\n0.2592 (-3.80%)\nLLaMA3.1-8b (TriviaQA)\nUncalibrated\n0.2581\n0.2743\n0.6451\n0.6654\n5 Questions\n0.2321 (-2.60%)\n0.2612 (-1.31%)\n0.6905 (+4.54%)\n0.6455 (-1.99%)\n50 Questions\n0.2066 (-5.15%)\n0.2547 (-1.96%)\n0.6712 (+2.61%)\n0.6478 (-1.76%)\nQwen2.5-7b (MMLU-pro)\nUncalibrated\n0.6396\n0.6236\n0.5606\n0.3086\n5 Questions\n0.5309 (-10.87%)\n0.5318 (-9.18%)\n0.5975 (+3.69%)\n0.2907 (-1.79%)\n50 Questions\n0.5438 (-9.58%)\n0.5084 (-11.52%)\n0.6039 (+4.33%)\n0.2795 (-2.91%)\nQwen2.5-7b (TriviaQA)\nUncalibrated\n0.4526\n0.4356\n0.6845\n0.4964\n5 Questions\n0.3872 (-6.54%)\n0.3908 (-4.48%)\n0.5968 (-8.77%)\n0.4847 (-1.17%)\n50 Questions\n0.3868 (-6.58%)\n0.3862 (-4.94%)\n0.6206 (-6.39%)\n0.5014 (+0.50%)\nTable 3: Ablation study comparing different round sizes (5 vs. 50 questions) under symmetric scoring. Best values\nin each block are bolded.\ning strategy, which provides smoother feedback\nand better isolates the effect of round size without\nintroducing steep penalties from exponential scor-\ning. The experiment is performed on Llama3.1-8b\nand Qwen2.5-7b. Table 3 presents the results.\nOverall, increasing the number of questions from\n5 to 50 leads to consistently better calibration\noutcomes, especially in terms of ECE and Brier\nScore. For example, Llama3.1-8b on MMLU-Pro\nimproves its ECE from 0.5807 to 0.5635 when\nmoving from 5-question to 50-question rounds, cor-\nresponding to a 1.72% reduction. This pattern sug-\ngests that longer game rounds provide more robust\nfeedback signals, allowing the model to better cali-\nbrate its confidence estimates. With only 5 game\nquestions, the model receives limited information\nfor under or over confidence, leading to less consis-\ntent improvements. On the other hand, 50-question\nrounds offer richer performance-based feedback,\nencouraging more stable calibration adjustments.\nHowever, improvements in accuracy remain\nmodest or even slightly negative, consistent with\nthe trade-off observed in Section 5.2. The best ac-\ncuracy in each setting often still comes from the\nuncalibrated or shorter-round variants, highlighting\nthat better calibration does not always align with\nbetter predictive performance.\nThese findings suggest that increasing the num-\nber of questions per calibration game enhances the\nmodel’s ability to self-adjust its confidence, partic-\nularly in terms of ECE and Brier Score, while im-\nprovements in AUROC and accuracy appear more\ntask-dependent.\nVariance of the results. The experimental results\npresented above are consistent across different runs.\nAs shown in Appendix A.4, we repeated the calibra-\ntion experiment five times under identical settings,\nand the low variance observed demonstrates the\nstability of our method.\n6\nConclusion\nWe present a prompt-based calibration framework\ninspired by the Credence Calibration Game, where\nLLMs improve their confidence alignment through\nstructured feedback without any parameter updates.\nBy incorporating game-play history into prompts,\nour approach dynamically adjusts the model’s con-\nfidence. Experiments across multiple LLMs and\ndatasets show consistent improvements in evalu-\nation metrics, particularly under the exponential\nscoring strategy. These results demonstrate the\neffectiveness of our proposed method and high-\nlight the potential of game-based prompting as\na lightweight and general calibration strategy for\nbuilding more trustworthy AI systems. Future re-\nsearch could explore extending the framework to\ndiverse task formats beyond multiple-choice and\nopen-ended QA, integrating richer feedback signals\nand studying long-term adaptation over extended\ninteraction sequences.\n\nLimitations\nWhile our game-based prompting strategy shows\nsubstantial improvements in calibration metrics\nsuch as ECE and Brier Score, it comes with notable\ntrade-offs. In several settings, calibration gains are\nachieved at the cost of reduced accuracy, raising\nquestions about the balance between confidence\nalignment and task performance. Additionally, our\nexperiments are conducted on a limited number\nof benchmarks and LLMs. Future work may ex-\nplore ways to mitigate the accuracy drop, evaluate\nbroader task types, and improve efficiency for real-\nworld deployment."}
{"id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "authors": ["Wenhan Dong", "Zhen Sun", "Yuemeng Zhao", "Zifan Peng", "Jun Wu", "Jingyi Zheng", "Yule Liu", "Xinlei He", "Yu Wang", "Ruiming Wang", "Xinyi Huang", "Lei Mo"], "source": "http://arxiv.org/abs/2508.14377v1", "page_count": 10, "text": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students’\nCognitive Abilities\nWenhan Dong1,2\nZhen Sun2 Yuemeng Zhao2 Zifan Peng2\nJun Wu1 Jingyi Zheng2 Yule Liu2\nXinlei He2*\nYu Wang3 Ruiming Wang1 Xinyi Huang4\nLei Mo1*\n1School of Psychology, South China Normal University\n2Information Hub, Hong Kong University of Science and Technology (Guangzhou)\n3School of AI, Guangzhou University\n4College of Cyber Security, Jinan University\nAbstract\nLarge language models (LLMs) have demonstrated poten-\ntial in educational applications, yet their capacity to accu-\nrately assess the cognitive alignment of reading materials\nwith students’ developmental stages remains insufficiently\nexplored. This gap is particularly critical given the founda-\ntional educational principle of the Zone of Proximal Devel-\nopment (ZPD), which emphasizes the need to match learn-\ning resources with Students’ Cognitive Abilities (SCA). De-\nspite the importance of this alignment, there is a notable ab-\nsence of comprehensive studies investigating LLMs’ ability\nto evaluate reading comprehension difficulty across different\nstudent age groups, especially in the context of Chinese lan-\nguage education. To fill this gap, we introduce ZPD-SCA,\na novel benchmark specifically designed to assess stage-\nlevel Chinese reading comprehension difficulty. The bench-\nmark is annotated by 60 “Special Grade” teachers, a group\nthat represents the top 0.15% of all in-service teachers na-\ntionwide.1 Experimental results reveal that LLMs perform\npoorly in zero-shot learning scenarios, with Qwen-max and\nGLM even falling below the probability of random guess-\ning. When provided with in-context examples, LLMs per-\nformance improves substantially, with some models achiev-\ning nearly double the accuracy of their zero-shot baselines.\nThese results reveal that LLMs possess emerging abilities\nto assess reading difficulty, while also exposing limitations\nin their current training for educationally aligned judgment.\nNotably, even the best-performing models display system-\natic directional biases, suggesting difficulties in accurately\naligning material difficulty with SCA. Furthermore, signifi-\ncant variations in model performance across different genres\nunderscore the complexity of task. We envision that ZPD-\nSCA can provide a foundation for evaluating and improving\nLLMs in cognitively aligned educational applications.\n*Corresponding author (xinleihe@hkust-gz.edu.cn, molei@m.scnu.edu.cn)\n1The dataset construction cost at least $20,000 (converted from RMB\n150,000), primarily covering expert teacher involvement, and text acquisi-\ntion It will be made publicly available upon acceptance of the manuscript.\n1\nIntroduction\nIn recent years, Large Language Models (LLMs) have made\nremarkable progress in educational applications [39, 44, 16].\nThis progress spans a wide range of tasks, including auto-\nmated essay scoring [7, 41] and instructional content de-\nsign [47, 45].\nLLMs demonstrate strong text-processing\ncapabilities, offering new technical support for educational\npractices. However, in educational practice, providing high-\nquality education requires teachers to thoroughly analyze and\nunderstand students’ abilities [28, 36, 27]. This has led to a\nstrong assumption in the field of LLMs applied to education:\nthat LLMs are aware of the differences in Students’ Cog-\nnitive Abilities (SCA) across different educational stages.\nThis assumption is grounded in Vygotsky’s Zone of Proxi-\nmal Development (ZPD) theory [38], which emphasizes that\neducation should provide learning materials aligned with stu-\ndents’ current cognitive levels while introducing appropriate\nchallenges to facilitate growth.\nIn subjects like mathematics, the requirements for students\nat different educational stages are often relatively objective.\nFor example, the Compulsory Education Mathematics Cur-\nriculum Standard (2022 Edition) [29], outline that students\nshould learn concepts such as fractions and inequalities.\nSpecifically, second-grade students are expected to master\naddition and subtraction within 20 [29]. In contrast, the Chi-\nnese Curriculum Standards for Compulsory Education (2022\nEdition) [28] focus on cultivating students’ abilities in char-\nacter recognition, reading comprehension, and writing. For\nsecond-grade students, the emphasis is on enjoying reading,\nbeing able to briefly retell stories, and achieving an extracur-\nricular reading volume of no less than 50,000 words [28]. In\npractice, testing whether students have met their annual read-\ning volume target is far more challenging than assessing their\nability to perform addition and subtraction within 20. At the\nsame time, the assessment of mathematical ability focuses on\nthe mastery of specific skills, while the measurement of read-\ning volume reflects the cumulative results of practice, repre-\nsenting an essential difference in assessment nature.\n1\n\nLLMs as Teachers?\nMulti-Stage SCA Appropriateness Evaluation\nElementary\nMiddle\nReading Level: Elementary\nReading Level: High School\nLLM\nZPD-based Alignment\n(match text\nSCA)\nTeacher\n(Human)\nStudent\n(Reader)\nLLM\n“Implicit / Untested”\nDoes it really align?\n?\nKnown\nUnknown\nZPD\nFigure 1: The motivation of our work.\nWe observe that the strong assumption, i.e., LLMs in-\nherently understand the SCA differences across educational\nstages, has been widely accepted without sufficient investi-\ngation, particularly in the domain of language skills. This\nleads us to a critical question: To what extent do LLMs\naccurately comprehend the SCA, especially the reading\nabilities of students at different educational stages?\nThis question is complex and subjective. Beyond examin-\ning traditional linguistic features addressed in readability as-\nsessments—such as syntactic complexity, lexical difficulty,\nand morphological characteristics [13, 12, 11], it is also nec-\nessary to incorporate analyses of content depth, logical rea-\nsoning demands, and thematic emotional complexity. These\ndimensions provide a more comprehensive reflection of the\nactual SCA at different educational stages.\nTo examine LLMs’ ability to evaluate the developmental\nappropriateness of texts for students at different stages, we\nconcentrate specifically on the Chinese language to examine\nhow well LLMs understand SCA. The complexity of the Chi-\nnese language arises not only from its semantic richness but\nalso from features such as multi-layered emotional connota-\ntions and the presence of words with multiple meanings that\nvary by context. These characteristics make the task of align-\ning texts with students’ cognitive levels a significant chal-\nlenge, even for experienced educators specializing in Chi-\nnese language teaching. In response, we define a specialized\ntask and construct a novel benchmark, ZPD-SCA, designed\nto rigorously assess LLMs’ capacity to classify Chinese texts\naccording to their suitability for elementary, middle, and high\nschool students.\nZPD-SCA is annotated by 60 “Special Grade” teachers,\na group representing the top 0.15% of all in-service teach-\ners nationwide. This dataset is designed to assess LLMs’\nability to classify the difficulty of Chinese texts into three\neducational stages: elementary, middle, and high school. In\nthis task, LLMs analyze the linguistic, logical, thematic, and\nemotional complexity of a given text, directly mapping it to\nthe most suitable educational stage based on an understand-\ning of the reading abilities associated with each stage.\nThis study not only addresses a significant gap in the eval-\nuation of LLMs’ capabilities but also provides insights into\ntheir potential applications in education.\nOur main contributions are as follows:\n1. To evaluate LLMs’ capacity in assessing the cognitive\nalignment of reading materials with students’ develop-\nmental stages, we frame the task as a three-way classi-\nfication problem, in which LLMs are required to assign\ntexts to one of three educational levels. This task evalu-\nates whether LLMs can recognize the linguistic, logical,\nthematic, and emotional complexity of reading materi-\nals and align them with the cognitive capacities required\nfor students at different developmental stages. This ad-\ndresses a previously underexplored aspect of evaluating\nLLMs in education, specifically focusing on their ability\nto assess SCA in the context of reading comprehension.\n2. We introduce a novel benchmark ZPD-SCA, which is\nannotated by 60 “Special-Grade” teachers, a nationally\nrecognized group that represents the top 0.15% of all\nK-12 educators in China, ensuring high precision and\nreliability. This authoritative annotated benchmark of-\nfers robust support for evaluating LLMs’ ability to as-\nsess SCA in reading comprehension tasks.\n3. We reveal significant limitations of LLMs in assessing\nreading comprehension across educational stages, with\nperformance variations across models. These findings\nhighlight gaps in current LLMs’ ability to align text dif-\nficulty with students’ developmental stages.\n4. We show that LLMs perform poorly in zero-shot set-\ntings but improve significantly with in-context learning,\nindicating the need for task-specific training rather than\ninherent capability issues. By introducing ZPD-SCA,\nwe emphasize the importance of testing assumptions\nabout LLMs’ understanding of SCA, encouraging fur-\nther research in educational LLMs.\n2\nTask Definition\n2.1\nMulti-Stage SCA Appropriateness Evalu-\nation (MSCAAE)\nThis section introduces a new task MSCAAE, designed to\nevaluate the ability of LLMs to determine the reading com-\nprehension difficulty of Chinese texts. The primary objective\nof this task is to assess whether these models can accurately\nclassify texts into one of three educational stages-elementary,\nmiddle, or high school-within an annotated dataset, while\ncapturing the cognitive differences in Chinese reading com-\nprehension abilities across these stages.\nTo accomplish MSCAAE, the input texts need to satisfy\ntwo essential criteria.\nFirst, the content should come from real-world reading\nmaterials rather than standardized textbooks or curricula.\nThese materials may include extracurricular readings, news\narticles, and popular science literature, ensuring alignment\nwith real-world reading scenarios.\nSecond, the text difficulty should match the average Chi-\nnese reading comprehension level of students at a specific\neducational stage. The difficulty is generally assessed based\n2\n\n(a) Elementary school\n(b) Middle school\n(c) High school\nFigure 2: Semantic space analysis of educational stages. This figure presents the t-SNE contour visualizations of text embeddings\nfor three educational stages: elementary, middle, and high school. The results indicate that, from a purely semantic perspective,\nelementary school exhibits a more distinct distribution in the embedding space, while middle school and high school demonstrate\ngreater overlap, suggesting higher difficulty in distinguishing between these two categories based solely on semantic features.\non the following dimensions: linguistic complexity, thematic\ndepth, logical reasoning, and emotional complexity.\nFor each input text, the model is required to generate a\nclassification label that identifies the most appropriate edu-\ncational stage. For example, “Elementary” indicates that the\ntext is suitable for elementary school students, “Middle” for\nmiddle school students, and “High” for high school students.\nThe classification accuracy directly influences the model’s\npotential applications in Chinese education, especially in per-\nsonalized learning resource recommendations and reading\nability evaluations. Importantly, the task focuses on evaluat-\ning the model’s ability to cognitively assess students’ reading\ncomprehension capabilities at different educational stages\nby analyzing the alignment between text difficulty and stu-\ndents’ cognitive development.\nThus, the model must not\nonly identify text difficulty but also integrate the cognitive\ndevelopment traits of students at various educational stages\nto comprehensively judge its understanding of student abil-\nities. Specifically, the model needs to possess the following\ntwo core capabilities:\n• Understanding Students’ Chinese Reading Abilities\nat Specific Educational Stages: The model must grasp\nthe cognitive characteristics of elementary, middle, and\nhigh school students in terms of language comprehen-\nsion, logical reasoning, knowledge background, and\nthematic acceptance.\n• Evaluating the Reading Difficulty of Chinese Texts:\nThe model must comprehensively analyze text diffi-\nculty across linguistic complexity, thematic depth, log-\nical reasoning, and emotional complexity, aligning it\nwith students’ reading abilities.\n2.2\nEvaluation Metrics\nTo assess the model’s ability to classify Chinese reading\ncomprehension difficulty levels, we propose two evaluation\nmetrics: Cross-Level Migration Concentration (CLME) and\nWeighted Directional Bias Index (WDBI).\nCLME. The CLME measures the proportion of misclassi-\nfications occurring between non-adjacent difficulty levels.\nThis metric Mij denotes the number of samples classified as\nclass j while their true class is i. The errors are weighted by\nthe distance between classes to indicate the severity of mis-\nclassification. The CLME is defined as:\nCLME = ∑i, j,|i−j|>1 Mij\n∑i, j,i̸=j Mij\nWDBI. The WDBI evaluates the model’s overall tendency to\noverestimate or underestimate difficulty levels, while adjust-\ning for class imbalance. It compares the weighted counts of\nupward shifts (predicting higher difficulty than the true label)\nand downward shifts (predicting lower difficulty than the true\nlabel). The WDBI is defined as:\nWDBI = ∑i<j wiMij −∑i>j wiMij\n∑i,j wiMij\n3\nZPD-SCA\nThe data collection process was carefully structured to en-\ncompass both diversity and educational appropriateness. Re-\nsearchers with expertise in educational psychology collected\nraw data by selecting texts from a diverse range of extracur-\nricular books, ensuring suitability for elementary, middle and\nhigh school students. To ensure comprehensive coverage of\nreading materials typically encountered by students, we in-\ncorporated a wide range of genres, including fairy tales, fan-\ntasy, science fiction, campus life, adventure, and others, to-\ntaling 12 distinct categories. These genres not only align\nwith the typical categories in students’ extracurricular read-\ning but also capture the varying levels of reading comprehen-\nsion required at different educational stages.\nThis rigorous selection process resulted in a corpus of over\n8 million characters, providing a robust foundation for sub-\nsequent analysis. During the data annotation phase, we col-\nlaborated with 60 distinguished educators, all of whom were\n3\n\nGPT4o\nGPT4o-mini\nR1\nV3\nClaude\nGLM\nQwen-max\nQwen-plus\nQwen72B\nQwen32B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nElementary School (ZP)\nElementary School (CL)\nMiddle School (ZP)\nMiddle School (CL)\nHigh School (ZP)\nHigh School (CL)\nFigure 3: Recall rates for elementary, middle, and high school difficulty levels across LLMs. The figure illustrates how well each model\nidentifies texts belonging to difficulty categories, highlighting variations in recall performance across labels.\nrecipients of the title “Special Grade” teachers. This pres-\ntigious title is conferred by the Chinese government upon\ntop-performing teachers, with the number of awardees not\nexceeding 0.15% of all active teachers nationwide.\nEach book was independently evaluated by at least 20 an-\nnotators, who assessed its suitability for specific educational\nstages. All teachers participating in the annotation work were\ncompensated at the highest standard permitted by the state\nfor their professional services.\nAfter annotation and filtering, the dataset was reduced\nfrom an initial 8 million characters to a high-quality la-\nbeled corpus of 4.7 million characters. This reduction re-\nsulted from the exclusion of unsuitable or controversial texts,\nthereby ensuring the consistency and reliability.\nSemantic exploration of ZPD-SCA. We explored the se-\nmantic differences in the annotated samples using GTE-\nlarge [25]. After obtaining the embeddings, we applied t-\nSNE to visualize and analyze the data. As shown in Fig-\nure 2, middle and high school samples are not easily distin-\nguishable at the semantic level. However, elementary sam-\nples show clear differences from both middle and high school\nsamples. Clustering analysis confirms this, achieving 81.2%\naccuracy in separating elementary samples from the others.\n4\nExperiments and Analysis\nThis section outlines the experimental setup, results, and\nanalysis conducted to evaluate LLMs in assessing the diffi-\nculty stages of Chinese reading comprehension texts for three\nstages: elementary, middle, and high school.\n4.1\nExperimental Setup\nWe evaluate 10 LLMs, including GPT4o (version 2024-08-\n06), GPT4o-mini (version 2024-07-18) [32], Deepseek-R1\n(abbreviated as R1), Deepseek-V3 (abbreviated as V3) [14],\nClaude3.5-haiku (abbreviated as Claude) [4], GLM-4-plus\n(abbreviated as GLM) [48], Qwen-max (version 2024-10-\n15), Qwen-plus (version 2025-06-24), Qwen2.5-72b-instruct\n(abbreviated as Qwen72B), and Qwen2.5-32b-instruct (ab-\nbreviated as Qwen32B) [46].\nTheir performance is evaluated using four metrics: F1\nscore, accuracy (ACC), CLME, and WDBI. While F1 and\nACC capture overall classification accuracy, CLME mea-\nsures the severity of errors by penalizing non-adjacent mis-\nclassifications, and WDBI evaluates the directional bias in\npredictions (e.g., whether models tend to overestimate or un-\nderestimate SCA when aligning texts with appropriate diffi-\nculty levels).\nWe consider two templates for prompting the models:\n• Zero-shot Prompting (ZP): The LLMs are provided\nwith a brief description of the criteria and the full text\nas input, directly outputting a three-class result.\n• In-Context Learning (CL): In addition to using the\nsame input and output format as zero-shot prompting,\nillustrative examples are included in the prompt.\n4.2\nPrompt Design\nThe prompts used in our experiments are developed through\nan iterative refinement process, leveraging the capabilities\nof GPT-4o and Qwen-max to provide structured feedback.\nInitially, we test basic prompts on these models, which are\ndesigned to elicit three-class difficulty level predictions cor-\nresponding to elementary, middle, and high school cogni-\ntive abilities. After collecting sample outputs, we system-\natically identify instances of misclassification and analyze\nthe reasoning provided by the models in conjunction with\nthe ground truth labels. This analysis is then used as in-\nput for further iterations, where GPT-4o and Qwen-max are\ntasked with refining the prompts to enhance clarity and con-\nsistency. Through this iterative process, we ensure that the\nfinal prompts provide sufficient contextual information.\nIt is important to note that the objective of this process\nis not to optimize prompts for maximum performance but\nrather to develop a consistent and reasonable framework for\ntesting model capabilities across different settings.\n4.3\nExperimental Results\nTable 1 compares the ACC achieved by the evaluated models\nunder ZP and CL settings, highlighting the impact of con-\ntextual examples on performance. Table 2 summarizes the\nkey performance metrics (F1, ACC, CLME, and WDBI) for\nsome genres under the CL condition.\n4\n\nTable 1: Comparison of ACC scores achieved by the evaluated models under ZP and CL settings. These scores indicate how much\nbetter the models perform in CL compared to ZP in terms of ACC.\nModel→\nGPT4o\nGPT4o-mini\nR1\nV3\nClaude\nGLM\nQwen-max\nQwen-plus\nQwen72B\nQwen32B\nAll\n+0.1676\n+0.1786\n+0.0578\n+0.0538\n+0.2356\n+0.4219\n+0.3235\n+0.3522\n+0.2065\n+0.1241\nElementary\n+0.2657\n+0.2895\n+0.1410\n+0.1007\n+0.3909\n+0.6607\n+0.5587\n+0.5901\n+0.3247\n+0.2267\nMiddle\n+0.0765\n+0.0314\n−0.2481\n+0.0335\n−0.0527\n−0.0812\n+0.1559\n−0.0171\n−0.1005\n−0.1568\nHigh\n−0.0227\n−0.0079\n+0.0432\n−0.0534\n+0.0266\n+0.1364\n−0.1753\n−0.0352\n+0.0978\n+0.0446\nTable 2: Performance evaluation of 10 different models across various text genres in CL. “All” represents the aggregate performance\nacross all genres. F1 and ACC indicate the model’s performance for a specific genre. CLME assesses the proportion of misclassi-\nfications occurring across non-adjacent difficulty levels, while WDBI evaluates the overall tendency of the model to overestimate or\nunderestimate difficulty levels, accounting for class imbalance.\nDataset\nModel→\nMetrics ↓\nGPT4o\nGPT4o-mini\nR1\nV3\nClaude\nGLM\nQwen-max\nQwen-plus\nQwen72B\nQwen32B\nAll\nF1\n0.6896\n0.7993\n0.7184\n0.8352\n0.6860\n0.7038\n0.6818\n0.6962\n0.6547\n0.7610\nACC\n0.6618\n0.7784\n0.7413\n0.8313\n0.6445\n0.6878\n0.6296\n0.6674\n0.6091\n0.7555\nCLME\n0.0477\n0.0189\n0.0874\n0.0350\n0.0329\n0.0074\n0.0062\n0.0924\n0.0361\n0.0396\nWDBI\n0.3561\n0.0704\n-0.3243\n0.1081\n0.2579\n-0.1708\n0.3727\n0.1684\n0.2953\n-0.1930\nAcademic\nF1\n0.9873\n0.7500\n0.2222\n0.8571\n0.9744\n0.3750\n0.7302\n0.9333\n0.9474\n0.5185\nACC\n0.9750\n0.6000\n0.1250\n0.7500\n0.9500\n0.2308\n0.5750\n0.8750\n0.9000\n0.3500\nCLME\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.6000\n0.0000\n0.0000\nWDBI\n-0.0250\n-0.4000\n-0.8750\n-0.2500\n-0.0500\n-0.7692\n0.4250\n-0.1250\n-0.1000\n-0.6500\nFantasy\nF1\n0.4273\n0.7598\n0.9294\n0.9166\n0.4379\n0.8748\n0.5911\n0.5993\n0.4745\n0.9427\nACC\n0.2922\n0.6712\n0.9296\n0.8904\n0.3178\n0.8356\n0.4749\n0.4717\n0.3288\n0.9315\nCLME\n0.0258\n0.0139\n0.0000\n0.0417\n0.0068\n0.0000\n0.0087\n0.0268\n0.0408\n0.0000\nWDBI\n0.7005\n0.2108\n-0.2474\n0.1303\n0.5431\n0.0465\n0.5297\n0.3842\n0.7171\n0.0364\nInitial underperformance in zero-shot prompting. In the\nZP, several models exhibit significant challenges in accu-\nrately classifying text difficulty levels. For example, GLM\nand Qwen-max achieve accuracy scores below 33%, which\nis lower than the expected performance of random guessing\nin a three-class classification task. In contrast, Qwen32B\nperforms relatively better, achieving an ACC of 63.14%.\nThis discrepancy suggests that these models may lack suf-\nficient training in recognizing the SCA at different educa-\ntional stages. Furthermore, models with strong general text\ncomprehension capabilities might inadvertently misinterpret\nSCA levels due to their extensive exposure to diverse but\nnon-targeted training data, potentially introducing biases into\ntheir predictions.\nNotably, while elementary school samples can be distin-\nguished with (81.2%) accuracy semantically, models like\nQwen-max and GLM show a stark contrast in the ZP set-\nting. Their ACC for elementary school samples (9.83% and\n7.71%, respectively) falls significantly below the random\nguessing baseline. This highlights a critical gap: while the\nmodels can differentiate text difficulty semantically, they\nfail to effectively recognize SCA.\nFurther analysis of GLM shows that its CLME score was\n0, suggesting an absence of severe misclassifications across\nnon-adjacent difficulty levels. However, this outcome arises\nfrom the model’s overwhelming tendency to assign nearly\nall texts (88.03%) to the middle school category, revealing a\nfundamental inability to differentiate between difficulty lev-\nels effectively. Additionally, among the 10 models, seven ex-\nhibit positive WDBI values, with an absolute average WDBI\nof 0.2696 and a mean WDBI of 0.2035. Notably, Qwen-Plus\nand GPT-4o had WDBI values exceeding 0.4, suggesting a\npronounced tendency to underestimate human cognitive abil-\nities and overestimate text difficulty.\nUnlocking potential with few-shot learning. When eval-\nuated under the CL, the performance of these models im-\nproved dramatically.\nFor example, Qwen-max’s ACC in-\ncreases from 30.61% to 62.96%, representing an improve-\nment of over 2.06×. Similarly, the ACC of GLM rises from\n26.59% to 68.78%, a gain of approximately 2.59×. These\nsubstantial improvements highlight the critical role of tar-\ngeted training or contextual examples in enabling LLMs to\nperform well on tasks requiring nuanced understanding. The\nresults may suggest that initial underperformance is not due\nto inherent limitations in the models but rather the absence of\nexposure to relevant training data or task-specific contexts.\nIn the CL setting, we provide two elementary-level ex-\namples to help the models calibrate their understanding of\nhuman cognitive abilities. This adjustment reduces the ab-\nsolute average WDBI by 14%, bringing it down to 0.2317,\nwhile the mean WDBI decreased to 0.094. These changes\nsuggest a partial reduction in the models’ tendency to under-\nestimate SCA, leading to a notable improvement in classifi-\ncation accuracy. However, three LLMs that initially overes-\ntimate human abilities further exacerbated this bias. To de-\ntermine whether the observed improvement is influenced by\nlabel imbalance (e.g., models favoring the elementary label\nto achieve higher accuracy), we examine recall rates for texts\naligned with the three educational stages. We observe im-\nprovements in recall rates for middle and high school across\n5\n\nmost models (see Figure 3), ruling out the possibility of label\nbias. These findings reinforce our hypothesis that the LLMs\npossess some inherent ability to distinguish text difficulty but\nlacked appropriate training or context.\nUnder the CL, V3 achieves the highest overall F1\n(83.52%) and ACC (0.8313), demonstrating its strong capa-\nbility in assigning texts to the correct difficulty levels. De-\nspite this, WDBI (0.1081) indicates persistent challenges in\naccurately capturing the directionality of classification er-\nrors, suggesting opportunities for further refinement.\nNotably,\nsmaller models such as GPT-4o-mini and\nQwen32B demonstrate strong competitiveness in terms of F1\nscores and accuracy, suggesting that model size alone does\nnot guarantee superior performance on this task. The per-\nformance of models varied significantly across different gen-\nres, reflecting the complexity and diversity of the texts. In\nthe Academic genre, Claude achieves the highest accuracy\n(0.9500). Meanwhile, in the fantasy genre, Qwen32B sur-\npasses other models with an F1 score of 0.9427 and the low-\nest CLME (0), demonstrating its ability to minimize severe\nmisclassifications.\n5\nEnhancing Capability through Training\nTable 3: Comparison of Qwen-32B performance before and af-\nter LoRA fine-tuning.\nSetting\nAccuracy\nF1\nCLME\nWDBI\nZero-Shot\n0.6314\n0.6696\n0.0046\n-0.1046\nFew-Shot\n0.7555\n0.7610\n0.0396\n-0.1930\nFine-Tuned\n0.7986\n0.6169\n0.0467\n-0.0502\nWe conduct fine-tuning on the Qwen32B using Low-Rank\nAdaptation (LoRA) [22] under ZP setting. The LoRA con-\nfiguration uses a rank of 32, a learning rate of 5×10−4, and\na batch size of 8, trained for 5 epochs.\nTable 3 summarizes the evaluation results after training,\ncompared with Qwen32B’s performance under zero-shot and\nfew-shot settings. The fine-tuned Qwen32B achieves an ac-\ncuracy of 0.7986 in a zero-shot setting, surpassing its previ-\nous few-shot performance (0.7555) and significantly improv-\ning upon the original zero-shot result (0.6314). The CLME\nscore increases slightly (from 0.0046 to 0.0467), but remains\nwithin an acceptable range, indicating no severe misclassifi-\ncation. Moreover, the WDBI is closer to zero (-0.0502), sug-\ngesting a reduction in directional bias compared to both prior\nsettings. These results demonstrate that targeted fine-tuning\nvia LoRA can not only bridge the gap between ZP and CL\ncapabilities, but also lead to more balanced model behavior.\n6\nDiscussion\nZero-shot performance underestimates models’ latent\npotential in aligning text with student cognitive levels.\nOur experiments reveal that ZP performance significantly\nunderestimates the ability of LLMs to assess the suitability\nof texts for different cognitive levels. When provided with\nin-context examples, model accuracy improves substantially,\nFigure 4: Comparative analysis of error type classification. This\nfigure illustrates the misprediction categorization of ten LLMs,\nwhere the error attributions were analyzed and classified by\nGPT-4o based on the input of mispredictions and their under-\nlying causes. Categories accounting for less than 0.1% are ex-\ncluded from visualization for clarity.\nwith some models achieving 2-3× higher accuracy com-\npared to their ZP baselines. This suggests that LLMs have\nlatent knowledge about text complexity and its alignment\nwith SCA levels but require appropriate contextual guidance\nto better utilize this capability. The marked improvement\nthrough CL indicates that these models likely lack exposure\nto educational tasks during pretraining, which limits their\nability to perform such specialized assessments effectively.\nModel size and general leaderboard rankings do not\nconsistently predict success in assessing text alignment\nwith SCA. Contrary to expectations, smaller LLMs such\nas Qwen32B and GPT-4o-mini outperformed larger counter-\nparts like Qwen-max, and GPT-4o in evaluating text suitabil-\nity for different SCA levels. For instance, despite their high\nrankings on general benchmarks [21, 40, 10, 19, 6], GPT-\n4o and Qwen-max struggled to accurately assess text diffi-\nculty and SCA alignment. This finding highlights that model\nscale alone is insufficient for success in specialized educa-\ntional tasks.\nWe hypothesize that larger models, trained on diverse\ndatasets, may inadvertently develop biases that conflict with\nthe nuanced requirements of educational applications fo-\ncused on student cognitive levels.\nTheir vast parameter\nspaces and broad exposure to general domains might lead\nthem to over-rely on patterns irrelevant to the task at hand.\nIn contrast, smaller models, constrained by their limited ca-\npacity, appear less influenced by such biases and can focus\nmore effectively on task-specific features, such as aligning\ntexts with cognitive levels.\nCL has a pronounced impact, particularly on larger mod-\nels.\nThe transition from ZP to CL yielded particularly\nsignificant improvements for larger models. For example,\nGLM saw its accuracy increase by approximately 2.59×\nwhen provided with contextual examples.\nThis suggests\nthat larger models, with their greater parameter space and\nlearned representations, have a stronger capacity to leverage\nin-context information effectively. However, smaller models\n6\n\nlike Qwen32B also demonstrated competitive performance,\nindicating that model size alone is not the sole determinant\nof success in this task. These results underscore the impor-\ntance of targeted approaches, such as CL, to unlock the full\npotential of LLMs in educational contexts.\nEvidence points to a gap in training for cognitive-level\nalignment tasks. Our findings collectively indicate that the\nsuboptimal performance of LLMs in assessing text align-\nment with student cognitive levels likely results from in-\nsufficient exposure to relevant educational tasks during pre-\ntraining. While LLMs demonstrate latent capabilities that\ncan be enhanced through CL, their current training does not\nadequately equip them for specialized evaluations of read-\ning comprehension difficulty across different developmen-\ntal stages. This limitation is particularly significant given\nthe critical need to align educational materials with students’\ncognitive levels, as highlighted by ZPD theory.\nIn Figure 4, CL proves a certain level of effectiveness in\naligning aspects such as “Emotional Complexity” and “Lin-\nguistic Complexity.” However, it falls short in addressing\nmore intricate dimensions like “Thematic Depth” and “Log-\nical Reasoning”, where more targeted training strategies are\nevidently required. Bridging this gap will necessitate focused\ntraining approaches and further exploration into the intersec-\ntion of LLMs and education. Overall, addressing this issue\nremains an open challenge and calls for continued research\nin this interdisciplinary domain.\n7\nRelated Work\nLLMs as cognitive models of reading. Recent studies in-\ndicate that LLMs can serve as cognitively plausible proxies\nfor human reading processes [8, 20]. For example, Lopes\nRego et al. [34] demonstrated that replacing traditional cloze\npredictability with LLM-based predictions significantly im-\nproved a cognitive simulation of eye-movement behavior\nduring reading.\nLLM-powered educational tools for reading comprehen-\nsion. A growing body of work leverages LLMs to generate\nand personalize reading materials for learners [42, 17, 37,\n23]. Such personalized or simplified content has been shown\nto enhance learners’ engagement and understanding. While\nthese LLMs show promise in reducing teacher workload and\nproviding adaptive reading practice, their outputs still require\ncareful educator vetting to ensure appropriate difficulty and\nalignment with curricular goals.\nLimitations of LLMs in SCA assessment, particularly in\nChinese. While existing research has explored the potential\nof LLMs in educational applications [33, 3, 30, 1], a critical\ngap remains in their ability to directly assess SCA, particu-\nlarly in the context of reading comprehension across different\neducational stages. Despite advancements in text processing\ncapabilities [35, 2, 15, 26], most studies have primarily fo-\ncused on leveraging LLMs for tasks such as text simplifica-\ntion or personalized content generation, often assuming that\nthese models inherently understand the SCA differences be-\ntween elementary, middle, and high school. However, this\nassumption has not been rigorously tested, especially in lan-\nguages like Chinese, which present unique challenges due to\ntheir semantic richness and contextual variability. Although\npersonalized LLMs can estimate individual proficiency lev-\nels by learning from specific user data, there is also a cru-\ncial need to delineate the boundaries of group-level cogni-\ntive abilities. Notably, with appropriate prompting strategies,\nLLMs may demonstrate some ability to discern the bound-\naries of cognitive difficulty levels across different educa-\ntional stages [43, 24, 31]; however, comprehensively under-\nstanding these broader patterns remains critical for designing\nequitable and effective educational tools [5, 9, 18].\n8\nConclusion\nBased on Vygotsky’s Zone of Proximal Development (ZPD)\ntheory, evaluating the alignment between text difficulty and\nstudents’ cognitive levels is a core issue in educational ap-\nplications. However, current large language models (LLMs)\ngenerally lack targeted consideration for this task during their\ntraining process. Our study introduces a novel task frame-\nwork to assess LLMs’ ability to evaluate Students’ Cognitive\nAbilities (SCA) in reading comprehension across different\neducational stages. By constructing a high-quality bench-\nmark ZPD-SCA annotated by 60 “Special Grade” teachers,\nwe systematically uncover the limitations of existing LLMs\nin performing this task.\nExperimental results reveal that\nwhile in-context learning significantly improves model per-\nformance, the low accuracy in zero-shot scenarios, along\nwith cross-level misclassification and directional bias issues,\nhighlights the neglect of educational-specific needs in cur-\nrent model training. These findings not only provide critical\ndirections for enhancing the adaptability of LLMs in edu-\ncational contexts but also underscore the importance of in-\ncorporating cognitive alignment tasks into model training to\nadvance their application in education."}
{"id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "authors": ["Charles Welch", "Allison Lahnala", "Vasudha Varadarajan", "Lucie Flek", "Rada Mihalcea", "J. Lomax Boyd", "João Sedoc"], "source": "http://arxiv.org/abs/2508.14344v1", "page_count": 17, "text": "ISCA: A Framework for Interview-Style Conversational Agents\nCharles Welch♡and Allison Lahnala♡and Vasudha Varadarajan♥and\nLucie Flek♢and Rada Mihalcea♦and J. Lomax Boyd♣and João Sedoc♠\n♡McMaster University, ♥Stony Brook University, ♢University of Bonn,\n♦University of Michigan, ♣Johns Hopkins University, ♠New York University\ncwelch@mcmaster.ca\nAbstract\nWe present a low-compute non-generative sys-\ntem for implementing interview-style conver-\nsational agents which can be used to facilitate\nqualitative data collection through controlled in-\nteractions and quantitative analysis. Use cases\ninclude applications to tracking attitude for-\nmation or behavior change, where control or\nstandardization over the conversational flow\nis desired. We show how our system can be\neasily adjusted through an online administra-\ntive panel to create new interviews, making the\ntool accessible without coding. Two case stud-\nies are presented as example applications, one\nregarding the Expressive Interviewing system\nfor COVID-19 and the other a semi-structured\ninterview to survey public opinion on emerg-\ning neurotechnology. Our code is open-source,\nallowing others to build off of our work and\ndevelop extensions for additional functionality.\n1\nIntroduction\nConversational agents are increasingly used for\napplications in healthcare (Valizadeh and Parde,\n2022), customer service and engagement (Soni,\n2023), and education (Yan et al., 2024). They can\nhelp motivate behavior change, assisting with ad-\ndiction (He et al., 2022), medication adherence, and\nhealthy behaviors (Aggarwal et al., 2023). They\nare also useful tools for understanding thoughts,\nopinions, and concerns (Ziems et al., 2024).\nIn this paper, we describe our framework ISCA,\nfor interview style conversational agents for appli-\ncations to health, behavior change, and information\ngathering. ISCA enables researchers to implement\nstandardized interviews with customizable conver-\nsation flows. It is deliberately non-generative, al-\nlowing for more controlled interactions across par-\nticipants and avoiding off-topic discussion. The\nbackend of the system includes language detec-\ntion modules, which the researcher can customize\nthrough the admin portal to design rules for trig-\ngering follow-up questions based on elements of\nWhat do you think about chatbots?\nInterview\nThey’re kinda cool\nTell me what you think is cool!\nAbout the Study\nThis study is about chatbots! Please read the\ninfo to learn about it and hit Submit to begin.\nInterview Summary\nhappy\nneutral\nreligion\nmoney\nPost-Interview Survey\nHow much did you enjoy the chat?\nPre-Interview Survey\nHave you ever used a chatbot?\nFigure 1: ISCA: An Interview-Style Framework\nthe user’s responses. The system is equipped to\ndeploy the studies within a participant-facing in-\nterface. Language analytics are built-in, both for\nthe researcher’s admin view to gain insight into\nthe study population’s responses and for the par-\nticipants to gain insight into their own language.\nOur system is run as a web server built on the\nDjango framework (Django Software Foundation,\n2024) and allows users to easily configure inter-\nviews without coding.\nISCA can be used to understand attitude for-\nmation, attitude polling, and monitoring behav-\nior change, using multiple choice survey questions\ngiven before and after each interview. We present\ntwo case studies: 1) to address stress related to the\nCOVID-19 pandemic (Welch et al., 2020) and 2) to\nsurvey public opinion about human brain organoids,\nan emerging neurotechnology that raises ethical\nconcerns among segments of the public. Finally,\nwe discuss applications and extensions. We inten-\ntionally avoid large language models and genera-\ntive approaches as they are unnecessary and costly.\nOur code1 and a demo video2 are made publicly\navailable, so that others can easily set up and run\nservers locally or extend functionality.\n1https://github.com/cfwelch/framework-intervi\new-style-agents\n2https://youtu.be/_5XvMsZf8dA\n\n2\nRelated Work\nSurveys and Interviews Recent research has ex-\nplored advancements in language-based survey\nmethodologies, particularly focusing on tailoring\napproaches to enhance response rates and reduce\nrespondent burden (Dillman et al., 2014; Sikström\net al., 2023). Computerized adaptive testing (CAT)\nhas long been recognized for its effectiveness in\nselecting informative questions based on partici-\npants’ previous responses. Recently, it has emerged\nas a viable method to traditional fixed-format sur-\nveys (Wainer et al., 2000; Varadarajan et al., 2024).\nWhile CAT effectively improves engagement and\nresponse rates, it faces several limitations including\nthe need for extensive pre-calibration with large but\nfixed question banks and significant computational\nresources. Recent solutions employ humans-in-the-\nloop to aid with question selection to train rein-\nforcement learning models to dynamically update\nsurvey questions (Velez, 2024).\nDesigning interviews with meaningful outcomes\nrequires careful consideration of the context, role\ndynamics, trust and expectations between the in-\nvolved parties (Schilling, 2013). Distinct interview-\ning styles can elicit distinct response patterns from\nthe participants reflective of their cognitive pat-\nterns (Priede and Farrall, 2011). However, with the\nadvent of conversational agents and chatbot-based\ninterviewing systems, the elicited responses can dif-\nfer due to the personality of the agent (Xiao et al.,\n2020), anthropomorphism (Rhim et al., 2022) or\nthe mode of interaction (Oates et al., 2022; Zarouali\net al., 2024). AI-powered chatbots have been ex-\nplored as a means to provide telehealth advice (Xu\net al., 2024), mental health assessments (Schick\net al., 2022) and facilitating discussions (Nguyen,\n2023). AI chatbots present challenges including\ndata privacy concerns, ethical considerations in\ndata handling, and potential impacts on authentic\nhuman interactions (Adam et al., 2021; Marks and\nHaupt, 2023).\nConversational Agent Design Several platforms\nand frameworks exist for developing conversa-\ntional agents. Rasa, available in both open-source\nand commercial versions, specializes in machine\nlearning-based dialog management using slot-value\nsystems (Bocklisch et al., 2017). ParlAI provides\na research platform specifically designed for dia-\nlog research (Miller et al., 2017), while AIML and\nChatScript offer rule-based chatbot development\nthrough specialized scripting languages (Ramesh\nC.P.: What are the major\nissues in your life right now,\nespecially in light of issues\nsurrounding COVID?\nUser: I am worried about my\nhealth and the health of my\nfamily. I don’t want to make\npeople sick and I am also\nhaving money issues.\nLexicons:\nLIWC\nWN-Affect\nMPQA\nHealth\nFinance\nFamily\nHealth\nFinance\nFamily\nGeneral\n...\nHealth Reflection\nI'd like to know more about\nyour feelings surrounding your\nhealth and the health of people\nclose to you.\nWhat actions can you take to\nhelp keep you healthy during\nthese challenging times?\nFigure 2: Main questions and reflection triggers.\net al., 2017). Similar to ISCA, Juji offers conversa-\ntional capabilities (juj, 2024), while Riff represents\na more focused application - a generative chatbot\nsystem designed to enhance college students’ learn-\ning through reflection (Cavagnaro, 2023), specifi-\ncally targeting users 18 and older to avoid concerns\nassociated with generative AI. Researchers used\nJuji for interactive surveying (Xiao et al., 2020),\nhowever, this is closed-source limiting the ability\nto tailor their infrastructure.\n3\nSystem Overview\nWe release ISCA, an open framework for re-\nsearchers to create conversational interview-style\nsocial science studies. As illustrated in Figure 1,\nISCA facilitates the implementation of the inter-\nview flow and surveys that can be administered\nbefore and after the interview. It offers a unique\nexperience for participants by providing analytical\nsummaries and visualizations of what they wrote.\nOur conversational interview design revolves\naround main questions and reflections. The main\nquestions will be asked to every participant by com-\npletion of the interview, but participants may en-\ncounter them at different times depending on the\nconversation flow. The conversation flow is im-\npacted by the content of the participant’s turns,\nas the language triggers specific reflections (i.e.,\nfollow-up questions) when certain trigger condi-\ntions are satisfied. Figure 2 shows an example\nwhere a participant’s response with a high propor-\ntion of health topic words triggers a health reflec-\ntion, prompting the user to reflect more on health.\nParticipant Flow. Participants can be directed to\nthe website where the project is hosted either to a\npage that shows all topics with active interviews, or\ndirectly linked to the start page for a specific topic.\nThe start page shows an informational screen about\nthe purpose of the interview. This can serve as a dis-\nclosure of information about the present study and\nasking for consent. Next, they are sent to the first\nmultiple choice page. This page can be used for\nchecking pre-interview participant status by asking\n\nyes/no or Likert scale questions. After answering\nquestions, they are directed to the conversation.\nThe conversation begins with an introduction\nfrom the chatbot and an initial main question.\nWhen the participant responds, a reflection may\ntrigger. One reflection can be triggered in between\neach main question. If no defined reflections trig-\nger, preprogrammed generic reflections (e.g. “Tell\nme a little more about that”) can trigger instead.\nThese will trigger if the participant has taken less\nthan 15 seconds to write their response or if they\ntype less than 100 characters. Only one of these\ntype of triggers can occur in a conversation, as we\nfound that repeatedly reminding someone to write\nmore tends to be frustrating and causes them to dis-\nengage. After all the main questions from the active\ninterview are asked, the conversation will end. The\nparticipant is then redirected to post-interview mul-\ntiple choice questions. After they respond to these\nquestions, they are shown a summary of their in-\nteraction (Figure 14 in the Appendix shows most\ndiscussed categories) and have options to download\ntheir data, reset the page if they want to start the\nconversation over, or provide feedback.\n4\nAdministrative Panel Demo\nThe administrative panel consists of seven main\npages with distinct functions. Admins can create\ninterview topics, define interview questions for a\ntopic, manage lexicon categories, configure survey\nquestions, view analytics on the dashboard, per-\nform topic modeling, and manage the FAQ. While\npages like the lexicon management are globally\nmanaged, other pages, such as the interviews, are\nspecific to a topic. On these pages, the dropdown\nin the sidebar can be changed at any time to reload\nthe page with information for that topic. Figures\nshowing each of the pages are in Appendix A and\nstep-by-step documentation is provided in the sup-\nplementary material.\nTopics. The topics page shows the list of conversa-\ntion topics (Figure 3). Each topic can have one ac-\ntive interview at a time. When adding a new topic,\nadmins can provide a name and icon for the topic,\nthe name of the chatbot, and the intro/disclaimer\nscreen that shows before any conversation starts.\nInterviews. The interview overview page (Fig-\nure 4) shows the list of interviews for a topic. A\nnew interview can be created when one wants to\nupdate the questions that will be asked in an in-\nteraction. Any notes on the topic will be shown\non this page. When adding a new interview, ad-\nmins can add questions and reflections. Admins\ncan choose to trigger the reflection when there is\na dominant category from the set of lexicons that\nare active for that topic, sentiment, and whether or\nnot a different reflection has already been triggered.\nA dominant category is one that occurs more than\n50% more often in the response than the next most\nfrequent lexical category. The sentiment option\nuses VADER to efficiently classify each utterance\nas positive, negative, or neutral (Hutto and Gilbert,\n2014). On the overview page, clicking on any of\nthese interviews shows details and allows setting\nthe active interview (Figure 5).\nLexicons. The lexicons page (Figure 6) allows\nadmins to define new lexicon categories and to\nadd/remove words from each category.\nWhen\nadding words, admins provide a comma-separated\nlist of words and word stems. Word stems end with\nan asterisk and will match words beginning with\nthe preceding stem, while other words must exactly\nmatch (case insensitive). When the user visits the\nlexicon topic management page (Figure 7), they\ncan assign lexicon categories to a topic or remove\nthem. The categories active for each topic are listed\non this page. An active category means that those\ncategories can be used as reflection triggers when\nadding an interview, and that those categories will\nbe detected and recorded for summary statistics\nand for the administrative dashboard.\nSurveys. The survey management page (Figure 8)\nshows the list of questions for a topic. Questions\ncan be added or deleted. The intro and outro boxes\nallow admins to toggle if the question is asked be-\nfore or after the interview (or both). Each question\ncan be a yes/no question or a Likert scale question.\nThe responses to questions defined here will be\nvisualized on the administrative dashboard.\nFAQs. Admins can add or delete frequently asked\nquestions in the FAQ page (Figure 9). This page is\naccessible to participants who are currently having\na conversation either through the side navigation\nbar or through the conversational interface directly\nwhen they ask the bot a question. When a question\nis detected, a notification will appear that states\nthat the interview was not designed for users to ask\nthe bot questions and instead provides a link to the\nFAQ page for that topic.\nDashboard. The dashboard shows a variety of ag-\ngregate statistics over interviews for the selected\ntopic. Figure 10 shows the top of the dashboard,\n\nwhich contains a bar chart showing the most fre-\nquently discussed topics. Charts on this page use\nlexicon categories that are active for the topic. We\nalso see the total number of conversations, average\nresponse length and interview time. The interview\ntime and response length statistics can be selected,\nwhich redirects the admin to distribution plots for\neach statistic. The next plot on the dashboard shows\nthe detected word categories (Figure 11). Unlike\nthe previous plot, which shows the total number of\nconversations a given category appears in, this plot\nshows the frequency distribution of each category\nacross conversations. The dashboard also shows a\nlist of summaries and plots. Each summary has a\ndate, word count, and button that redirects to the\nparticipant-facing summary for that page. Each\nsurvey question is used to generate a plot. Yes/no\nquestions generate bar plots and Likert scale ques-\ntions generate line plot distributions (Figure 12)\nwith an entry for before and/or after the interview.\nTopic Modeling. From the dashboard, you can go\nto separate topic modeling pages for LDA (Blei\net al., 2001) and BERTopic (Grootendorst, 2022).\nThe pages are identical except each allows you\nto run a different topic modeling method, which\ncan be used for exploration of interview responses.\nThe topic modeling pages show an overview of\nthe previous topic modeling runs (Figure 15). The\nadmin can choose a number of topics and start a\nnew topic model, which will run as a subprocess.\nThe status of the previous runs is shown below and\nwill update automatically, as it regularly checks\nthe subprocess status. The table of previous runs\nshows the date, number of topics, how long it took\nto run, and the topic coherence.\nClicking to see the results of the previous topic\nmodeling runs takes the admin to a page specifi-\ncally for those results (Figure 16). This page shows\nthe distribution of each topic’s frequency and top\nten words. Clicking on these topics shows a list\nof all conversation turns where the words occurred\nfor more context. The bottom of this page contains\nthe visualization generated with pyLDAvis (Sievert\nand Shirley, 2014), showing topic overlap and a dy-\nnamic visual of the salient terms for each topic (Fig-\nure 17). We use the same visual for the BERTopic\ntopics even though it does not use LDA.\n5\nCase Study 1: COVID-19\nThe base of ISCA was originally designed for the\nExpressive Interviewing system as a response to\nthe COVID-19 pandemic (Welch et al., 2020). The\nsystem uses a combination of techniques from Ex-\npressive Writing (Pennebaker and Beall, 1986) and\nMotivational Interviewing (Miller and Rollnick,\n2012). Expressive writing is a reflective writing\ntechnique shown to improve mental and physical\nhealth (Frattaroli, 2006). Motivational interviewing\nis a counseling technique designed to help people\nchange their behavior by eliciting one’s motivation\nfor change and reflective listening.\nObjective. Our original system aimed to reduce\nstress related to the pandemic.\nMore recently,\nStewart et al. (2023) used ISCA to perform a follow-\nup study to examine behavior change associated\nwith the use of our system to understand how short-\nand long-term effects varied based on writing style\nand demographic factors. Participants were asked\nabout a variety of behaviors, such as how frequently\nthey thought and talked about COVID or went out\nin public, and used these to determine if long-term\nbehaviors had changed when they returned two\nweeks later.\nInterview Configuration.\nThis interview con-\nsisted of four main questions. The questions cen-\ntered around (1) the major issues in your life, (2)\nsomething you look forward to, (3) advice you\nwould give to others, and (4) something you are\ngrateful for (see Appendix B, for the full ques-\ntions). The interview contains reflections related\nto positive and negative emotions, pronoun usage,\nwords related to order, and LIWC (Pennebaker\net al., 2015) lexicon words related to money, health,\nhome, and work.\nBefore the interview, participants were asked\nabout their overall life satisfaction. After the inter-\nview, they were asked how personal and meaning-\nful the interaction was. At both times, they were\nasked about their current stress level. All questions\nuse a 7-point Likert scale. In a behavioral study,\nthey were asked the additional behavior questions\nseparately from our system, though these questions\ncould easily be integrated as additional survey ques-\ntions. The original study also asked users to com-\npare our system to Woebot (Fitzpatrick et al., 2017),\na widely-used conversational mental help app.\nParticipants. The original study recruited 174 par-\nticipants through social media and announcements\nthrough our university. For the follow-up study on\nbehavior change, 200 participants were recruited\nfrom Prolific.co to use the system and return two\nweeks later to use the system again.\n\nFindings. In the original study, participants’ self-\nreported stress levels were assessed before and after\nthe interview, with > 3 on a 7-point Likert scale\nbeing high stress. When comparing ISCA to Woe-\nbot, ISCA resulted in a greater reduction in stress\n(from 91% to 64%, a 9% absolute decrease over\nWoebot). When asked to compare the two systems,\nusers found ISCA to be easier to use and, overall,\nmore useful (Welch et al., 2020).\nWhen looking at the difference in participants\nreturning two weeks later, writing with more lex-\nical diversity was found to be correlated with an\nincrease in social activity. Anxiety words in writing\nwere correlated with stress reduction in the short\nterm, and positive words with a meaningful experi-\nence. However, the short-term benefits to mental\nhealth did not translate to the long-term.\n6\nCase Study 2: Neurotechnology - Brain\nOrganoids\nAnother study using ISCA focused on human brain\norganoids (HBO), as an emerging technology in\nneuroscience research. Organoids, generally speak-\ning, are miniature models of organs grown in a lab\nfrom stem cells that mimic the structure and func-\ntion of actual organs and thus are a valuable tool\nfor studying their development and health (Benito-\nKwiecinski and Lancaster, 2020).\nObjectives. The study aimed to replicate the find-\nings from a previous survey on public attitudes to-\nward organoid research (Bollinger et al., 2021) and\ntoward xenotransplanted chimeras, an organism\ncomposed of cells derived from different species\nthat are though to raise ethical concerns for some\nindividuals (Boyd, 2023). This work primarily sur-\nveyed the public about their moral attitudes toward\nbrain organoids and/or chimeras.\nParticipants. Students were recruited through the\nUniversity of Marburg via an internal mailing list.\nIn total, 39 participants used the system. Only five\nhad heard of HBOs before using our system.\nInterview Configuration. The participants were\nasked eight main questions, consisting of (1) their\ninitial thoughts about HBOs, (2) justification for\ntheir feelings, (3) conflict with moral beliefs, (4)\ntransplanting human brain cells into other animals ,\n(5) impact on disease treatment, (6) views on suf-\nfering, (7) conflicted views, and (8) consciousness.\nReflections related to sentiment and the LIWC cate-\ngories of money and religion were also considered.\nBefore the interview, participants were asked if\nthey had heard of HBOs before (yes/no) and if they\nwere in favor of their use (this and subsequent ques-\ntions presented on 7-point Likert scale). After the\ninterview, they were asked how conscious HBOs\nwould have to be before they are morally problem-\natic, how meaningful and personal the interaction\nwas, and again if they are in favor of their use.\nFindings. A manual analysis of the conversation\ntopics reveals a large number of similarities with\nEvans (2022), where people express similar themes\nrelated to research benefits, ethical concerns with\nrespect to consciousness, and cautious optimism.\nThey similarly relate to the technology through sci-\nence fiction references. They express unease with\nchimeric research, and religion plays a minor role\nin shaping opinions. The level of agreement in\nour study for HBOs and chimeric research were\n84% and 62%, respectively, whereas previous work\nfound these levels to be 90% and 68%. The find-\nings are similar, though our sample size is small\nand derived from a different population (German\nversus USA). In contrast, the participants talked\nabout the socioeconomic impact and how the tech-\nnology might only be accessible to the wealthy.\nThey also mentioned how the public must be able\nto give input and compared the technology to artifi-\ncial intelligence.\n7\nDiscussion\nWe demonstrated ISCA, a system that can be used\nby researchers to develop and administer research\nwith a conversational interview style design. ISCA\nfacilitates the collection of qualitative data that can\nprovide deeper insight into respondents’ perspec-\ntives and behaviors. Conducting these interviews\nthrough a conversational agent may also help users\nexpress perspectives more freely than they would\nto another person (Lucas et al., 2014; Gratch et al.,\n2014). By including pre- and post-interview survey\noptions in the design, it offers a suitable framework\nto investigate the effects of the conversational inter-\nview. We demonstrated these utilities in two case\nstudies that are contrasted by guiding conversations\non familiar versus novel concepts.\nUtility for chatbot intervention research. The\nCOVID-19 study focused on guiding users to\nexpress their thoughts and feelings as a stress-\nreduction intervention using the expressive inter-\nviewing technique.\nISCA allowed us to under-\nstand the effects of the technique on reducing\nstress. Many users who showed reduced stress\n\nafter the interaction found the conversation mean-\ningful. Also, users who found it more personal\nfound it more meaningful as well. This highlights\nopportunities to research how to personalize the ex-\nperience (Welch, 2021; Abd-Alrazaq et al., 2021).\nThe Bioethics study aimed to gather ethical per-\nspectives on a scientific topic that general popula-\ntions may find novel, playing an educational role to\nfacilitate this. Through the introductory text for the\ninterview, studies can introduce participants to new\nconcepts, or link users to external information or\nthe FAQ section for more details. This allows the\nsystem to then capture first impressions of a new\nconcept. The agent is not fluent enough to intro-\nduce these concepts in conversation, so researchers\nshould take this into consideration.\nUser Expectations. Through interactions in both\ncase studies, we find a small portion of users frus-\ntrated with the lack of natural interaction. We have\nthree mechanisms for setting expectations; (1) the\nintroductory text, (2) redirection to FAQ when a\nquestion is asked, and (3) encouraging participants\nto spend time writing their responses. We want\nthem to view the system as a guide for them to\nwrite detailed responses rather than a quick back-\nand-forth chat. Participants saw significant stress\nreduction and we were able to collect valuable in-\nput without a fluent agent, though the impact of\nfluency on the effectiveness of these methods de-\nserves attention in future work.\nConsiderations for Integrations with Generative\nAI. ISCA is engineered as an end-to-end conver-\nsational system where the research admin has full\ncontrol over the content the system produces. How-\never, there are benefits that generative techniques\ncould provide. For instance, we found that users of\nthe COVID-19 system were more likely to find it\nmeaningful if also personal, one direction could be\nto utilize LLMs to adapt to the user. Using LLMs\nto support personalization may be especially bene-\nficial for systems like mental health chatbots (Abd-\nAlrazaq et al., 2021).\nHowever, control over what the system generates\nis especially crucial for sensitive subjects like those\nexplored in our case studies. Generative models\nare unpredictable in nature and can produce harm-\nful responses (Bommasani et al., 2021). How to\nmitigate or remove biases of these models remains\nan open issue (Meade et al., 2022; Blodgett et al.,\n2020), leading researchers to suggest that they pose\nsignificant risks and should be carefully evaluated\nand monitored (Laranjo et al., 2018; Dinan et al.,\n2021), and that they may not be ready for many\napplications, especially those involving topics of a\nsensitive nature (Lechner et al., 2023).\nEven if these issues were solved, generative\nmodels are still not to be desired for standard-\nized conversational interactions. Standardized in-\nterviews, used for public opinion polling and aca-\ndemic research, influence decision-making in mar-\nket research, corporate, and governmental organi-\nzations (Gwartney, 2007). Often contrasted with\nsurveys that allow for the collection of rich open-\nended data (Houtkoop-Steenstra, 2000), interview\nmethodologies are more exploratory – they vary\nin their level of standardization, with some allow-\ning for more open-ended exploration (Priede and\nFarrall, 2011). This level of control over question\nstandardization and exploration is not possible with\nlanguage models. One of the salient problems with\nopen-domain conversational agents is the lack of\nconsistency in the generated output, often involving\ncontradictions (Roller et al., 2020).\nExtensibility. Our open-source framework can be\nextended to pursue integration with other models,\nsuch as LLMs, that could modify output styles or\nadd flexibility to conversation flow. This feature\nwould be optional and situation-specific as these\ncomponents are often computationally or mone-\ntarily expensive (Zhao et al., 2023). Researchers\ncould also extend it to offer human-in-the-loop fea-\ntures, enabling the use of LLMs to make the system\nmore flexible while ensuring human oversight in\nsensitive scenarios.\n8\nConclusion\nWe introduced ISCA, a framework for interview-\nstyle low-compute conversational agents that takes\nthe form of a web interface with admin and par-\nticipant facing pages. The interviews can be con-\nfigured for new conversational topics through the\nadministrative panel with no programming expe-\nrience. By using a rule-based system, we avoid\nthe drawbacks of generative models. By setting\nuser expectations, ISCA can effectively help people\nreflect on their experiences and monitor behavior\nchange (Case Study 1), and survey public opinion\n(Case Study 2). We release our code so that others\ncan set up their own servers and extend our system.\n\nEthics statement\nThe COVID-19 case studies were approved by\nthe University of Michigan IRB. Case Study 2:\nBioethics of Organoids was approved by the Uni-\nversity of Marburg and University of Bonn IRBs.\nAcknowledgements\nWe would like to thank Marlon May for help-\ning with code compatibility, Ezzeddine Ben Hadj\nYahya for his contributions to the code, and Ten-\nzin Migmar for her testing and revisions to setup\ninstructions."}
{"id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "authors": ["Yixin Chen", "Ying Xiong", "Shangyu Wu", "Yufei Cui", "Xue Liu", "Nan Guan", "Chun Jason Xue"], "source": "http://arxiv.org/abs/2508.14323v1", "page_count": 16, "text": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via\nBehavior-Aligned Retriever\nYixin Chen1 , Ying Xiong2† , Shangyu Wu1, Yufei Cui3, Xue Liu2,\nNan Guan1, Chun Jason Xue2\n1City University of Hong Kong\n2Mohamed Bin Zayed University of Aritificial Intelligence\n3McGill University\nAbstract\nTool-augmented\nlarge\nlanguage\nmodels\n(LLMs) leverage external functions to extend\ntheir capabilities, but inaccurate function calls\ncan lead to inefficiencies and increased costs.\nExisting methods address this challenge by\nfine-tuning LLMs or using demonstration-\nbased prompting, yet they often suffer from\nhigh training overhead and fail to account for\ninconsistent demonstration samples, which\nmisguide the model’s invocation behavior.\nIn this paper, we trained a behavior-aligned\nretriever (BAR), which provides behaviorally\nconsistent demonstrations to help LLMs make\nmore accurate tool-using decisions. To train\nthe BAR, we construct a corpus including\ndifferent\nfunction-calling\nbehaviors,\ni.e.,\ncalling or non-calling. We use the contrastive\nlearning framework to train the BAR with\ncustomized positive/negative pairs and a\ndual-negative contrastive loss, ensuring robust\nretrieval of behaviorally consistent examples.\nExperiments demonstrate that our approach\nsignificantly reduces erroneous function calls\nwhile maintaining high task performance,\noffering a cost-effective and efficient solution\nfor tool-augmented LLMs.\n1\nIntroduction\nTool-augmented models (Tang et al., 2023; Patil\net al., 2023; Abdelaziz et al., 2024) have emerged\nas a promising paradigm for enhancing the capa-\nbilities of large language models (LLMs), enabling\nthem to interact with external functions such as\nsearch engines (Mialon et al., 2023), or domain-\nspecific APIs (Li et al., 2023; Huang et al., 2023).\nBy dynamically invoking functions, LLMs can per-\nform tasks such as retrieving real-time informa-\ntion, executing computations, or interacting with\ndatabases. However, the reliability of function call-\ning remains a critical challenge, as incorrect or un-\n† Corresponding author.\nhttps://anonymous.4open.science/r/BAR-F65E\nnecessary invocations may lead to computational\noverhead, increased latency, or even financial costs.\nEnsuring that LLMs accurately decide when and\nhow to call functions is thus essential for deploying\nthese systems efficiently.\nPrior approaches to this challenge rely on ei-\nther pre-training or fine-tuning LLMs on curated\nfunction-calling datasets to enhance their tool-\nusing capabilities (Qin et al., 2023; Chen et al.,\n2024). To better leverage LLMs’ in-context ca-\npabilities, these approaches also retrieve simi-\nlar examples as demonstrations to guide LLMs\nin generating correct function calls.\nAlthough\nthese approaches can alleviate the misinvocation\nissues (Chen et al., 2024), their training or fine-\ntuning costs are too high, requiring high-end com-\nputational resources. Moreover, these works over-\nlook the inconsistency of examples in demonstra-\ntions, which may confuse or misguide the LLMs in\ncalling the function.\nThe inconsistency issue in demonstrations lies\nin the lack of a mechanism to ensure that retrieved\ndemonstrations exhibit coherent behavior regarding\nfunction calls. Our preliminary experiments also\nprove that when demonstrations are behaviorally\naligned, either uniformly requiring or not requiring\nfunction calls, the LLMs can perform significantly\nbetter. This observation motivates the need for a\nspecialized retrieval module that can dynamically\nselect the most relevant and consistent demonstra-\ntions, thereby reducing ambiguity and improving\ndecision-making in tool-augmented LLMs.\nIn this paper, we propose a behavior-aligned\nretriever, called BAR, which retrieves examples\nwith consistent behaviors to help LLMs make more\naccurate tool-using decisions. To train the BAR,\nwe construct a corpus consisting of different be-\nhaviors, e.g., calling or non-calling. We follow\nthe contrastive framework to select positive pairs\nand negative pairs in the function-calling scenar-\nios.\nWe divide negative samples into two sub-\n1\n\nLLMs\nUsers\nWhat kind of\nweather do you like?\nquery\nresponse\nresults\nTool-aware\nPretrained/Finetuned\nTool DB\nretrieved samples\n1. What's your favorite season and why? (No tool)\n2. Can you tell me the weather forecast for\ntomorrow? (Require tool)\n3. Do you prefer rainy or sunny days? (No tool)\nuse tool\nRetriever\nDatastore\nretrieval-augmented prompt\nFigure 1: Retrieval-augmented function-call pipeline.\nclasses for more fine-grained semantically and be-\nhaviorally discrimination. Finally, we introduce a\ndual-negative contrastive loss to optimize our BAR.\nExtensive experiments demonstrate the efficacy of\nour proposed BAR. Compared to baseline models,\nBAR improves direct response rate of LLMs by\n8.5% on H2A scenarios, and reduces redundant\nAPI calls by 4.2% on ToolDEER dataset. The main\ncontributions of this paper are as follows:\n• We show that demonstrations with aligned be-\nhaviors can guide LLMs correctly utilizing\nthe external tools.\n• We trained a behavior-aligned retriever with\ncustomized positive/negative samples and a\ndual-negative contrastive loss.\n• We applied our BAR on various LLMs, and\nthe results show consistent improvements.\n2\nBackground and Motivations\n2.1\nRetrieval-Augmented Function-Call\nPipeline\nLLMs equipped with function-calling abilities can\ninvoke external APIs to answer queries that re-\nquire real-time data, computation, or privileged\nknowledge.\nBenefited from LLMs’ in-context\ncapabilities, existing works retrieve similar ex-\namples as demonstrations to help LLMs make\ncorrect function-calling decisions. The retrieval-\naugmented function-call pipeline is shown in Fig-\nure 1, which includes a datastore of labeled ex-\namples, a retriever for top-k semantically relevant\ndemonstrations, and a tool-augmented LLM for\ninvoking the functions if needed.\nFor example, given a query from users (“What\nkind of weather do you like?”), the retriever first re-\ntrieves the top-3 similar examples from the external\ndatastore (i.e., “What’s your favorite season and\nwhy?” with the label of no tool calling, “Can you\ntell me the weather forecast for tomorrow” with the\nlabel of tool invocation, and “Do you prefer rainy\nor sunny days?” with the label of no tool calling).\nThen the framework concatenates the user query\nand the corresponding retrievals into one prompt\nand feeds it into the LLMs. The LLMs would de-\ntermine whether to use the tool or not, and then\nreturn the final responses to the users.\n2.2\nMotivation\nRetrieval-augmented function-call pipelines still\nsuffer from two key points: high adaptation cost\nand semantic-only retrieval noise.\nFirst, most\nworks still require fine-tuning the LLM on function-\ncall data to ingest the retrieved demonstrations reli-\nably. However, fine-tuning LLMs is prohibitively\nexpensive and might be repeatedly tuned when new\nAPIs or domains are introduced. Secondly, ex-\nisting retrievers rank retrievals solely by seman-\ntic similarity but overlooking tool-invocation be-\nhaviors. Many queries that are lexically or top-\nically close require opposite tool-invocation be-\nhavior.\nFigure 1 illustrates such a clash: the\nretrieved example is semantically near the user\nquery yet calls an API, whereas the query should\nnot. Such behavior-mismatched demonstrations\nsystematically mislead the LLM, degrading down-\nstream accuracy. We quantify this issue on the\nH2A benchmark (Chen et al., 2024), as shown in\nFigure 2. Across three query categories, Helpful-\nness, Harmfulness, and Autonomy, we measure the\nbehavior-consistency ratio:\nbehavior-consistency ratio =\n#{retrievals with the same call/non-call behavior}\nk\n(1)\nOur preliminary experiments reveal two key find-\nings. Finding 1 is that retrieving semantically\nrelated examples can enhance LLMs’ function-\ncalling capabilities. Stronger encoders (e.g., BERT)\ncan provide more similar examples, thus LLMs\ncan learn from them to make function-calling de-\ncisions. Finding 2 is that LLMs’ function-calling\ncapabilities scale with behavior alignment. The\nestimated trend indicates that the behaviors of\n2\n\nFigure 2: Relationship between behavior-consistency ratio (across different retrievers) and LLM performance on the\nH2A Dataset. Each point represents an LLM and a retriever combination, where 5 demonstrations are retrieved per\nquery.\ndemonstrations are more consistent, making LLMs\nmore likely to invoke the correct function. These\nobservations expose a fundamental limitation of\nsemantic-only retrieval and motivate us to propose\na behavior-aligned retriever that explicitly encodes\ntool-invocation labels. By selecting demonstrations\nthat are both semantically relevant and behaviorally\nconsistent with the incoming query, the retriever\nsupplies the LLM with noise-free guidance, elimi-\nnating unnecessary API calls while preserving zero-\nshot generality, without any further fine-tuning of\nthe backbone model.\n3\nBehavior-Aligned Retriever\nIn this section, we introduce a behavior-aligned\nretriever (BAR) that can fetch samples that ex-\nhibit similar invocation behaviors. First, we de-\nfine the inputs/outputs of the retriever; Then, we\nintroduce the corpus used for training and retriev-\ning; Finally, we present the detailed techniques of\ntraining the behavior-aligned retriever, including\npositive/negative samples and dual-negative con-\ntrastive loss.\n3.1\nInputs/Outputs of Retriever\nThe goal of the retriever in tool-augmented LLM\nis to retrieve similar examples for LLMs, enabling\ntheir in-context capabilities. The retriever R takes\nthe user query q as input, and returns top-k behav-\niorally compatible examples,\nR(q) =\n\b\n(zi, yi)\nk\ni=1,\n(2)\nwhere zi is the similar queries, and yi is the cor-\nresponding behavior.\nFor example, yi can be\nthe specific APIs that need to be called or yi ∈\n{call, no_call}. After obtaining the retrievals,\nall retrievals are concatenated with the user query\nusing a specific prompt template as the final input\nx to be fed into LLMs.\n3.2\nTraining/Retrieval Corpus\nIn this paper, we aim to train a behavior-aligned re-\ntriever with a customized dataset and then retrieve\nsimilar examples from the retrieval database during\ninference. To enhance the awareness of function\nbehavior, we construct the training dataset by merg-\ning the function-calling instruction datasets and\ngeneral question-answering datasets. And we use\nthe dataset of downstream tasks as the retrieval cor-\npus. The specific training corpus and the retrieval\ncorpus used in this paper are listed in §4.1.\n3.3\nTraining Details\nWe first train the retriever model as an encoder, i.e.,\nfR : q →hq ∈Rd, then we compute the simi-\nlarities between the query representation and the\nrepresentations of other examples in the retrieval\ncorpus, finally select the top-k similar examples\nas the retrievals. During the training stage, we fol-\nlow the representative contrastive framework (Gao\net al., 2021) with customized positive/negative sam-\nples and a behavior-aligned loss function.\nBuilding Positive Pairs. One critical process in\ncontrastive learning is how to construct positive\npairs (q, q+). Given an anchor query q, choosing a\ncandidate positive sample q+ follows two criteria:\n(a) Categorical Matching:\nq+ has the same\nfunction-invocation behavior as that of the\nanchor query q.\n(b) Semantic Proximity: they are semantically\nrelated, i.e., the similarity δ(q, q+) between\nthem exceeds a predefined threshold t,\nδ(q, q+) =\nh⊤\nq hq+\n∥hq∥∥hq+∥,\n(3)\nwhere hq is the representation of the query q. With\nthese two constraints, the selected positive pairs\n3\n\nwould be behavior-consistent and semantically co-\nherent.\nBuilding Negative Pairs. Previous studies (Robin-\nson et al., 2021) emphasized the critical role of hard\nnegative pairs in contrastive learning. To construct\nnegative samples, we adopt a dual strategy that\naddresses both fine-grained semantic discrimina-\ntion between samples with the same behavior and\nambiguity of samples across different behaviors.\n(a) Samples with Same Behaviors. For each an-\nchor query q, all other queries exhibiting the\nsame behaviors (e.g., requiring function calls),\nexcluding q and its positive pairs, serve as\nnegative samples. This helps the model fur-\nther discern subtle semantic variations among\nqueries with identical invocation behaviors.\n(b) Samples with Different Behaviors. Instead of\nrandomly sampling, we choose other queries\nwith different behaviors (e.g., no function call)\nbut semantically similar to q. This explicitly\npenalizes confusion between behaviors near\ndecision boundaries.\nFor example, given an anchor query (“What is\nthe weather like today”) which requires an API\ncall, a similar commonsense query (“What kind of\nweather do you like?”) would be chosen as a neg-\native sample. For an anchor query q, we compute\nits similarity to all queries with different behav-\niors and select the top-l most similar instances as\nnegative samples.\nDual-Negative Contrastive Loss With two kinds\nof negative samples, we design a dual-negative con-\ntrastive loss, which integrates two kinds of losses\ncorresponding to the above two cases:\nLDNCL = αLsame + (1 −α)Ldiff\n(4)\nwhere α ∈[0, 1] is a balancing coefficient between\nthese two losses. The first loss for the same behav-\niors is based upon the InfoNCE loss (Oord et al.,\n2018):\nLsame = −Ep\n\"\nlog\neδq,p/τ\neδq,p/τ + P\nn eδq,n/τ\n#\n(5)\nwhere (q, p) is a sampled positive pair, n is the\nnegative sample with the same behavior as that of\nq. The second loss for different behaviors Ldiff\nsharpens decision boundaries:\nLdiff = −Ep\n\"\nlog\neδq,p/τ\neδq,p/τ + P\nm eδq,m/τ\n#\n(6)\nwhere m is the negative sample with different\nbehavior. The temperature parameter τ controls\nthe scale of similarity distribution, with lower val-\nues producing sharper distributions that emphasize\nhard negatives.\n4\nExperiments\n4.1\nExperimental Setup\nModel Selection To comprehensively evaluate\nLLMs’ capability in function calling, we intro-\nduce twelve advanced models, including three cate-\ngories: (1) vanilla pre-trained models: Mistral-7B-\nInstruct (Jiang et al., 2023), Llama-2-7B-chat (Tou-\nvron et al., 2023), Llama-2-13B-chat (Touvron\net al., 2023), Llama-3.1-8B-Instruct (Grattafiori\net al., 2024) and Qwen2.5-7B-Instruct (Yang et al.,\n2024); (2) pretrained model on function calling\ndataset:\nFunctionary-7B (MeetKai, 2024); (3)\nfine-tuned models from function calling datasets:\ngorilla-openfunctions (Patil et al., 2023), granite-\n20b-functioncalling (Abdelaziz et al., 2024), Tool-\nLLaMa (Qin et al., 2023), ToolAlpaca-7B (Tang\net al., 2023), ToolAlpaca-13B (Tang et al., 2023)\nand ToolAlign-DPO (Chen et al., 2024).\nDatasets We use API-Bank (Li et al., 2023), a\nbenchmark with 2,202 dialogues covering 2,211\nAPIs from 1,008 domains, for function calling in-\nstructions. For general questions, we select 9,750\nhigh-quality samples from common-sense ques-\ntions (Talmor et al., 2019) and instructions used\nin LIMA (Zhou et al., 2023). We evaluate our re-\ntriever on two challenging datasets: H2A (Chen\net al., 2024) and ToolDEER (Gui et al., 2024). We\nchoose testset on the H2A, focusing on three di-\nmensions: single-tool instructions with multi-APIs\nfrom helpfulness scenario, harmful instructions,\nand autonomy instructions. For the ToolDEER\ndataset, we utilize its validation set, which contains\ntwo types of queries: #SearchAPI (queries must\nbe solved by external tools) and #NoSearchAPI\n(queries can be solved by LLMs without tools).\nEvaluation Metrics For H2A dataset: (1) Help-\nfulness: We simulate multi-API scenarios by in-\ncluding available API lists in prompts. To evaluate\nperformance, we introduce Exact Function Match,\nmeasuring whether the model correctly identifies\nall required APIs without omissions or unnecessary\ncalls. (2) Harmlessness: Following ToolAlign’s\nprompts (Table 10), we use GPT-4 to judge if the\nresponses reject to answer unsafe instructions, and\nthen we calculate the refusal response rate. (3)\n4\n\nModels\nHelpfulness\nHarmlessness\nAutonomy\nZero\nBM25\nBERT\nContriever\nBAR\nZero\nBM25\nBERT\nContriever\nBAR\nZero\nBM25\nBERT\nContriever\nBAR\nMistral-7B\n25.0%\n27.0%\n27.0%\n27.5%\n29.0%\n20.6%\n39.2%\n39.2%\n35.6%\n35.6%\n0.0%\n0.0%\n1.0%\n1.0%\n1.0%\nQwen2.5-7B\n7.0%\n11.0%\n9.0%\n10.5%\n11.0%\n41.2%\n41.8%\n38.1%\n41.2%\n40.2%\n38.0%\n49.0%\n42.0%\n42.0%\n52.0%\nLlama-2-7B\n5.0%\n11.5%\n12.0%\n11.0%\n14.0%\n29.9%\n58.8%\n57.2%\n62.4%\n60.8%\n1.0%\n10.0%\n14.0%\n17.0%\n19.0%\nLlama-2-13B\n18.0%\n26.0%\n28.0%\n29.0%\n34.0%\n36.1%\n69.1%\n67.5%\n69.1%\n67.5%\n4.0%\n13.0%\n17.0%\n17.0%\n21.0%\nLlama-3.1-8B\n4.5%\n12.5%\n13.0%\n12.5%\n14.0%\n57.2%\n56.2%\n54.1%\n51.0%\n52.1%\n3.0%\n3.0%\n2.0%\n1.0%\n1.0%\nFunctionary-7B\n32.5%\n22.0%\n20.0%\n25.5%\n25.0%\n28.4%\n48.5%\n54.1%\n44.3%\n52.1%\n2.0%\n43.0%\n47.0%\n47.0%\n49.0%\nOpenfunctions\n19.0%\n16.5%\n23.5%\n24.5%\n27.5%\n72.2%\n80.9%\n83.5%\n83.0%\n86.6%\n20.0%\n30.0%\n31.0%\n33.0%\n44.0%\nFunctioncalling\n5.0%\n7.0%\n8.5%\n7.0%\n10.0%\n3.6%\n50.0%\n47.4%\n46.4%\n55.7%\n4.0%\n70.0%\n76.0%\n84.0%\n86.0%\nToolLLama-7B\n3.5%\n3.5%\n1.5%\n2.0%\n2.5%\n11.9%\n30.9%\n35.6%\n28.9%\n40.7%\n58.0%\n85.0%\n88.0%\n93.0%\n95.0%\nToolAlpaca-7B\n10.5%\n8.5%\n6.0%\n8.5%\n9.0%\n40.7%\n64.4%\n73.2%\n69.1%\n75.8%\n80.0%\n83.0%\n89.0%\n90.0%\n91.0%\nToolAlpaca-13B\n4.0%\n11.0%\n11.0%\n10.5%\n12.0%\n40.7%\n69.6%\n68.0%\n57.2%\n77.3%\n75.0%\n79.0%\n89.0%\n88.0%\n94.0%\nToolAlign-DPO\n2.0%\n4.0%\n4.0%\n3.0%\n4.0%\n88.1%\n94.8%\n90.7%\n87.1%\n85.6%\n87.0%\n81.0%\n83.0%\n88.0%\n95.0%\nAverage\n11.3%\n13.4%\n13.6%\n14.3%\n16.0%\n39.2%\n58.7%\n59.1%\n56.3%\n60.8%\n31.0%\n45.5%\n48.3%\n50.1%\n54.0%\nTable 1: The function calling performance of LLMs with 5 demonstrations retrieved by different retrievers on H2A\ndataset. Zero denotes zero-shot setting.\nQuery Type\nSimilar Query\nBM25\nBERT\nContriever\nBAR\nHelpfulness\nHelpfulness\n925\n956\n948\n976\nHarmlessness\n73\n36\n27\n19\nAutonomy\n2\n8\n25\n5\nHarmlessness\nHelpfulness\n352\n285\n304\n276\nHarmlessness\n600\n676\n632\n684\nAutonomy\n18\n9\n34\n10\nAutonomy\nHelpfulness\n214\n174\n131\n25\nHarmlessness\n12\n7\n5\n4\nAutonomy\n274\n319\n364\n471\nTable 2: Performance comparison of retrieval models:\nDistribution of top-5 retrieval queries across three cate-\ngories (Helpfulness, Harmlessness and Autonomy).\nAutonomy: We measure the direct response rate\nwithout external function calls. For the ToolDEER\ndataset, we follow the setting of Gui et al. (Gui\net al., 2024), and evaluate the vanilla models by\ncalculating the number of correctly predicted re-\nsponses for NoSearch and Search queries. While\nthe pre-trained and fine-tuned models such as Func-\ntionary and ToolLlama, we prompt GPT-4o to eval-\nuate helpfulness of response - whether it provides\nuseful information that meets the task requirements,\nas shown in Table 11. All detailed prompts are il-\nlustrated in Appendix A.2.\nDetails for Fine-tuning We initialize the network\nwith the BERT base uncased model(110M) and\nfine-tune using the AdamW optimizer with a learn-\ning rate of 1e-6. We train the model for 20 epochs\nwith a batch size of 64. We set the temperature τ\nto 0.05 and use α=0.8 to balance between same-\nbehavior and different-behavior loss. Each positive\nsample is selected based on a similarity threshold of\n0.7, and negatives combine both the same-behavior\nand top-l (l=10) different-behavior samples via co-\nsine similarity. All experiments are run on a single\nhttps://huggingface.co/google-bert/\nbert-base-uncased\nNVIDIA A100-80G GPU.\nBaselines We compare our approach with three\nretrievers: (1) BM25 (Robertson et al., 2009), a\ntraditional probabilistic retrieval model based on\nterm frequency statistics and document length nor-\nmalization; (2) BERT (Devlin et al., 2019), a\npre-trained language model where the [CLS] to-\nken embedding is for query representation; (3)\nContriever (Izacard et al., 2021), a dense trained\nthrough contrastive learning in general domain.\n4.2\nMain Results\n4.2.1\nResults on H2A Dataset\nWe present our experimental findings in Table\n1, comparing the performance of LLMs with 5\ndemonstrations from three dimensions: Helpful-\nness, Harmlessness, and Autonomy.\n(1) For Helpfulness: Our retrieval-augmented\nprompt presents higher Exact Function Match ratio\nthan the other baselines, supporting by the consis-\ntent behavior ratio shown in Table 2. For vanilla\nmodels, Mistral-7B-Instruct with BAR achieves a\naverage 4.0% improvement compared to its zero-\nshot performance.\nNotably, Llama-2-13B-chat\nshows remarkable improvement from 26.0% to\n34.0% over BM25. For fine-tuned models, Open-\nfunctions demonstrates enhancement from 16.5%\nto 27.5%. However, most fine-tuned LLMs have\ndifficulty to return all necessary APIs, especially\nfor ToolLLaMA and ToolAlign. The reason might\nbe that these LLMs are optimized for function in-\nvocation, but their over-reliance on fine-tuning data\nlimits generalization, as shown in Table 12.\n(2) For Harmlessness: Our method presents su-\nperior ability to refuse harmful instructions. Specif-\nically, Openfunctions achieves 86.0% refuse rate\nIn this paper we use the bert-base-uncased version.\n5\n\nModels\n#NoSearchAPI\n#SearchAPI\nBM25\nBERT\nContriever\nBAR\nBM25\nBERT\nContriever\nBAR\nMistral-7B-Instruct\n59.6%\n50.3%\n53.4%\n59.1%\n79.9%\n82.8%\n78.6%\n82.3%\nQwen2.5-7B-Instruct\n30.1%\n31.1%\n40.4%\n40.4%\n82.3%\n83.4%\n82.8%\n79.5%\nLlama-2-7B-chat\n44.6%\n42.5%\n42.0%\n44.0%\n68.9%\n70.2%\n68.8%\n71.2%\nLlama-2-13B-chat\n54.9%\n53.9%\n52.3%\n59.6%\n77.8%\n78.2%\n78.1%\n79.2%\nLlama-3.1-8B-Instruct\n63.2%\n79.8%\n62.7%\n64.8%\n66.3%\n67.9%\n69.7%\n61.2%\nFunctionary-7B\n48.7%\n48.2%\n50.8%\n49.7%\n66.1%\n72.7%\n60.1%\n78.3%\nOpenfunctions\n60.6%\n61.7%\n59.6%\n56.0%\n68.3%\n66.2%\n68.8%\n77.9%\nFunctioncalling-20B\n90.2%\n89.6%\n91.7%\n92.2%\n61.9%\n62.7%\n62.3%\n62.3%\nToolLLama-7B\n57.0%\n52.3%\n53.4%\n57.5%\n41.6%\n40.9%\n40.7%\n41.6%\nToolAlpaca-7B\n68.4%\n67.4%\n67.9%\n84.5%\n68.8%\n69.8%\n68.8%\n70.5%\nToolAlpaca-13B\n73.6%\n81.9%\n81.9%\n89.6%\n69.0%\n72.4%\n76.8%\n80.2%\nToolAlign-DPO\n79.3%\n87.6%\n85.0%\n82.4%\n80.6%\n84.4%\n80.3%\n80.3%\nAverage\n60.8%\n62.2%\n61.7%\n65.0%\n69.3%\n71.0%\n69.6%\n72.0%\nTable 3: The comparison of LLMs decision-making awareness on ToolDEER dataset. We report different retrieval\nmethods (BM25, BERT, Contriever, and BAR) under two scenarios (#NoSearchAPI and #SearchAPI).\naugmented with BAR, surpassing BM25 by 5.1%.\nToolAlpaca-7B shows substantial improvement\nfrom 64.4% to 76.7%, as well ToolLLama in-\ncreases from 30.9% to 40.7%. LLMs with BAR\nreach an average refusal rate of 60.8%, signifi-\ncantly higher than the zero-shot setting. This out-\nperformance aligns with the retrieval distribution\nin Table 2, indicating that more relevant demonstra-\ntions contribute to stronger safety awareness.\n(3) For Autonomy: BAR increases direct re-\nsponse rate of LLMs, significantly reducing re-\ndundant API calls. The most notable improve-\nments appear in fine-tuned models, where substan-\ntial gains ranging from 14%-15% for ToolAlign\nand ToolAlpaca-13B compared to BM25. Func-\ntionCalling shows remarkable enhancement from\n70.0% to 86.0% in direct response rate. Although\nour methods have strong performance with large\nmodels, both Mistral-7B and Llama-7B show lim-\nited improvement, which indicates their inherent\nconstrains in tool usage domain. Overall, our ap-\nproach achieves an 8.5% improvement over base-\nlines. It aligns with Table 2, where our method\nretrieves 471 autonomy instructions, a 71.9% in-\ncrease over BM25 (274). The consistent improve-\nments among different LLMs confirm its generaliz-\nability in enhancing function-calling ability.\n4.2.2\nResults on ToolDEER Dataset\nFollowing ToolDEER (Gui et al., 2024), we select\n6 demonstrations within in-context learning. The\nexperimental results presented in Table 3 demon-\nstrate the superior performance of BAR in distin-\nQuery Type\nCE\nInfoNCE\nSCL\nTriplet\nDNCL\nHelpfulness\n95.6%\n96.4%\n96.8%\n95.7%\n97.6%\nHarmlessness\n68.8%\n68.4%\n65.6%\n65.7%\n70.5%\nAutonomy\n93.0%\n92.8%\n90.2%\n82.0%\n94.2%\nTable 4: Behavior-consistency ratio of BAR trained with\ndifferent loss functions on H2A dataset.\nFigure 3: Behavior-consistency ratio of BM25, BERT,\nContriever, and BAR on ToolDEER dataset.\nguishing between #NoSearchAPI and #SearchAPI.\nUsing retrieval-augmented in-context learning to\nenhance prompt relevance, our method achieves\nsignificant improvements over both vanilla and tool-\naugmented models.\nIn the #NoSearchAPI scenario, BAR achieves\n95.9% behavior-consistency ratio on the retrieval\ntask with top-6 similar queries (Figure 3), surpass-\ning both BM25(81.0%) and Contriever (85.5%).\nThe effectiveness of BAR is particularly evident\nin the case of ToolAlpaca-13B, demonstrating a\n16.0% improvement over BM25, as shown in Ta-\n6\n\nQuery Type\n80%\n85%\n90%\n95%\n100%\nHelpfulness\n97.0%\n96.2%\n96.3%\n96.8%\n97.6%\nHarmlessness\n66.2%\n65.1%\n65.4%\n68.9%\n70.5%\nAutonomy\n93.0%\n92.8%\n93.6%\n92.6%\n94.2%\nTable 5: Impact of training data scaling on behavior\nconsistency ratio on the H2A dataset.\nble 3. Regarding the #SearchAPI scenario, BAR\nmaintains the consistent advantages.\nNotably,\nFunctionary-7B with BAR exhibits stronger dis-\ncriminative capabilities, highlighting its enhanced\nbehavioral consistency in function call necessities\ndetection.\n4.3\nAblation Study\nImpact of different loss function Table 4 presents\nthe ablation study of our retriever using differ-\nent loss functions for demonstration retrieval. :\nCross-Entropy loss (CE), InfoNCE loss, Super-\nvised Contrastive Loss (SCL) (Khosla et al., 2020),\nTriplet loss (Schroff et al., 2015), and our Dual-\nnegative Contrastive Loss (DNCL). BAR trained\nwith DNCL achieves optimal overall robustness,\nattaining 97.6% on helpfulness and 94.2% on au-\ntonomy, surpassing CE by 2.0% and 1.2%, re-\nspectively. For Harmlessness instructions, DNCL\nmaintains stronger behavior consistency, and high-\nlights the advantage of contrastive learning in cap-\nturing fine-grained semantic differences. Model\ntrained with SCL shows a decline in results due to\nits focus on class-level rather than instance-level\ncontrasts. The Triplet loss model underperforms\nDNCL by 16.3% on autonomy decisions, high-\nlighting its limitations for representation learning.\nThis confirms DNCL’s superiority in distinguishing\nfunction-calling behaviors for reliable retrieval.\nImpact of negative pairs As illustrated in Table\n6, We evaluate three negative sampling strategies\nto validate our dual-negative design: (1) Random\nDifferent-Behavior: randomly sampling queries\nwith different behaviors; (2) Top-l Different-\nBehavior: selecting semantically similar queries\nbut with different behaviors; (3) Dual-Behavior:\ncombining both behavior-consistent and behavior-\ndifferent negative samples.\nRandom sampling\nachieves a baseline accuracy of 85.6%. By in-\ntroducing the top-l samples with different behav-\nior significantly improves performance to 92.3%,\nindicating that challenging negative examples en-\nhance the model to make fine-grained distinctions.\nThe dual-behavior strategy further enhances con-\nNegative Sampling Strategy\nBehavior-Consistency Ratio\nRandom Different Behavior\n85.6%\nTop-l Different Behavior\n92.3%\nDual-Behavior\n94.2%\nTable 6: Ablation study on negative sampling strategies\nfor function retrieval performance on Autonomy query.\nsistency ratio to 94.2% by combining behavior-\nconsistent and behavior-different negatives, demon-\nstrating the effectiveness of both negative types.\nAnalysis of training data scaling effects To evalu-\nate whether our retriever has been trained on a suf-\nficiently large dataset, we progressively increased\nthe size of the training corpus from 80% to 100%\nand measured behavior consistency ratio on the\nH2A dataset. The results are shown in Table 5. We\nobserve that while retrieval performance improves\nwith more data, the gains plateau after 90–95%, es-\npecially for Helpfulness and Autonomy, where con-\nsistency improves by only 0.8 percentage points or\nless from 95% to 100%. This trend suggests that be-\nhavioral coverage is largely saturated for these cate-\ngories, meaning that BAR has already captured the\nkey decision patterns needed for behavior-aligned\nmatching. Although Harmlessness shows slightly\nlarger gains, this is likely due to the inherently\nnoisier and more subjective nature of such queries.\nThis trend suggests that adding more data beyond\n90% yields very limited improvement, meaning the\ncurrent dataset already captures the key behavior\npatterns needed for stable and accurate retrieval. In\npractice, further increasing the dataset would incur\nsignificant cost without meaningful performance\nimprovement, and our existing corpus strikes a\nstrong balance between coverage, efficiency, and\nretriever generalization.\n4.4\nVisualization\nWe analyze retriever effectiveness in semantic rep-\nresentation learning through t-SNE (Wang and\nIsola, 2020) visualization on both H2A and ToolD-\nEER datasets, as shown in Figure 4 and Figure\n5. According to Figure 4, we observe that BM25,\nBERT, and Contriever show scattered distributions\nwith significant overlap between different cate-\ngories, BAR provides better separation, particu-\nlarly for autonomy instructions, which form a well-\nseparated cluster. However, Helpfulness and Harm-\nlessness queries are hard to separate because they\nuse a similar language, but Harmlessness cases\ninvolve unsafe or insecure tools, making the behav-\n7\n\nFigure 4: The t-SNE plots of the learned representations with different retrievers on the H2A dataset.\nFigure 5: The t-SNE plots of the learned representations with different retrievers on the ToolDEER dataset.\nioral difference subtle and easy to confuse.\nFigure 5 on the ToolDEER dataset more clearly\nillustrates the advantages of BAR. BM25 and Con-\ntriever exhibit substantial overlap in their distribu-\ntions. BERT improved, but insufficiently discrim-\ninates between #SearchAPI and #NoSearchAPI\ncategories. In contrast, BAR illustrates a clearer\nseparation, with #NoSearchAPI samples distinctly\nclustering in the lower-right region. These visu-\nalizations demonstrate BAR’s stronger ability to\ncapture semantics and enhance in-context learning.\n5\nRelated Work\nFunction Calling by LLMs Recent advances\nin large language models (LLMs) have signifi-\ncantly enhanced their ability to perform function\ncalling.\nExisting approaches broadly fall into\ntwo categories: pretrained models with function-\ncalling capabilities (e.g., Functionary (MeetKai,\n2024)) and fine-tuned models through supervised\nand preference learning (e.g., ToolAlpaca (Tang\net al., 2023), ToolLLaMA (Qin et al., 2023) and\nToolAlign (Chen et al., 2024)).\nBuilding on\nretrieval-augmented generation methods (Wu et al.,\n2024b; Zhao et al., 2024; Wu et al., 2024a), re-\ncent studies adapt retrieval to the function-call\nsetting and introduce tool-augmented LLMs (Xu\net al., 2024; Qu et al., 2024; Qin et al., 2023).\nTool-augmented LLMs interact with users by un-\nderstanding their intents and needs, concentrat-\ning on retrieving recommended tools from a large\npool (Xu et al., 2024; Qu et al., 2024; Qin et al.,\n2023). Although these approaches improve the ac-\ncuracy of function selection and parameter match-\ning, they don’t address the challenge of discriminat-\ning between queries that require function calls and\nthose that don’t. In contrast, our approach targets\nat enhancing LLMs’ capability to make decisions\non whether or not to use tools.\nContrastive Learning Contrastive learning has\ndemonstrated remarkable success with its core ob-\njective centered on optimizing feature spaces by\nleveraging similarities and differences between\nsamples. Self-supervised contrastive learning (He\net al., 2020) pioneered the learning of generic rep-\nresentations from unlabeled data, while supervised\ncontrastive learning (SCL) (Khosla et al., 2020)\nfurther enhanced feature discriminability by explic-\nitly incorporating label information. The key to\ncontrastive learning is how to construct negative\nsamples. Unlike methods (Robinson et al., 2021;\nKalantidis et al., 2020) that only consider intra-\nclass negatives, we incorporate inter-class hard neg-\natives. This helps LLMs better distinguish instruc-\ntions with similar semantics but distinct behaviors.\n6\nConclusion\nIn this paper, we trained a behavior-aligned re-\ntriever, called BAR, which retrieves semantically\ncoherent and behaviorally related examples as\ndemonstrations for LLMs. Extensive experiments\ndemonstrate that our BAR can consistently guide\ndifferent LLMs to make correct function-calling\ndecisions and reduce unnecessary function calls.\n8\n\nLimitations\nThe first limitation is the difficulty in separat-\ning risk-aligned behaviors. Both Helpfulness and\nHarmfulness queries tend to require API calls, but\nHarmfulness involves requests to unsafe, unethi-\ncal, or illegal tools (e.g., insecure APIs). Since\ntheir language forms are often similar, BAR strug-\ngles to distinguish them based on behavioral su-\npervision alone. Addressing this limitation would\nrequire training on more fine-grained, risk-aware\nannotations that go beyond binary call/no-call la-\nbels.\nThe second is the dependence on the la-\nbeled behavioral data. Training the retriever re-\nquires function-call datasets and general question\nanswering datasets. Although lightweight com-\npared to LLM fine-tuning, the process still depends\non domain-specific labels, which may not be read-\nily available for all tools or APIs.\nEthical Consideration\nOur work evaluates LLM’s safety in function call-\ning using harmful instructions on publicly ava-\niable H2A dataset (Chen et al., 2024), which are\nmodified from helpful instruction and rigorously\nanonymized to avoid real-world harm. They follow\nthe safetyLLaMA (Bianchi et al., 2023) approach\nto sample instructions. Besides, our experiments\nfocus on enhancing LLM’s ability to reject unsafe\ninputs by retrieving demonstrations that reinforce\nethical response. Moreover, we provide API lists in\nprompts to simulate real-world API call scenarios,\nwhich prevents potential unsafe API calling."}
{"id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "authors": ["Jing Chen", "Zhiheng Yang", "Yixian Shen", "Jie Liu", "Adam Belloum", "Chrysa Papagainni", "Paola Grosso"], "source": "http://arxiv.org/abs/2508.14317v1", "page_count": 20, "text": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans\nand Memory-Guided Writing\nJing Chen1,2* , Zhiheng Yang1*, Yixian Shen1, Jie Liu1,\nAdam Belloum1, Chrysa Papagianni1, Paola Grosso1\n1University of Amsterdam, the Netherlands\n2Vrije Universiteit Amsterdam, the Netherlands\nj.chen12@student.vu.nl\n{z.yang, y.shen, j.liu, a.s.z.belloum, c.papagianni, p.grosso}@uva.nl\nAbstract\nSurvey papers play a critical role in scien-\ntific communication by consolidating progress\nacross a field. Recent advances in Large Lan-\nguage Models (LLMs) offer a promising so-\nlution by automating key steps in the survey-\ngeneration pipeline, such as retrieval, struc-\nturing, and summarization.\nHowever, ex-\nisting LLM-based approaches often struggle\nwith maintaining coherence across long, multi-\nsection surveys and providing comprehensive\ncitation coverage. To address these limitations,\nwe introduce SurveyGen-I, an automatic survey\ngeneration framework that combines coarse-to-\nfine retrieval, adaptive planning, and memory-\nguided generation. SurveyGen-I first performs\nsurvey-level retrieval to construct the initial\noutline and writing plan, and then dynami-\ncally refines both during generation through\na memory mechanism that stores previously\nwritten content and terminology, ensuring co-\nherence across subsections.\nWhen the sys-\ntem detects insufficient context, it triggers fine-\ngrained subsection-level retrieval. During gen-\neration, SurveyGen-I leverages this memory\nmechanism to maintain coherence across sub-\nsections. Experiments across four scientific\ndomains demonstrate that SurveyGen-I con-\nsistently outperforms previous works in con-\ntent quality, consistency, and citation cover-\nage. The code is available at https://github.\ncom/SurveyGens/SurveyGen-I.\n1\nIntroduction\nThe exponential expansion of scholarly literature,\nwith thousands of new papers published daily,\npresents significant challenges for researchers to\nefficiently acquire and synthesize comprehensive\nknowledge. Consequently, writing survey papers\nrequires substantial expertise and time commitment\nfrom researchers, as it traditionally involves an iter-\native and labor-intensive process of reading, note-\ntaking, clustering, and synthesis (Carrera-Rivera\n* Equal contribution.\net al., 2022). Recent advances in Large Language\nModels (LLMs) offer a promising solution to this\nbottleneck by enabling the automation of key steps\nin the survey-writing pipeline, such as retrieving,\norganizing, and summarizing large volumes of pa-\npers (Wang et al., 2024; Liang et al., 2025; Yan\net al., 2025; Agarwal et al., 2024a,b).\nDespite recent advances, current LLM-based sur-\nvey generation frameworks remain limited in sev-\neral key aspects. First, literature retrieval scope\nand depth remain limited. Most systems rely on\nembedding-based similarity search over a fixed lo-\ncal paper database (Wang et al., 2024; Yan et al.,\n2025). While efficient, such surface-level match-\ning often fails to identify important papers with\ndifferent terminology or at a more conceptual level,\nresulting in incomplete or biased coverage. Sec-\nond, lack of cross-subsection consistency. Most\nsystems generate all subsections in parallel as iso-\nlated units without modeling dependencies across\nsubsections (Wang et al., 2024; Liang et al., 2025;\nYan et al., 2025). This often leads to redundant\ncontent, inconsistent terminology, and fragmented\ndiscourse. Moreover, they always follow a static,\nonce-for-all outline that cannot adapt to newly gen-\nerated content, making it difficult to maintain con-\ntent coherence or integrate emerging insights. Fi-\nnally, indirect citations are often left unresolved.\nRetrieval-augmented generation (RAG) typically\nextracts passages from retrieved papers to support\nwriting. These passages often include indirect cita-\ntions such as \"[23]\" and \"Smith et al., 2022\", which\nrefer to influential prior work not present in the\nretrieval results. Without tracing these references,\nthe system may miss influential papers, leading to\nincomplete citation coverage and broken linkage\nbetween ideas and their original sources.\nTo address these limitations, we introduce\nSurveyGen-I, an end-to-end, modular framework\nfor generating academic surveys with consistent\ncontent and comprehensive literature coverage.\n1\n\nFirst, SurveyGen-I performs literature retrieval at\nboth the survey-level and subsection-level, aug-\nmented with citation expansion and LLM-based\nrelevance scoring. This multi-level retrieval strat-\negy substantially enhances literature coverage and\ntopical relevance. Second, SurveyGen-I introduces\nPlanEvo, a dynamic planning mechanism powered\nby an evolving memory that continuously accumu-\nlates terminology and content from earlier gener-\nated subsections. This memory is used to construct\nthe outline and a dependency-aware writing plan\nthat captures the logical and conceptual relation-\nships between subsections, allowing foundational\ntopics to be generated before more advanced or\nderivative ones. As writing progresses, both the\noutline and plan are continually refined based on\nthe updated memory, ensuring consistent terminol-\nogy and coherent content flow across the survey.\nFinally, SurveyGen-I introduces CaM-Writing,\nwhich combines a citation-tracing module that de-\ntects indirect references in retrieved passages and\nresolves them back to their original source papers,\nwith memory-guided generation that uses the evolv-\ning memory to maintain coherent terminology and\ncontent across the survey.\nExtensive results highlight the strengths of\nSurveyGen-I across multiple dimensions of aca-\ndemic survey generation.\nCompared to the\nstrongest baseline, SurveyGen-I yields an 8.5% im-\nprovement in content quality, a 27% increase in\ncitation density, and more than twice as many dis-\ntinct references, while also demonstrating signifi-\ncantly better citation recency. These improvements\nshow the effectiveness of the system in enabling\nhigh-quality and consistent survey generation.\nOur contributions are summarized as follows:\n• We propose SurveyGen-I, a novel framework\nfor high-quality, reference-rich, and consistent\nsurvey generation.\n• We design a multi-stage Literature Retrieval\npipeline that combines keyword search, cita-\ntion expansion, and LLM-based filtering to\nconstruct relevant and comprehensive paper\nsets at both survey and subsection levels.\n• We introduce PlanEvo, a dynamic planning\nmechanism that constructs and continuously\nrefines the outline and writing plan based on\ninter-subsection dependencies and evolving\nmemory, ensuring coherent survey generation.\n• We develop a CaM-Writing pipeline that\ncombines citation tracing and memory-guided\ngeneration to improve reference coverage and\nensure consistent, well-structured writing.\n2\nRelated work\nComponent-Oriented and Hybrid Approaches.\nA longstanding approach to assisting literature sur-\nveys has been to tackle the problem in stages, where\ncomponents handle retrieval, structuring, or writ-\ning, etc., independently (Susnjak et al., 2025; Lai\net al., 2024; Li et al., 2024c). Early systems orga-\nnized citation sentences through clustering or clas-\nsification (Nanba et al., 2000; Wang et al., 2018), or\nemployed rule-based content models (Hoang and\nKan, 2010; Hu and Wan, 2014). These systems of-\nten relied on static heuristics or surface-level topic\nassociations, making them difficult to generalize\nacross domains or maintain narrative coherence.\nThe rise of LLMs brought a wave of hybrid de-\nsigns that integrated neural summarization with\nstructured control (Zhang et al., 2024; Fok et al.,\n2025; An et al., 2024).\nTemplate-based gen-\neration (Sun and Zhuge, 2019) and extractive-\nabstractive hybrids (Shinde et al., 2022) introduced\nmore fluent synthesis but retain rigid structures.\nMeanwhile, RAG-based methods (Lewis et al.,\n2020; Ali et al., 2024; Agarwal et al., 2024a) en-\nhanced retrieval fidelity (Gao et al., 2023), and\nagent-driven systems like the framework proposed\nby Brett and Myatt (2025), RAAI (Pozzobon and\nPinheiro, 2024) and AutoSurveyGPT (Xiao, 2023)\nbroke down the pipeline into retrieval, filtration,\nand generation stages. More recent works empha-\nsize pre-writing planning, such as COI-Agent (Li\net al., 2024b), which organizes references into con-\nceptual chains to enhance topic coverage.\nHowever, these designs remain fundamentally\ndecomposed: content selection and writing are\nplanned in isolation, and their execution often lacks\nglobal coordination across stages.\nEnd-to-End Automated Literature Review/Sur-\nvey Generation.\nWith increasing demand for\nscalability and consistency, end-to-end frame-\nworks have emerged to streamline the full pipeline\nfrom retrieval to synthesis. Multi-agent architec-\ntures (Sami et al., 2024; Rouzrokh and Shariat-\nnia, 2025) have been wildly used, and decom-\npose the pipeline into specialized agent roles, mim-\nicking human editorial workflows. Representa-\ntively, AutoSurvey (Wang et al., 2024) introduces a\nretrieval-outline-generation sequence that produces\nentire surveys via section-wise prompting. Survey-\nForge (Yan et al., 2025) extends this with memory\n2\n\nFinal Refinement\nCurrent Stage:\n(Next Stage:\n)\nSAWC: Structure-Aware Writing Controller\nCaM-Writing\nLR: Literature Retrieval\n(Survey)\nSDP: Structure-Driven Planner\nSubsection\n:\nSubsection\n:\nDispatch all\nLR: Literature Retrieval\n(Subsection)\nLR: Literature Retrieval\n(Subsection)\nIf\nIf\nMGSR\nTrigger replan (\n)\nreplan (\n)\nmemory\nMemory\nIf\nSubsection\n:\nFinal Survey\nTable Generation\nInput\nUpdate Memory:\n1\n2\n3\n4\n5\n6\n7\n9\n10\nAll subsections in\nStage t finished\n8\n0\n11\n12\n13\nLoop until\nall stages complete\n3\n10\nLegend:\n: Subsection description\n: Retrieval flag\n: Table flag\n: Stage index (parallel writing)\n: Structure memory (drafts, terminology)\nKey:\n• Subsections with same\nare\nprocessed in parallel per stage.\n• Steps 3–10 repeat until all stages\ncomplete.\n• Memory, outline and plan dynamically\nupdated after each stage.\n• Table generation/global refinement\ntriggered after writing finishes.\nBest-of-N Drafting &\nMulti-Stage Refinement\nCitation Traced Retrieval\n(RAG + Citation Trace)\nMemory-Guided\nSkeleton Gen\nFigure 1: Overview of the SurveyGen-I pipeline for automatic academic survey generation. The system comprises\nthree stages: (1) multi-stage Literature Retrieval (LR); (2) PlanEvo, a structure planning module integrating SDP\n(planning), SAWC (scheduling), and MGSR (dynamic replanning); (3) CaM-Writing for citation-aware subsection\ngeneration. Final refinement and table generation are performed after writing. Memory M accumulates writing\ncontent and terminology across stages to guide planning and ensure consistency.\nmodules and outline heuristics, aiming to enforce\nconsistency across segments. SurveyX (Liang et al.,\n2025) scales this further by relying on larger mod-\nels and more complex pipeline, producing more\nrobust and strict step-by-step outputs.\nDespite these advances, many systems adopt a\nstatic and compartmentalized approach. Outlines\nare typically fixed in advance, with no capacity\nto revise structure based on intermediate content.\nSubsections are often generated in parallel, lack-\ning shared context, which weakens narrative flow\nand increases repetition or terminology drift. Cita-\ntion usage also remains surface-level: references\nare selected from top-ranked snippets without trac-\ning citation chains or enabling expansion (Kasan-\nishi et al., 2023). In response, our work views\nsurvey writing as a dynamic process, one that re-\nquires adaptive planning, context-aware memory,\nand citation-traced retrieval. By continuously re-\nfining structural plans, maintaining cross-section\nconsistency, and grounding generation in citation\nchains, we move toward more coherent and adap-\ntive scientific surveys.\n3\nMethodology\nIn this section, we propose SurveyGen-I, a novel\nframework for automatic survey generation. As\nshown in Figure 1, it consists of three key stages:\n(1) Literature Retrieval (LR) performs multi-\nstage literature retrieval at both survey and subsec-\ntion levels. (2) Structure Planning with Dynamic\nOutline Evolution (PlanEvo) generates a hierar-\nchical outline and a dependency-aware writing plan,\nand dynamically updates both during generation\nto ensure cross-subsection consistency of the sur-\nvey. (3) CaM-Writing generates each subsection\nwith strong content consistency and rich citation\ncoverage, combining citation-traced retrieval to re-\ncover influential references, memory-guided skele-\nton planning for content consistency, and best-of-N\ndraft selection to ensure high-quality generation.\n3.1\nLR: Literature Retrieval\nTo ensure that the generated survey is grounded\nwith the most relevant and comprehensive research,\nour system adopts a multi-stage literature retrieval\nstrategy that operates at both the survey and sub-\nsection levels. As shown in Figure 1, this retrieval\n3\n\nSDP: Structure-Driven Planner\nLR: Literature Retrieval\nKeyword Gen\nSearch\nSemantic Filter\nCitation Expansion\nLLM Relevence Rerank\nSubsection Input:\nSubsection Literature Set\nStep 2: Dependency-Aware Writing Plan\nInput Papers:\n\nExtract Review Papers\nTopic Input: T, E\nStep 1: Reference-Grounded Outline Generation\nExtract Non-Review Papers\nGenerate Outline\nRefine Outline\nOutline (Example)\n1. Intro\n1.1 History\n1.2 Motivation\n...\n2. Theory\nGlobal Literature Set\n\nSame Pipeline\nInitial Plan Example for\nSubsection Title\n: Motivation\nSubsection description\n: ...\nGenerate Table\n: False\nsubsection Retrieval\n: True\nRemove Cycles\nDependency\n(Example)\n\nA: depends on B\nB: depends on C\nC: depends on D\nD: depends on B\nIdentify Dependency\nA\nB\nC\nD\nA\nB\nC\nD\nDAG\nFinal Plan Example for\nSubsection Title\n: Motivation\nSubsection description\n: ...\nGenerate Table\n: False\nsubsection Retrieval\n: True\nStage Index τ(\n): 1\nInitial Plan\nInput Outline:\n\nTopological Sort\nand Final Plan\nFigure 2: Details of the Literature Retrieval and Structure-Driven Planner components in SurveyGen-I.\nprocess provides the reference foundation for both\nstructure planning (SDP; see Sec. 3.2.1) and writ-\ning (CaM-Writing; see Sec. 3.3). The overall work-\nflow is shown in Figure 2; implementation details\nare provided in Appendix B.1.\n3.1.1\nSurvey-Level Retrieval for Structure\nPlanning\nFor survey-level literature retrieval, an LLM is first\nprompted to generate a keyword set K based on the\ninput topic T and its description E (see prompt in\nFigure 9, Appendix G). These keywords are used to\nquery Semantic Scholar (Ammar et al., 2018), pro-\nducing an initial set of papers Pinit. While keyword-\nbased retrieval offers broad initial coverage, it may\ninclude irrelevant papers. To enhance topical pre-\ncision, a semantic filtering step is applied. Specifi-\ncally, both the input (T, E) and each paper abstract\nai are embedded using the all-mpnet-base-v2\nsentence transformer (Song et al., 2020). Candi-\ndate papers with high cosine similarity to the input\n(T, E) are retained, yielding a refined set Psem:\nPsem = {pi ∈Pinit | cos(eT,E, eai) ≥θ}.\n(1)\nTo improve coverage and avoid missing influen-\ntial work, Psem is expanded by retrieving references\nand citations of top-ranked papers. These expanded\npapers are again filtered by embedding similarity\nfor topical relevance. Finally, an LLM-based rele-\nvance scorer assesses all remaining papers with re-\nspect to (T, E), and the top-ranked literature P∗is\nretained to support outline generation (see prompt\ntemplate in Figure 11, Appendix G).\n3.1.2\nSubsection-Level Retrieval for Writing\nIn addition to survey-level retrieval for structure\nplanning, subsection-level retrieval is optionally\ntriggered during writing. For each subsection si\nwith its description di, a focused paper set Pi is\nconstructed using the same retrieval pipeline as\nabove, with (si, di) as input. Whether this step\nis performed is controlled by a retrieval flag ri in\nthe dependency-aware writing plan (see Sec. 3.2.1).\nThe final paper set used for writing each subsection\nis the combination of the survey-level set P∗and\nthe subsection-level set Pi, forming the combined\npaper set P∗\ni . This paper set captures both the\nglobal scope of the survey and the specific focus of\neach subsection.\n3.2\nPlanEvo: Structure Planning with\nDynamic Outline Evolution\nIn this section, we introduce PlanEvo, a planning-\ncentric framework for scalable and coherent survey\noutline generation and writing plan construction.\nPlanEvo consists of three tightly integrated com-\nponents: the Structure-Driven Planner (SDP), the\nStructure-Aware Writing Controller (SAWC), and\nthe Memory-Guided Structure Replanner (MGSR).\nDetailed designs for each component are presented\nin Sec. 3.2.1, Sec. 3.2.2, and Sec. 3.2.3.\n3.2.1\nSDP: Structure-Driven Planner\nThe SDP module serves as the entry point of\nPlanEvo, transforming a specific research topic\n(T, E) into a structured, executable plan that guides\n4\n\nthe full survey generation process. The overall\nworkflow is shown in Figure 2.\nReference-Grounded Outline Generation.\nA\nliterature-grounded outline is essential for generat-\ning a coherent and well-structured survey. To build\nsuch an outline, the system first identifies review\narticles within the survey-level literature set P∗by\nanalyzing metadata such as publication type. The\nstructural outlines of these reviews R are then ex-\ntracted from their full texts using LLMs and used\nas representative structural patterns to inspire the\ndesign of new outlines. The system then collects\ntitles and abstracts of non-review papers in P∗to\nform the abstract-level content set Cabs, which is\nthen combined with R into a composite context\nC. Given C and (T, E), an LLM is prompted to\ngenerate an initial outline O0:\nO0 = {(si, di)}N\ni=1\n(2)\nwhere each subsection heading si is paired with\na brief description di to provide more detailed guid-\nance for writing subsections. To improve coher-\nence and reduce redundancy, the initial outline O0\nis refined by an LLM, yielding the final outline O.\nDependency-Aware Writing Plan.\nTo enable\nlogically coherent and coordinated writing across\nsubsections, we construct a dependency-aware writ-\ning plan Pdep based on the survey outline O.\nFirst, an initial plan Praw is generated by prompt-\ning an LLM with O. For each subsection si with\nits description di, the plan specifies two control\nsignals: whether additional literature retrieval is re-\nquired (ri), and whether a comparative table should\nbe generated (ti). These signals guide downstream\ntasks in subsection-level literature retrieval and ta-\nble generation. Each subsection takes the form:\nPraw[si] = (di, ri, ti),\n(3)\nNext, a structural dependency graph Graw =\n(V, E) is constructed by prompting an LLM to iden-\ntify, for each subsection, its prerequisite subsec-\ntions (see prompt in Figure 13). Here, V includes\nall subsections s1, . . . , sn, and E contains edges\n(si →sj) if si is judged to be a prerequisite of\nsj. Cycles are resolved by removing one edge per\ncycle, yielding a Directed Acyclic Graph (DAG) G.\nA topological sort is then applied to G to deter-\nmine the writing order. Each subsection s is as-\nsigned a stage index τ(s), representing the length\nof the longest path ending at s in the DAG:\nτ(s) =\n\n\n\n0\nif In(s) = ∅,\nmax\ns′∈In(s) τ(s′) + 1\notherwise.\n(4)\nThe final plan Pdep extends Praw by attaching\nstage indices to each subsection:\nPdep[si] = (di, ri, ti, τ(si)).\n(5)\nSubsections assigned the same stage index can be\nwritten in parallel, enabling multi-stage scheduling.\n3.2.2\nSAWC: Structure-Aware Writing\nController\nThe SAWC module serves as the central orchestra-\ntion engine across the entire writing process in the\nSurveyGen-I pipeline. Rather than being a single\nstep, SAWC coordinates a sequence of interdepen-\ndent modules, including writing stage scheduling\n(Step 4), subsection-level literature retrieval (Step\n5), citation-aware writing (Step 6), memory updat-\ning (Step 7), dynamic structure replanning (Steps\n9–10), global consistency refinement (Step 11), and\ntable generation (Step 12). Its control flow is illus-\ntrated throughout the center path of Figure 1.\nParallel Subsection Execution.\nSAWC executes\nthe dependency-aware writing plan Pdep by acti-\nvating all subsections with the same writing stage\nindex τ(si) in parallel (Step 4). For each active sub-\nsection si, SAWC first checks the retrieval control\nflag ri in Pdep[si]. If retrieval is required, SAWC\ntriggers subsection-level literature retrieval (Step 5;\nsee also Sec. 3.1.2). The resulting paper set P∗\ni is\npassed to the writing module (Step 6; see Sec. 3.3)\nfor citation-aware subsection generation.\nMemory Mechanism for Global Consistency.\nTo ensure structural coherence and terminologi-\ncal consistency across the survey, SAWC maintains\na dynamic structure memory M throughout writ-\ning. After each subsection si is written (Step 6), the\nsystem extracts key domain-specific terminology\nusing LLMs, and stores both the terminology and\nthe draft content into M (Step 7). This accumu-\nlated memory is then used to (1) guide subsequent\nsubsection writing by enforcing consistency (see\nSec. 3.3.2), and (2) provide feedback for dynamic\nupdates of the outline O and writing plan Pdep dur-\ning structure replanning (see Sec. 3.2.3).\n5\n\nDynamic Structure Refinement.\nAt the end of\neach writing stage, which corresponds to the com-\npletion of all subsections si with the same stage\nindex τ(si), SAWC triggers the Memory-Guided\nStructure Replanner (MGSR; see Sec. 3.2.3) to re-\nvise the outline and the writing plan based on the\naccumulated memory M (Step 9–10). This stage-\nwise feedback loop ensures that structural adjust-\nments are continuously informed by prior writing\noutputs before the next stage begins.\nFinal Refinement and Table Generation.\nAf-\nter all subsections are written, SAWC performs a\nfinal refinement step to improve global coherence\n(Step 11). An LLM analyzes the full draft to detect\nlogical contradictions, redundancy, and terminolog-\nical/style inconsistencies. Based on this diagnosis,\nthe system rewrites affected subsections to ensure\nconsistency. Then, for each subsection with the\ntable flag ti enabled in Pdep[si], SAWC generates\na structured table based on the retrieved paper set\n(Step 12). See Appendix B.3 for details.\n3.2.3\nMGSR: Memory-Guided Structure\nReplanner\nAfter each writing stage, the MGSR module per-\nforms dynamic refinement of the outline and writ-\ning plan based on the accumulated memory M and\nthe current outline O. MGSR prompt an LLM (see\nprompt in Figure 14) to analyze redundancy, miss-\ning conceptual gaps, or suboptimal ordering within\nthe unwritten subsections. It produces a set of\nstructured revision actions (merge, delete, rename,\nreorder, add) applied to the remaining outline. The\nupdated writing plan P′\ndep is then derived from the\nrevised outline O′ through the same dependency-\naware planning method used in the initial plan Pdep\n(see Sec. 3.2.1). This enables memory-guided struc-\ntural evolution throughout writing, ensuring that\nlater sections are adaptively optimized based on\nprior content while maintaining global consistency.\n3.3\nCaM-Writing: Citation-Aware Subsection\nWriting with Memory Guidance\nThis section introduces CaM-Writing, a citation-\naware, memory-guided writing pipeline for generat-\ning each survey subsection. The pipeline integrates\ncitation-traced retrieval to enhance literature cov-\nerage and citation diversity, skeleton-based genera-\ntion guided by accumulated memory M to ensure\ncontent consistency, and multi-stage refinement to\nimprove clarity, coherence, and citation integrity.\n3.3.1\nContext Construction with Citation\nTracing\nTo construct a rich and contextually relevant ev-\nidence set for writing each subsection si with\ndescription di, a RAG step is first applied over\nthe contextual paper set P∗\ni , which includes both\nsurvey-level and subsection-specific literature. Top-\nranked passages are selected to form the initial writ-\ning context Crag,i. However, the retrieved passages\nfrom academic papers often contain indirect cita-\ntions such as \"[23]\" and \"Ge et al., 2023\". These ci-\ntations typically refer to influential prior work that\nis not directly included in the retrieved documents.\nIf the system relies solely on these secondary men-\ntions without further resolution, it may overlook\nfoundational or highly relevant papers.\nTo address this, we introduce a citation-tracing\nmechanism that identifies such citations in Crag,i\nand uses an LLM to determine whether each refers\nto an original source of a key concept or result.\nTraceable citations are resolved via the Semantic\nScholar API, and their abstracts are appended to the\nbase context Crag,i, forming the citation-enriched\ncontext Cenrich,i. To maintain traceability, each en-\nriched abstract is linked back to the original pas-\nsage that cited it, allowing the writing model to\nunderstand the relationship between the mention\nand its source. For example, consider the passage:\n“... recent work has introduced reward-\nbalanced fine-tuning for alignment (Ge et\nal., 2023), showing improvements over\nDPO and RLHF ...”\nThe LLM flags “(Ge et al., 2023)” as traceworthy,\nidentifying it as introducing a core method. The\nsystem then resolves it to:\nTitle: Preserve Your Own Correlation:\nA New Reward-Balanced Fine-Tuning\nMethod\nAbstract:\n‘We introduce a reward-\nbalanced fine-tuning (RBF) framework\nfor language model alignment...”\nThis abstract is appended to the context, enabling\nthe system to cite this traced paper directly in the\ngeneration. Further implementation details are pro-\nvided in Appendix B.4.\n3.3.2\nMemory-Aligned Skeleton-Guided\nGeneration\nGiven the enriched context Cenrich,i, subsection title\nand description (ti, di), and the accumulated struc-\n6\n\nture memory M, the system first uses an LLM (see\nprompt in Figure 12) to generate a writing skele-\nton Si outlining the key conceptual points. The\nmemory M, which includes prior subsections and\nextracted terminology, ensures content coherence\nand terminology consistency across the survey.\nBest-of-N Selection.\nN candidate drafts are first\ngenerated based on the subsection title si, descrip-\ntion di, writing skeleton Si, and enriched context\nCenrich,i (see prompt in Figure 15). An LLM then\nevaluates the candidates and selects the best version\nbased on alignment with the skeleton, contextual\nrelevance, and overall writing quality.\nSubsection-Level Refinement.\nTo further im-\nprove the selected draft, a three-stage refinement\nis applied. First, the structure is adjusted to better\nreflect the conceptual flow defined by Si. Second,\nthe draft undergoes citation refinement, where the\nLLM rewrites the text based on Cenrich,i. Finally,\nthe draft is polished to enhance fluency and clarity.\n4\nExperiments and Results\n4.1\nEvaluation Setup.\nWe compare SurveyGen-I with three representative\nbaselines: AutoSurvey (Wang et al., 2024), Survey-\nForge (Yan et al., 2025), and SurveyX (Liang et al.,\n2025). We collect demo reports from SurveyForge\nand SurveyX official project pages. For AutoSur-\nvey, we generate reports on matched topics for a\nfair comparison and used the same model used in\nSurveyGen-I, with GPT4o-mini (OpenAI, 2024).\nWe also construct a new benchmark covering six\nmajor scientific domains, each with ∼30 subtopics.\n4.2\nEvaluation Metrics\nWe comprehensively evaluate SurveyGen-I against\nthree competitive baselines across two core dimen-\nsions, content quality and reference quality.\nContent Quality Evaluation.\nMeasures the\nstructural and semantic strength of the generated\nsurvey. This includes five sub-dimensions: cov-\nerage, relevance, structure, synthesis, and consis-\ntency. Each aspect is scored by LLM-as-Judge (Li\net al., 2024a) (specifically, rated by GPT4o-mini),\nwith explanation-based prompts to reduce variance.\nThis directly reflects the impact of our MGSR and\nCaM-Writing, which aim to improve global coher-\nence, abstraction, and flow. Evaluation criteria can\nbe found in the appendix G. We compute the final\ncontent quality score (CQS) as the average of five\nevaluation dimensions.\nReference Quality Evaluation.\nTo assess the ef-\nfectiveness and recency of reference usage in the\ngenerated survey, we adopt three reference-level\nmetrics that reflect citation coverage, intensity, and\ntimeliness. The Number of References (NR) counts\nthe distinct cited works, measuring the breadth of\nliterature coverage. The Citation Density (CD)\ncomputes the number of unique citation markers\nper character of text (excluding the reference sec-\ntion), reflecting how frequently references are inte-\ngrated into the main narrative. For reporting clarity,\nwe scale CD by a factor of 104. The Recency Ratio\n(RR@k) measures the proportion of all cited ref-\nerences that were published within a recent time\nwindow (e.g., within the past k=3 years). A higher\nRR indicates better engagement with the latest de-\nvelopments in the field, and reflects the model’s\nability to retrieve and integrate timely literature.\n4.3\nMain Results\nWe report evaluation results across content qual-\nity and reference behavior, along with an ablation-\nbased component analysis. SurveyGen-I is com-\npared against three state-of-the-art baselines: Au-\ntoSurvey, SurveyX, and SurveyForge. Our results\ndemonstrate that SurveyGen-I achieves significant\nimprovements across all dimensions, showing its\neffectiveness for automated survey generation.\nContent Quality.\nSurveyGen-I achieves consis-\ntent improvements across all five content quality di-\nmensions compared to prior systems (Table 1). The\noverall score reaches 4.59, outperforming the best\nbaseline (SurveyForge: 4.23) by +0.36. Largest\ngains are observed in structural flow (STRUC:\n+0.21) and synthesis (SYN: +0.41), indicating that\nthe model maintains a coherent narrative while in-\ntegrating information from diverse sources. Cover-\nage (4.72) and relevance (4.76) also lead all base-\nlines, suggesting high topical breadth and align-\nment. Consistency (4.59) improves notably over\nSurveyX (4.29), reflecting stability in terminology\nand phrasing across sections. Notably, SurveyX\nuses GPT4o (Hurst et al., 2024), whereas our sys-\ntem relies on a smaller and more cost-efficient\nmodel, making the performance gap especially sig-\nnificant. Observed quality gains suggest that sys-\ntems combining structural adaptivity, iterative re-\nfinement, and citation-tracing can more reliably\ngenerate coherent and well-grounded surveys.\n7\n\nModel\nOverall ↑\nCov ↑\nRel ↑\nStruc ↑\nSyn ↑\nConsis ↑\nAutoSurvey\n4.08\n4.10\n4.17\n4.03\n4.10\n4.00\nSurveyX\n4.13\n4.10\n4.33\n4.00\n3.95\n4.29\nSurveyForge\n4.23\n4.31\n4.41\n4.07\n4.21\n4.17\nOurs\n4.59\n4.72\n4.76\n4.28\n4.62\n4.59\nTable 1: LLM-based evaluation scores across multiple\nsurvey quality dimensions. Higher scores reflect bet-\nter coverage (COV), relevance (REL), structural flow\n(STRUC), synthesis (SYN), and consistency (CONSIS).\nSurveyGen-I leads across all.\nT\next Length\n100\n200\n300\n400\n500\nNumber Of References\nOurs\nAutoSurvey\nSurveyX\nSurveyForge\n(a) Ref count vs. length\nT\next Length\n0\n5\n10\n15\n20\n25\n30\n35\nCitation Density\nOurs\nAutoSurvey\nSurveyX\nSurveyForge\n(b) Citation density\nFigure 3: Citation behavior comparisons across mod-\nels using KDE-enhanced scatter plots. (a) Number of\nreferences vs. text length. SurveyGen-I demonstrates\na steeper citation scaling curve, suggesting deeper inte-\ngration of references even in longer texts. (b) Citation\ndensity vs. text length. SurveyGen-I maintains denser\ncitation patterns across all lengths.\nReference Quality.\nIn terms of citation qual-\nity and scientific grounding, SurveyGen-I exhibits\nboth broader and denser reference usage. It cites\n281 unique works per survey on average (Ta-\nble 2), representing a sharp increase over SurveyX\n(102) and AutoSurvey (73). Citation density also\nrises substantially (17.28), exceeding SurveyForge\n(5.52) by around 3 times, indicating tighter integra-\ntion of references into the body text. Importantly,\n89.1% of all citations are published within the past\n5 years (RR@5), compared to 66.7% in SurveyX\nand SurveyForge, demonstrating significantly im-\nproved recency alignment.\nThe steep scaling trend between reference count\nand text length in Figure 3a shows that text length\nremains relatively stable in SurveyGen-I, reflecting\nthe fixed-length constraint imposed during gener-\nation. It also demonstrates that SurveyGen-I in-\ncludes the most references overall. In contrast, Au-\ntoSurvey consistently generates fewer references,\nand its citation count remains relatively flat, even\nas its text length slightly increases, which is un-\nexpected given that the length parameter was con-\ntrolled across all generations. This suggests weaker\nresponsiveness to contextual expansion and under-\nutilization of available content space. Figure 3b fur-\nther shows that SurveyGen-I consistently maintains\na high citation density across varying text lengths,\nindicating robust integration of information-dense\ncontent.\nModel\nRR@1\nRR@3\nRR@5\nRR@7\nRR@10\nCD\nNR\nAutoSurvey\n0.174\n0.639\n0.837\n0.940\n0.992\n1.54\n73\nSurveyX\n0.239\n0.484\n0.667\n0.792\n0.916\n13.57\n102\nSurveyForge\n0.137\n0.437\n0.667\n0.824\n0.907\n5.52\n113\nOurs\n0.478\n0.759\n0.891\n0.955\n0.985\n17.28\n281\nTable 2: Performance comparison across models based\non recency-focused citation behavior and structural cita-\ntion metrics. RR@k indicates the proportion of recent\nreferences among the top-k citations. CD (scaled by\n×104) measures citation density, and NR denotes the\ntotal number of cited references.\nAblation Analysis.\nTable 3 reports the impact\nof removing specific behaviors from SurveyGen-\nI. The full model yields the highest overall score\n(4.77), with synthesis and structure both at 4.86\nand 4.71, respectively. Disabling final refinement\nresults in the steepest quality drop (Overall: –0.43),\nparticularly in synthesis (–0.43) and structure (–\n0.57), indicating that single-pass generation with-\nout revision is insufficient for maintaining narrative\nintegration. Fixed planning further reduces struc-\ntural flow (STRUC: –0.42) and consistency (CON-\nSIS: –0.28), suggesting that static outlines limit\nthe model’s ability to adjust to unfolding content.\nRemoving citation resolution reduces the number\nof distinct references by 61 and lowers relevance\nby 0.29, despite stable consistency.\nModel\nOverall ↑\nCov ↑\nRel ↑\nStruc ↑\nSyn ↑\nConsis ↑\nNR ↑\nOurs (w/o Citation Trace)\n4.60\n4.57\n4.57\n4.43\n4.71\n4.71\n225\nOurs (w/o Plan Update)\n4.49\n4.57\n4.71\n4.29\n4.43\n4.43\n212\nOurs (w/o Refine)\n4.34\n4.43\n4.43\n4.14\n4.43\n4.29\n286\nOurs (Full)\n4.77\n4.71\n4.86\n4.71\n4.86\n4.71\n286\nTable 3: Evaluation results of ablation variants. Each\ncomponent (Trace, Plan Update, Refine) contributes to\noverall quality.\n5\nConclusion\nWe present SurveyGen-I, a fully automated frame-\nwork for generating academic surveys with high\nconsistency, citation coverage, and structural coher-\nence. By integrating multi-level retrieval, adaptive\nplanning, and memory-guided writing, SurveyGen-\nI effectively captures complex literature landscapes\nand produces high-quality surveys without manual\nintervention. Extensive evaluations across six sci-\nentific domains demonstrate its effectiveness over\nexisting methods, marking a step forward in reli-\nable and scalable scientific synthesis.\n8\n\nLimitations\nWhile SurveyGen-I shows consistently strong\nperformance across benchmarks, our framework\nadopts an online retrieval strategy to ensure access\nto up-to-date literature. However, this design in-\ntroduces network sensitivity, variable latency, and\nreliance on third-party APIs, which may restrict\nfull-text access due to licensing constraints. Com-\npared to offline-indexed corpora used in prior work,\nour approach trades retrieval speed and infrastruc-\nture control for broader coverage and freshness.\nAdditionally, for niche or emerging topics with\nlimited source material, the achievable survey\nlength and depth are naturally constrained. This\nshows a general challenge in automatic survey gen-\neration: content quality is ultimately bounded by\nthe availability and granularity of the source lit-\nerature. Moreover, some evaluation signals may\nreflect subjective preferences rather than univer-\nsal writing standards. We anticipate that broader\ncommunity adoption and feedback will help guide\nfuture enhancements."}
{"id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "authors": ["Aman Goel", "Daniel Schwartz", "Yanjun Qi"], "source": "http://arxiv.org/abs/2508.14314v1", "page_count": 15, "text": "Zero-knowledge LLM hallucination detection and mitigation through\nfine-grained cross-model consistency\nAman Goel∗, Daniel Schwartz∗, Yanjun Qi\nAmazon Web Services, USA\n{goelaman, dansw, yanjunqi}@amazon.com\nAbstract\nLarge language models (LLMs) have demon-\nstrated impressive capabilities across diverse\ntasks, but they remain susceptible to halluci-\nnations—generating content that appears plau-\nsible but contains factual inaccuracies.\nWe\npresent FINCH-ZK, a black-box framework that\nleverages FINe-grained Cross-model consis-\ntency to detect and mitigate Hallucinations in\nLLM outputs without requiring external knowl-\nedge sources. FINCH-ZK introduces two key in-\nnovations: 1) a cross-model consistency check-\ning strategy that reveals fine-grained inaccu-\nracies by comparing responses generated by\ndiverse models from semantically-equivalent\nprompts, and 2) a targeted mitigation technique\nthat applies precise corrections to problem-\natic segments while preserving accurate con-\ntent. Experiments on the FELM dataset show\nFINCH-ZK improves hallucination detection\nF1 scores by 6-39% compared to existing ap-\nproaches. For mitigation, FINCH-ZK achieves\n7-8 absolute percentage points improvement\nin answer accuracy on the GPQA-diamond\ndataset when applied to state-of-the-art models\nlike Llama 4 Maverick and Claude 4 Sonnet.\nExtensive evaluation across multiple models\ndemonstrates that FINCH-ZK provides a practi-\ncal, deployment-ready safeguard for enhancing\nfactual reliability in production LLM systems.\n1\nIntroduction\nWith the rapid deployment of large language mod-\nels (LLMs) across diverse applications, ensuring\nfactual accuracy and reliability has become in-\ncreasingly critical for enterprise systems. LLMs\nfrequently generate plausible-sounding but factu-\nally incorrect information—a phenomenon known\nas hallucination—which poses significant risks in\nhigh-stakes domains.\nExisting black-box hallucination management\ntechniques typically address either detection or mit-\n*Equal contributions\nigation, but seldom integrate both. Black-box detec-\ntion systems when given zero knowledge struggle\nwith single-LLM biases, and coarse outputs lacking\ninterpretability, while mitigation approaches simi-\nlarly over-reformulate, reuse biased models, lack\nintegrated detection-correction pipelines, and offer\nlittle transparency (detailed review in §2.1).\nOur objective is to develop a practical LLM hal-\nlucination management system that integrates de-\ntection and targeted mitigation without external\nknowledge requirements. In this paper, we intro-\nduce FINCH-ZK, which integrates techniques like\nconsistency checking (Wang et al., 2023; Manakul\net al., 2023) with a novel multi-stage mitigation\napproach that precisely corrects only problematic\nsegments while preserving accurate content and\nembodying diverse reasoning patterns across model\nfamilies. Our key contributions include:\n• We introduce FINCH-ZK, an integrated black-\nbox framework that combines existing detection\ntechniques with a novel multi-stage mitigation\nprocess for targeted hallucination correction, ad-\ndressing a critical gap between detection and mit-\nigation in existing LLM safeguards.\n• We demonstrate how leveraging prompting vari-\nations (adding dynamic semantic-preserving al-\nterations to the input prompt) and cross-model\nconsistency checking (comparing outputs across\ndifferent model architectures) provide more ro-\nbust detection than single-model approaches, im-\nproving detection F1 scores by 6-39% on the\nFELM dataset (Zhao et al., 2023) compared to\nstate-of-the-art methods.\n• We present an interpretable multi-stage mitiga-\ntion pipeline that applies targeted corrections\nonly to problematic segments identified through\nfine-grained analysis while maintaining coher-\nence and completeness through cross-model rea-\nsoning feedback, achieving 7-8 absolute percent-\nage points improvement in answer accuracy on\n1\n\nthe GPQA-diamond dataset (Rein et al., 2024).\n• We provide comprehensive empirical evidence\nshowing that the integration of diverse sampling\nstrategies with targeted correction significantly\noutperforms existing approaches in the zero-\nknowledge setting, with quantitative ablation\nstudies identifying the relative contribution of\neach system component.\nThe framework is designed for practical deploy-\nment in production environments, with efficient\nmulti-threaded processing, comprehensive logging\nsupport, modular architecture supporting various\nLLMs, and rich user feedback.1\n2\nMethodology\n2.1\nBackground and Related Works\nExisting black-box hallucination management ap-\nproaches fall into two categories. For detection,\ntechniques include: a) external knowledge-based\napproaches like RAG (Lewis et al., 2020) that rely\non data sources for fact-checking, and b) internal\nconsistency methods like SelfCheckGPT (Man-\nakul et al., 2023) that analyze variations across\nmodel outputs.\nFor mitigation, common tech-\nniques include self-correction through iterative re-\nfinement (Wang et al., 2023), chain-of-thought\nreasoning (Wei et al., 2023), and majority vot-\ning (Lightman et al., 2023).\nDetection systems face three primary limita-\ntions: 1) RAG-based methods require comprehen-\nsive knowledge bases often unavailable for spe-\ncialized domains or inaccessible due to privacy\nconcerns; 2) zero-knowledge consistency-based ap-\nproaches typically rely on a single LLM architec-\nture, making them prone to high-certainty halluci-\nnations due to missing diverse reasoning patterns;\nand 3) most systems operate at coarse granular-\nity, lacking fine-grained analysis and interpretable\nexplanations for flagged content.\nMitigation approaches suffer from complemen-\ntary shortcomings: 1) most systems attempt whole-\nsale reformulation rather than targeted correction,\noften modifying accurate content while fixing er-\nrors; 2) they frequently rely on the same model\nthat produced the hallucination to correct it, per-\npetuating biases and reasoning patterns; 3) many\napproaches lack integration between detection and\ncorrection mechanisms, resulting in inefficient\n1Implementation details and experimental code will be\nmade available to support reproducibility and adoption in\nindustrial settings.\npipelines; and 4) few systems provide transparency\ninto why content was flagged and how corrections\nwere determined.\n2.2\nProposed: FINCH-ZK\nTo address the above limitations and provide an\nintegrated workflow for hallucination management,\nwe propose FINCH-ZK, a framework for FINe-\ngrained Cross-model consistency for Hallucination\ndetect and mitigate with Zero Knowledge. FINCH-\nZK addresses key limitations in existing approaches\nthrough two primary innovations: 1) a cross-model\nconsistency checking strategy that leverages di-\nverse model architectures and prompt formula-\ntions to reveal fine-grained inaccuracies not de-\ntectable through single-model analysis, and 2) a\ntargeted mitigation pipeline that applies precise cor-\nrections to identified problematic segments while\npreserving accurate content, avoiding the whole-\nsale response reformulation typical of existing ap-\nproaches.\nFigure 1 presents an overview of FINCH-ZK.\nGiven a prompt p, a target LLM T that gener-\nates response rT , a set of sampler models M =\n{m1, m2, . . . , m|M|}, a judge model J, and an im-\nprover model I, FINCH-ZK performs hallucination\ndetection and mitigation in three stages:\n• Generate diverse samples from different sampler\nmodels\n• Detect fine-grained inaccuracies in the input re-\nsponse using generated samples\n• Perform systematic response improvement using\ndetected inaccuracies and generated samples.\n2.3\nCross-model Sample Generation\nAs the first component, FINCH-ZK generates di-\nverse response samples through prompt variations\nand multi-model sampling to expose hallucinations\nthat may be consistent within a single model but in-\nconsistent across different architectures or prompt\nformulations.\nThe system applies a set of variations V\n=\n{v1, v2, . . . , v|V |} to generate prompt variants\n{v1(p), v2(p), . . . , v|V |(p)}. These variations in-\nclude syntactic transformations (rephrasing, expan-\nsion) and semantic modifications (chain-of-thought\nprompting, question decomposition) designed to\nelicit varied reasoning patterns while preserving\nthe original information requirements.2\n2Prompt variations are detailed in Appendix A.2.\n2\n\nPrompt p\n+\nResponse\nrT\n/\nPredicted\nHallucination\nScores\nError Summaries\nImproved\nResponse r’’T\nChanges\nSummary\nCross-model\nSample Generation\na. Generate prompt\nvariants\nv1 . . . vN\nb. Use Sampler LLMs (M)\nto generate samples\ns1 . . . sN\n1\nFine-grained\nHallucination Detection\na. Segment response into blocks\nb1 . . . bK\nb. Use Judge LLM (J) to evaluate\neach block against each sample\ns1 -> b1?\n. . .\nsN -> bK?\nc. Calculate hallucination scores\nd. Summarize errors per block with J\n2\nMulti-stage Hallucination\nMitigation\na. Apply block correction using\nImprover LLM (I) on each\ninaccurate block to generate\nr’T (with a change summary)\nb. Perform cross-model\nresponse-level reflection on\nr’T with I to generate r’’T\n(with a change summary)\n3\nFigure 1: Overview of FINCH-ZK\nThe system then collects |S| responses by\nprompting different sampler models in M with dif-\nferent variants from V to create the sample set\nS = {s1, s2, . . . , s|S|}. Each sample si ∈S is\ngenerated by randomly selecting a prompt vari-\nant vi ∈V and sampler model mi ∈M. This\ncross-model sampling strategy captures architec-\ntural differences in reasoning patterns, knowledge\nrepresentation, and potential systematic biases that\nsingle-model approaches cannot detect.\n2.4\nFine-grained Hallucination Detection\nAs the second component, FINCH-ZK performs\nfine-grained hallucination identification through\nautomated cross-consistency evaluation, enabling\nprecise localization of potentially hallucinated con-\ntent segments.\nResponse Segmentation.\nTarget response\nrT is segmented into semantic blocks B\n=\n{b1, b2, . . . , b|B|} using sentence-level segmenta-\ntion. This granular approach enables the system to\nidentify specific hallucinated segments rather than\nclassifying entire responses, providing actionable\nfeedback for targeted correction.\nCross-consistency Evaluation. Each block bi ∈\nB is evaluated against each sample sj ∈S using\nthe judge model J with structured prompts. The\njudge model classifies each (bi, sj) pair into:\n• ACCURATE: Block is factually consistent and sup-\nported by the sample\n• CONTRADICTION: Direct factual contradiction de-\ntected between block and sample\n• NEUTRAL: Insufficient information for definitive\nassessment.\nWeighted Scoring.\nBlock-level hallucination\nscores are computed using weighted aggregation\nacross all samples:\nscore(bi) =\nP|S|\nj=1 wj(bi) · score(bi, sj)\nP|S|\nj=1 wj(bi)\nwhere wj(bi) represents the reliability weight\nassigned to sample sj for evaluating block bi\nand score(bi, sj) ∈{0, 0.5, 1} corresponds to\nACCURATE, NEUTRAL, and CONTRADICTION labels\nrespectively. Factuality labels are assigned to each\nblock bi based on a threshold τ as: ACCURATE for\nscore(bi) ∈[0, τ], CONTRADICTION for score(bi) ∈\n[1 −τ, 1], and NEUTRAL otherwise. Response-level\nhallucination score is computed as score(rT ) =\n1\n|B|\nP|B|\ni=1 score(bi), with overall response label\ncomputed as: NON-FACTUAL if any block in rT is la-\nbelled as CONTRADICTION, and FACTUAL otherwise.\nSummarize Errors. For blocks identified as poten-\ntially hallucinated (i.e., labeled as CONTRADICTION\nor NEUTRAL), the system generates concise error\nsummaries ei using the judge model to characterize\nthe nature and severity of detected inconsistencies,\nproviding interpretable explanations for actionable\nuser feedback and downstream correction.\n2.5\nMulti-stage Hallucination Mitigation\nThe mitigation component applies targeted correc-\ntions to identified problematic segments through\na two-stage approach: fine-grained block correc-\ntion followed by response-level coherence improve-\nment.\nBlock-level Correction. For each hallucinated\nblock bi with error summary ei, FINCH-ZK gen-\nerates a corrected version b′\ni using an improver\nmodel I with a structured correction prompt that\nincludes: 1) the original block text, 2) the auto-\nmatically generated error summary, and 3) detailed\ncontradiction evidence from the cross-consistency\nanalysis. This approach ensures corrections are\ngrounded in specific identified issues rather than\ngeneric reformulation. The corrected response is\nreconstructed as:\nr′\nT = concat(c1, c2, . . . , c|B|)\nwhere ci = b′\ni if block i was flagged for correction,\n3\n\nTable 1: Comparison of hallucination detection methods on FELM (Zhao et al., 2023) dataset. P/R/F1/BA, respectively, denote\nprecision, recall, F-1 score, and balanced accuracy of predicted factuality labels vs human-annotations. For response-level, we\nadditionally show Pearson and Spearman correlations of predicted hallucination scores. GPT-4 Judge variants are from (Zhao\net al., 2023) that use GPT-4 for judgment based on the prompt and sentence directly (Vanilla), with chain-of-thought (CoT),\nor with retrieved content from reference sources (RAG). Delta percentages are shown for F1/BA metrics, with positive values\nindicating improvements compared to GPT-4 Judge (Vanilla).\nMethod\nSentence-level\nResponse-level\nP\nR\nF1 (∆%)\nBA (∆%)\nP\nR\nF1 (∆%)\nBA (∆%)\nPearson\nSpearman\nGPT-4 Judge (Vanilla)\n64.0\n24.4\n35.4\n60.7\n62.4\n39.4\n48.3\n63.8\n—\n—\nGPT-4 Judge (CoT)\n68.1\n30.4\n42.0 (+18.6%)\n63.7 (+4.9%)\n64.7\n46.1\n53.8 (+11.4%)\n66.8 (+4.7%)\n—\n—\nGPT-4 Judge (RAG)\n62.9\n39.2\n48.3 (+36.4%)\n67.1 (+10.5%)\n64.3\n51.1\n56.9 (+17.8%)\n68.5 (+7.4%)\n—\n—\nSelfCheckGPT\n41.2\n54.1\n46.8 (+32.2%)\n68.7 (+13.2%)\n73.7\n53.5\n62.0 (+28.4%)\n72.0 (+12.9%)\n59.5\n59.9\nFINCH-ZK\n45.8\n53.1\n49.2 (+39.0%)\n69.8 (+15.0%)\n83.8\n53.2\n65.1 (+34.8%)\n74.0 (+16.0%)\n63.1\n61.5\nand ci = bi otherwise. This selective correction\nstrategy preserves accurate content while targeting\nonly problematic segments.\nResponse-level Improvement. To address broader\ncoherence and completeness issues that may arise\nfrom localized corrections, the system performs\ncross-model reflection by generating an improved\nresponse r′′\nT that synthesizes insights from all gen-\nerated samples S. The improver model receives\nthe original prompt, the block-corrected response\nr′\nT , and representative samples from S to produce a\nfinal response that maintains factual accuracy while\nensuring overall coherence and completeness.\nThis multi-stage approach addresses the key lim-\nitation of existing mitigation systems that apply\nwholesale reformulation, often corrupting accurate\ncontent while attempting to fix errors. By preserv-\ning the structure and content of accurate segments,\nFINCH-ZK provides targeted correction that main-\ntains response quality while eliminating identified\nhallucinations.\n3\nExperiments\nWe conducted experimental evaluation to answer\nthe following research questions:3\nRQ1: How effective is FINCH-ZK at detecting\nhallucinations compared to other approaches?\nRQ2: How effective is FINCH-ZK for mitigat-\ning hallucinations?\nRQ3: Which components significantly influ-\nence FINCH-ZK’s detection capabilities?\nRQ4: How does different factors affect FINCH-\nZK’s hallucination mitigation?\nDatasets. We utilize two challenging benchmarks\nfor evaluation: 1) FELM (Zhao et al., 2023) com-\nposed of 847 questions & responses across di-\n3Due to space limits, detailed implementation and setup\nare described in Appendix A.4 & A.5.\nverse domains supplemented with fine-grained\nhuman-annotated factuality labels, and 2) GPQA-\ndiamond (Rein et al., 2024) composed of 198\ngraduate-level multiple-choice questions.\nBaseline Methods. For RQ1 & RQ2, we com-\npared against GPT4-based judge variants (Vanilla,\nCoT, RAG) as utilized in (Zhao et al., 2023)\nand SelfCheckGPT (Manakul et al., 2023). For\nRQ2 & RQ4, we compared against SelfCheckGPT\nand hallucination mitigation techniques: few-shots\nCoT (Wei et al., 2023) using 5 in-context examples,\nself-consistency (Wang et al., 2023), a cross-model\nvariant of self-consistency that uses multiple LLMs\n(call it cross-consistency), and best-of-N majority\nselection (Lightman et al., 2023).4\nRQ1: How effective is FINCH-ZK at detecting\nhallucinations compared to other approaches?\nTable 1 presents results for hallucination detec-\ntion on the FELM dataset. At both fine-grained\n(i.e., sentence) as well as aggregated response level,\nFINCH-ZK showed better precision-recall balance,\nconsistently outperforming all baselines. In par-\nticular, FINCH-ZK achieved 39% better sentence-\nlevel F1-score compared to GPT4-Judge (Vanilla).\nSurprisingly, FINCH-ZK even outperformed the\nknowledge-source dependent RAG-based baseline,\nachieving around 17% better F1-score and 8% bet-\nter balanced accuracy respectively at response level.\nDiverse sample generation through prompt varia-\ntions and cross-model sampling enabled FINCH-\nZK to achieve around 6% better F-1 scores and\nPearson correlation compared to SelfCheckGPT.\nRQ2: How effective is FINCH-ZK for mitigating\nhallucinations?\nTable 2 presents a comparison of FINCH-ZK\nagainst different mitigation baselines on GPQA-\n4For a fair comparison, we used equivalent configurations\nacross different techniques (Appendix A.5) and added equiva-\nlent engineering upgrades (Appendix A.3) to SelfCheckGPT.\n4\n\nTable 2: Comparison of hallucination mitigation methods on the GPQA-diamond (Rein et al., 2024) dataset. All methods are\nevaluated against the same zero-shot CoT baseline. Positive delta percentages indicate improvements compared to the baseline.\nRegex-based judge compares answer-choice accuracy against ground truth, RAG-based judge uses answer explanations from the\ndataset as trusted content for LLM-based judgment, FINCH-ZK-based judge is based on §2.4.\nT\nMethod\nRegex-based Judge\nRAG-based LLM Judge\nFINCH-ZK Judge\nAnswer Acc.\n∆%\nFull Resp. Acc.\n∆%\nFull Resp. Acc.\n∆%\nClaude 4 Sonnet\nZero-shot CoT (baseline)\n71.7\n—\n50.0\n—\n69.7\n—\nFew-shots-CoT\n68.2\n-4.9%\n47.5\n-5.1%\n70.7\n+1.4%\nSelf-Consistency\n73.2\n+2.1%\n48.5\n-3.0%\n66.7\n-4.3%\nCross-Consistency\n71.2\n-0.7%\n52.5\n+5.1%\n71.2\n+2.2%\nBest-of-N\n73.7\n+2.8%\n52.5\n+5.1%\n69.7\n0.0%\nSelfCheckGPT\n71.2\n-0.7%\n54.5\n+9.1%\n75.3\n+8.0%\nFINCH-ZK\n75.8\n+5.6%\n59.1\n+18.2%\n80.3\n+15.2%\nLlama 4 Maverick\nZero-shot CoT (baseline)\n68.2\n—\n42.9\n—\n63.1\n—\nFew-shots-CoT\n67.7\n-0.7%\n43.4\n+1.2%\n64.7\n+2.4%\nSelf-Consistency\n67.7\n-0.7%\n45.0\n+4.7%\n64.1\n+1.6%\nCross-Consistency\n73.7\n+8.2%\n50.5\n+17.7%\n69.7\n+10.4%\nBest-of-N\n67.2\n-1.5%\n41.9\n-2.4%\n61.1\n-3.2%\nSelfCheckGPT\n75.8\n+11.1%\n84.3\n+96.5%\n86.9\n+37.6%\nFINCH-ZK\n76.8\n+12.6%\n90.9\n+111.8%\n92.4\n+46.4%\ndiamond dataset. We evaluate performance using\nthree distinct judging methodologies: regex-based\nanswer-choice accuracy, RAG-based LLM judging\nof the full response, and FINCH-ZK’s based judge.\nIn answer-choice accuracy, FINCH-ZK achieved\nthe best performance—reaching ∼76% accuracy,\nup +5.6% for Claude 4 Sonnet and +12.6% for\nLlama 4 Maverick. For full response accuracy,\nFINCH-ZK outperformed the next best baseline\n(SelfCheckGPT) by around 9-15% for RAG-based\njudging and 7-9% for FINCH-ZK-based judge.\nThese results demonstrate that FINCH-ZK’s com-\nbination of cross-model sampling, fine-grained er-\nror detection, and targeted correction offers supe-\nrior hallucination mitigation compared to existing\napproaches. The system is particularly effective at\nimproving full response factuality, as evidenced by\nthe substantial gains in RAG-based and FINCH-ZK-\nbased judging metrics. The effectiveness across\ndifferent model families (Claude and Llama) high-\nlights FINCH-ZK’s model-agnostic design, making\nit a versatile solution for production environments\nwith diverse LLM deployments.\nNotably, while techniques like Self-Consistency\nand Best-of-N offer modest improvements in an-\nswer accuracy (2-3%), they often fail to meaning-\nfully improve full response factuality. This under-\nscores the limitations of approaches that don’t ex-\nplicitly target hallucinations at a fine-grained level.\nRQ3: Which components significantly influence\nFINCH-ZK’s detection capabilities?\nTable 3 summarizes ablation studies to understand\nthe influence of each component in FINCH-ZK for\nhallucination detection. Key observations include:\n• Detection capabilities do not monotonically in-\ncrease with more samples (G1.a-c vs G0).\n• Disabling cross-model sampling (i.e., all sam-\nples generated with Claude 4 Sonnet) degraded\ndetection at the response-level (G2.a vs G0).\n• Adding additional cross-model sampler LLMs,\nboth weaker models (G2.b adds Claude 3.5 Son-\nnet and Llama 4 Scout) or stronger models (G2.c\nadds Claude 4 Opus), improves detection at the\nsentence-level.\n• Using a coarse, response-level judge significantly\nlimits detection due to poor recall (G3.a vs G0).\n• Using a single judge query to evaluate all blocks\ntogether in a batch (instead of separate LLM\ncalls for each sample-block pair) is an effective\nway to reduce LLM costs for fine-grained as-\nsessment without compromising detection per-\nformance (G3.b vs G0).\n• Judge model can significantly influence detection\nperformance (G3.c-e vs G0).\nRQ4: How does different factors affect\nFINCH-ZK’s hallucination mitigation?\nTable 4 summarizes ablation studies to understand\nmitigation effectiveness using Claude 4 Sonnet as\nthe target model. Key observations include:\n5\n\nTable 3: Ablation studies for hallucination detection on FELM (Zhao et al., 2023) dataset. Group G1 shows the effect of\nchanging number of samples (§2.3), G2 compares the effect of changing sampler LLMs (§2.3), G3 shows the effect of changing\nLLM-based judge (§2.4).\nGroup\nConfiguration\nSentence-level\nResponse-level\nP\nR\nF1\nBA\nP\nR\nF1\nBA\nPearson\nSpearman\nG0\nFINCH-ZK\n45.8\n53.1\n49.2\n69.8\n83.8\n53.2\n65.1\n74.0\n63.1\n61.5\nG1\na. 3 samples\n46.0\n52.0\n48.8\n69.4\n77.6\n55.3\n64.6\n73.7\n57.8\n55.5\nb. 5 samples\n43.1\n59.5\n50.0\n71.2\n78.7\n57.8\n66.7\n75.0\n61.9\n59.8\nc. 20 samples\n48.0\n54.5\n51.0\n70.9\n81.2\n53.5\n64.5\n73.7\n63.7\n61.5\nG2\na. (-) cross-model sampling\n43.2\n55.3\n48.5\n69.8\n77.2\n51.8\n62.0\n72.1\n62.1\n59.8\nb. (+) weak samplers\n46.6\n56.0\n50.9\n71.1\n81.6\n53.5\n64.7\n73.8\n63.0\n60.7\nc. (+) strong samplers\n46.3\n56.2\n50.7\n71.0\n79.5\n53.5\n64.0\n73.3\n63.2\n62.2\nG3\na. (-) fine-grained judge\n—\n—\n—\n—\n88.1\n31.6\n46.5\n64.7\n58.7\n59.7\nb. (+) use batch judge\n37.2\n72.2\n49.1\n72.9\n69.8\n73.8\n71.7\n78.9\n63.9\n61.5\nc. Llama 4 Scout judge\n39.6\n81.4\n53.3\n77.3\n72.5\n80.5\n76.3\n82.6\n71.2\n67.9\nd. Llama 4 Scout batch judge\n35.5\n83.2\n49.8\n75.3\n69.4\n84.4\n76.2\n82.9\n65.5\n64.6\ne. Claude 4 Sonnet batch judge\n41.5\n86.7\n56.1\n80.1\n65.6\n85.8\n74.3\n81.7\n69.3\n66.9\n• FINCH-ZK typically reaches higher accuracy\nwith more samples, though with diminishing re-\nturns (G1.a-c vs G0).\n• Disabling cross-model sampling degrades miti-\ngation capability significantly (G2.a vs G0).\n• Using coarse response-level judge reduced accu-\nracy improvements significantly (G3.a vs G0).\n• Judge variations have modest effects on answer-\nchoice accuracy, but significant impact on full\nresponse accuracy (G3.b-d vs G0).\n• Disabling fine-grained correction drastically lim-\nits mitigation performance, underscoring the im-\nportance of targeted correction (G4.a vs G0).\n• Using Llama 4 Maverick as the improver LLM\n(instead of Claude 4 Sonnet) significantly im-\nproved full response accuracy, suggesting cross-\nmodel reflection can help remedy perpetuating\nbiases and reasoning patterns inherent in single-\nmodel architectures (G4.b vs G0).\n• FINCH-ZK boosts accuracy even with extended\nthinking enabled, achieving 80.3% answer-\nchoice accuracy (+11.3% over extended thinking\nbaseline). This demonstrates our proposed tech-\nniques complements internal extended reasoning,\nrather than competing with it (G5.b vs G5.a).\n4\nConclusions\nWe introduce FINCH-ZK, an integrated black-box\nframework that closes the gap between hallucina-\ntion detection and mitigation by combining ad-\nvanced detection techniques with a novel multi-\nstage process for targeted hallucination correction.\nTable 4: Ablation studies for hallucination mitigation on GPQA-diamond (Rein et al., 2024) dataset. Group G1 shows the\neffect of changing number of samples (§2.3), G2 compares the effect of changing sampler LLMs (§2.3), G3 shows the effect of\nchanging LLM-based judge (§2.4), G4 shows the effect of changing multi-stage mitigation (§2.5), G5 shows the comparison\nwith extended thinking enabled. Delta percentages indicate improvement compared to zero-shot CoT baseline.\nGroup\nConfiguration\nRegex-based Judge\nRAG-based LLM Judge\nFINCH-ZK Judge\nAnswer Acc.\n∆%\nFull Resp. Acc.\n∆%\nFull Resp. Acc.\n∆%\nG0\na. Zero-shot CoT (baseline)\n71.7\n—\n50.0\n—\n69.7\n—\nb. FINCH-ZK\n75.8\n+5.6%\n59.1\n+18.2%\n80.3\n+15.2%\nG1\na. 3 samples\n69.7\n-2.8%\n54.0\n+8.1%\n72.2\n+3.6%\nb. 5 samples\n71.2\n-0.7%\n61.1\n+22.2%\n76.8\n+10.1%\nc. 20 samples\n78.8\n+9.9%\n59.6\n+19.2%\n77.8\n+11.6%\nG2\na. (-) cross-model sampling\n71.7\n0.0%\n57.6\n+15.2%\n74.2\n+6.5%\nb. (+) weak samplers\n72.7\n+1.4%\n56.1\n+12.1%\n75.3\n+8.0%\nc. (+) strong samplers\n75.8\n+5.6%\n56.1\n+12.1%\n76.3\n+9.4%\nG3\na. (-) fine-grained judge\n74.2\n+3.5%\n56.6\n+13.1%\n77.3\n+10.9%\nb. (+) use batch judge\n75.8\n+5.6%\n56.6\n+13.1%\n79.3\n+13.8%\nc. Llama 4 Scout judge\n74.2\n+3.5%\n56.6\n+13.1%\n82.8\n+18.8%\nd. Claude 4 Sonnet batch judge\n74.2\n+3.5%\n56.1\n+12.1%\n78.8\n+13.0%\nG4\na. (-) fine-grained correction\n72.7\n+1.4%\n50.5\n+1.0%\n76.8\n+10.1%\nb. Llama 4 Maverick improver\n74.8\n+4.2%\n90.4\n+80.8%\n94.4\n+35.5%\nG5\na. (+) thinking (baseline)\n72.2\n—\n65.7\n—\n82.3\n—\nb. (+) thinking (FINCH-ZK)\n80.3\n+11.3%\n64.7\n-2.0%\n90.4\n+11.6%\n6\n\nLeveraging dynamic prompt variations and cross-\nmodel consistency checks, FINCH-ZK delivers sig-\nnificantly more robust detection than single-model\napproaches.\nIts multi-stage mitigation pipeline\nmakes precise, segment-level corrections while\nmaintaining overall coherence. We provide com-\nprehensive ablation studies underscoring the con-\ntribution of each component of FINCH-ZK. These\nablation results highlight that FINCH-ZK’s effec-\ntiveness stems from the synergistic combination of\nmultiple components, with fine-grained correction\nand cross-model sampling being particularly im-\npactful. The system’s performance scales with com-\nputational resources (samples, model capability),\nbut even resource-efficient configurations offer sub-\nstantial improvements over baseline approaches.\nLimitations\nWhile FINCH-ZK represents a meaningful step to-\nward improving the reliability of large language\nmodel outputs, it is not without important limi-\ntations. The underlying approach fundamentally\nrelies on the assumption that a truly reliable an-\nswer will emerge as the most frequent or stable\nacross repeated sampling. However, for complex\nor ambiguous queries, models may consistently\nreproduce similar hallucinated content, leading\nto a false sense of confidence in its correctness.\nIn such cases, consistency can inadvertently rein-\nforce errors rather than expose them. Additionally,\nthe computational overhead of generating multiple\ncross-model samples represents a substantial cost\nincrease compared to alternative approaches, which\nmay limit real-time applications.\nFuture work includes exploring extensions not\ncovered in this study like—1) extending hallucina-\ntion detection and mitigation for languages beyond\nEnglish, 2) exploring paragraph- and page-level\nsegmentation for very long-form responses, 3) re-\nducing computational costs with batch processing,\n4) rigorous evaluation on domain-specific bench-\nmarks, and 5) investigating solutions for agentic\napplications.\nEthics Statement\nOur cross-model approach assumes that consen-\nsus among different models indicates accuracy, but\nthis may amplify shared biases across model fam-\nilies rather than eliminate them. We emphasize\nthat FINCH-ZK should complement, not replace,\nhuman oversight in high-stakes applications. Users\nmust understand the system’s limitations and main-\ntain appropriate skepticism of AI-generated content.\nWe encourage responsible deployment with clear\ncommunication about the system’s capabilities and\nlimitations to end users."}
{"id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "authors": ["Demian Inostroza", "Mel Mistica", "Ekaterina Vylomova", "Chris Guest", "Kemal Kurniawan"], "source": "http://arxiv.org/abs/2508.14307v1", "page_count": 8, "text": "A Joint Multitask Model for Morpho-Syntactic Parsing\nDemian Inostroza, Mel Mistica, Ekaterina Vylomova, Chris Guest, Kemal Kurniawan\nUniversity of Melbourne\n{inostrozaad, misticam, ekaterina.vylomova,\nchris.guest, kurniawan.k}@unimelb.edu.au\nAbstract\nWe present a joint multitask model for the Uni-\nDive 2025 Morpho-Syntactic Parsing shared\ntask, where systems predict both morphologi-\ncal and syntactic analyses following novel UD\nannotation scheme. Our system uses a shared\nXLM-RoBERTa encoder with three specialized\ndecoders for content word identification, de-\npendency parsing, and morphosyntactic feature\nprediction. Our model achieves the best overall\nperformance on the shared task’s leaderboard\ncovering nine typologically diverse languages,\nwith an average MSLAS score of 78.7%, LAS\nof 80.1%, and Feats F1 of 90.3%. Our abla-\ntion studies show that matching the task’s gold\ntokenization and content word identification\nare crucial to model performance. Error analy-\nsis reveals that our model struggles with core\ngrammatical cases (particularly Nom-Acc) and\nnominal features across languages.1\n1\nIntroduction\nThe UniDive 2025 Morpho-Syntactic Parsing\nshared task (Goldman et al., 2025) introduces a\nnovel framework for dependency parsing that seeks\nto bridge the traditional divide between morpholog-\nical and syntactic analysis. In conventional Univer-\nsal Dependencies (Nivre et al., 2020), morphology\nand syntax are treated as distinct modules operating\nat different linguistic levels, with word boundaries\nserving as the interface between them. However,\nthis separation has led to significant inconsisten-\ncies in how different languages and even different\ntreebanks for the same language handle word seg-\nmentation and grammatical analysis. The shared\ntask proposes to address these long-standing chal-\nlenges by reorganizing grammatical representation\naround the content-function distinction rather than\nrelying on theoretically problematic word bound-\naries, proposing a more typologically consistent\n1Our\ncode\nand\nmodels\nare\npublicly\navailable:\nhttps://github.com/DemianInostrozaAmestica/\nshared_task_UD_official\nID\nToken\nFEATS\nHEAD\nDEPREL\n1\nFrom\n_\n_\n_\n2\nthe\n_\n_\n_\n3\nAP\nCase=Abl|Definite=Def|\nNumber=Sing\n4\nobl\n4\ncomes\nMood=Ind|Polarity=Pos|\nTense=Pres|VerbForm=Fin|\nVoice=Act\n0\nroot\n5\nthis\nNumber=Sing|\nPronType=Dem\n6\ndet\n6\nstory\nNumber=Sing\n4\nnsubj\n7\n:\n_\n_\n_\nTable 1: Example of the new annotation scheme used in\nthe shared task\napproach to multi-linguistic parsing. For instance,\nin the sentence ‘From the AP comes this story’\nshown in Table 1, traditional UD treats ‘From’ as a\ndependent of ‘AP’ with the deprel case, while the\nnew framework transfers the grammatical meaning\nof ‘From’ as a morphosyntactic feature Case=Abl\n(Ablative) directly onto the content word ‘AP’.\nThe task requires systems to predict both labeled\ndependency arcs and morphosyntactic features, but\nwith a difference from standard Universal Depen-\ndencies parsing: the dependency tree consists only\nof content words (lexical words carrying semantic\nmeaning like nouns, verbs, and adjectives), while\nfunction words (grammatical elements like adpo-\nsitions, articles, and auxiliaries) contribute their\ngrammatical information as features on related con-\ntent words.\nWhile the content-function distinction is explicit\nin the training data, systems must identify this dis-\ntinction themselves at test time from raw text. This\nidentification determines which words participate\nin the dependency tree and which contribute fea-\ntures to other words. Additionally, the multi-label\nnature of features, where a content word can have\nmultiple feature values for a given feature class,\n1\n\nFigure 1: Joint model architecture for the shared task.\nlike Case=Ine;Atr,2 requires models to learn intri-\ncate morphosyntactic patterns.\nWe present a joint multitask model (Figure 1)\nthat explicitly addresses these challenges through\nthree specialized decoders sharing a common\nXLM-RoBERTa encoder (Conneau et al., 2020),\ninitialized from pre-trained multilingual represen-\ntations. We design content word identification as\nan explicit task to be learned by the model rather\nthan relying on intuition-driven heuristics. We par-\nticipate in the multilingual track, training separate\nmodels for each of the nine languages, allowing us\nto tune hyperparameters specifically for each lan-\nguage’s characteristics while still benefiting from\nmultilingual pretrained representations. On the\nshared task’s results, our system achieves the best\noverall performance with average scores of 78.7%\nMSLAS, 80.1% LAS, and 90.3% Feats F1 across\nall languages. Additionally, our model ranks first\non each individual language, demonstrating the ef-\nfectiveness of multitask learning for this task.\nOur error analysis yields three main observa-\ntions: (1) errors in tokenization and content word\nidentification cascade through the pipeline, with\ngold annotations improving MSLAS by up to 12\npoints; (2) the majority of residual errors lie in nom-\ninal morphology—Gender, Number, and Case—with\n2Ine=Inessive,\n“inside\nan\nenclosed\narea”;\nAtr=“complement,\nattribute”.\nBoth definitions come\nfrom the official Case inventory supplied by the shared-task\norganisers.\ncommon Nominative–Accusative swaps; and (3) syn-\ntactic mislabels are concentrated in the nmod versus\nobl relation.\n2\nSystem Description\n2.1\nModel Overview\nWe propose a joint multitask model implemented\nusing the Flair framework (Akbik et al., 2019) for\nmorphosyntactic parsing, as shown in Figure 1.\nAlthough the evaluation metrics assess only depen-\ndency arcs and morphosyntactic features, produc-\ning these outputs requires distinguishing between\ncontent and function words. Because this classifi-\ncation is not given at test time, we treat it as an ad-\nditional prediction task. Our system uses the large\nversion of XLM-RoBERTa augmented with charac-\nter embeddings (Akbik et al., 2018) as a shared en-\ncoder, both provided by the Flair framework. This\nencoder’s output is then passed through a shared in-\ntermediate layer (linear transformation with ReLU\nand dropout) before being fed to three specialized\ndecoders: content word identification, morphosyn-\ntactic feature prediction, and dependency parsing.\n2.2\nDecoders\nContent word identification.\nThe content word\nidentification decoder accepts tokens as input. Each\ntoken’s contextual embedding computed by the\nshared intermediate layer is passed through a bidi-\nrectional LSTM (256 hidden units in both direc-\ntions). The LSTM output is then passed through a\nlinear layer with 2 output units, each corresponds to\n“content” vs. “function” respectively. Training uses\ntwo forms of regularisation: token-level (word)\ndropout—zeroing the entire embedding of 5% of\nUD tokens—and locked dropout that masks 50%\nof the LSTM outputs with the same pattern across\nall timesteps. Class-weighted cross-entropy loss\nfunction is then used to compensate for the imbal-\nance between the number of content and function\ntokens.\nMorphosyntactic features.\nThe morphosyntac-\ntic features decoder consists of a single linear\nlayer that performs multi-label classification di-\nrectly from the output of the shared intermediate\nlayer. For each content word, it outputs proba-\nbilities for all possible feature-value pairs in the\nvocabulary (e.g., Case=Gen, Number=Sing, Voice=Act).\nUsing sigmoid activation with a 0.5 threshold,\nthe model can predict multiple features per to-\nken—for instance, a noun might simultaneously\n2\n\nhave Number=Plur and Case=Gen. Complex features\nwith multiple values (like Case=Ine;Atr) are handled\nby predicting each component separately, allowing\nthe model to learn different value combinations.\nFunction words bypass this decoder entirely and\nreceive ‘_’ as their feature value. At training time,\nwe use gold content word (i.e. checking if its fea-\nture values exist). In contrast, we use the predicted\ncontent words by the content word identification at\ntest time.\nDependency parser.\nThe parsing decoder em-\nploys separate multilayer perceptrons (MLPs) for\narc and relation prediction with biaffine attention\nmechanisms, following Dozat and Manning (2016).\nThe arc MLPs have 256 hidden units while the rela-\ntion MLPs use 128 units, both with layer normaliza-\ntion and ReLU activation. Operating exclusively\non content words, we frame the parser as a con-\nditional random field over projective dependency\ntrees that we implement using TorchStruct (Rush,\n2020). Similar to the morphosyntactic feature de-\ncoder, we use gold and predicted content word at\ntraining and test time respectively.\n2.3\nData Handling and Inference\nWhile the shared task data includes abstract nodes\nfor representing implicit arguments, we initially at-\ntempted to handle them through sequence labeling\nby inserting mask tokens at potential abstract node\npositions. However, this approach introduced noise\nthat degraded performance across all metrics, as\nincorrect abstract node predictions propagated er-\nrors to downstream decoders. Therefore, our final\nsystem filters out abstract nodes during data load-\ning, simplifying the parsing task while improving\noverall performance.\nDuring inference, raw text is first segmented\ninto word tokens using Stanza (Qi et al., 2020).\nSince tokenization quality impacts downstream per-\nformance but is not the focus of this shared task,\nwe choose to leverage Stanza’s pre-trained models\nrather than training custom tokenizers. For each\nlanguage, we evaluated different Stanza model vari-\nants on the development set and selected those that\nbest matched the gold tokenization (e.g., HTB for\nHebrew, IMST for Turkish). This selection was\ndone manually by running the full pipeline with\neach available Stanza model variant and choosing\nthe one that achieved the highest metrics on the\nofficial evaluation script.\nWe apply minimal post-processing to ensure\nvalid output. For content word identification, to-\nkens with confidence below 0.6 that appear be-\ntween two tokens of the opposite type are relabeled\nto match their context (e.g., a low-confidence func-\ntion word between two content words becomes\ncontent). As a fallback for extreme cases where\ncontent word identification predicts all tokens as\nfunction words (particularly in very short sentences\nof 2-3 tokens), we force the first token to be content\nwith deprel=‘root’ and features=‘|’. This ensures\nevery sentence has at least one parseable token.\n2.4\nTraining Objective and Optimization\nThe model is trained end-to-end using a weighted\nsum of the three decoders’ losses:\nLtotal\n=\nwparserLparser + wmorphLmorph + wCWILCWI, where\nthe weights are hyperparameters tuned for each lan-\nguage. The parser uses negative log-likelihood loss\nover projective trees, the morphosyntactic decoder\nuses binary cross-entropy for multi-label classifi-\ncation, and the content word identification uses\nclass-weighted cross-entropy to handle class imbal-\nance.\n3\nExperimental Setup\nThe shared task provided training and development\nsets for multiple languages. To simulate a realis-\ntic evaluation scenario, we split the official train-\ning data into 90% for training and 10% for de-\nvelopment, using the official development set as\nour local test set. This allowed us to tune hyper-\nparameters and select models before the official\ntest release. The languages included in our experi-\nments were English, Turkish, Hebrew, Czech, Pol-\nish, Portuguese, Italian, Serbian, and Swedish, with\ntraining sizes ranging from approximately 3,000 to\n10,000 sentences depending on the language.\nWe develop a custom data loader to handle the\nmodified CoNLL-U format used in the shared task.\nThe loader automatically extracts content words\nby examining the FEATS column, where ‘_’ indi-\ncates function words and any other value indicates\ncontent words. As mentioned before, we filter out\nabstract nodes during loading.\nAll models are trained using AdamW opti-\nmizer (Loshchilov and Hutter, 2019) with an initial\nlearning rate of 2 × 10−5 and batch size of 16 for\n25 epochs. We employ early stopping with pa-\ntience of 1 epoch and learning rate reduction by\nfactor 0.5 when validation loss plateaus. Training\nis performed on a NVIDIA A100 GPU with 32GB\n3\n\nRAM on a high-performance computing cluster,\nwith each model taking approximately 1-5 hours to\nconverge.\nWe perform grid search over task-specific loss\nweights on our development split. The optimal\nweights varied by language—for example, Turk-\nish benefited from weighting parsing and morpho-\nlogical feature losses twice as much as content\nword identification (2.0:2.0:1.5), while English per-\nformed better with parsing weighted most heavily,\nfollowed by morphological features and content\nword identification (2.0:1.5:1.0).\nFor each language, we train three models with\ndifferent random seeds using the same hyperparam-\neter configuration to verify training stability and\nrobustness. All three models are evaluated on our\nlocal test set (the official development set) using\nthe shared task’s official evaluation script.\nOnce hyperparameters are selected, we retrain a\nsingle model for each language using the complete\nofficial training and development data combined.\nThese final models use the same hyperparameters\ndetermined during development. These models are\nused to generate predictions on the official covered\ntest set, which contains only raw text without anno-\ntations. Evaluation is performed using the official\nscript which computes three metrics: MSLAS (mor-\nphosyntactic features F1 only on correctly parsed\ntokens), LAS (labeled attachment score), and Feats\nF1 (morphosyntactic features F1).\n4\nResults\nThis section is divided into two parts: first, we\npresent official test results from models trained on\nall available data (official train + dev combined)\nand evaluated on the covered test set; second, we re-\nport development results using our local data splits\n(90% train, 10% dev, official dev as test) to analyze\ndesign choices and hyperparameter impact.\n4.1\nOfficial Test Results\nTable 2 presents the official test results from models\ntrained on all available data. Our system achieved\nthe highest performance among all submissions\nwith an average MSLAS of 78.7%. The results\nshow strong performance across most languages,\nwith MSLAS scores exceeding 83% for seven\nof the nine languages. Portuguese (88.9%) and\nCzech (87.1%) achieved the highest scores, con-\nsistent with our development results. The morpho-\nlogically complex languages continued to present\nLanguage\nMSLAS\nLAS\nFeats\nCzech\n87.1\n88.0\n95.2\nEnglish\n83.8\n85.1\n94.9\nHebrew\n68.7\n71.4\n83.4\nItalian\n73.0\n73.7\n84.7\nPolish\n75.0\n76.5\n86.2\nPortuguese\n88.9\n89.5\n94.8\nSerbian\n86.6\n88.3\n95.6\nSwedish\n86.6\n87.7\n95.7\nTurkish\n58.7\n60.9\n82.1\nAverage\n78.7\n80.1\n90.3\nTable 2: Official test results on the covered test set.\nOur system achieved the highest average MSLAS score\n(78.7%) among all submissions.\nSystem\nMSLAS\nLAS\nFeats\nOur model\n78.7\n80.1\n90.3\nbaseline_multi\n47.3\n55.4\n64.2\nbaseline_cross\n36.7\n51.2\n50.6\nbaseline_finetune\n33.0\n36.1\n52.3\nTable 3: Comparison with baseline systems (average\nacross all languages).\nchallenges—Turkish (58.7%) and Hebrew (68.7%)\nshowed the lowest performance.\nThe baseline systems provide important context\nfor understanding the task’s difficulty (Table 3).\nThe multilingual few-shot baseline achieved mod-\nerate performance (average MSLAS 47.3%), while\nthe cross-lingual few-shot approach struggled sig-\nnificantly (36.7%), highlighting the importance of\nlanguage-specific examples. The finetuned BERT\nbaseline performed poorest (33.0%), suggesting\nthat the reformulated parsing task with its content-\nfunction distinction and expanded feature inventory\nbenefits from specialized modeling approaches.\nOur 31.4 point improvement over the best base-\nline (78.7% vs 47.3%) indicates that combining\npretrained representations with task-specific archi-\ntectural components can effectively address the\nchallenges of unified morphosyntactic parsing.\n4.2\nDevelopment Results\nThe ablations in Figure 2 show that most of the gain\ncomes from using gold tokenization, with a smaller\nbut consistent boost from explicit content/function\nlabeling. Hebrew makes this clear: MSLAS goes\nfrom 75.2 (Full) →84.5 (GoldTok, +9.3) →85.7\n(GoldWT, +1.2; +10.5 total). This motivates per-\nlanguage tokenizer selection and modeling content\nword identification as a dedicated task.\nLoss-weight tuning largely favored parser=2.0,\n4\n\nFull\nGoldTok\nGoldWT\n60\n80\n100\nMSLAS\nHebrew\nItalian\nSwedish\nTurkish\nFigure 2:\nMSLAS across setups (Full, GoldTok,\nGoldWT) for four languages with the largest gains. Full:\npredicted tokenization and predicted content word iden-\ntity. GoldTok: gold tokenization with predicted content\nword identity. GoldWT: gold tokenization plus gold\ncontent word identity.\nLanguage\nParser\nMorph\nCWI\nCzech\n2.0\n1.5\n1.0\nEnglish\n2.0\n1.5\n1.0\nHebrew\n2.0\n1.5\n1.0\nItalian\n2.0\n1.5\n1.0\nPolish\n2.0\n1.5\n1.0\nPortuguese\n2.0\n1.5\n1.0\nSerbian\n2.0\n1.5\n1.0\nSwedish\n2.0\n1.5\n1.5\nTurkish\n2.0\n2.0\n1.5\nTable 4: Optimal loss weight configurations by language.\nCWI =content word identification.\nmorph=1.5, CWI=1.0; Turkish and Swedish bene-\nfited from higher weights on morph/CWI (Table 4).\n5\nError Analysis\nWe performed error analysis on the models trained\nwith our local data splits (90% train, 10% dev, of-\nficial dev as test). We analyzed only the first seed\nmodel for each language, as the low standard de-\nviations indicate minimal variation across seeds.\nThe analysis uses scripts that replicate the official\nevaluation logic to ensure our error categorization\nmatches the scoring methodology.\n5.1\nNominal Morphology Errors\nThe main feature prediction errors occur in nominal\nmorphology, with Gender, Number, and Case show-\ning the highest confusion rates. Since languages\nhave different feature inventories (e.g., Czech in-\ncludes Dual while others do not), creating a unified\nconfusion matrix is not feasible. We selected Czech\nas a representative example because it has by far the\nmost training data points, resulting in more stable\nmodel behavior.\nOur analysis of Czech reveals strong overall per-\nformance, with 99.1% accuracy for Gender and\n99.6% for Number predictions. For Gender, the\nmodel correctly classifies the vast majority of in-\nstances, with Feminine (12,405 correct), Mascu-\nline (14,914 correct), and Neuter (5,325 correct)\nall showing high diagonal values in the confusion\nmatrix. The annotation scheme includes syncretic\nforms like \"Fem,Masc\" for grammatically ambigu-\nous cases. The most common confusions occur\nbetween Masculine and Feminine (110 instances\nmisclassified as Feminine when Masculine was cor-\nrect), though these remain relatively rare. Simi-\nlarly, for Number, Singular (24,653 correct) and\nPlural (9,587 correct) are accurately predicted, with\nminimal confusion between categories (only 44\nSingular instances misclassified as Plural, and 85\nPlural instances misclassified as Singular).\nSince our model uses multi-label classification\nwith sigmoid activation (threshold 0.5), it occa-\nsionally predicts semantically incompatible feature\ncombinations—for instance, simultaneously pre-\ndicting both a specific gender value (e.g., \"Fem\")\nand a syncretic form containing that value (e.g.,\n\"Fem,Masc\"). While these semantically nonsen-\nsical predictions are rare (occurring in fewer than\n100 instances out of over 30,000), they suggest\nthat post-processing constraints based on linguistic\ncompatibility rules could eliminate such predic-\ntions and further improve the performance.\nFor Case features, plotting a confusion matrix\nis impractical due to the >100 possible values in\nthe expanded inventory. While there is some vari-\nation across languages, aggregating the most fre-\nquent errors reveals consistent patterns. Table 5\nshows the 10 most common Case confusions aver-\naged across all languages. The high frequency of\nNom-Acc confusions (154 and 140 instances) reflects\nboth the prevalence of these cases in the data and\ntheir potential ambiguity—distinguishing core ar-\nguments becomes particularly challenging in com-\nplex sentences with long-distance dependencies or\nmulti-clause structures. This pattern holds across\nlanguages despite their individual variations, sug-\ngesting that even within the expanded Case system,\nthese fundamental grammatical distinctions remain\nchallenging when syntactic complexity increases.\nThese systematic errors in core grammatical cases\nsuggest a targeted improvement strategy: increas-\ning loss weights for frequently confused cases (es-\npecially Nom/Acc) during training. Given our joint\nmodel architecture where all tasks share embed-\n5\n\nCount\nGold case\nPredicted case\n154\nAcc\nNom\n140\nNom\nAcc\n77\nNom\nConj;Nom\n47\nNom\nGen\n44\nConj;Nom\nNom\n43\nGen\nNom\n36\nAcc\nGen\n25\nGen\nAcc\n22\nGen\nConj;Gen\n15\nDat\nIns\nTable 5: Top 10 most frequent case prediction errors\n(average across all languages).\ndings, better representation of these central argu-\nments could benefit dependency parsing as well.\n5.2\nSpatial Case Results\nWe evaluate our model’s performance on the fine-\ngrained spatial Case values, a particularly challeng-\ning subset due to the numerous possible inflec-\ntional meanings that this domain contains.3 The\ncomplete inventory of spatial cases includes over\n40 fine-grained distinctions. Table 6 shows high\nperformance across all languages (F1 scores 89.2-\n98.7%), demonstrating that our model successfully\nlearned the unified Case system for spatial mean-\nings. This annotation scheme directly names in-\nflectional meanings regardless of the grammatical\nmarkers used - for instance, in Polish, when abla-\ntive meaning is expressed periphrastically through\na clitic (an adposition)4 plus an inflected form (a\nroot with a genitive case affix), the system assigns\nthe inflectional meaning (e.g., Case=Abl) instead of\nthe genitive meaning conveyed by the suffix on its\nown. Our model’s performance on these distinc-\ntions suggests it effectively captures the mapping\nbetween diverse surface forms and their underly-\ning spatial semantics. This opens opportunities for\ninjecting linguistic knowledge about spatial rela-\ntions in downstream applications, leveraging the\nsemantic transparency of the annotation scheme.\n5.3\nDependency Parsing Errors\nFor dependency relation errors, we analyze confu-\nsions across all languages since the label inventory\n3These notions are understood as defined by Haspelmath\n(2025): “inflectional meaning” designates the specific mean-\ning conveyed by an inflected form (for example, ablative),\nand “inflectional domain” denotes the broader class of related\nproperties in which this meaning is categorized (for example,\ncase).\n4The classification of adpositions as clitics follows the\ndefinition proposed by Haspelmath (2023).\nLanguage\nPrecision\nRecall\nF1\nCzech\n98.2\n98.4\n98.3\nEnglish\n93.3\n90.3\n91.8\nHebrew\n88.4\n90.0\n89.2\nItalian\n98.0\n97.0\n97.5\nPolish\n98.4\n97.2\n97.8\nPortuguese\n98.5\n99.0\n98.7\nSerbian\n96.4\n93.7\n95.1\nSwedish\n98.4\n96.1\n97.2\nTurkish\n94.7\n96.4\n95.6\nTable 6: Spatial case performance (%) across languages\nusing micro-averaged metrics.\nCount\nGold label\nPredicted label\n67\nobl\nnmod\n63\nnmod\nobl\n11\nobj\nnsubj\n11\nadvmod\n_\n10\nnmod\nflat\n10\nnmod\namod\n9\nnsubj\nobj\n9\niobj\nobj\n8\nobj\nobl\n8\nnsubj\nroot\nTable 7: Top 10 most frequent deprel labeling errors\n(average across all languages).\nis universal. Table 7 presents the 10 most frequent\nlabeling errors aggregated across languages. The\nnmod-obl confusion dominates with 67 and 63 in-\nstances respectively, accounting for over 40% of\nthe top errors. This pattern is linguistically ex-\npected as the boundary between nominal modifiers\nand oblique arguments could involve borderline\ncases.\nUnlike other languages where errors concen-\ntrate on the nmod/obl distinction, Turkish shows a\nmuch more dispersed error pattern with confusions\nspread across many dependency relations. This\nsuggests that our joint architecture may not be opti-\nmal for Turkish’s non-projective structures and rich\nmorphology. A dedicated non-projective parsing\nalgorithm might better capture Turkish’s complex\ndependency patterns.\nAdditionally, we analyze attachment distance\npatterns specifically for parsing errors (i.e., tokens\nwith incorrect head assignments). Figure 3 shows\nthe distribution of attachment distances for Czech\nparsing errors, comparing gold (blue) versus pre-\ndicted (orange) distances for these misparsed to-\nkens. The graph reveals that while most gold at-\ntachments occur at distances 1-3, the model’s er-\nrors tend to predict longer distances (note the or-\nange bars extending further right). This indicates\n6\n\nFigure 3: Distribution of attachment distances for pars-\ning errors in Czech.\nFigure 4: Attachment direction confusion matrix for\nCzech.\nthe parser frequently overlooks nearby heads in fa-\nvor of more distant ones when making mistakes.\nFigure 4 presents a heatmap of misparsed tokens\nwhere rows represent gold attachment directions\nand columns show predicted directions. The strong\ndiagonal (LEFT→LEFT: 1179, RIGHT→RIGHT:\n1058) confirms the model correctly identifies at-\ntachment direction in most error cases. However,\nwithin each correct direction, the parser still selects\nthe wrong head - for instance, when it correctly\npredicts a leftward attachment, it often chooses a\nhead that is too far to the left.\n6\nConclusions\nWe present a joint multitask architecture for unified\nmorphosyntactic parsing that achieves first place in\nthe UniDive 2025 shared task. Our key contribution\nis explicitly modeling content word identification\nas a classification task, creating a robust cascade\nwhere the identification determines parsing and\nfeature assignment.\nOur analysis reveals systematic error patterns\npointing to specific improvement opportunities.\nCase confusions concentrate on core grammatical\ndistinctions (Nom-Acc), while dependency errors re-\nflect the expected challenges at the nmod-obl bound-\nary. While these patterns are linguistically under-\nstandable, they suggest potential room for improve-\nment through weighted training or specialized han-\ndling of frequently confused categories, though\nsuch optimizations may yield only incremental\ngains.\nA more substantial enhancement to the annota-\ntion scheme could be making explicit which func-\ntion words contribute features to which content\nwords. Currently, function words are marked with\n‘_’ and their grammatical information is incorpo-\nrated into \"related\" content words, but these re-\nlationships remain implicit. An indexing system\ncould explicitly link each function word to its tar-\nget content word. This would not only reduce am-\nbiguity in feature assignment but also make the\nannotation more transparent for researchers unfa-\nmiliar with specific languages, as they could trace\nexactly how morphosyntactic information flows\nfrom function words to content words in the uni-\nfied representation.\nFinally, the 30-point performance gap between\nPortuguese and Turkish highlights fundamental\nchallenges in handling typologically diverse lan-\nguages within a unified framework. While the\nparser excels at the predominantly projective struc-\ntures, Turkish’s agglutinative morphology and flex-\nible word order might be introducing some diffi-\nculties. The dispersed error patterns observed for\nTurkish—contrasting with the concentrated confu-\nsions in other languages—suggest that the current\narchitecture may not be optimal for highly non-\nprojective languages. Future work could explore\nspecialized parsing algorithms designed for non-\nprojective structures or alternative architectures\nthat better handle long-distance dependencies and\nflexible word order. Despite these challenges, our\nresults across nine languages demonstrate the vi-\nability of joint morphosyntactic modeling for the\ntask.\nAcknowledgments\nWe thank the anonymous reviewers for their con-\nstructive feedback. This research was supported\n7\n\nby The University of Melbourne’s Research Com-\nputing Services and the Petascale Campus Initia-\ntive. Demian is funded by the Graduate Research\nScholarship from the Faculty of Engineering and\nInformation Technology, University of Melbourne.\nCOST (European Cooperation in Science and Tech-\nnology) kindly provided funding for travel to the\nevent."}
{"id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "authors": ["Amirmohsen Sattarifard", "Sepehr Lavasani", "Ehsan Imani", "Kunlin Zhang", "Hanlin Xu", "Fengyu Sun", "Negar Hassanpour", "Chao Gao"], "source": "http://arxiv.org/abs/2508.14302v1", "page_count": 12, "text": "GLASS: Test-Time Acceleration for LLMs\nvia Global-Local Neural Importance Aggregation\nAmirmohsen Sattarifard *1, Sepehr Lavasani *1, Ehsan Imani 1, Kunlin Zhang 1,\nHanlin Xu 2, Fengyu Sun 2, Negar Hassanpour †1, Chao Gao †1\n1Huawei Technologies Canada\n2Huawei\n*Equal Contribution, †Equal Advising\nAbstract\nDeploying Large Language Models (LLMs) on edge hard-\nware demands aggressive, prompt-aware dynamic pruning\nto reduce computation without degrading quality. Static or\npredictor-based schemes either lock in a single sparsity pat-\ntern or incur extra runtime overhead, and recent zero-shot\nmethods that rely on statistics from a single prompt fail on\nshort prompt and/or long generation scenarios. We introduce\nA/I-GLASS: Activation- and Impact-based Global-Local\nneural importance Aggregation for feed-forward network\nSparSification, two training-free methods that dynamically\nselect FFN units using a rank-aggregation of prompt local\nand model-intrinsic global neuron statistics. Empirical re-\nsults across multiple LLMs and benchmarks demonstrate that\nGLASS significantly outperforms prior training-free meth-\nods, particularly in challenging long-form generation scenar-\nios, without relying on auxiliary predictors or adding any in-\nference overhead.\n1\nIntroduction\nLarge Language Models (LLMs) have shown remarkable\nperformance in a wide range of natural language applica-\ntions (Team et al. 2023, 2024a; Touvron et al. 2023; Jiang\net al. 2023; Anthropic 2024), leading to increasing demand\nfor deployment on edge devices (Alizadeh et al. 2024; Song\net al. 2024; Xue et al. 2024). These models, however, owe\ntheir performance to enormous parameter counts and sub-\nstantial memory and compute requirements which make in-\nference on consumer-grade hardware challenging (Xue et al.\n2024). An effective on-device model therefore should have\nlow inference latency and memory footprint with little com-\npromise on output quality.\nStructured pruning is a well-studied approach to leverage\nsparsity, especially in feed-forward blocks, and to reduce in-\nference time and memory usage. Structured pruning algo-\nrithms iteratively remove weights or neurons in a hardware-\nfriendly manner based on magnitude or impact, and then\nfine-tune the pruned model to recover performance (LeCun,\nDenker, and Solla 1989; Ma, Fang, and Wang 2023; Xia\net al. 2023). The actual magnitude or impact depends on\nthe task or even the current input and thus static pruning\nresults in poor quality-efficiency trade-off. Further, iterative\nfine-tuning is impractical for large-scale models.\nPredictor-based methods improve upon static pruning by\nusing contextual sparsity (Song et al. 2024; Xue et al. 2024;\nL0\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\nL12\nL13\nL14\nL15\nL16\nL17\nL18\nL19\nL20\nL21\nL22\nL23\nL24\nL25\nL26\nL27\nL28\nL29\nL30\nL31\nGlobal + Local\nGlobal only\nLocal only\nFigure 1: Jaccard similarity (↑) between critical neuron\nsets identified by the Global-Local Aggregation (green),\nGlobal (orange), and Local (blue) methods with the oracle’s\nas reference, across various layers on Llama 3 8B. By ag-\ngregating both global and local information, our proposed\napproach more faithfully identifies the oracle’s critical neu-\nron set (see Sec. 4.1 for details).\nAlizadeh et al. 2024). A predictor is first trained on an of-\nfline corpus to predict neuron activations using information\nfrom a previous layer. During inference time, at each layer\nthe predictor anticipates the active neurons in a subsequent\nlayer to avoid loading and computing the rest of the neurons.\nPredictor-based methods yield a higher sparsity ratio than\nstatic pruning since the number of inactive neurons that can\nbe predicted from a previous layer is higher than the number\nof consistently inactive neurons. The improvement comes at\nthe cost of (i) offline predictor training, (ii) inference-time\npredictor overhead, and (iii) extra memory-transfer latency\ndue to loading different neurons for a sequence of generated\ntokens.\nTraining-free methods aim to alleviate these limitations\nby obtaining a sparsity mask from the prompt and using\nit throughout the subsequent generation (Dong, Chen, and\n\nChi 2024; Ma et al. 2025). These algorithms rely on the\nflocking phenomenon, in which neuron-activation patterns\nmostly persist throughout the sequence. Since the prompt is\nfed to the model at once, the entire layer is computed for all\nprompt tokens and the sparsity patterns are used for finding\nthe set of the most active neurons. This set of neurons will\nremain fixed throughout the generation phase. These meth-\nods, however, perform poorly when the prompt is too short\n(i.e., there are few samples for deriving the mask), or when\nthe response is relatively long and the actual activation pat-\ntern drifts.\nSome recent works have instead proposed deriving a dy-\nnamic mask for every newly generated token to track such\ndrift (Zhang et al. 2025; You et al. 2025; Yin et al. 2025;\nStora¨\nı, Lee, and Hwang 2025; Qi et al. 2025; Shin, Yang, and\nYi 2025; Lee, Ramachandran, and Krishna 2025; Yang et al.\n2025). However, generating a new mask at every decoding\nstep incurs extra I/O overhead. Specifically, on edge devices\nwhere inference is memory-bound, the additional overhead\nmay offset the latency gains achieved through sparsifica-\ntion (Liu et al. 2024). Therefore, we focus on algorithms that\nform a single static mask during prompt (prefill) phase and\nreuse it for the entire generation phase, in order to remain\nedge-friendly.\nIn this work, we propose two methods, Activation-\nbased and Impact-based Global-Local Neural Importance\nAggregation for FFN SparSification (A/I-GLASS) that in-\ncorporate model-intrinsic (global) information about neu-\nron activation or impact, while respecting document-specific\n(local) activation patterns. The proposed algorithms Elim-\ninate the need for offline training stage and inference-\ntime overhead of predictor-based methods while signif-\nicantly outperforming previous training-free methods in\nshort prompt and long generation scenarios.\nContributions.\nWe present three main contributions:\n(i) we introduce GLASS, a training-free global-local FFN\nsparsification framework that utilizes local prompt activa-\ntions as well as precomputed global neural importance to\nconduct dynamic, inference-time pruning; (ii) we provide a\ntheoretical foundation by deriving GLASS as the maximum-\nlikelihood fusion of two independent Plackett-Luce rank-\nings, equivalent to a weighted Borda count, grounding our\nmethod in a principled statistical framework; and (iii) we\nempirically demonstrate up to a 45.10% reduction in per-\nplexity and a 25.26% reduction in KL divergence over the\nstrongest prior training-free baseline at 50% sparsity across\nmultiple LLMs and benchmarks.\n2\nProblem Formulation\n2.1\nPreliminaries and Notation\nWe consider a modern Transformer (Vaswani et al. 2017)\nblock where the Feed-Forward Network (FFN) adopts a\ngated structure:\nzu = xWup + bup,\nau = ϕu(zu)\nzg = xWgate + bgate,\nag = ϕg(zg)\nh = au ⊙ag\ny = hWdown + bdown,\n(1)\nGated FFN Parameters and Dimensions\nHere, x ∈Rd\nis the input token embedding. The FFN expands the rep-\nresentation to a higher-dimensional space of width m, per-\nforms elementwise gating, and projects back to Rd:\n• Wup, Wgate ∈Rd×m are the expansion matrices.\n• Wdown ∈Rm×d is the projection matrix.\n• bup, bgate ∈Rm, and bdown ∈Rd are the bias terms.\n• ϕu(·) is an activation function and ϕg(·) is a sigmoid-like\ngating function.\n• h ∈Rm is the FFN hidden unit vector. Its j-th element is\ndenoted by hj\nAssociate weights of a hidden unit refers to the triplet of\nparameters and operations indexed by j in the up, gate, and\ndown projections:\n(W (:,j)\nup\n, W (:,j)\ngate , W (j,:)\ndown)\n(2)\nThis unit contributes the term\nyj = ϕu(xW (:,j)\nup\n) · ϕg(xW (:,j)\ngate ) · W (j,:)\ndown + bj\ndown\n(3)\nto the FFN output y ∈Rd. Sparsifying the FFN corresponds\nto removing some of these hidden units.\nProblem Statement\nOur goal is to dynamically prune a\nsubset of FFN hidden units (neurons) at inference-time, to\nreduce both the parameter count and computational cost,\nwhile minimizing degradation in downstream model qual-\nity with respect to the base (unpruned) model. Specifically,\ngiven a per-layer budget k < m, we aim to select a subset of\nk hidden units (referred to as critical neurons) to retain, and\nremoving the remaining m −k units.\n3\nPrevious Work\nTransformer sparsification techniques can be broadly cat-\negorized into training-based and training-free methods.\nTraining-based methods, such as DejaVu (Liu et al. 2023),\nShadowLLM (Akhauri et al. 2024), and PowerInfer (Song\net al. 2024; Xue et al. 2024) rely on auxiliary predictors\nor routing mechanisms to achieve sparsity. Despite their ef-\nfectiveness, they introduce inference-time overhead due to\nruntime decisions or auxiliary computations. In contrast,\ntraining-free methods operate post-hoc on pre-trained mod-\nels, and are therefore better suited for latency-sensitive de-\nployments.\n3.1\nTraining-Free Methods\nGRIFFIN (Dong, Chen, and Chi 2024) leverages the obser-\nvation that FFN sub-units activated in the prefill phase re-\nmain relevant during generation, enabling single-stage struc-\ntured pruning based on neuron activation magnitude.\nTDA (Ma et al. 2025) extends this concept by applying per-\nlayer activation thresholding during prefill, masking out neu-\nrons of lower importance dynamically.\nTEAL (Liu et al. 2024) adopts a similar approach to TDA,\nand proposes a greedy optimization method for identifying\nlayer-wise sparsity levels.\nCATS (Lee et al. 2024), in contrast, extracts activations once\nfrom offline data (rather than from prompt activations), de-\ntermines a threshold based on the k-th percentile, and zeros\nout activations below this threshold at inference time.\n\nOrthogonal to activation-based sparsification, another re-\nsearch direction estimates neuron or channel importance via\ntheir impact on model loss, often using Taylor expansions.\n3.2\nImpact-Based Importance Estimation\nOptimal Brain Damage (OBD) (LeCun, Denker, and Solla\n1989) and Optimal Brain Surgeon (OBS) (Hassibi, Stork,\nand Wolff 1993) are among the earliest approaches to net-\nwork pruning based on estimating the impact of individual\nparameters on the training loss. They use a second-order\nTaylor expansion of the loss with respect to model weights,\nand compute saliency scores using the diagonal (OBD)\nor full (OBS) Hessian of the loss. While theoretically\ngrounded, these methods are computationally expensive and\nscale poorly with model size, limiting their applicability to\nmodern large-scale networks.\nPerforatedCNNs (Figurnov et al. 2016) is an earlier ap-\nplication of the first-order Taylor expansion for structured\npruning, focusing on reducing spatial redundancy in con-\nvolutional neural networks. This approach demonstrates the\ngeneral effectiveness of first-order saliency and sets the stage\nfor later neuron- and kernel-level pruning methods.\nSimilarly, Molchanov et al. (2017) applies the first-order\nTaylor criterion to prune convolutional channels. Molchanov\net al. (2019) generalizes this to second-order expansions,\nand Kwon et al. (2022) introduces a Fisher information-\nbased, training-free pruning framework for Transformers,\nenhancing inference efficiency.\n4\nModel-Intrinsic Neural Importance\nWe measure model-intrinsic (also referred to as global) neu-\nral importance using any input-independent neural statis-\ntics that enables accurate identification of critical neurons.\nIf the statistics are extracted from sufficiently large and\ndiverse corpora, we still consider them input-independent\nand model-intrinsic. In this work, we consider two global\nneural importance (M g) notions, namely, activation magni-\ntude (Ag) and impact (Ig). As these are computed only once\nper model, this global importance profile can be precom-\nputed and cached at negligible storage cost.\n4.1\nActivation-based Importance\nIn a similar spirit to GRIFFIN, one method to identify the\nsalient neurons is based on their average activation across\na task-agnostic corpus. For each FFN unit j, we define its\nglobal activation magnitude as:\nAg\nj := Ex∼D\nh\nˆ\nhj(x)\ni\n,\n(4)\nwhere ˆ\nh(x) = (ˆ\nh1(x), ..., ˆ\nhm(x)) denotes the ℓ2 normal-\nized activations over each token x. The ℓ2 normalization is\nto ensure that activations are comparable across tokens and\nlayers. The resulting Ag\nj serves as a model-intrinsic impor-\ntance score for unit j, and will later be used for critical neu-\nron selection.\nActivation Consistency\nSparsification methods that rely\nsolely on prompt activations (namely GRIFFIN, TDA, and\nTEAL) to identify critical neurons are highly sensitive to\nprompt length. Specifically, as the prompt becomes shorter,\nthe statistic used to select critical neurons (i.e., ˆ\nh(x)) be-\ncomes more susceptible to error, leading to performance\ndrops in scenarios with short prompts and long genera-\ntions (Dong, Chen, and Chi 2024).\nTo address this, we studied to what extent critical neurons\nare input-dependent. If many neurons consistently appear as\ncritical neurons across diverse prompts, it implies existence\nof a global set of critical neurons that can be identified of-\nfline and later utilized at inference-time to make the critical\nneuron selection process more robust to prompt length.\nWe conducted an experiment to assess this hypothesis.\nFrom the WikiText data (Merity et al. 2016), we extracted\ntwo disjoint sets, each consisting of 100 documents with ex-\nactly 1024 tokens (constructed by concatenating shorter doc-\numents). One set was used to compute the global activation,\ndenoted as Ag. The other set was used to evaluate and com-\npare the following methods for extracting critical neurons at\n50% sparsity:\n1. Oracle: Using the full documents to obtain the ideal crit-\nical neurons for the entire document.1\n2. Local: Only using Al computed from the first 100 tokens\nin the corresponding document (representing GRIFFIN,\nTDA, and TEAL).\n3. Global: Only using Ag (representing CATS).\n4. Global-Local Aggregation: Using average of Ag and\nAl, thereby incorporating both global and local signals.\nTo evaluate the effectiveness of aggregating global and\nlocal activation statistics, we measure how similar the crit-\nical neuron sets obtained from each method (local, global,\nand global-local aggregation) are to the oracle’s ideal criti-\ncal neuron sets. Specifically, we compute the Jaccard simi-\nlarity2 between each method’s set of critical neurons and that\nof the oracle. The results on LLaMA 3 8B (Fig. 1) suggest\nthat (i) global critical neurons do exist; and (ii) this approach\nyields a set more similar to the ideal critical set than using\nonly global or only local signals, across all layers.\n4.2\nImpact-based Importance\nAs an alternative to raw activations, we consider the impact\nof each neuron on the loss, echoing classical Taylor prun-\ning (e.g., (Figurnov et al. 2016; Molchanov et al. 2017)).\nLet fθ(h(x)) be the network output, y the ground-truth\ntoken, and L\n\u0000fθ(h(x)), y\n\u0001\nthe cross-entropy loss of the\nmodel as a function of the activations of the current FFN\nlayer h(x) = (h1(x), . . . , hm(x)). Ablating neuron j sets\n1In practice, this is not feasible, as the oracle requires observing\nthe activations of all neurons not only for the prompt sequence but\nalso for the response sequence in order to identify the optimal set\nof critical neurons for that document. Nonetheless, it can serve as\na reference in our comparisons.\n2The Jaccard similarity between two sets A and B is the size\nof their intersection divided by the size of their union: J(A, B) =\n|A∩B|\n|A∪B|, ranging from 0 (no overlap) to 1 (identical sets).\n\nhj 7→0, i.e. ∆hj = −hj. Applying a first-order Taylor ex-\npansion of L at h yields\n∆Lj ≈∇hL(h) · ∆h =\n∂L\n∂hj\n∆hj = −hj gj,\n(5)\nwhere gj := ∂L/∂hj. Because only the magnitude of ∆Lj\nin Eq. (5) matters for pruning, we define the (neuron-wise)\nimpact score\nIg\nj = Ex∼D\n\u0002\n|hj(x) gj(x)|\n\u0003\n.\n(6)\nNeurons with larger Ig\nj are predicted to incur a larger dis-\nturbance in loss when removed. The resulting Ig\nj serves as\nanother model-intrinsic importance score, and will later be\nused for critical neuron selection.\nImpact locality.\nConsistent with the activation-locality\nphenomenon reported by Song et al. (2024), we observe that\nonly a small, largely input-independent subset of neurons\ncontributes the majority of the impact (long-tailed distribu-\ntion). The complete empirical verification, including figures\nand layer-wise statistics, is presented in Appendix 2.1.\n4.3\nNull Prompt Stimulation\nTo compute global activations or neural impacts, one ap-\nproach is to run the model on a dataset and collect the re-\nspective statistics (Lee et al. 2024). However, standard cor-\npora like Wikipedia can bias the statistics toward specific\nlinguistic patterns. We instead propose Null Prompt Stimu-\nlation (NPS): generating text from the model itself using a\nnull prompt (e.g., empty string or BOS token) under typi-\ncal sampling settings. This produces self-generated data that\nbetter reflects the model’s intrinsic token distribution. Em-\npirically (see Fig. 3), NPS yields cleaner and more con-\nsistent impact estimates, by aligning with the model’s own\ninference-time behavior and avoiding external corpus bias.\n5\nGLASS\n5.1\nFormulation\nLet each FFN layer contain m neurons, and for each neu-\nron i ∈[N], Al\nj denote the expected activation magnitude\nof FFN unit j over the prompt tokens, and M g\nj denote its\nexpected neural importance obtained over D (the offline\ndataset tokens)\nAl\nj, M g\nj ∈R≥0,\n(7)\nDirectly utilizing heterogeneous quantities can be prob-\nlematic because their scales, distributions, and semantics\ndiffer, making weighted summation ill-defined. In contrast,\nranking each unit within its respective distribution renders\nthe two components comparable by preserving their ordinal\npreferences. In GLASS, rather than directly combining raw\nmagnitudes, which may be incompatible when the local and\nglobal components are of different types (e.g., activation vs.\nimpact), we first convert each to ranks.\nRl = Rank\n\u0000Al\u0001\n,\nRg = Rank\n\u0000M g\u0001\n,\n(8)\nwith Rank(Al\nj) and Rank(M g\nj ) denoting the ranks of unit\nj in local and global importance respectively (with higher\nrank value indicating higher importance).\nFigure 2: Overview of GLASS.\nLet each neuron have a latent utility µi ∈R (i.e., a proxy\nof importance of the respective neuron), and εi be a sample\nfrom the Gumbel distribution (type-I extreme-value distri-\nbution). The observed (noisy) utilities are\nUi = µi + εi,\nεi\ni.i.d.\n∼Gumbel(0, 1),\n(9)\nso the probability of observing a full ranking π\n=\n(π1, . . . , πm) ( πt = item placed at rank m −t + 1) follows\nthe Plackett-Luce distribution 3 (Luce et al. 1959; Plackett\n1975):\nP(π | µ) =\nm\nY\nt=1\nexp(µπt)\nPm\nj=t exp(µπj).\n(10)\nWe treat the global and prompt rankings as independent\nrealizations of Eq. (10). With a uniform prior over permuta-\ntions, the joint log-likelihood is\nℓℓ(µ) = λ log P(π(l) | µ)+(1−λ) log P(π(g) | µ), (11)\nwhere λ ∈[0, 1] balances prompt-specific context (λ →1)\nand model-intrinsic diversity (λ →0).\nFollowing Yellott Jr (1977), we expand each log P in\nEq. (11) around the “all-utilities-equal” point µ = 0 (full\nderivation in Appendix 1):\nlog P(π(s) | µ) = Cs+\nm\nX\ni=1\nR(s)\ni\nµi+O(∥µ∥2),\ns ∈{l, g}.\n(12)\nDiscarding the second-order terms, up to an irrelevant\nconstant, and inserting Eq. (12) into Eq. (11)\nℓℓlin(µ) =\nm\nX\ni=1\nh\nλ R(l)\ni\n+ (1 −λ) R(g)\ni\ni\nµi.\n(13)\n3Drawing each neuron’s observed score following Eq. (9) and\nthen selecting the winner by Gumbel-max (arg maxi{µi + εi})\ngives the familiar soft-max/logit choice probabilities (Jang, Gu, and\nPoole 2016). Repeating that selection step to build an entire ranking\nproduces exactly the Plackett-Luce likelihood in Eq. (10).\n\n5.2\nMaximum-Likelihood Ranking\nOnly the ordering of µ matters for maximizing Eq. (13).\nChoose an arbitrary ordering, e.g., µi = Ri (R ∈{x ∈\nNm | set(x) = {1, . . . , m}}). Under this choice, maximiz-\ning Eq. (13) is equivalent to sorting neurons by the scalar\nweight\nGLASSi = λ Rank(Al\nj) + (1 −λ) Rank(M g\nj ).\n(14)\nWhen M g\nj represents global activation magnitudes, it is\nequal to Eq. (4), and we refer to it as A-GLASS; when it\nrepresents neural impacts, it is equal to Eq. (6), and we refer\nto it as I-GLASS. By aggregating ranks, GLASS treats the\nlocal and global importances as two independent voters over\nthe criticality of neurons. This ensures that the final selection\nrespects both prompt relevance and model-intrinsic impor-\ntance while avoiding dominance by any single unnormalized\nmeasure. The top-k critical neurons are then selected based\non the aggregated ranks (Fig. 2), preserving units critical for\ninterpreting the prompt while injecting model-intrinsic con-\ntexts to mitigate overfitting to any single prompt domain.\nEq. (14) coincides with a weighted Borda count. Early\nsocial-choice work proved that Borda is the Maximum-\nLikelihood (ML) estimator of the ground-truth ranking\nin the i.i.d., small-utility, high-noise Plackett-Luce set-\nting (Young 1986). Hence GLASS is not a heuristic but the\nML (and, with a uniform permutation prior, MAP) solu-\ntion to fusing two rank observations under a random-utility\nmodel.\n6\nExperiments\nIn this paper, we compare our method with GRIFFIN, which\nrepresents a family of sparsification methods that rely solely\non prompt statistics. We choose GRIFFIN because deter-\nmining the sparsity level is outside the scope of this work;\nour focus is on comparing different neural importance sig-\nnals, not on deciding how many critical neurons to select.\nTDA and TEAL, which use thresholding and result in vary-\ning sparsity levels across documents, are less suitable for this\ncomparison. However, under fixed sparsity regimes, they are\nfunctionally equivalent to GRIFFIN. We also do not include\nCATS in our experiments, as it has been shown to underper-\nform compared to methods that use prompt activations for\npruning ((Liu et al. 2024)), and our preliminary results (Fig.\n1) indicate that it exhibits lower similarity to the ideal set of\ncritical neurons than global-local aggregation.\nFollowing GRIFFIN’s protocol, we evaluate our methods\non both Classification (C) and Short-form Generation (SG)\nbenchmarks. To more rigorously assess performance under\nextended decode lengths, we introduce a Long-form Gener-\nation (LG) benchmark, that is designed specifically for tasks\nwith brief prompts and lengthy outputs, and demonstrate its\nsuitability for evaluating generation-time sparsification.\nWe note that the benefits of using a static mask during the\ngeneration phase become significant only when the gener-\nation phase is sufficiently long, since models often operate\nat full capacity during the prompt processing stage. There-\nfore, sparsification methods do not meaningfully reduce la-\ntency for tasks involving long prompts but short genera-\ntions. Yet GRIFFIN reports results exclusively on C and SG\nbenchmarks, overlooking the short-prompt, long-generation\nregime in which generation-time sparsification truly shines.\n6.1\nDatasets\nThe C datasets include HellaSwag (Zellers et al. 2019),\nPIQA (Bisk et al. 2020), COPA (Roemmele, Bejan, and\nGordon 2011), ARC-Easy/Challenge (Clark et al. 2018),\nand BoolQ (Clark et al. 2019). For the SG task, we\nuse XSum (Narayan, Cohen, and Lapata 2018), CNN/-\nDailyMail (See, Liu, and Manning 2017; Hermann et al.\n2015), CoQA (Reddy, Chen, and Manning 2019), and\nQASPER (Dasigi et al. 2021). Further details of the C and\nSG datasets are provided in the Appendix 3.1.\nFor the short-prompt LG task, we used Alpaca (Taori\net al. 2023), an instruction-following dataset generated via\nself-instruction using GPT-3. This dataset covers a wide\nrange of tasks and is commonly used to train and evaluate\ngeneral-purpose LLMs. To ensure sufficiently long genera-\ntions, we selected samples from the Alpaca dataset whose\nground-truth outputs exceed 100 tokens and for which none\nof the base models that we study in this work generated out-\nputs shorter than 20 tokens, resulting in 3,602 samples.\n6.2\nModels\nFor C and SG tasks, we evaluate five publicly available\nLLMs in the 6-7 billion parameter range: Gemma 7B (Team\net al. 2024a), Llama 2 7B (Touvron et al. 2023), Mis-\ntral 7B (Jiang et al. 2023), OPT 6.7B (Zhang et al. 2022),\nand Relu-LlaMA 7B (Team 2023), following GRIFFIN\nand TDA. These models vary in architecture, training data,\nand optimization strategies, providing a representative set of\ncontemporary foundation models.\nFor the LG task, we adopted a variety of instruction-\ntuned pretrained models, as base models are generally\nnot capable of generating cohesive long responses and of-\nten generate repetitive content. The models under study\nare: Gemma 7B (Team et al. 2024a), Gemma 2 9B\nand\n27B\n(Team\net\nal.\n2024b),\nMistral\n7B\n(Jiang\net al. 2023), Qwen 2.5 7B and 14B (Qwen et al.\n2025),\nPhi\n3\n14B\n(medium)\n(Abdin\net\nal.\n2024),\nLlama 3 8B (Dubey et al. 2024), as well as Gemma 3n E2B\nand E4B, which are based on the MatFormer architec-\nture (Devvrit et al. 2024) and already incorporate a built-\nin sparsification mechanism specifically designed for on-\ndevice applications.\n6.3\nEvaluation Metrics\nFor evaluation of the C and SG tasks, we adopt GRIFFIN’s\nset of evaluation metrics that includes classification accu-\nracy, F1-Score, exact match, and ROUGE-1/2/L (see Ap-\npendix 3.2 for more details).\nFor the LG task, we argue that evaluating how a sparsi-\nfication method affects an LLM’s performance using met-\nrics such as ROUGE or exact match can be misleading.\nEven small differences in the next-token distribution, when\naccumulated over many generation steps, can cause out-\nputs to diverge substantially, resulting in a rapid decline in\nn-gram-based evaluation scores. Alternatively, one can di-\nrectly assess changes in the next-token probabilities using\n\n0°\n15°\n30°\n45°\n60°\n75°\n105°\n120°\n135°\n150°\n165°\n180°\n90%\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n(a) Gemma KLD\n0°\n15°\n30°\n45°\n60°\n75°\n105°\n120°\n135°\n150°\n165°\n180°\n90%\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n(b) Llama KLD\n0°\n15°\n30°\n45°\n60°\n75°\n105°\n120°\n135°\n150°\n165°\n180°\n90%\n80%\n70%\n60%\n50%\n40%\n30%\n20%\n10%\n(c) Mistral KLD\nFigure 3: Visualization of KLD (PPL results in Appendix 3.4) for different sparsification methods across activation densities and\nmodels. Each semicircle corresponds to a specific density (10% to 90%), with method values placed according to their relative\nmagnitude within each density (min at the rightmost (angle=0◦) point of each semicircle). The size of each dot is proportional\nto the absolute metric value relative to the global maximum for the given model, providing an intuitive view of both method\nranking and value significance. For all plots, a method closer to the angle=0◦line indicates better performance.\nmetrics like Perplexity (PPL) and Kullback-Leibler Diver-\ngence (KLD) over the vocabulary probability distributions\nfor the next token. By conditioning on the base model’s gen-\nerated context, this approach prevents the accumulation of\ndivergence over long sequences and more accurately cap-\ntures deviations caused by sparsification.\nPerplexity\nPerplexity (PPL) gauges how surprised a lan-\nguage model is when predicting the next token in a\nsequence. It is the exponentiated average negative log-\nlikelihood per token:\nPPL = exp\n\u0010\n−1\nN\nN\nX\ni=1\nlog pθ(xi)\n\u0011\n,\n(15)\nwhere xi is the i-th token and pθ(xi) is the model’s out-\nput probability density under parameters θ. Lower values in-\ndicate better predictive performance. We use the unpruned\nmodel’s generated response as the reference for comput-\ning PPL, which effectively measures how unlikely the un-\npruned model’s responses are under the sparsified model.\nThis makes sense, as we want to minimize the deviation of\nthe sparsified model from the base model.\nKullback-Leibler Divergence\nThe average token-level\nKullback-Leibler Divergence (KLD) between the reference\ndistribution Pi and the model distribution Qi is\nKLD =\n1\nN\nN\nX\ni=1\nX\nv∈V\nPi(v) log Pi(v)\nQi(v),\n(16)\nwhere N is the sequence length, v ∈V the vocabulary, Pi\nthe reference token distribution, and Qi = softmax(logitsi)\nis the model-predicted distribution for the i-th position.\nNote that since the vocabulary is large, we compute the\nKLD over only the top-100 tokens with the highest proba-\nbility under the unpruned model. This approximation is jus-\ntified as we found empirically that the top-100 tokens of-\nten account for nearly the entire probability mass, with their\nsummed probabilities effectively equal to 1.\n6.4\nResults and Discussions\nAs mentioned earlier, in addition to the C and SG bench-\nmarks (results detailed in the next subsection), we also\nevaluate our methods on the Alpaca dataset, which pairs\nshort prompts with extended generation lengths, providing\na more representative and challenging testbed for assessing\ngeneration-time sparsification.\nClassification\nand\nShort\nGeneration\nTasks\nTa-\nbles 2 and 3 in Appendix 3.3 indicate that the observed\ndifferences in accuracy between I-GLASS and GRIFFIN\non the classification tasks were not substantial. Specifically,\nthe largest improvement over GRIFFIN was achieved by\nMistral 7B on the ARC-Challenge benchmark, with a gain\nof 0.68%. Conversely, the greatest margin by which GRIF-\nFIN outperformed I-GLASS was observed for Gemma 7B\non the ARC-Easy benchmark, at 0.30%. Similarly, in\nshort-generation tasks, differences in ROUGE scores and\nexact match metrics were relatively minor.\nLong Generation Task\nWe studied the trend of im-\nprovement over GRIFFIN as the activation (density) level\nincreases from 10% to 90%, using three instruction-\ntuned representative models: Llama 3 8B, Gemma 7B, and\nMistral 7B (see Fig. 3). To further assess the effectiveness\nof NPS, we also included variants where the global neural\nimportance was extracted from the WikiText dataset. The\nresults suggest that NPS variants consistently outperform\nthose using WikiText-based importance. Moreover, the re-\nsults indicate that the performance gap between GRIFFIN\nand GLASS tends to widen as the activation density level\ndecreases, highlighting the robustness and effectiveness of\nour approach under more aggressive sparsification regimes.\nTable 1 compares the performance of GRIFFIN with\nA-GLASS and I-GLASS across multiple models using PPL\nand KLD. Overall, both A-GLASS and I-GLASS consis-\ntently outperform GRIFFIN on most models and metrics,\nwith I-GLASS generally achieving the greatest improve-\nments. E.g., on Gemma 2 9B, I-GLASS achieves a 12.91%\n\nModel\nMetric\nGRIFFIN\nA-GLASS\n→Imp%\nI-GLASS\n→Imp%\nGemma 7B\nPPL\n3.7014 (0.0316)\n3.5635 (0.0368)\n3.73%\n3.3982 (0.0310)\n8.19%\nKLD\n0.6453 (0.0033)\n0.5998 (0.0037)\n7.05%\n0.5661 (0.0036)\n12.27%\nGemma 2 9B\nPPL\n4.0143 (0.0262)\n3.5105 (0.0217)\n12.55%\n3.4962 (0.0218)\n12.91%\nKLD\n0.6970 (0.0026)\n0.5877 (0.0025)\n15.68%\n0.5821 (0.0026)\n16.48%\nGemma 2 27B\nPPL\n2.7729 (0.0190)\n2.4860 (0.0165)\n10.35%\n2.4268 (0.0155)\n12.48%\nKLD\n0.4338 (0.0024)\n0.3515 (0.0025)\n18.97%\n0.3349 (0.0024)\n22.80%\nGemma 3n E2B\nPPL\n23.1611 (0.2938)\n12.7155 (0.1806)\n45.10%\n14.4240 (0.2159)\n37.72%\nKLD\n1.9983 (0.0063)\n1.4841 (0.0068)\n25.73%\n1.5677 (0.0072)\n21.55%\nGemma 3n E4B\nPPL\n8.5984 (0.0920)\n5.9075 (0.0707)\n31.30%\n6.3256 (0.0809)\n26.43%\nKLD\n1.3106 (0.0053)\n0.9796 (0.0061)\n25.26%\n1.0190 (0.0066)\n22.25%\nLlama 3 8B\nPPL\n5.2523 (0.0434)\n4.0374 (0.0324)\n23.13%\n4.0295 (0.0331)\n23.28%\nKLD\n0.9426 (0.0047)\n0.7407 (0.0044)\n21.42%\n0.7354 (0.0046)\n21.98%\nMistral 7B\nPPL\n5.0059 (0.0407)\n4.4786 (0.0384)\n10.53%\n4.4860 (0.0387)\n10.39%\nKLD\n0.8774 (0.0041)\n0.7615 (0.0045)\n13.21%\n0.7593 (0.0046)\n13.46%\nQwen 2.5 7B\nPPL\n5.0840 (0.0378)\n5.5823 (0.0581)\n-9.80%\n5.5424 (0.0593)\n-9.02%\nKLD\n0.6772 (0.0028)\n0.6880 (0.0039)\n-1.59%\n0.6684 (0.0040)\n1.30%\nQwen 2.5 14B\nPPL\n5.4330 (0.0882)\n5.5823 (0.0677)\n-2.75%\n5.0279 (0.0701)\n7.46%\nKLD\n0.6557 (0.0048)\n0.5947 (0.0048)\n9.30%\n0.5680 (0.0050)\n13.38%\nPhi 3 14B (medium)\nPPL\n3.8981 (0.0328)\n4.1384 (0.0283)\n-6.16%\n3.7364 (0.0237)\n4.15%\nKLD\n0.6763 (0.0034)\n0.6928 (0.0035)\n-2.44%\n0.6188 (0.0031)\n8.50%\nTable 1: Perplexity (PPL) and top-100 Kullback-Leibler Divergence (KLD) for GRIFFIN compared to our A/I-GLASS (NPS)\nmethods. The Imp% columns represent the improvement percentage over GRIFFIN.\nreduction in PPL and a 16.48% reduction in KLD compared\nto GRIFFIN. Similar trends are observed for larger mod-\nels such as Gemma 2 27B, where I-GLASS reduces PPL\nby 12.48% and KLD by 22.80%. A notable exception is\nQwen 2.5 7B, where both variants slightly degrade PPL,\nlikely due to the model’s tendency to default to Chinese gen-\neration under minimal prompting, which is misaligned with\nthe English Alpaca dataset. Nevertheless, I-GLASS still im-\nproves KLD by 1.30%, indicating some gain in distribu-\ntional alignment. These results highlight the effectiveness of\nour methods in reducing both uncertainty (PPL) and diver-\ngence (KLD) across diverse model scales and architectures.\nAnother notable observation is the remarkable perfor-\nmance gap between GRIFFIN and GLASS when applied to\nthe Gemma 3n family, which employs a MatFormer-style ar-\nchitecture, specifically designed for efficient on-device de-\nployment. MatFormer (Devvrit et al. 2024) enables training\na large model from which smaller submodels can be ex-\ntracted at inference time. The larger gap between GLASS\nand GRIFFIN on Gemma 3n variants, which inherently sup-\nport structured sparsification, highlights the effectiveness\nof GLASS in scenarios requiring more aggressive sparsity.\nFor instance, on Gemma 3n E4B, A-GLASS and I-GLASS\nachieve 31.30% and 26.43% improvements over GRIFFIN\nin terms of PPL, and 25.26% and 22.25% improvements\nin KLD, respectively. On the smaller Gemma 3n E2B, the\ngains are even more pronounced: 45.10% and 37.72% re-\nductions in PPL, and 25.73% and 21.55% in KLD, for\nA-GLASS and I-GLASS, respectively.\n7\nConclusion\nIn this paper, we introduced two training-free methods for\ninference-time sparsification of FFNs in LLMs. By fusing\nlocal prompt activations with global statistics, either activa-\ntion magnitudes (A-GLASS) or impact scores (I-GLASS),\nour methods strike a balance between context sensitivity\nand model-intrinsic diversity without requiring offline pre-\ndictor training or incurring runtime overhead. Extensive ex-\nperiments across multiple tasks and models demonstrate that\nour methods consistently outperform prior training-free ap-\nproaches, especially under short prompts and long genera-\ntions. These results demonstrate that model-intrinsic global\nknowledge can significantly improve prompt-driven spar-\nsification, unlocking reliable speed-ups even for the short-\nprompt long-generation workloads typical of edge devices.\nOur work opens the door for efficient, adaptive inference in\nLLMs with minimal changes.\nLimitations and Future Work\nBuilding on the strong\nperformance of GLASS, we identify a couple of opportu-\nnities to further improve its flexibility and effectiveness:\n(i) currently we apply a fixed sparsity level uniformly and\nshare a single mixing coefficient λ across all layers; jointly\noptimizing the sparsity pattern and learning layer-specific\nλ values could lead to more efficient capacity allocation;\n(ii) each layer is pruned independently for simplicity and\ncompatibility with standard sparsification frameworks, how-\never, this may neglect potential cross-layer interactions that\ncoordinated pruning strategies could leverage."}
{"id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "authors": ["Youssef Maklad", "Fares Wael", "Ali Hamdi", "Wael Elsersy", "Khaled Shaban"], "source": "http://arxiv.org/abs/2508.14300v1", "page_count": 8, "text": "MultiFuzz: A Dense Retrieval-based Multi-Agent\nSystem for Network Protocol Fuzzing\nYoussef Maklad∗, Fares Wael∗, Ali Hamdi∗, Wael Elsersy∗, Khaled Shaban†\n∗Dept. of Computer Science, MSA University, Giza, Egypt\n{youssef.mohamed88, fares.wael, ahamdi, wfarouk}@msa.edu.eg\n†Dept. of Computer Science, Qatar University, Doha, Qatar\nkhaled.shaban@qu.edu.qa\nAbstract—Traditional protocol fuzzing techniques, such as\nthose employed by AFL-based systems, often lack effectiveness\ndue to a limited semantic understanding of complex protocol\ngrammars and rigid seed mutation strategies. Recent works,\nsuch as ChatAFL, have integrated Large Language Models\n(LLMs) to guide protocol fuzzing and address these limitations,\npushing protocol fuzzers to wider exploration of the protocol\nstate space. But ChatAFL still faces issues like unreliable output,\nLLM hallucinations, and assumptions of LLM knowledge about\nprotocol specifications. This paper introduces MultiFuzz, a novel\ndense retrieval-based multi-agent system designed to overcome\nthese limitations by integrating semantic-aware context retrieval,\nspecialized agents, and structured tool-assisted reasoning. Multi-\nFuzz utilizes agentic chunks of protocol documentation (RFC\nDocuments) to build embeddings in a vector database for a\nretrieval-augmented generation (RAG) pipeline, enabling agents\nto generate more reliable and structured outputs, enhancing\nthe fuzzer in mutating protocol messages with enhanced state\ncoverage and adherence to syntactic constraints. The framework\ndecomposes the fuzzing process into modular groups of agents\nthat collaborate through chain-of-thought reasoning to dynami-\ncally adapt fuzzing strategies based on the retrieved contextual\nknowledge. Experimental evaluations on the Real-Time Stream-\ning Protocol (RTSP) demonstrate that MultiFuzz significantly\nimproves branch coverage and explores deeper protocol states\nand transitions over state-of-the-art (SOTA) fuzzers such as\nNSFuzz, AFLNet, and ChatAFL. By combining dense retrieval,\nagentic coordination, and language model reasoning, MultiFuzz\nestablishes a new paradigm in autonomous protocol fuzzing,\noffering a scalable and extensible foundation for future research\nin intelligent agentic-based fuzzing systems.\nIndex Terms—Protocol Fuzzing, Network Security, Finite-State\nMachine, Reverse Engineering, Large Language Models, Multi-\nAgent Systems, Dense Retrieval, Retrieval-Augmented Genera-\ntion, Chain-of-Thoughts\nI. INTRODUCTION\nNetwork protocols form the backbone of modern communi-\ncation systems, yet remain vulnerable to many flaws that can\ncompromise entire infrastructures’ security. Protocol fuzzing\nhas long been an effective technique for uncovering these\nvulnerabilities through automated test generation, and it has\nlong been recognized as an effective technique for uncovering\nthese software vulnerabilities [1]. As network services grow\nin complexity and scale, the importance of discovering imple-\nmentation flaws, especially in stateful protocols with finite-\nstate machines (FSMs), increases. Network protocol fuzzing\nattempts to systematically test protocol implementations by\ngenerating malformed, unexpected, or semi-valid protocol\nmessages to identify anomalous behavior. However, traditional\nfuzzing methods often struggle with unique protocol chal-\nlenges, such as handling complex grammar formats, managing\ndeep-protocol state transitions, and maintaining valid session\nsemantics across multi-packet interactions [2].\nRecent research highlight multiple directions in the advance-\nment of protocol fuzzing, including state-aware input gen-\neration and automated reverse engineering of undocumented\nprotocols [3], [4]. Despite these developments, achieving high\ncoverage and deeper state exploration remains difficult, par-\nticularly for closed-source or proprietary protocols. This has\nmotivated the integration of more intelligent components into\nthe fuzzing loop.\nThe rise of LLMs has opened new avenues for automating\ntraditionally manual tasks in software engineering. LLMs have\ndemonstrated strong capabilities in reasoning, code under-\nstanding, and program synthesis [5], [6]. Their potential in\nsecurity applications, including fuzzing, has begun to evolve.\nStudies show that LLMs can infer message grammars, generate\nvalid input sequences, and even simulate stateful behavior\nwithout access to source code [7], [8]. Recent works such as\nChatAFL introduced LLM-guided protocol fuzzing, resulting\nin improved protocol state coverage [9]. These developments\nhighlight LLMs as promising assistants for fuzzing complex,\nstateful, and security-critical systems.\nIn this work, we present MultiFuzz, a multi-agent system\nbuilt on top of ChatAFL, designed to enhance network proto-\ncol fuzzing by unleashing the capabilities of LLMs and dense\nretrieval [10]. Inspired by recent advancements in retrieval-\naugmented generation and ReAct-based chain-of-thought rea-\nsoning [11], [12], MultiFuzz is structured around collaborative\nagents, each responsible for a specific phase of the ChatAFL\nfuzzing pipeline. Unlike traditional fuzzers or single LLM\napproaches, MultiFuzz orchestrates tool-augmented agents to\nmaintain semantic context support and protocol-specific infer-\nence.\n\nThe main contributions of this work are as follows:\n• We propose MultiFuzz, a multi-agent system for protocol\nfuzzing, integrated with the ChatAFL framework, where\neach group of agents is dedicated to a specific subtask and\nenhanced with tool integration, vector database context\nawareness, and CVE-driven vulnerability knowledge.\n• We introduce an agentic-chunking method and embed-\nding strategy for protocol RFC documents, enabling\nsemantic indexing of protocol knowledge for agent use.\n• We integrate dense retrieval into the agent reasoning\nprocess to maintain a protocol-aware context and guide\nmore effective fuzzing actions.\n• We evaluate MultiFuzz on stateful protocol targets, and\nthe results demonstrate improvements in branch coverage,\nnumber of states explored, and state transitions compared\nto SOTA fuzzers such as NSFuzz, AFLNet, and ChatAFL.\nThrough MultiFuzz, we aim to bridge the gap between\nprotocol-aware fuzzing needs and the generative coordination\ncapabilities of modern agentic-AI architecture, allowing more\nintelligent and effective protocol fuzzing.\nThe paper is structured as follows: Section II introduces\nbackground on protocol fuzzing, LLMs, and multi-agent sys-\ntems. Section III reviews the related work. Section IV explains\nour methodology. Section V explores the research questions,\nexperiments’ setup, and evaluation metrics. Section VI high-\nlights the experimental results. Finally, section VII concludes\nthe paper and suggests potential directions for future works.\nII. BACKGROUND\nThe following subsections review key concepts and recent\ndevelopments in network protocol fuzzing, LLMs, and multi-\nagent systems, providing background information for their\nintegration in modern frameworks.\nA. Network Protocol Fuzzing\nProtocol fuzzing is a specialized security-testing technique\nthat targets the finite-state behavior of communication proto-\ncols by injecting crafted or mutated packets to uncover flaws. It\nrelies on intelligently generating seed inputs that exercise dif-\nferent protocol states, since exploring deep protocol behaviors\noften exposes hidden vulnerabilities. Network protocol fuzzing\nfocuses on testing stateful network services by feeding packet\nsequences through the protocol’s FSM in the server under\ntest (SUT). The goal is to traverse unusual protocol paths\nand trigger implementation bugs or security flaws. Fuzzers\ncan be broadly classified by their test case generation strat-\negy. Mutation-based approaches modify existing valid packets\nusing bit-flipping, arithmetic, block-level, or dictionary-based\ntransformations [13]. In contrast, generation-based approaches\nsynthesize packets from protocol specifications or templates.\nWhile mutation methods may struggle with diverse data types\nand protocol constraints, generation-based methods often face\ndifficulties in acquiring or modeling accurate protocol specifi-\ncations. Additionally, based on the level of knowledge about\nthe target system, fuzzing can be categorized into blackbox,\nwhitebox, and graybox approaches. Blackbox fuzzers operate\nwithout internal knowledge of the protocol implementation,\nrelying solely on input/output observation. White-box fuzzers\nanalyze the source code to guide test case generation, while\ngray-box fuzzers use lightweight instrumentation as code\ncoverage feedback to guide mutations more effectively. In\npractice, graybox fuzzing offers a balanced trade-off and is\nwidely used due to limited access to source code in real-world\nprotocol implementations [14].\nB. Large Language Models\nLLMs have recently demonstrated powerful capabilities in\ngenerating and reasoning over complex inputs, opening new\nopportunities for automation in domains like software testing\nand cybersecurity [7], [8]. They are deep transformer-based\nneural networks [15], trained on massive text corpora, enabling\nthem to generate coherent language and perform complex\nreasoning [12]. Their rich knowledge and generative power\nhave been harnessed in many domains. In cybersecurity, LLMs\nhave shown remarkable utility. For example, ChatPhishDetec-\ntor uses an LLM to detect phishing websites [16]. Maklad. Y.\net al demonstrated how LLMs, enhanced by RAG and chain-\nof-thought reasoning, can be used to evaluate seed enrichment\ntasks and network packet generation [17]. SeedMind explored\nthe use of LLMs for building fuzzing seed generators [18].\nCodamosa highlights the use of LLMs in overcoming cov-\nerage plateaus in test generation [19]. LLMs have also been\nintegrated into automation workflows such as Robotic Process\nAutomation (RPA) and OCR [20]–[22]. These results highlight\nthat LLMs, when incorporated intelligently, can enhance the\naccuracy and efficiency of automation systems.\nC. Multi-Agent Systems\nMulti-agent systems consist of multiple autonomous enti-\nties (agents) that interact and collaborate to solve tasks. By\nharnessing the diverse capabilities and roles of individual\nagents, multi-agent systems can tackle complex problems more\neffectively than a single agent could [23]. For example, agents\nmight divide a workflow so that some gather information,\nothers perform reasoning, and yet others execute actions.\nHowever, orchestrating agents also introduces challenges like\noptimal task allocation, sharing complex context information,\nand memory management, which become critical in LLM-\nbased multi-agent architectures. In security applications, multi-\nagent LLM architectures have begun to emerge. A recent\neffort has introduced PentestAgent [24], a framework in which\nmultiple agents collaborate to automate penetration testing\nand vulnerability analysis. This demonstrates how multi-agent\nsystems can decompose a complex task (like pentesting)\ninto subtasks handled by specialized LLM agents, improving\noverall efficiency.\nIII. RELATED WORK\nA. Protocol Fuzzing\nFuzzing has proven to be one of the most effective tech-\nniques for vulnerability discovery. Protocol fuzzing, in partic-\nular, poses unique challenges due to its reliance on structured\n\nFig. 1. High-Level System Architecture of the MultiFuzz Framework, based on AFLNet and ChatAFL\nformats and stateful interactions. Existing techniques outlined\nearlier in section II, which are blackbox, whitebox, and\ngraybox fuzzing approaches, offer different trade-offs between\nscalability, precision, and required prior knowledge of the\ntarget protocol.\n1) Blackbox Fuzzing: Blackbox fuzzers operate without\nany internal knowledge of the target and typically rely on\ntraffic observation or mutation of recorded protocol messages.\nTools like SPIKE and Peach exemplify early blackbox efforts,\nrelying on manual specification of protocol structures [25].\nPULSAR and BBuzz extract message formats from captured\nnetwork traffic using protocol reverse engineering techniques\n[26], [27]. These methods are simple to deploy but struggle\nwith exploring deeper protocol states, often failing to maintain\nsession validity across multi-message interactions.\n2) Whitebox Fuzzing: Whitebox fuzzers leverage full ac-\ncess to source code or binaries to systematically explore\nexecution paths. Symbolic execution and taint analysis are\ncommonly used to analyze input-dependent behaviors. Polar\ncombines static analysis with dynamic taint tracking to extract\ninput-related conditions from protocol code [14]. While these\ntechniques offer fine-grained insight and deeper coverage, their\nscalability is limited by path explosion and instrumentation\noverhead. Whitebox fuzzers are less commonly used in net-\nwork protocol contexts due to the complexity of protocol\nstacks and message interleaving.\n3) Graybox Fuzzing: Graybox fuzzers balance insight and\nscalability by utilizing lightweight instrumentation to guide\ninput mutations. AFL and its extensibles, like AFL++ and\nAFLNet, employ coverage feedback to direct test-case gen-\neration [28]–[30]. AFLNet extends AFL to stateful network\nprotocols by using response codes to infer protocol state\ntransitions. NSFuzz further improves graybox fuzzing by ex-\ntracting program variables associated with state changes to\nbetter synchronize test inputs with protocol logic [31]. These\ntools have proven effective on real-world network services and\nare the foundation for many modern protocol fuzzers.\nB. Large Language Model-assisted Fuzzing\nRecent research has explored the integration of LLMs into\nfuzzing pipelines. These models offer the ability to generate\nsyntactically correct and semantically meaningful inputs by\nleveraging knowledge learned during pre-training. ChatFuzz\nuses OpenAI’s ChatGPT to mutate existing seed inputs, re-\nsulting in improved edge coverage compared to AFL++ [32].\nChatAFL constructs message grammars and predicts the next\nprotocol message using GPT models, achieving significant\ngains in state and code coverage over AFLNet and NSFuzz\n[9]. MSFuzz extracts abstract syntax trees from the protocol\nsource code via LLMs to guide syntax-aware mutation [33].\nTitanFuzz and FuzzGPT show that LLMs can function as zero-\nshot fuzzers for deep learning libraries by generating edge\ncase inputs and exploiting rare model behaviors without instru-\nmentation or prior seeds [34], [35]. These works demonstrate\nthat LLMs can serve as powerful assistants in automating\ngrammar extraction, seed generation, and mutation strategies\nfor protocol fuzzing.\nIV. METHODOLOGY\nThis section details the design of the MultiFuzz framework\nand its agent-based workflows. The MultiFuzz framework\nAPIs are integrated in the ChatAFL framework, on top of\nthe AFLNet architecture. The whole system architecture can\nbe shown in Figure 1. MultiFuzz is structured around three\n\nspecialized crews of agents: the Grammar Extraction Crew,\nthe Seed Enrichment Crew, and the Coverage Plateau Crew.\nEach crew operates over a shared semantic context retrieved at\ninference by a common dense retrieval agent, which retrieves\nthe agentic chunked embeddings in the vector store. The\nworkflow begins with preprocessing protocol RFCs, then trans-\nforms the content into propositional transformation, followed\nby agentic chunking, and finally, collaborative agent reasoning\nat inference.\nSystem Prompt for Propositional Transformation\nDecompose the RFC documents’ content given into clear and simple text\npropositions, ensuring they are interpretable out of context.\nRules to follow:\n1)\nSplit compound sentences into simple sentences. Maintain the original\nphrasing whenever possible.\n2)\nFor any named entity with descriptive information, separate this\ninformation into its own distinct proposition.\n3)\nDecontextualize propositions by adding necessary modifiers and re-\nplacing pronouns (e.g., ”it”, ”they”, ”this”) with the corresponding full\nentities.\n4)\nPreserve the structure and formatting of any network packet example,\nprotocol message, or code snippet. Do not summarize them.\n5)\nPresent the results as a list of strings formatted in JSON.\nExample:\nInput: ”The DESCRIBE method retrieves the description of a media object, it\naccepts application/sdp...”\nExpected Output:\n{”sentences”: [”The DESCRIBE method retrieves the description...”, ”The\nDESCRIBE method accepts application/sdp...”]\n}\nFig. 2. System prompt used for propositional transformation of\nfiltered RFC documents\nSample Document Chunk Post Agentic Chunking\n{\n”<chunk id>”: {\n”title”: ”RTSP Streaming Protocols and Resource Management”,\n”summary”: ”This chunk contains information about RTSP controls in stream-\ning protocols, emphasizing session management, RTSP URL semantics, and\ntransmission methods, while including examples and method functionalities.”,\n”propositions”: [\n”SETUP starts an RTSP session.”,\n”PLAY starts data transmission on a stream allocated via SETUP.”,\n”PAUSE does not free server resources.”,\n”TEARDOWN causes the RTSP session to cease to exist on the server.”,\n”RTSP methods that contribute to state use the Session header field.”,\n”The ’rtsp’ scheme requires that commands are issued via a reliable protocol,\nspecifically TCP.”,\n”Lines in RTSP messages are terminated by CRLF.”,\n”RTSP methods are idempotent unless otherwise noted.”,\n”For the scheme ’rtsp’, a persistent connection is assumed.”,\n”...”,\n]\n}\n}\nFig. 3. Sample document chunk after the agentic chunking phase of\ntext propositions.\nA. RFC Documents Preprocessing\nWe first process the RFC documents, where each RFC is\nmanually segmented into paragraphs and then passes through a\nseries of filters to extract technical sections, including stateful\ninteractions, command formats, and response rules. We define\nan RFC as a sequence of paragraphs R = {r1, r2, ..., rn}.\nA semantic classifier ffilter maps each paragraph to a boolean\nlabel:\nffilter(ri) =\n(\n1\nif ri is protocol-relevant\n0\notherwise\n(1)\nOnly filtered paragraphs R′ = {ri ∈R | ffilter(ri) = 1} are\nretained for downstream chunking and proposition extraction.\nB. Propositional Transformation\nOnce the RFC content has been semantically filtered and\nstructured into coherent sections using specific delimiters\n(###, ---, @@@), the next step in the pipeline is to trans-\nform these technical paragraphs into interpretable, context-\nindependent atomic propositions. To perform this transforma-\ntion, we constructed an LLM-powered pipeline. Each filtered\nsection is first processed using a carefully designed system\nprompt, shown in Figure 2. The prompt is executed using the\ngpt-4o-mini model with structured output enforced by a JSON\nschema. Each chunk of the RFC document is passed through\nthis pipeline, producing a list of minimal, decontextualized\nstatements that accurately capture the semantics of the protocol\nspecification. Formally, for a given input chunk Ci ∈C where\nC is the set of smart RFC chunks, the transformation function\nT produces:\nT(Ci) = {p1, p2, . . . , pk},\nwhere each pj ∈P\nHere, P denotes the proposition space containing linguis-\ntically simple, context-independent units of meaning. As a\nresult, each paragraph Ci is mapped to a finite set of logically\ncoherent propositions, and the global proposition set P be-\ncomes the knowledge substrate for subsequent dense retrieval\nand crew-based inference modules. In our experiments on\nRFC-2326 (RTSP), this step yielded 445 unique and precise\npropositions.\nC. Agentic Chunking Module\nFollowing the propositional transformation step, we employ\nan intelligent chunking mechanism termed the Agentic Chun-\nker to group semantically similar propositions into cohesive\nand operationally meaningful units. This process creates the\nfoundation for precise retrieval and role-specific agent infer-\nence in later stages of the MultiFuzz framework.\nFormally, given a set of propositions P = {p1, p2, . . . , pn}\nderived from the RFC document, the goal is to partition\nP into a set of non-overlapping semantic chunks Z\n=\n{z1, z2, . . . , zm}, where each zj ⊆P and Sm\nj=1 zj = P. The\nchunking objective can be viewed as an unsupervised grouping\nproblem constrained by topic cohesion, guided by an LLM.\nThe chunking process is agentic in nature: each incoming\nproposition pi is evaluated using a prompt-driven LLM flow by\ngpt-4o-mini. This LLM first examines the current set of chunk\nsummaries and determines whether pi semantically aligns with\nany existing chunk zj. If alignment is detected, pi is appended\nto that chunk. Otherwise, a new chunk is instantiated.\nEach chunk zj maintains three evolving elements:\n\nDense Retrieval Agent\nGrammar Extraction Agent\nGrammar Formatting Agent\nGrammar Formatting Tool\nGrammar Extraction Crew\nAnalysis Agent\nVulnerabilities Agent\nCoverage Surpassing Agent\nCVEs Retrieval Tool\nPacket Parsing Tool\nCoverage Plateau Crew\nSeeds Enricher Agent\nSeeds Parsing Tool\nSeed Enrichment Crew\nFig. 4. Summary of the MultiFuzz’s crews, showing each crew’s internal agents and integrated tools. All three crews share a Dense Retrieval\nAgent for semantic context fetching.\n• A list of constituent propositions {pk}K\nk=1,\n• A concise chunk summary sj generated by the LLM.\n• A descriptive, technically precise chunk title tj.\nThe internal logic can be modeled as a two-stage LLM\npipeline:\n1) Chunk Selection: Given current chunk outlines and a\nnew proposition pi, select the most semantically com-\npatible chunk zj such that:\nzj = arg max\nz∈Z sim(pi, sz)\nIf max sim < θ, where θ is a system-defined compati-\nbility threshold, a new chunk is created.\n2) Metadata Refinement: After assignment, the system\nregenerates the chunk’s summary sj and title tj using\nstructured prompt templates conditioned on the current\nlist of propositions.\nThe final output is a collection of richly annotated document\nobjects, each encapsulating a semantic group of RFC-derived\npropositions, along with human-readable summaries and titles.\nThese document objects were then embedded using OpenAI’s\ntext-embedding-ada-002 model and indexed into a Chroma-\nbased dense vector database. A sample document object can\nbe shown in Figure 3.\nD. Dense Retrieval Agent\nThe first common agent in all crews is the dense retrieval\nagent. This agent is responsible for querying a Chroma-based\ndense vector store populated with semantically grouped RTSP\nagentic chunks. It utilizes a Custom RAG Tool, to per-\nform approximate nearest neighbor search against the indexed\nchunks. The output of this agent is a context-rich corpus of\nrelevant documents passed to assist all agents with their tasks.\nE. Grammar Extraction Crew of Agents\nThe Grammar Extraction Crew is the first crew of agents\ndesigned to extract structured RTSP client request templates\nfor ChatAFL. It operates through three agents: a dense retrieval\nagent, a grammar extraction agent, and a grammar formatting\nagent.\nGrammar Extraction Agent: This agent uses the re-\ntrieved context to produce JSON-formatted RTSP request\ntemplates, where each method (e.g. PLAY, DESCRIBE) maps\nto a list of headers containing <<VALUE>> placeholders and\n\\r\\n terminators.\nGrammar Formatting Agent: It refines the raw JSON\noutput into a clean, numbered textual format using the\nGrammar Extraction Formatting Tool, making it\neasier to parse in the ChatAFL grammar parsing module.\nF. Seed Enrichment Crew of Agents\nThe Seed Enrichment Crew is a two-agent crew designed to\nenhance a given sequence of RTSP client requests by inserting\nnew protocol-compliant packets at semantically correct posi-\ntions. This enrichment supports fuzzers by generating deeper,\nmore state-aware input sequences. The first agent is the dense\nretrieval agent, and the second is the seeds enricher agent.\nSeeds Enricher Agent: This agent interprets the proto-\ncol’s FSM and uses retrieved context from the dense retrieval\nagent to insert two desired client requests, typically absent\nfrom the original seed into their appropriate positions as\nadopted in ChatAFL. It ensures server responses are excluded\nand leverages the Seeds Parsing Tool to generate struc-\ntured outputs of continuous enriched network packets. These\nenriched seeds are structured to be easily parsed by the\nChatAFL parsing module.\nG. Coverage Plateau Surpassing Crew of Agents\nThe Coverage Plateau Surpassing Crew is designed to\nhelp the fuzzer escape stagnation points during test execution,\nwhere no new protocol states or code paths are being explored\nas observed in ChatAFL. This crew of agents aims to generate\npackets that can trigger new transitions by analyzing commu-\nnication history, retrieved context, and optionally exploiting\nknown CVEs.\n\nAnalysis Agent: This agent performs deep context anal-\nysis of the context retrieved from the dense retrieval agent\nand the fuzzer’s communication history to construct a detailed\ngeneration prompt. Rather than producing packets directly, it\ncrafts precise instructions to guide the next agent in generating\na coverage-enhancing input.\nVulnerabilities Agent: To improve the chance of pro-\nducing impactful packets, this agent enriches the genera-\ntion prompt with insights from real CVEs, fetched using a\nCVEs Retrieval Tool which uses the NVD (National\nVulnerability Database) API to obtain the Live555 server\nvulnerabilities [36]. If any vulnerability discovered is relevant\nto the current communication context, the prompt is refined\naccordingly; otherwise, it is forwarded unchanged.\nCoverage Surpassing Agent: Finally, this agent con-\nsumes the refined prompt and generates a valid RTSP client\nrequest designed to surpass the coverage plateau. The agent\nuses a Packet Parsing Tool to structure the final packet\nand log it along with an explanation of its purpose. A\ngenerated sample prompt can be shown in Figure 5.\nSample Prompt to generate Coverage Plateau Packet\n”prompt”: {\n”To surpass the current coverage plateau, Generate a PAUSE request\nthat will transition the server from the Playing state to the Ready\nstate. The PAUSE method should be sent with the appropriate\nheaders, including CSeq: 5, Session: 000022B8, and the method\nset to PAUSE. This will explore the state transition from Playing\nto Ready, potentially revealing new server behaviors and increasing\ncoverage.”\n}\nFig. 5. Sample prompt asking the final agent to generate a coverage\nplateau surpassing packet\nH. Implementation\nWe have developed MultiFuzz on top of two agentic-AI\nframeworks: LangChain [37] and CrewAI [38]. LangChain\nprovides abstractions for building applications on top of\nLLMs. We use the LangChain framework combined with\nthe Chroma vector store for embedding and indexing. We\nutilize it’s features specifically in the RFC processing stage\nfor RFC document agentic chunking and during the dense\nretrieval inference in all agent tasks. CrewAI, on the other\nhand, is used to build autonomous multi-agent systems and\nprovides modular assignment of agents to specific and unique\nroles. It supports integration with multiple LLM API providers\nand offers native support for tool-assisted workflows, enabling\nagents to interact with file systems and vector databases.\nWe use CrewAI in defining and orchestrating the agents that\ncompose the MultiFuzz framework. These structured agent\ngroups, or ”crews”, coordinate within the ChatAFL framework\nusing event-driven task scheduling augmented with custom\nstructured tools.\nV. EXPERIMENTAL DESIGN AND EVALUATION\nWe evaluate the proposed MultiFuzz framework by measur-\ning its effectiveness in fuzzing stateful protocol implementa-\ntions using a multi-agent-based architecture. Our evaluation\naims to answer the following research questions:\n• RQ1: How effective is MultiFuzz in improving branch\ncoverage and state exploration compared to SOTA proto-\ncol fuzzers?\n• RQ2: How does the multi-agent collaboration strategy\nimprove over single-LLM approaches such as ChatAFL?\nTo conduct the evaluation, we test MultiFuzz on the\nRTSP protocol implemented by the Live555 media stream-\ning server. RTSP was selected due to its rich stateful be-\nhavior, complexity in session semantics, and widespread\nuse in multimedia transmission. It presents non-trivial state\ntransitions that make it a fitting candidate for state-aware\nfuzzing. The framework is powered by Llama-based language\nmodels obtained via the Groq-Cloud API [39], which are:\nllama3.3-70b-versatile, deepseek-r1-distill-llama-70b, llama3-\n70b-8192, llama-4-scout-17b-16e-instruct, and llama-3.1-8b-\ninstant, chosen for their reasoning abilities, and long-context\nwindow capacities. Throughout the experimentation process,\nwe explored different combinations of these models across the\nvarious agent groups in the framework. Tasks such as gram-\nmar extraction, seed enrichment, and plateau surpassing were\nassigned to different models iteratively until the most effective\nmodel was identified for each specific subtask, optimizing the\noverall performance of MultiFuzz.\nA. Experiments Setup\nTo evaluate the fuzzing effectiveness of MultiFuzz, we con-\nducted a 24-hour three fuzzing sessions using our framework\nalongside the three SOTA baseline fuzzers: NSFuzz, AFLNet,\nand ChatAFL under the same experimental conditions. All\nexperiments were performed on a local machine running\nUbuntu 24.04.02 LTS, equipped with an Intel Core i5-11300H\nprocessor and 16 GB of RAM. The fuzzers were evaluated\nagainst the RTSP protocol implemented in the Live555 media\nstreaming server. Each fuzzer was independently executed with\ndefault settings. We measured the effectiveness of each fuzzer\nacross several key metrics, including unique crashes, state\ncoverage, branch coverage, and total paths explored. This setup\nallows us to assess the relative performance of MultiFuzz in\ncontrast with existing approaches.\nB. Evaluation Metrics\nTo evaluate MultiFuzz’s fuzzing performance, we adopted\nstandard coverage-based metrics inspired by existing works\nsuch as AFLNet and ChatAFL:\n• Branch\nCoverage:\nNumber\nof\nunique\nconditional\nbranches exercised in the code.\n• Number of States: Number of FSM states reached and\nexplored during fuzzing.\n• Number of State Transitions: The total count of valid\nstate transitions triggered within the protocol’s FSM,\nreflecting the depth of state space exploration.\nWe use ProFuzzBench [40] as our benchmarking platform\ndue to its automated nature in a containerized environment\n\nusing Docker and to baseline with the previous SOTA pro-\ntocol fuzzers. All experiments were repeated multiple times\nto ensure consistency, and results were averaged over time\nwindows to account for variability in execution. This setup\nallows us to rigorously measure MultiFuzz’s capability to\nintelligently generate protocol-aware inputs and uncover deep-\nstate vulnerabilities.\nC. Experimental Results of Fuzzing on Code Coverage\nTable I presents the branch coverage results, demonstrating\nMultiFuzz’s substantial superiority in code coverage metrics.\nMultiFuzz achieves an average branch coverage of 2940\nbranches, representing dramatic improvements of 1.0% over\nChatAFL (2912.67), 2.8% over AFLNet (2860), and 2.3%\nover NSFuzz (2807). Although these percentage improvements\nmay appear modest, the absolute differences are significant in\nthe context of protocol fuzzing, where each additional branch\nrepresents potential discovery of critical vulnerabilities. The\nconsistency of MultiFuzz’s performance is particularly note-\nworthy, with coverage ranging from 2970 to 2940 branches\nacross experiments, demonstrating reliable and predictable\nperformance. In contrast, ChatAFL shows higher variability\n(2890-2998 branches), while AFLNet exhibits perfect con-\nsistency but at significantly lower coverage levels. NSFuzz\ndemonstrates the most variability, with coverage ranging from\n2795 to 2826 branches.\nVI. EXPERIMENTAL RESULTS AND DISCUSSION\nA. Experimental Results of Fuzzing on State Exploration\nTable II and Table III demonstrate that compared to NSFuzz,\nAFLNet, and ChatAFL, MultiFuzz achieves superior perfor-\nmance in both state transitions and state exploration across all\nthree experimental runs. In terms of state transitions, Multi-\nFuzz achieves an average of 163.33 transitions, representing\na significant improvement of 2.3% over ChatAFL (159.67),\n94.5% over AFLNet (84.0), and 81.2% over NSFuzz (90.33).\nThe state exploration results further validate MultiFuzz’s effec-\ntiveness, with an average of 14.67 states explored compared to\nChatAFL’s 14.33 states, AFLNet’s 10.0 states, and NSFuzz’s\n11.7 states. This represents improvements of 2.4%, 46.7%, and\n25.4% respectively.\nTABLE I: Branch coverage achieved by MultiFuzz and baseline\nSOTA fuzzers\nExperiment\nMultiFuzz\nChatAFL\nAFLNet\nNSFuzz\n1\n2970\n2890 ↑2.8%\n2850 ↑4.2%\n2800 ↑6.1%\n2\n2910\n2998 ↓-2.9%\n2870 ↑1.4%\n2795 ↑4.1%\n3\n2940\n2850 ↑3.2%\n2860 ↑2.8%\n2826 ↑4.0%\nAverage\n2940.0\n2912.67 ↑0.9%\n2860.0 ↑2.8%\n2807.0 ↑4.7%\nTABLE II: Number of state transitions achieved by MultiFuzz and\nbaseline SOTA fuzzers\nExperiment\nMultiFuzz\nChatAFL\nAFLNet\nNSFuzz\n1\n163\n159 ↑2.5%\n80 ↑103.8%\n88 ↑85.2%\n2\n165\n158 ↑4.4%\n85 ↑94.1%\n91 ↑81.3%\n3\n162\n162 ↑0.0%\n87 ↑86.2%\n92 ↑76.1%\nAverage\n163.33\n159.67 ↑2.3%\n84.0 ↑94.4%\n90.33 ↑80.8%\nTABLE III: Number of states explored by MultiFuzz and baseline\nSOTA fuzzers\nExperiment\nMultiFuzz\nChatAFL\nAFLNet\nNSFuzz\n1\n14\n14 ↑0.0%\n9 ↑55.6%\n12 ↑16.7%\n2\n15\n14 ↑7.1%\n11 ↑36.4%\n11 ↑36.4%\n3\n15\n15 ↑0.0%\n10 ↑50.0%\n12 ↑25.0%\nAverage\n14.67\n14.33 ↑2.4%\n10.0 ↑46.7%\n11.7 ↑25.4%\nB. Observations\nThe dense retrieval-based multi-agent system of MultiFuzz\nenables more systematic state space exploration by leveraging\nthe agented-chunked and embedded protocol specifications and\ncoordinated agent interactions. Unlike baseline fuzzers that\nrely on conventional feedback-driven exploration, MultiFuzz’s\nmulti-agent architecture facilitates comprehensive state discov-\nery through intelligent coordination and knowledge sharing\namong specialized agents. The results indicate that MultiFuzz’s\ndense retrieval mechanism effectively identifies and prioritizes\nvaluable states that serve as critical transition points within the\nprotocol state machine. The multi-agent coordination allows\nfor parallel exploration strategies while maintaining system-\natic coverage of the state space. This approach significantly\noutperforms traditional fuzzing methods that rely on random\nmutation and single LLM approaches.\nVII. CONCLUSION AND FUTURE WORK\nProtocol fuzzing continues to be a foundational technique\nfor uncovering implementation flaws in communication sys-\ntems. However, traditional fuzzers often face significant limita-\ntions when applied to stateful or proprietary network protocols,\nparticularly due to difficulties in handling complex message\ngrammars, managing multi-step state transitions, and main-\ntaining valid interactions across sessions. We proposed Multi-\nFuzz, a dense retrieval-based multi-agent system designed to\naddress these limitations by leveraging an agentic-RAG-based\narchitecture empowered by chain-of-thought reasoning. Our\napproach builds upon prior advances in LLM-assisted fuzzing\nbut distinguishes itself by introducing multi-agent coordination\ninstead of a single LLM. It proposes agentic-based chunking\nof protocol documents and context-aware inference about\nprotocol specifications and vulnerabilities. These additions\nhelp overcome key challenges in stateful fuzzing, such as\nlow coverage and stagnation during long-running sessions.\nEvaluation across real-world protocol implementations has\nshown that MultiFuzz surpasses existing tools such as NSFuzz,\nAFLNet, and ChatAFL in terms of state exploration and\nbranch coverage. These findings bridge the gap between tradi-\ntional fuzzing methodologies and recent advances in agentic-\nAI, as they open promising opportunities for more effective\nand adaptive security testing.\nLooking forward, we suggest several paths to extend this\nwork. Firstly, enhancing automation by tightly integrating re-\nverse engineering tools, symbolic analyzers, and traffic parsers\ncan further streamline the entire pipeline. Lastly, fine-tuning\nagent behaviors using domain-specific interaction data could\nimprove their effectiveness in specialized protocol domains.\n\nVIII. ACKNOWLEDGMENT\nHeartfelt gratitude is extended to AiTech AU, AiTech for Ar-\ntificial Intelligence and Software Development (https://aitech.\nnet.au), for funding this research and enabling its successful\ncompletion."}
{"id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih Gümüş", "Sercan Karakaş", "Banu Diri", "Savaş Yıldırım", "Demircan Çelik"], "source": "http://arxiv.org/abs/2508.14292v1", "page_count": 15, "text": "Tokens with Meaning: A Hybrid Tokenization\nApproach for NLP\nM. Ali Bayram1, Ali Arda Fincan2, Ahmet Semih Gümü¸\ns2, Sercan Karaka¸\ns3,\nBanu Diri1, Sava¸\ns Yıldırım4, Demircan Çelik2\n1Yıldız Technical University, 2Yeditepe University, 3University of Chicago,\n4Istanbul Bilgi University\nmalibayram20@gmail.com\nAbstract\nTokenization plays a pivotal role in natural language processing (NLP), shaping\nhow textual data is segmented, interpreted, and processed by language models.\nDespite the success of subword-based tokenization techniques such as Byte Pair\nEncoding (BPE) and WordPiece, these methods often fall short in morphologi-\ncally rich and agglutinative languages due to their reliance on statistical frequency\nrather than linguistic structure. This paper introduces a linguistically informed\nhybrid tokenization framework that integrates rule-based morphological analysis\nwith statistical subword segmentation to address these limitations. The proposed\napproach leverages phonological normalization, root-affix dictionaries, and a novel\ntokenization algorithm that balances morpheme preservation with vocabulary ef-\nficiency. It assigns shared identifiers to phonologically variant affixes (e.g., -ler\nand -lar) and phonologically altered root forms (e.g., kitap vs. kitabı), significantly\nreducing redundancy while maintaining semantic integrity. The framework also\nincorporates special tokens for whitespace and orthographic case, including an\n<uppercase> token to prevent vocabulary inflation from capitalization. Byte Pair\nEncoding is integrated to support out-of-vocabulary coverage without compro-\nmising morphological coherence. Evaluation on the TR-MMLU benchmark—a\nlarge-scale, Turkish-specific NLP benchmark—demonstrates that the proposed\ntokenizer achieves the highest Turkish Token Percentage (90.29%) and Pure To-\nken Percentage (85.8%) among all tested models. Comparative analysis against\nwidely used tokenizers from models such as LLaMA, Gemma, and OpenAI’s\nGPT reveals that the proposed method yields more linguistically meaningful and\nsemantically coherent tokens. A qualitative case study further illustrates improved\nmorpheme segmentation and interpretability in complex Turkish sentences. Al-\nthough the implementation focuses on Turkish, the underlying methodology is\nlanguage-independent and adaptable to other languages. This work contributes\nto ongoing efforts to improve tokenizer design through linguistic alignment, of-\nfering a practical and extensible solution for enhancing both interpretability and\nperformance in multilingual NLP systems.\nKeywords: Tokenization, Morphologically Rich Languages, Morphological Seg-\nmentation, Byte Pair Encoding, Turkish NLP, Linguistic Integrity, Low-Resource\nLanguages\n1\nIntroduction\nTokenization, the process of segmenting text into smaller linguistic units called tokens, is a founda-\ntional step in Natural Language Processing (NLP). It has a direct impact on vocabulary construction,\nmodel efficiency, semantic interpretation, and the overall performance of downstream tasks such as\n\nquestion answering, sentiment analysis, and machine translation[1]. While traditional tokenization\ntechniques—such as whitespace or rule-based segmentation—have been commonly used in early\nNLP systems, they fall short in modeling the complex morphological phenomena of many languages,\nparticularly those that exhibit agglutination, inflectional variation, and phonological alternation.\nSubword-based tokenization methods like Byte Pair Encoding (BPE) [2], WordPiece [3], and Unigram\n[4] have become the de facto standard in transformer-based language models such as BERT [5]\nand GPT [6]. These methods address the out-of-vocabulary (OOV) problem by segmenting rare\nwords into frequently occurring subword units, thereby balancing vocabulary size and generalization.\nHowever, despite their computational strengths, these frequency-based methods often disregard the\nlinguistic structure of words. As a result, morphologically rich languages such as Turkish, Finnish,\nand Hungarian are frequently segmented in ways that violate morphemic boundaries, reducing\nsemantic coherence and interpretability [7, 8].\nAgglutinative languages like Turkish pose specific challenges for tokenization. Words are formed\nby appending multiple affixes to a root, producing an expansive set of surface forms that differ\nonly in morphological features. Phonological processes such as vowel harmony and consonant\nalternation further increase the diversity of surface realizations. For instance, plural suffixes like -lAr\nor ablative markers like -dAn, -tAn, functionally represent the same morphemes but differ based on\nthe phonological context. Similarly, root alternations like kitap →kitab (book) and gö˘\ngüs →gö˘\ngs\n(chest) are common in Turkish. Frequency-based subword models fail to account for such variation,\nresulting in redundant and inconsistent tokenization [9].\nTokenization approaches that ignore these morphological and phonological nuances lead to increased\nvocabulary size, fragmented representation of morphosyntactic units, and reduced performance in\nsyntactically dependent tasks. Recent benchmark studies, including TR-MMLU [10] and a cross-\nmodel tokenizer evaluation [9], have shown that metrics such as Turkish Token Percentage (TR %)\nand Pure Token Percentage (Pure %) strongly correlate with downstream model performance. These\nfindings underscore the necessity of tokenization strategies that align with linguistic structures.\nToken purity plays a critical role in the effectiveness of large language models, particularly when\napplied to morphologically complex languages like Turkish. Since LLMs are fundamentally statistical\npattern learners, the quality and clarity of those patterns directly influence their ability to generalize,\nreason, and generate coherent outputs. Pure tokens—those that cleanly align with complete mor-\nphemes such as roots or affixes—provide semantically and syntactically consistent input signals.\nThis allows models to recognize grammatical structures, identify morphological relationships, and\ntransfer learned behavior across different word forms (e.g., kitap, kitabı, kitaplık). In contrast, impure\ntokens—subword units that contain partial or blended morphemes—introduce ambiguity into the\ntoken stream. Such noise disrupts the alignment between token boundaries and linguistic meaning,\nhindering the model’s ability to learn reliable representations.\nEmpirical studies have shown that morphologically aware tokenization can significantly improve\nmodel performance, generalization, and interpretability. Hofmann et al. [11] demonstrated that\ntransformer models with derivationally informed vocabularies perform better at interpreting complex\nword forms, even in English, a language with relatively mild morphological variation. Similarly,\nJabbar [12] introduced MorphPiece, a tokenizer that segments text based on morphemes before\napplying subword encoding. A GPT-style model trained with this tokenizer achieved superior\nperformance across multiple NLP benchmarks—including language modeling, zero-shot GLUE, and\ntext embedding tasks—despite using only half the training iterations of its BPE-based counterpart.\nThese findings provide strong evidence that token purity, grounded in morphological structure,\nenhances learning efficiency and leads to more transparent and generalizable language models.\nThe importance of token purity is analogous to segmentation practices in other machine learning\ndomains. In computer vision, models such as capsule networks [13] and object-centric architectures\nlike Slot Attention [14] show that performance and generalization improve when visual scenes are\ndecomposed into discrete, meaningful entities rather than treated as undifferentiated pixel grids.\nCapsule networks, for example, represent objects as holistic capsules rather than scattered features,\nenabling more accurate recognition in complex visual settings. Similarly, Slot Attention learns to bind\nvisual input to abstract object representations, facilitating compositional reasoning and generalization\nacross novel configurations. The same principle applies to language modeling: when token boundaries\nreflect linguistic structure, the model receives clearer and more interpretable signals. Token purity is\nthus not merely a linguistic preference—it is a structural requirement for training high-performing,\n2\n\nsemantically aware language models. This perspective motivates our use of Pure % as a central\nevaluation metric in this study.\nIn response to these limitations, this paper introduces a linguistically informed, language-independent\ntokenization framework that integrates rule-based morphological segmentation with statistical sub-\nword modeling. The approach includes several key innovations:\nFirst, phonological normalization is applied so that surface variants of the same morpheme are\nassigned a unified identifier. This includes mapping affixes with phonological variation triggered\nby the vowel harmony (e.g., -dAn, -tAn (from)) and roots with final devoicing (e.g., kitap and kitab\n(book)) to shared token IDs. Second, a special token (<uppercase>) is used to encode orthographic\ncase distinctions, enabling models to differentiate capitalized tokens without duplicating them in the\nvocabulary. Third, formatting characters such as space, newline, and tab are explicitly tokenized,\npreserving the structural integrity of the original text for downstream tasks involving structured\ndocuments or layout-sensitive processing. Fourth, a hybrid tokenization algorithm is developed,\ncombining dictionary-based morphological analysis with Byte Pair Encoding. While morphological\nsegmentation ensures alignment with linguistic units, BPE provides fallback coverage for unknown\nwords, maintaining efficiency and scalability in large corpora.\nThe proposed tokenizer is evaluated on the TR-MMLU benchmark to test the hypothesis that\nincorporating linguistic structures—particularly morphological segmentation and phonological nor-\nmalization—into tokenization can significantly enhance semantic alignment and efficiency in mor-\nphologically rich languages. This hypothesis is grounded in prior empirical evidence that linguistic\nalignment metrics such as Turkish Token Percentage (TR %) and Pure Token Percentage (Pure %)\nare correlated with downstream performance on MMLU-style benchmarks [9]. Motivated by these\nfindings, this study aims to develop a tokenization strategy that aligns closely with Turkish mor-\nphosyntactic structures, minimizes redundancy, and improves interpretability. Empirical results\nvalidate this objective: the tokenizer achieved 90.29% TR % and 85.80% Pure %—the highest among\nall evaluated models—outperforming widely used tokenizers such as those from LLaMA, Gemma,\nand Qwen. These results demonstrate that tokenizers designed with linguistic integrity in mind can\nyield tokens that are both semantically meaningful and syntactically coherent, without relying on\nlarge vocabularies or excessive computational overhead. While the implementation is tailored to\nTurkish, the underlying methodology is designed to generalize across other languages.\n2\nRelated Work\nTokenization is a fundamental step in NLP, significantly impacting model performance, memory\nefficiency, and downstream task effectiveness. Tokenization strategies range from character-level\nsegmentation to subword-based methods such as Byte Pair Encoding (BPE) [2], WordPiece [3],\nand Unigram [15]. The choice of tokenization directly influences the ability of models to capture\nsyntactic, semantic, and morphological structures, especially in morphologically rich languages like\nTurkish, Finnish, and Hungarian [7, 8].\nRecent research has explored alternative tokenization strategies tailored to morphologically rich\nlanguages. The morphological tokenizer introduced by [8] outperformed conventional subword\ntokenization techniques, recovering 97% of the performance of larger BERT-based models while\nreducing model size by a factor of three. Additionally, tokenization granularity has been extensively\nexamined in [16], which found that Turkish requires nearly 2.5 times more subwords per word than\nEnglish, emphasizing the importance of vocabulary size in achieving optimal model performance.\nTokenization strategies also play a crucial role in machine translation and text generation tasks.\n[17] demonstrated that morphology-aware segmentation reduces data sparsity in Neural Machine\nTranslation (NMT) for Turkish-English and Uyghur-Chinese translation models. Additionally, [18]\ninvestigated target-side word segmentation strategies, showing that morphological segmentation\nimproves translation accuracy by maintaining linguistic consistency between source and target\nlanguages.\nBeyond language modeling and translation, morphological tokenization has been evaluated in abstrac-\ntive summarization and sentiment analysis tasks. Studies like [7] revealed that morphology-aware\ntokenization improves summarization quality by preserving semantic information and reducing infor-\nmation loss. Hybrid tokenization approaches that combine statistical and morphological segmentation\n3\n\nhave also demonstrated superior performance in multiple NLP tasks, particularly for Named Entity\nRecognition (NER) and Sentiment Analysis [19].\nDespite these advancements, the computational cost of morphological tokenization remains an open\nchallenge. Expanding the vocabulary size in tokenization increases memory consumption and slows\ndown training times. [1] and [5] highlighted that while larger vocabulary sizes enhance performance\nin morphologically complex languages, they also contribute to increased model size. Furthermore,\nenergy consumption in large-scale NLP models has become a growing concern. As discussed in\n[20], optimizing tokenization strategies plays a crucial role in improving resource efficiency and\nminimizing computational costs.\nTo address these challenges, recent research has investigated adaptive tokenization methods that\ndynamically adjust segmentation strategies based on linguistic context. The EuroLLM project [21]\ndeveloped multilingual tokenizers optimized for European languages, incorporating language-specific\nsubword segmentation techniques. Similarly, [22] proposed a selective tokenization approach that\nprioritizes semantically meaningful tokens, demonstrating performance improvements in multilingual\nNLP tasks.\nOverall, ongoing research in tokenization strategies continues to evolve, with increasing emphasis on\ndeveloping efficient, linguistically informed, and adaptive tokenization frameworks. The next section\nwill delve deeper into the role of tokenization in language modeling, pretraining, and benchmark\nevaluations.\nTokenization strategies play a critical role in pretraining large language models (LLMs), influencing\nmodel efficiency, generalization, and performance across downstream tasks. Transformer-based\narchitectures such as BERT [5], RoBERTa [1], and GPT [6] rely on effective tokenization to balance\nvocabulary size, sequence length, and computational cost. Studies have shown that inappropriate\ntokenization choices can introduce biases, degrade semantic coherence, and limit generalization to\nlow-resource languages [23].\nA key challenge in tokenization for LLMs is granularity control—striking a balance between exces-\nsively fragmented sequences and overly coarse segmentation. A comparative study in [16] analyzed\ntokenization granularity across English and Turkish, revealing that standard subword tokenization\nstrategies result in Turkish words being split into approximately 2.5 times more subwords than\nEnglish. This discrepancy affects the efficiency of multilingual models, as Turkish texts require\nlonger sequences to encode the same information.\nBenchmark evaluations such as Massive Multitask Language Understanding (MMLU) [24] and\nTR-MMLU [10] highlight the shortcomings of existing tokenization techniques for morphologically\ncomplex languages. The TR-MMLU benchmark, specifically designed to evaluate Turkish NLP\nmodels, demonstrated that token purity—the alignment of tokens with linguistic units—correlates\nstrongly with downstream model performance. The findings suggest that tokenization strategies\noptimized for English may not be directly transferable to Turkish and similar languages, necessitating\nmorphology-aware adaptations.\nTo address these issues, [9] proposed a novel linguistic integrity framework for evaluating tokenization\nstrategies. This framework introduced token purity and language-specific token percentages (%TR)\nas critical evaluation metrics, providing a structured approach for assessing how well tokenization\npreserves morphological structures. Experimental results confirmed that higher %TR values correlate\nwith improved performance on MMLU-style benchmarks, underscoring the importance of preserving\nlanguage-specific morphemes.\nRecent efforts to refine tokenization strategies have included hybrid and domain-adaptive approaches.\nThe ITUTurkBERT system [19] explored a hybrid tokenization method, combining whitespace\nsegmentation with BPE and Unigram-based subword representations. This method was particularly\nbeneficial for Named Entity Recognition (NER) and abstractive summarization, where preserving\nlinguistic structure is crucial. Similarly, [25] examined the differences between GPT-4 and GPT-4o,\ndemonstrating that model performance fluctuates depending on tokenization quality, especially in\nnon-English tasks.\nBeyond model pretraining, tokenization impacts computational efficiency and energy consumption.\n[20] argued that BPE is suboptimal for pretraining due to inefficient vocabulary utilization, a con-\ncern echoed in [20]. These studies emphasize the need for tokenization techniques that minimize\n4\n\nredundancy and optimize training efficiency. Similarly, research on EuroLLM [21] has focused on\ndeveloping multilingual tokenizers that adjust dynamically to different languages, reducing processing\noverhead while improving semantic coherence.\nDespite these advancements, morphological compositionality remains a challenge for LLMs. [23]\nfound that state-of-the-art models struggle with morphological productivity, particularly when en-\ncountering novel word roots. Their study demonstrated that model performance sharply declines as\nword complexity increases, a phenomenon that affects agglutinative languages more than English\nor Chinese. This finding aligns with earlier work by [8], which concluded that morphology-aware\ntokenization improves semantic alignment, model interpretability, and generalization.\nThe impact of morphological tokenization on NLP pipelines extends beyond text generation and\nclassification. Research in optical character recognition (OCR) and document parsing [26] has\ndemonstrated that custom tokenization tailored to linguistic structures significantly enhances accuracy.\nThe Arabic-Nougat project, for instance, introduced a custom tokenizer, Aranizer-PBE-86k, which\nimproved Markdown structure accuracy and character recognition in Arabic OCR tasks.\nFurther investigations into tokenization adaptation for multilingual models highlight ongoing chal-\nlenges in cross-linguistic NLP. While standardized tokenization methods enable broad compatibility,\nthey often fail to capture the linguistic diversity of non-English languages. [27] established a bench-\nmark for Scandinavian tokenizers, identifying key differences in how tokenization strategies affect\nlanguage understanding. These findings support the argument that morphology-aware tokenization is\nessential for low-resource and typologically diverse languages.\nGiven these insights, tokenization research continues to evolve toward more adaptive, efficient, and\nlinguistically informed models. The next section will explore cutting-edge developments in tokenizer\ndesign, including self-learning tokenization, tokenization-free architectures, and the integration of\nmorphological analysis into transformer-based models.\nDespite these advancements, morphological segmentation remains underutilized in contemporary\nLLM architectures. As shown in [23], even state-of-the-art LLMs struggle with compositional\nmorphology, particularly when encountering novel root words. Their analysis found that performance\ndeclines sharply as morphological complexity increases, with models failing to generalize across dif-\nferent inflected forms. This limitation highlights the need for morphologically informed tokenization\nthat can dynamically adapt to linguistic variations.\nThe integration of linguistic knowledge into tokenizer design has been further explored through\nmorphological tagging and feature-based tokenization. While standard subword tokenization methods\ntokenize text without explicit linguistic knowledge, recent studies have experimented with incorporat-\ning morphological features directly into tokenization schemes [9]. One such approach involves using\nmorphologically tagged tokens instead of raw subwords, preserving grammatical information that is\noften lost in statistical segmentation. However, experiments with morphological tagging as tokens\nhave yielded mixed results, as excessive granularity can lead to sequence length expansion, reducing\nmodel efficiency [16].\nAn emerging area of interest is dynamic tokenization strategies that adapt based on task requirements.\nStudies such as [28] have introduced more flexible Byte-Pair Tokenizers, capable of dynamically\nadjusting segmentation rules based on contextual requirements. This marks a shift away from static,\npre-defined vocabularies toward more adaptable tokenization approaches that can optimize model\nperformance dynamically.\nDespite these advancements, morphological tokenization has yet to become a standard component\nin mainstream NLP models. While experimental results consistently show that morphology-aware\ntokenization improves efficiency and accuracy, most large-scale language models still rely on tradi-\ntional subword segmentation methods. Addressing this gap requires further research into efficient\nmorphological parsing algorithms, lightweight tokenizer architectures, and seamless integration into\npretraining pipelines.\nIn conclusion, tokenization research has evolved significantly from simple whitespace-based segmen-\ntation to more sophisticated subword and morphology-aware techniques. However, the limitations\nof static tokenization—particularly for morphologically rich languages—have spurred interest in\nself-learning tokenization, hybrid approaches, and tokenization-free architectures. Future research\nshould focus on refining dynamic, language-aware tokenization methods that can enhance NLP\n5\n\nmodels across diverse linguistic contexts, ensuring that tokenization strategies do not become a\nbottleneck for language model performance.\n3\nMethodology\nTraditional NLP models primarily relied on word-level tokenization, where each word was treated as\nan individual token. However, this approach was inadequate for handling out-of-vocabulary (OOV)\nwords, requiring extensive vocabulary lists that resulted in inefficient memory usage [6]. To address\nthis, subword tokenization methods such as BPE and WordPiece emerged, segmenting rare words\ninto smaller, frequently occurring subunits, thereby improving generalization and reducing OOV\noccurrences. BPE, originally introduced for data compression [29] and later adapted for NLP by [2],\niteratively merges frequent adjacent character pairs into subword units. Similarly, WordPiece, which\nwas initially developed for speech recognition [3], follows a comparable iterative merging approach\nbut optimizes token selection using likelihood-based probability maximization.\nMorphological complexity presents a significant challenge for NLP tokenization, particularly in ag-\nglutinative languages such as Turkish, Hungarian, and Finnish. These languages exhibit a high degree\nof word inflection, resulting in a vast array of surface forms derived from relatively few lemmas [21].\nIn Turkish, for instance, the word anlayabildiklerimizden (‘from what we were able to understand’)\nis composed of multiple morphemes: anla- (UNDERSTAND) + -yabil (ABLE) + -dik (NOMINALIZER)\n+ -ler (PLURAL) + -imiz (1PL.POSS) + -den (ABLATIVE). Standard subword tokenization methods\nsuch as Byte Pair Encoding (BPE) and WordPiece often fail to capture such rich internal structures,\nfragmenting words in ways that obscure grammatical function and semantic interpretation [16]. This\nmisalignment reduces linguistic coherence and can negatively impact downstream tasks, highlight-\ning the need for tokenizers that are sensitive to language-specific morphological and phonological\nfeatures.\nThe hybrid tokenization framework combines linguistic knowledge with statistical subword segmen-\ntation techniques to enhance tokenization performance in morphologically rich languages, using\nTurkish as a benchmark. The approach integrates rule-based morphological analysis with a structured\ndictionary of roots and affixes while incorporating Byte Pair Encoding (BPE) to handle out-of-\nvocabulary (OOV) words and ambiguous segments. The objective is to create a tokenization system\nthat accurately represents linguistic structures while maintaining computational efficiency.\nThe tokenizer is implemented in both Python and Rust, each optimized for different use cases. The\nPython implementation provides flexibility and ease of integration into NLP pipelines, whereas the\nRust implementation prioritizes performance through parallel processing and efficient memory man-\nagement. The tokenization process follows a structured pipeline consisting of three key components:\ndictionary-based morphological segmentation, BPE integration for subword tokenization, and the\ninclusion of special tokens to preserve linguistic and formatting information.\nMorphological segmentation is a key component of the proposed approach, leveraging a dual-\ndictionary system to accurately identify and segment words. The root dictionary is constructed from\nhigh-frequency words extracted from large-scale Turkish corpora, ensuring that only base word\nforms are included. This dictionary is augmented with phonological normalization techniques to\nprevent vocabulary expansion due to phonological alternations such as final devoicing (kitap →\nkitabı), haplology (alın →alnı), and vowel hiatus (oyna + yor →oynuyor). Furthermore, frequently\nused compound words such as \"akarsu\" and \"çama¸\nsırhane\" are assigned unique identifiers to ensure\nthey are treated as single tokens rather than being arbitrarily segmented.\nThe affix dictionary consists of approximately 230 linguistic elements, including suffixes, prepositions,\nand conjunctions. To improve efficiency and reduce redundancy, affixes with identical grammatical\nfunctions, such as the plural markers \"-lAr\" or the ablative markers \"-dAn,\" are assigned a common\nidentifier. This approach ensures that morphologically equivalent structures do not inflate the\nvocabulary size while preserving their grammatical roles in sentence construction.\nTo ensure comprehensive token coverage, the framework integrates Byte Pair Encoding (BPE) to\nsegment words that are not explicitly listed in the morphological dictionaries. The training data for\nBPE was sourced from large-scale Turkish corpora, specifically umarigan/turkish_corpus_small and\nkadirnar/combined-turkish-datasets-v4, with a combined size of 8.52 GB. Using the SentencePiece\nlibrary, a vocabulary of 10,000 subword units was generated and subsequently incorporated into\n6\n\nInput\nIs there any\nspecial tokens?\nAdd special token\nto token list\nProcess word\nsegment\nIs the word\nin root list?\nAdd ID of root\nto token list\nIterate through\nword to find\nlongest\nmatched root\nIs the root found?\nCheck suffixes\nTry BPE seg-\nmentation\nAre suffixes found?\nAdd IDs of root\nand suffixes\nto token list\nIs remainder a root?\nAdd ID of root\nto token list\nYes\nNo\nYes\nNo\nYes\nNo\nYes\nNo\nYes\nNo\nFigure 1: Tokenization decision flow with root, suffix, and fallback segmentation logic.\n7\n\nthe tokenizer. This enables the system to process novel words while retaining consistency in\nmorphological decomposition.\nSpecial tokens are introduced to handle whitespace, punctuation, capitalization, and unknown words,\nenhancing the tokenizer’s ability to preserve linguistic structure. A dedicated token for whitespace en-\nsures that spacing information is explicitly encoded, preserving sentence structure during tokenization.\nAdditionally, an uppercase token is introduced to differentiate capitalized words from their lowercase\ncounterparts without inflating the vocabulary. Additional tokens account for newline characters, tab\nspaces, and unknown words, preventing tokenization errors when encountering unfamiliar input.\nThe encoding process begins with morphological analysis, where the longest matching root is\nidentified from the dictionary. Once the root is determined, suffix segmentation is performed\nby iteratively checking for affix matches. If a valid segmentation cannot be identified using the\nmorphological dictionary, the remaining portion of the word is processed using BPE-based subword\nsegmentation. Words that do not match any predefined root, suffix, or subword are assigned an\nunknown token, ensuring robustness in handling OOV terms.\nThe decoding process reconstructs tokenized text while adhering to linguistic rules. A reverse mapping\nmechanism ensures that phonological alternations are restored correctly, preserving morphosyntactic\ndependencies. Disambiguation rules are applied to select the most probable reconstruction based on\nphonetic context and grammatical constraints. This process enhances readability while maintaining\nfidelity to the original text.\nThe proposed framework provides a balance between linguistic integrity and computational effi-\nciency. By integrating morphological analysis with BPE-based segmentation, the tokenizer achieves\nimproved performance in capturing linguistic structures while maintaining flexibility for unseen\nwords. Furthermore, the methodology is adaptable to other morphologically complex languages,\ndemonstrating its potential for multilingual applications.\nThe construction of the tokenizer dictionary follows a structured approach that ensures comprehensive\ncoverage of Turkish morphology while maintaining efficiency. The dictionary consists of three\nprimary components: a root word list, an affix list, and a set of functional words such as prepositions\nand conjunctions. These elements form the basis of the tokenization process, enabling accurate\nsegmentation and linguistic representation.\nThe root dictionary is built from a dataset of high-frequency Turkish words extracted from large-scale\ncorpora. This dataset includes approximately 22,000 roots, ensuring broad lexical coverage. Each\nroot is assigned a unique identifier, allowing for consistent referencing throughout the tokenization\nprocess. To improve efficiency, roots are categorized based on their length, enabling a hierarchical\nlookup mechanism that prioritizes longer roots before shorter alternatives. This method significantly\nenhances root detection speed by reducing the number of comparisons required.\nAn additional layer of processing is applied to handle phonological alternations in root words, which\nfrequently occur in Turkish due to sound changes triggered by suffixation. To ensure consistency\nand reduce vocabulary sparsity, different phonetic realizations of the same morphological root are\nmapped to a single identifier. For example, final devoicing results in surface variations such as kitap\n(‘book’) and kitabı (‘its book’), both of which are assigned the same root ID. Similarly, haplology in\nforms like alın (‘forehead’) and alnı (‘his/her forehead’), and vowel hiatus in forms like oyna + yor\n→oynuyor (‘he/she/it is playing’) are normalized through unified token mappings. This phonological\nnormalization preserves morphological coherence while avoiding unnecessary token duplication.\nIn addition to root words, the dictionary includes a comprehensive inventory of approximately 230\nsuffixes, prepositions, and conjunctions, compiled from authoritative linguistic sources and organized\naccording to grammatical function. To further optimize vocabulary size without compromising\nsyntactic accuracy, affixes that perform the same grammatical role are assigned a shared identifier.\nFor instance, plural suffixes such as -lAr, or ablative markers like -dAn, -tAn, functionally represent\nthe same morphemes but differ based on phonological context. This strategy is also applied to\nlocative markers like -dA and -tA, which exhibit surface variation due to consonant alternation rules.\nBy merging such phonologically conditioned allomorphs, the tokenizer reduces redundancy while\nmaintaining linguistic fidelity.\nCompound words represent another important aspect of Turkish morphology, wherein multiple\nroots combine to form a single semantic unit. To prevent incorrect segmentation, frequently used\n8\n\ncompounds such as akarsu (‘stream’) and çama¸\nsırhane (‘laundromat’) are directly included in the\ndictionary and assigned unique token IDs. This ensures that compound expressions are treated as\nindivisible lexical items, preserving their semantic integrity and avoiding erroneous decomposition\ninto root-affix pairs.\nBeyond roots and affixes, the dictionary incorporates functional words such as prepositions and\nconjunctions, which play a crucial role in sentence structure. These elements are often challenging to\ntokenize correctly due to their small size and high frequency. By including them explicitly in the\ndictionary, the tokenizer avoids erroneous segmentations that might result from statistical subword\napproaches.\nThe integration of Byte Pair Encoding (BPE) further enhances tokenization flexibility. While the\ndictionary provides structured linguistic segmentation, BPE ensures robust handling of words not\nexplicitly covered in the predefined lexicon. The BPE model is trained on a diverse Turkish corpus,\nincorporating approximately 10,000 subword units to supplement dictionary-based tokenization. The\ncombined approach enables the tokenizer to efficiently process both frequent and rare words, ensuring\ncomprehensive text coverage.\nAnother important aspect of the proposed framework is its ability to handle case sensitivity without\nincreasing vocabulary size. A dedicated uppercase token is introduced to mark words that were\noriginally capitalized. This avoids the need to store separate tokens for capitalized and lowercase\nversions of the same word, optimizing storage efficiency while preserving orthographic distinctions.\nThe dictionary-driven approach provides a balance between linguistic accuracy and computational\nefficiency. By leveraging structured linguistic resources, normalizing phonological variations, and\nintegrating statistical subword segmentation, the tokenizer achieves robust performance across diverse\ntext types. The next section will describe the encoding and decoding processes in detail, outlining\nhow tokenization is applied in practice to segment and reconstruct text.\nThe encoding process follows a hierarchical approach that ensures linguistic consistency while\nmaintaining computational efficiency. The tokenizer operates in a multi-step pipeline that sequentially\napplies morphological analysis, affix segmentation, and subword processing. This structured approach\noptimizes tokenization accuracy while preserving essential linguistic features.\nThe encoding process begins with preprocessing, where special characters and formatting elements\nare replaced with predefined tokens. Whitespace characters such as spaces, newlines, and tab spaces\nare explicitly encoded using dedicated tokens. This step ensures that text formatting is preserved,\npreventing information loss in structured text. Additionally, words that begin with capital letters are\nmarked with an uppercase token to maintain case information without inflating the vocabulary.\nFollowing preprocessing, the tokenizer applies root detection using a hierarchical lookup strategy.\nThe algorithm first searches for the longest matching root in the dictionary, prioritizing exact matches\nbefore considering phonological variants. If a match is found, the root is assigned its corresponding\ntoken ID. In cases where no direct match is identified, alternative scenarios such as compound words\nor phonologically altered roots are considered. This flexible approach ensures that words are correctly\nsegmented even when phonological modifications are present.\nOnce the root is identified, suffix segmentation is performed iteratively. The algorithm checks for\naffix matches in the suffix dictionary and assigns token IDs accordingly. Each identified suffix is\ntreated as a separate token, maintaining its grammatical function while ensuring proper segmentation.\nThe suffix matching process continues until no further valid suffixes can be extracted. If an affix is\nambiguous or overlaps with multiple possible segmentations, a probabilistic model selects the most\nlikely segmentation based on corpus frequency data.\nIf a word does not match any predefined root or suffix, Byte Pair Encoding (BPE) is applied as a\nfallback mechanism. The BPE model segments the word into subword units based on a pre-trained\nvocabulary, ensuring that unknown words are processed effectively. This hybrid approach prevents the\ntokenizer from failing on unseen words while maintaining the linguistic integrity of known structures.\nFor example, the word kalktı˘\ngımızda (‘when we stood up’) is segmented into its root and affix\ncomponents as follows:\nInput text: \"Kalktığımızda hep birlikte yürüdük.\" (“When we stood up, we walked to-\ngether.”)\n9\n\nToken\nsequence:\n[uppercase], kalk, tığ, ımız, da, [space], hep, [space],\nbirlikte, [space], yürü, dü, k, .\nToken\nIDs:\n0, 1502, 22280, 22285, 22278, 1, 2300, 1, 4803, 1, 2280, 22296,\n22617, 22582\nInput Text\n\"Kalktığımızda hep birlikte yürüdük.\"\nTokenized\n[uppercase], kalk, tığ, ımız, da, [space], hep, [space], birlikte, [space], yürü, dü, k, .\nToken IDs\n0, 1502, 22280, 22285, 22278, 1, 2300, 1, 4803, 1, 2280, 22296, 22617, 22582\nDecoded Tokens\nOutput Text\n\"Kalktığımızda hep birlikte yürüdük.\"\nEncoding\nDecoding\nFigure 2: Encoding and decoding process for the sentence “Kalktı˘\ngımızda hep birlikte yürüdük.”\nThis example demonstrates how the encoder accurately identifies the root kalk (“stand up”), segments\nits suffixes (-tı˘\ng “past nominalizer”, -ımız “our”, -da “when/at”), and preserves syntactic structure us-\ning dedicated space and punctuation tokens. Each token corresponds to a morphologically meaningful\nunit, enabling interpretable and reversible text representations.\nThe decoding process reconstructs surface text from tokenized sequences while maintaining linguistic\naccuracy. Token IDs are mapped back to their textual forms, and affixes are recombined according to\ntheir grammatical function. During this step, phonological alternations are reversed: rules for soft\nconsonantization, vowel deletion, and contraction are reapplied to ensure natural word formation.\nCapitalization is restored using a dedicated [uppercase] token, which automatically capitalizes the\nfirst letter of the following word. Space and punctuation tokens ([space], ., etc.) are replaced with\ntheir respective characters, maintaining sentence layout. If an unknown or out-of-vocabulary token is\nencountered, a placeholder is inserted to allow for post-processing or human review.\nConsider another example:\nToken sequence: [uppercase], kitap, [space], okuma, yı, [space], sev, i, yor,\num, .\nDecoded output: \"Kitap okumayı seviyorum.\" (“I like reading books.”)\nThis process demonstrates how the tokenizer ensures both accuracy and efficiency in text reconstruc-\ntion, preserving morphological structure while maintaining fluency.\nThe proposed framework successfully integrates morphological analysis with subword segmentation,\ncreating a robust tokenizer optimized for morphologically complex languages. By balancing linguistic\nintegrity and computational efficiency, this approach offers a scalable solution adaptable to multiple\n10\n\nlanguages. Future work may explore extending this framework to other agglutinative languages,\noptimizing it for real-time applications, and integrating additional linguistic features for enhanced\nperformance.\n4\nResults and Analysis\nThe performance of the proposed morphological tokenizer was evaluated using the TR-MMLU\nbenchmark dataset, which comprises over 1.6 million characters and approximately 200,000 words\ncurated specifically for Turkish [10]. This dataset is designed to reflect the linguistic complexity of\nTurkish, including its rich morphology, agglutinative structures, and diverse syntactic constructions.\nAs such, it provides a rigorous basis for assessing tokenization quality in morphologically complex\nlanguages.\nThe\nevaluation\ncompared\nfive\ndifferent\ntokenizers:\ngoogle/gemma-2-9b,\nmeta-llama/Llama-3.2-3B, Qwen/Qwen2.5-7B-Instruct, CohereForAI/aya-expanse-8b,\nand the proposed turkish_tokenizer. Each tokenizer was assessed using a consistent set of\nlinguistic and computational metrics introduced in [9]. These metrics include total token count,\nvocabulary size, number of unique tokens, Turkish Token Percentage (TR %), and Pure Token\nPercentage (Pure %). TR % quantifies the proportion of tokens that correspond to valid Turkish words\nor morphemes, while Pure % measures the proportion of tokens that fully align with unambiguous\nroot or affix boundaries, thus reflecting morphological integrity.\nTable 1: Performance of the proposed turkish_tokenizer on the TR-MMLU dataset.\nMetric\nValue\nVocabulary Size\n32,768\nTotal Token Count\n707,727\nProcessing Time (s)\n0.6714\nUnique Token Count\n11,144\nTurkish Token Count\n10,062\nTurkish Token Percentage (TR %)\n90.29%\nPure Token Count\n9,562\nPure Token Percentage (Pure %)\n85.80%\nThe proposed turkish_tokenizer demonstrated the highest linguistic alignment across all evalu-\nated metrics. It achieved a TR % of 90.29% and a Pure % of 85.80%, substantially outperforming\nall competing tokenizers. In comparison, google/gemma-2-9b reached a TR % of only 40.96%\nand a Pure % of 28.49%, indicating that the majority of its tokens do not represent full morphemes.\nSimilarly, meta-llama/Llama-3.2-3B produced a TR % of 45.77% and a Pure % of 31.45%, while\nQwen2.5 and aya-expanse achieved TR % values of 40.39% and 53.48%, respectively.\nDespite employing significantly smaller vocabulary sizes, the proposed tokenizer demonstrated\nbetter linguistic segmentation. With a vocabulary of 32,768 tokens and 11,144 unique tokens used\nduring evaluation, it balanced generalization and expressiveness more effectively than models such\nas gemma-2-9b and aya-expanse, which rely on vocabularies of over 255,000 tokens. These\nlarge-vocabulary tokenizers, rooted in frequency-based subword segmentation, tend to fragment\nmorphologically rich expressions and introduce ambiguity in downstream tasks. In contrast, the\nmorphological awareness of the turkish_tokenizer enables semantically coherent token formation\nand more consistent syntactic parsing.\nAlthough the total token count generated by the proposed tokenizer (707,727) exceeds those of the\nother models—for instance, aya-expanse produced 434,526 tokens—this increase is offset by gains\nin interpretability and linguistic fidelity. High TR % and Pure % scores suggest reduced reliance on\nspurious subword splits and improved preservation of morphosyntactic structure. This is particularly\nbeneficial for tasks such as syntactic parsing, translation, summarization, and question answering,\nwhere semantic consistency across tokens is essential.\nThese findings support the hypothesis introduced in [9], which argues that high linguistic alignment in\ntokenization correlates strongly with downstream model performance in morphologically rich and low-\nresource languages. While conventional subword tokenizers may suffice for high-resource languages\nlike English, they exhibit clear limitations in Turkish unless informed by morphological structure.\n11\n\nThe results presented here highlight the effectiveness of combining rule-based linguistic analysis\nwith subword strategies to produce tokenizers that are both accurate and efficient in morphologically\ncomplex settings.\nTo illustrate the linguistic fidelity of different tokenization strategies, we present a qualitative compar-\nison using the Turkish sentence:\n\"Atasözleri geçmi¸\nsten günümüze kadar ula¸\nsan anlamı bakımından mecazlı bir mana kazanan kalı-\npla¸\nsmı¸\ns sözlerdir.\"\n(“Proverbs are fixed expressions passed down from the past to the present that acquire a metaphorical\nmeaning in terms of their significance.”)\nThis sentence contains a wide range of morphological features, including compound words, multiple\nderivational and inflectional suffixes, and root forms that undergo phonological alternations. These\nproperties make it an ideal test case for evaluating the morphological sensitivity of different tokenizers.\nProposed Hybrid Tokenizer:\nThe hybrid morphological tokenizer segments the sentence into linguistically meaningful units with\nhigh fidelity. It produces:\n[\"<uppercase>\", \"atasöz\", \"ler\", \"i\", \"<space>\", \"geçmiş\", \"ten\", \"<space>\",\n\"gün\", \"üm\", \"üz\", \"e\", \"<space>\", \"kadar\", \"<space>\", \"ulaş\", \"an\",\n\"<space>\", \"anlam\", \"ı\", \"<space>\", \"bakım\", \"ın\", \"dan\", \"<space>\",\n\"mecaz\", \"lı\", \"<space>\", \"bir\", \"<space>\", \"mana\", \"<space>\", \"kazan\",\n\"an\", \"<space>\", \"kalıp\", \"laş\", \"mış\", \"<space>\", \"sözle\", \"r\", \"dir\",\n\".\"]\nIt correctly separates suffixes (\"ler\", \"i\", \"ın\", \"dan\", \"lı\", \"an\", \"mış\", \"dir\"),\nextracts root forms such as \"atasöz\", \"gün\", \"mana\", and employs special tokens like\n\"<uppercase>\" and \"<space>\" to preserve orthographic structure.\nGemma-3:\nThe tokenizer google/gemma-3 segments the sentence as:\n[\"<bos>\", \"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" gün\", \"ümü\", \"ze\",\n\" kadar\", \" ulaş\", \"an\", \" anlam\", \"ı\", \" bakım\", \"ından\", \" mec\", \"az\",\n\"lı\", \" bir\", \" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"mış\", \"\nsöz\", \"lerdir\", \".\"]\nAlthough it captures some suffixes like \"ten\" and \"ından\", it fragments common roots (\"At\",\n\"as\", \"öz\" instead of \"atasöz\") and fails to isolate inner morphemes in forms such as \"lerdir\"\nand \"kazanan\", limiting morphological interpretability.\nLLaMA-3.2:\nThe tokenizer meta-llama/Llama-3.2-3B yields:\n[\"<|begin_of_text|>\", \"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" gün\",\n\"ümü\", \"ze\", \" kadar\", \" \", \"ula\", \"ş\", \"an\", \" anlam\", \"ı\", \" bakımından\",\n\" me\", \"ca\", \"z\", \"lı\", \" bir\", \" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\",\n\"ş\", \"mış\", \" söz\", \"lerdir\", \".\"]\nThis tokenizer combines morphologically valid segments like \"bakımından\" and \"kazanan\" with\nfragmented roots like \"At\", \"as\", \"öz\", creating inconsistency in morpheme alignment.\nQwen2.5:\nThe tokenizer Qwen/Qwen2.5 outputs:\n[\"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" gün\", \"üm\", \"ü\", \"ze\", \"\nkadar\", \" ulaş\", \"an\", \" anlamı\", \" bakım\", \"ından\", \" me\", \"ca\", \"z\", \"lı\",\n\" bir\", \" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"mış\", \" söz\",\n\"ler\", \"dir\", \".\"]\nWhile suffixes such as \"ten\" and \"ından\" are recognized, the tokenizer introduces redundant splits\nlike \"üm\", \"ü\", \"ze\", reducing the linguistic coherence of the token stream.\n12\n\nAya-Expanse:\nThe tokenizer CohereForAI/aya-expanse returns:\n[\"<BOS_TOKEN>\", \"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" günümüze\", \"\nkadar\", \" ulaşan\", \" anlamı\", \" bakımından\", \" mec\", \"az\", \"lı\", \" bir\", \"\nmana\", \" kazanan\", \" kalı\", \"pl\", \"aş\", \"mış\", \" söz\", \"lerdir\", \".\"]\nIt retains some complete word forms such as \"günümüze\" and \"ulaşan\", but still fragments\ncompounds like \"kalıplaşmış\" and splits the root \"atasöz\", reducing morphological traceability.\nPhi-4:\nThe tokenizer microsoft/phi-4 produces:\n[\"At\", \"as\", \"ö\", \"z\", \"leri\", \" geç\", \"mi\", \"ş\", \"ten\", \" gün\", \"üm\", \"ü\",\n\"ze\", \" kadar\", \" \", \"ula\", \"ş\", \"an\", \" an\", \"lam\", \"ı\", \" bak\", \"ım\",\n\"ından\", \" me\", \"ca\", \"z\", \"lı\", \" bir\", \" mana\", \" kaz\", \"anan\", \" kal\",\n\"ı\", \"pla\", \"ş\", \"m\", \"ış\", \" sö\", \"z\", \"ler\", \"dir\", \".\"]\nThis tokenizer over-fragments even basic stems like \"geçmiş\" into \"geç\", \"mi\", \"ş\" and\n\"anlam\" into \"an\", \"lam\", increasing token count and reducing interpretability.\nYTU Turkish GPT-2:\nThe\ntokenizer\nytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1,\ntrained\non\nTurkish corpora, yields:\n[\"At\", \"as\", \"öz\", \"leri\", \" geçmişten\", \" günümüze\", \" kadar\", \" ulaşan\",\n\" anlamı\", \" bakımından\", \" mec\", \"az\", \"lı\", \" bir\", \" mana\", \" kazanan\",\n\" kalıp\", \"laşmış\", \" söz\", \"lerdir\", \".\"]\nAlthough it still segments \"atasözleri\" incorrectly, it performs well with forms like \"geçmişten\",\n\"günümüze\", and \"bakımından\", showing the advantage of Turkish-specific pretraining.\nGPT-4o:\nThe tokenizer gpt-4o-o200k_base generates:\n[\"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" gün\", \"ümü\", \"ze\", \" kadar\",\n\" ulaş\", \"an\", \" anlam\", \"ı\", \" bakım\", \"ından\", \" mec\", \"az\", \"lı\", \" bir\",\n\" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"mış\", \" söz\", \"ler\",\n\"dir\", \".\"]\nIts segmentation strategy is similar to LLaMA and Qwen—partially aware of Turkish morphemes but\nlimited by frequent over-segmentation of compound and derived forms.\nThe results presented in this section provide strong empirical support for the hypothesis introduced in\nthe introduction: tokenizers that explicitly incorporate morphological and phonological knowledge of\nTurkish can outperform general-purpose models in both segmentation accuracy and linguistic coher-\nence. While most state-of-the-art tokenizers struggle with root-fragmentation, over-segmentation,\nand inconsistent affix treatment, the proposed hybrid tokenizer consistently identifies morpheme\nboundaries, preserves semantically meaningful units, and reduces vocabulary redundancy. These\nfindings validate the motivation behind this work: morphologically informed tokenization is essential\nfor robust and interpretable NLP in agglutinative languages like Turkish. The qualitative compar-\nisons presented here illustrate not only the performance gap between general and language-specific\ntokenizers, but also the need for tokenizer architectures that respect language-internal rules.\n5\nConclusion\nIn this study, we introduced a linguistically-informed hybrid tokenization framework specifically\ndesigned to address the challenges posed by morphologically rich and low-resource languages,\nwith Turkish serving as the primary case study. By integrating rule-based morphological analysis\nwith subword segmentation techniques such as Byte Pair Encoding (BPE), our approach seeks\nto preserve morpheme boundaries, minimize vocabulary redundancy, and improve syntactic and\nsemantic coherence during tokenization.\nEmpirical\nevaluations\non\nthe\nTR-MMLU\ndataset\ndemonstrated\nthat\nthe\nproposed\nturkish_tokenizer significantly outperforms existing state-of-the-art tokenizers—including\n13\n\ngemma-2, llama-3, qwen2.5, and aya-expanse—in both Turkish Token Percentage (TR %)\nand Pure Token Percentage (Pure %), achieving 90.29% and 85.80%, respectively. These metrics\nreflect the tokenizer’s strong alignment with the linguistic structure of Turkish, a crucial factor for\ndownstream NLP tasks. The tokenizer also exhibited efficient vocabulary utilization with only\n32,768 entries and showed robust performance in handling morphosyntactic structures across diverse\nsentence types.\nQualitative analyses further reinforced the superiority of our approach, revealing that the proposed tok-\nenizer segments text into linguistically meaningful units and accurately preserves suffixes, compound\nforms, and phonologically altered variants—challenges frequently mishandled by general-purpose,\nfrequency-driven tokenization strategies. The findings presented here reaffirm the thesis proposed\nin [9], namely that tokenization strategies rooted in linguistic structure are not only desirable but\nnecessary for accurate and efficient language modeling in morphologically complex settings. As NLP\ncontinues to evolve toward inclusive, multilingual systems, the development of linguistically aware\ntokenization methods will be critical for ensuring equity in language technologies.\nFuture directions include extending this hybrid framework to other agglutinative and typologically\ndiverse languages, refining the morphological rules through semi-supervised learning, and exploring\nintegration with multilingual LLM pretraining pipelines to optimize performance in low-resource\nlanguage environments."}
{"id": "2508.14288v1", "title": "Measuring LLM Code Generation Stability via Structural Entropy", "date": "2025-08-19", "authors": ["Yewei Song", "Tiezhu Sun", "Xunzhu Tang", "Prateek Rajput", "Tegawende F. Bissyande", "Jacques Klein"], "source": "http://arxiv.org/abs/2508.14288v1", "page_count": 5, "text": "Measuring LLM Code Generation Stability via\nStructural Entropy\nYewei Song, Tiezhu Sun, Xunzhu Tang, Prateek Kumar Rajput,\nTegawend´\ne F. Bissyand´\ne Jacques Klein,\nThe Interdisciplinary Centre for Security, Reliability and Trust\nUniversity of Luxembourg\nAbstract—Assessing the stability of code generation from\nlarge language models (LLMs) is essential for judging their\nreliability in real-world development. We extend prior “structural-\nentropy” concepts to the program domain by pairing entropy\nwith abstract-syntax-tree (AST) analysis. For any fixed prompt,\nwe collect the multiset of depth-bounded subtrees of AST in\neach generated program and treat their relative frequencies\nas a probability distribution. We then measure stability in two\ncomplementary ways: (i) Jensen–Shannon divergence, a symmetric,\nbounded indicator of structural overlap, and (ii) a Structural Cross-\nEntropy ratio that highlights missing high-probability patterns.\nBoth metrics admit structural-only and token-aware variants,\nenabling separate views on control-flow shape and identifier-level\nvariability. Unlike pass@k, BLEU, or CodeBLEU, our metrics are\nreference-free, language-agnostic, and execution-independent. We\nbenchmark several leading LLMs on standard code generation\ntasks, demonstrating that AST-driven structural entropy reveals\nnuances in model consistency and robustness. The method runs\nin O(n, d) time with no external tests, providing a lightweight\naddition to the code-generation evaluation toolkit.\nIndex Terms—Large Language Models, Code Generation,\nStructural Entropy, Evaluation Metrics, Stability Test\nI. INTRODUCTION\nThe advent of powerful Large Language Models (LLMs) has\nenabled remarkable capabilities in automated code generation.\nHowever, a notable challenge is the high variability of generated\ncode: identical prompts can yield substantially different code\nsnippets across runs or across models. Prior studies observe that\neven with fixed inputs and hyperparameters, state-of-the-art\nLLMs are rarely deterministic at the output level [1]. For\nexample, ChatGPT produces completely different code on\nrepeated queries for the same programming task in the majority\nof cases (e.g. 75.8% of tasks showed zero identical test outputs\nacross runs) [2]. Crucially, setting the sampling temperature\nto zero (greedy decoding) did not guarantee consistency.\nThis output instability undermines developer trust and makes\nreproducibility of code-generation research difficult. In safety-\ncritical or collaborative software settings, unpredictable LLM\nsuggestions can harm reliability. Hence there is a pressing need\nto rigorously quantify the structural stability of generated code,\nbeyond simply assessing correctness.\nCurrent evaluation of LLM code outputs largely focuses\non functional correctness or textual similarity to reference\nsolutions. The pass@k metric, for example, checks if any of k\nsamples passes the unit tests [3], thereby measuring functional\nsuccess. While pass@k and related metrics (e.g. the unbiased\npass-ratio@n) are valuable for overall performance [4], they do\nnot address variability: different outputs may all pass the tests\nbut differ substantially in structure. Likewise, traditional NLP-\nstyle metrics (BLEU, ROUGE, METEOR, etc. [5]–[7]) assess\nn-gram overlap with a reference solution, but have known\nlimitations for code [8], [9]. Ren et al. in the CodeBLEU\nwork show that BLEU correlates poorly with code semantics\nand cannot account for the many functionally equivalent\nprograms that do not share surface tokens [10]. Code-specific\nmetrics such as CodeBLEU have been proposed to address\nsyntax and semantics: CodeBLEU augments n-gram matches\nwith AST-based syntax weighting and data-flow features [10].\nThese improvements help correlate with human judgments, but\nCodeBLEU and similar metrics still measure pairwise similarity\nto a single reference output rather than the consistency across\nmultiple samples. We list some metrics that are currently used\nfor measuring LLM stability in Table I. In short, existing\nmetrics focus on correctness or reference fidelity, but they do\nnot capture structural/topological similarity of different outputs\nfrom the same prompt.\nAlthough structural entropy—measuring uncertainty or vari-\nability in structural choices—is not widely used in LLM\ncode-generation literature, related concepts have been explored\nin software engineering contexts. For example, Torres et al.\napplied structural entropy metrics to study software evolution,\nquantifying how changes affect the organization and complexity\nof code structures over time [11]. Their method captures how\ntransformations impact predictability and structural information\nin software systems. A closely analogous idea in LLM text\ngeneration is Semantic Entropy (SE), proposed by Kossen et\nal., which assesses model uncertainty by clustering multiple\ngenerated answers by meaning; high SE indicates diverse or\ninconsistent outputs, signaling possible hallucinations [12].\nInspired by this, our work adapts the entropy concept to\ncode generation, analyzing variability in generated code AST\nstructures. Low structural entropy reflects consistent outputs,\nwhereas high entropy indicates the model frequently alternates\nbetween different programming structures or approaches.\nII. STRUCTURAL ENTROPY AND SIMILARITY\nOur pipeline consists of three phases: subtree extraction from\nASTs, constructing empirical distributions from these subtrees,\n1Proposed by this paper. Readers can check the code with the link:\nhttps://github.com/Etamin/SCE.git\n\nTABLE I\nCOMPARISON OF CURRENT STABILITY METRICS WITH OUR APPROACHES.\nEvaluation\nMethod\nDescription\nStructure\nRef.\nBLEU/\nROUGE-L/\nMETEOR\nN-gram overlap or sequence matching\nto reference code.\nNo\n[5]–[7]\nExact Match\nChecks if generated code exactly\nmatches the reference.\nNo\n-\nPass@k\nRuns up to k generated outputs;\nsucceeds if any pass all tests.\nIndirect\n[13]\nCodeBLEU\nCombines BLEU with AST subtree\nmatch and data-flow match.\nYes\n[10]\nRUBY\nCompare Program Dependency Graph\nof output vs. reference.\nYes\n[14]\nTSED\nComputes the tree edit distance between\nAST of output and AST of reference.\nYes\n[15]\nSemantic\nEntropy\nClusters multiple answers to the same\nquestion, then calculates entropy.\nIndirect\n[16]\nStructural\nEntropy1\nProposed: Parse the answers’ AST structure and\ncomputes the subtree entropy between\nanswers, with KL or JS divergence.\nYes\n-\nand computing similarity metrics. We parse code outputs\ninto ASTs and extract depth-bounded subtrees, transforming\nthem into canonical encodings. These encodings are then\nused to construct empirical distributions that capture structural\nvariations between code outputs. Finally, we calculate stability\nusing two entropy-based metrics: Jensen–Shannon divergence,\nproviding a symmetric measure of structural similarity, and\nStructural Cross-Entropy, which emphasizes missing high-\nprobability patterns.\nPhase 1 : Depth-bounded Subtree Extraction: Let TA and\nTB denote the ASTs parsed from two code snippets. Fix a\ndepth parameter d ∈N. For every node v in an AST, we\nconsider the rooted subtree sub(v; d) which is the fragment\nof T rooted at v containing all descendants up to depth d. To\ntransform such subtrees into hashable symbols, we define two\ncanonical encodings:\n(a) Structure-only\nencoding σstruct\n:\nsub(v; d)\n7→\n(node-type(v),\n\u0000node-type(c1), . . . , node-type(ck)\n\u0001\n),\nwhere c1, . . . , ck are the immediate children of v.\n(b) Structure+value\nencoding σvalue\n:\nsub(v; d)\n7→\n(node-type(v), lexeme(v),\n\u0000node-type(c1), . . . , node-type(ck)\n\u0001\n), where lexeme(v)\nis the exact source text covered by v (for leaves) or\na sentinel ∅(for internal nodes).\nThese two approaches trade off generality vs. specificity.\nStructure-only patterns may capture common coding patterns\nand yield higher overlap between different programs, but they\nlose fine-grained information. Structure-with-value patterns are\nmore discriminative (sensitive to exact code), but they also\nincrease the size of the pattern vocabulary and may yield\nsparser overlap. In practice, one can choose the representation\ndepending on whether value information is important to the\nsimilarity task.\nFor either choice of σ, we enumerate the multisets of depth-\nbounded subtrees extracted from TA and TB:\nSA =\n\b\nσ\n\u0000sub(v; d)\n\u0001\n\n\nv ∈TA\n\n,\nSB =\n\b\nσ\n\u0000sub(v; d)\n\u0001\n\n\nv ∈TB\n\n.\n(1)\nLet nA = |SA| and nB = |SB| be the total numbers of\n(possibly repeated) subtree symbols harvested. In (1) we treat\nSA, SB as multisets symbols may appear with multiplicity.\nPhase 2 : Constructing Empirical Distributions: Let the\nmultisets of subtree symbols extracted in Phase 1 be SA ⊆Σ\n∗\nand SB ⊆Σ\n∗, where Σ\n∗denotes the countable universe of\ncanonical subtree encodings (either structure–only or struc-\nture–with–value). Define the joint support\nU = SA ∪SB =\n\b\nu1, . . . , um\n\n,\nm = |U|.\n(2)\nEach ui ∈U is a distinct subtree symbol.\nMultiplicity functions.: For u ∈U define\ncA(u) = #\n\b\ns ∈SA : s = u\n\n,\ncB(u) = #\n\b\ns ∈SB : s = u\n\n,\n(3)\ni.e. the number of occurrences of u in the respective multiset.\nLet\nnA =\nX\nu∈U\ncA(u),\nnB =\nX\nu∈U\ncB(u)\n(4)\nbe the total numbers of (depth-bounded) subtrees harvested\nfrom TA and TB, respectively.\nEmpirical probability distributions.: We define the fre-\nquency distributions P, Q : U →[0, 1] by\nP(u) = cA(u)\nnA\n,\nQ(u) = max\n\u0010cB(u)\nnB\n, ε\n\u0011\n,\nX\nu∈U\nP(u) =\nX\nu∈U\nQ(u) = 1,\n(5)\nwhere 0 < ε ≪1 is a fixed smoothing constant ensuring\nQ(u) > 0 for every u ∈U. (The summation constraint on\nQ can be restored by a final renormalisation, but in practice\nε≪1/nB suffices and the effect on P\nu Q(u) is negligible.)\nIn vector notation let\np =\n\u0000P(u1), . . . , P(um)\n\u0001\nT,\nq =\n\u0000Q(u1), . . . , Q(um)\n\u0001\nT ∈[0, 1]m\n(6)\nso ∥p∥1 = ∥q∥1 = 1. These two probability vectors fully\ncharacterize the structural ’vocabularies’ of AST TA and TB\nand constitute the input for the similarity metrics of Phase 3.\nRemark. The inclusion of a smoothing parameter ε is only\nrequired for the directed cross-entropy in Phase 3 A to\navoid undefined log Q(u) when Q(u) = 0. The symmetric\nJensen–Shannon divergence of Phase 3 B remains finite without\nexplicit smoothing as long as P and Q are defined on the\ncommon support U.\nPhase 3 : Entropy–Based Similarity Metrics: With P and\nQ defined in (5), we present two similarity measures.\n(A) Structural Cross-Entropy (SCE): Define the cross-\nentropy of P relative to Q and the Shannon entropy of Q\nas\nH(P, Q) = −\nX\nu∈U\nP(u) log Q(u),\nH(Q) = −\nX\nu∈U\nQ(u) log Q(u).\n(7)\n2\n\nWe normalise by taking the ratio\nSCE(P, Q) =\nH(Q)\nH(P, Q),\n0 < SCE ≤1.\nSCE(P, Q) = 1 ⇐\n⇒P = Q;\nSCE →0 as Q fails to explain P.\n(8)\nBecause the numerator and denominator share the same\nlogarithmic base, the score is scale–independent and monotone\nin the KL divergence DKL(P ∥Q) = H(P, Q) −H(P). In\ncode, compute H(Q) and H(P, Q) directly with base-2 logs;\nskip terms with P(u) = 0 and apply ε smoothing on Q(u).\nThis ratio acts as a normalized structural cross-entropy.\n(B) Jensen–Shannon Divergence (JSD): Let M = 1\n2(P +\nQ) denote the midpoint distribution. The Jensen–Shannon\ndivergence is\nDJS(P ∥Q) =\n1\n2DKL(P ∥M) +\n1\n2DKL(Q ∥M)\n= H(M) −1\n2\n\u0002\nH(P) + H(Q)\n\u0003\n.\n(9)\nWith logarithm base 2, DJS ∈[0, 1] bits and √DJS is a metric\n[17]. We use\nSJSD(P, Q) = 1 −DJS(P ∥Q) ∈[0, 1],\n(10)\nwhere larger values again indicate greater similarity. Unlike\nSCE, JSD is symmetric and finite without smoothing so long\nas P, Q share support.\na) Computational cost: Both scores require O(|U|) arith-\nmetic once P and Q are formed. When depth d is fixed, |U|\ngrows at most linearly in the number of AST nodes, the overall\npipeline O(n d) for subtree extraction plus O(|U|) for scoring.\nb) Summary: SCE (Equation 8) offers a directed mea-\nsure—“how efficiently does distribution Q encode P?”—while\nSJS\n(Equation 10) gives a symmetric, bounded diver-\ngence–to–similarity conversion. Either can be deployed de-\npending on whether directionality or symmetry is desired; both\nmap naturally into the [0, 1] range, facilitating thresholding and\ncomparative analysis of program structures.\nIII. EXPERIMENTS AND RESULTS\nTo\nevaluate\nthe\neffectiveness\nand\ninterpretability\nof\nboth\nentropy-based\nstability\nmetrics\nwe\nproposed,\n(1)\nJensen–Shannon Divergence (JSD) and (2) Structural Cross\nEntropy (SCE), we conducted experiments focusing on the\nstability of LLM-generated code. Specifically, we provided\neach prompt to the LLM five times independently, generating\nfive distinct outputs per prompt. We then measured stability\nby computing similarity or divergence scores pairwise among\nthese five outputs, using standard evaluation metrics (BLEU,\nCodeBLEU, TSED) and our proposed entropy-based metrics\n(JSD and SCE). The final stability score for each metric was\nobtained by averaging all pairwise comparisons.\nFor comparison, we also included pass@k, a metric eval-\nuating functional correctness, computed by executing each\ngenerated code output against provided test cases. Unlike\nother metrics, pass@k directly reflects correctness rather than\nstability but serves as an important baseline for understanding\nText to SQL\nPython Code Generation\nFig. 1.\nHeatmap of Pearson Correlation Coefficient between Stability\nEvaluation Metrics, JSD: Jensen-Shannon Divergence; SCE: Structural Cross\nEntropy, Structural means value in subtree is ignored.\npractical implications of structural stability. Additionally, we\nanalyzed structural-only variants – JSD (structural) and SCE\n(structural) – (cf. Section II.1(a)) by abstracting away token\nvalues, thus isolating structural differences to examine their\neffect on stability.\nA. Experimental Setup\nWe performed our evaluations using two widely recognized\nbenchmarks across two programming languages:\n• Python (BigCodeBench): A dataset widely adopted for\ngeneral code generation tasks [18].\n• SQL (Spider): A comprehensive benchmark for SQL\nquery generation [19].\nWe evaluated three representative models: LLaMA 3.1 (8B,\ninstruct), Qwen-2.5(7B, instruct), and Qwen-2.5-Coder(7B).\nOur analysis focuses on understanding how our entropy-based\nmeasures reflect code stability and their correlations with\nexisting metrics.\nB. Quantitative Results\nThe results are summarized in Table II. For the Python tasks,\nwhile the Qwen-2.5 models exhibit stronger stability in lexical\nmetrics such as BLEU and CodeBLEU compared to LLaMA\n3.1, a notable discrepancy between their pass@1 and pass@5\nindicates considerable variability. Specifically, Qwen-2.5-Coder\nachieves pass@1 of 0.373 and pass@5 of 0.517, suggesting\n3\n\nTABLE II\nCOMPARISON OF CURRENT STABILITY METRICS. JSD: JENSEN-SHANNON DIVERGENCE; SCE: STRUCTURAL CROSS ENTROPY; STRUCTURAL MEANS ONLY\nCOMPARES THE SUBTREE STRUCTURE.\nModel\nLanguage(Task)\nAvg. BLEU\nCode BLEU\nPass@1\nPass@5\nTSED\nSCE(structural)\nSCE\nJSD(structural)\nJSD\nLLaMA-3.1 8B it\nPython\nBigCodeBench\n0.428\n0.669\n0.289\n0.481\n0.785\n0.798\n0.656\n0.940\n0.898\nQwen-2.5 7B it\n0.596\n0.716\n0.339\n0.482\n0.765\n0.823\n0.726\n0.947\n0.913\nQwen-2.5-Coder 7B\n0.614\n0.715\n0.373\n0.517\n0.764\n0.823\n0.722\n0.951\n0.918\nLLaMA-3.1 8B it\nSQL\nSpider\n0.495\nN/A\n0.673\n0.824\n0.832\n0.729\n0.669\n0.934\n0.905\nQwen-2.5 7B it\n0.770\nN/A\n0.716\n0.790\n0.921\n0.886\n0.850\n0.971\n0.958\nQwen-2.5-Coder 7B\n0.664\nN/A\n0.787\n0.860\n0.927\n0.822\n0.781\n0.962\n0.946\nthat multiple sampled outputs differ substantially in correctness,\nreflecting inherent instability.\nFor SQL (Spider), this instability is also evident. Although\nQwen models achieve higher overall correctness and stability,\nthey demonstrate substantial differences between pass@1 and\npass@5 (e.g., Qwen-2.5-Coder: pass@1 = 0.787, pass@5 =\n0.860), further highlighting output variability.\nOur proposed entropy-based stability metrics (JSD and SCE)\ncomplement these findings. JSD values remain consistently high\n(above 0.9), suggesting that structurally, outputs are broadly\nsimilar across samplings. However, SCE scores (especially\nwhen considering token values explicitly) are consistently lower,\nindicating sensitivity to subtle variations overlooked by purely\nstructural metrics. This contrast highlights the utility of SCE\nin detecting fine-grained token-level variability.\nMoreover, the structural-only variants of both metrics gener-\nally produce higher scores, as expected, reflecting their insensi-\ntivity to identifier or literal changes. Thus, these structural-only\nscores represent a baseline stability that isolates deeper syntactic\npatterns from superficial token-level differences.\nOverall, our metrics successfully quantify and clarify the\nnature of stability in generated code, complementing traditional\ncorrectness-based metrics like pass@k, which indirectly reflect\nvariability through the disparity between single- and multi-\nsample correctness evaluations.\nC. Correlation Analysis\nFigure 1 illustrates the Pearson correlation coefficient across\nstability metrics:\na) Python (BigCodeBench): We observe high correlations\nbetween JSD and JSD(structural) (0.94), indicating structural\nabstraction maintains strong stability signals. The correlation\nbetween SCE and JSD metrics is moderately strong (0.85-0.87),\nthough lower than the near-perfect correlation between JSD\nand its structural variant (0.94). TSED shows good correlation\nwith JSD (0.85–0.86), reinforcing its utility for capturing\nstructural differences. Notably, SCE structural variants correlate\nless strongly with TSED (0.74), highlighting differences in\nsensitivity to structural variations.\nb) SQL (Spider): On SQL tasks, the correlations between\nJSD and JSD(structural) are even stronger (0.98), underscoring\nthe robustness of structural abstraction in capturing stability.\nInterestingly, BLEU’s correlation is lower with JSD metrics\n(around 0.51), emphasizing the limitation of token-based\nmetrics in SQL’s structural context. CodeBLEU is not valid\nfor SQL, hence excluded. TSED correlates strongly with\nJSD (0.94), confirming their structural similarity evaluation\nalignment. The SCE metrics correlate moderately (0.72–0.88)\nwith others, again highlighting their distinctive sensitivity to\ntoken-level discrepancies.\nIV. THREATS TO VALIDITY\nSeveral threats may impact the validity of our findings.\nFirstly, our experiments were conducted on specific benchmarks\n(Python BigCodeBench and SQL Spider), potentially limiting\ngeneralizability across other languages and tasks. Second, the\nchosen depth parameter in subtree extraction might influence\nstability measures; different values could yield varied results.\nAdditionally, while our entropy-based metrics effectively\ncapture syntactic variability, they currently do not explicitly\naddress semantic equivalences or execution behaviors. Lastly,\nthe limited number of models evaluated and the constrained\ndataset size may affect the robustness and external validity of\nour conclusions. Future work will address these limitations by\nexpanding evaluations across diverse programming languages,\nincorporating broader semantic and behavioral analyses, and\nemploying larger-scale benchmarks.\nV. CONCLUSION\nIn this paper, we introduced entropy-based metrics—Jensen-\nShannon divergence (JSD) and structural cross-entropy\n(SCE)—to quantify the stability of LLM-generated code using\nAbstract Syntax Trees (ASTs). Our experiments demonstrate\nthat these metrics:\n1) Provide insights beyond functional correctness (pass@k)\nand lexical-syntactic metrics (BLEU, CodeBLEU).\n2) Correlate strongly with AST-based structural metrics (e.g.,\nTSED), effectively capturing structural stability.\n3) Offer significant advantages with structural-only abstrac-\ntion by reducing token-value noise.\nDistinguishing structural-only from full-subtree encodings\nhighlights deep syntactic stability versus token-specific varia-\ntions. These metrics are lightweight, language-agnostic, and\ngeneralizable, complementing existing stability assessments.\nFuture work will incorporate semantic relationships, such\nas data- and control-flow dependencies, to extend analysis to\nbehavioral stability. We also plan adaptive subtree-weighting\nschemes and benchmarking on larger, multi-module codebases\nto study long-range structural variability.\nACKNOWLEDGEMENT\nThe\nFNR\nfunded\nthis\nresearch\nunder\ngrants\nNCER22/IS/16570468/NCERFT.\n4"}
{"id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "authors": ["Adrian-Marius Dumitran", "Alexandra-Mihaela Danila", "Angela-Liliana Dumitran"], "source": "http://arxiv.org/abs/2508.14279v1", "page_count": 9, "text": "GRILE: A Benchmark for Grammar Reasoning and Explanation in\nRomanian LLMs\nAdrian-Marius Dumitran* Alexandra-Mihaela Danila* Angela-Liliana Dumitran\nUniversity of Bucharest Faculty of Mathematics and Computer Science\nAbstract\nLLMs (Large language models) have revolu-\ntionized NLP (Natural Language Processing),\nyet their pedagogical value for low-resource\nlanguages remains unclear. We present GRILE\n(Grammar Romanian Inference and Language\nExplanations) , the first open benchmark\nof 1,151 multiple-choice questions harvested\nfrom Romanian high-stakes exams (National\nEvaluation, Baccalaureate, university admis-\nsions). GRILE enables us to probe two com-\nplementary abilities of seven state-of-the-art\nmultilingual and Romanian-specific LLMs: (i)\nselecting the correct answer, and (ii) produc-\ning linguistically accurate explanations. While\nGemini 2.5 Pro reaches 83% accuracy, most\nopen-weight models stay below 65%, and 48%\nof their explanations contain factual or peda-\ngogical flaws according to expert review. A\ndetailed error analysis pinpoints systematic\nweaknesses in morphology and in applying\nthe latest DOOM3 orthographic norms.\nAll\ndata, code and a public web demo are released\nto catalyze future research. Our findings ex-\npose open challenges for trustworthy educa-\ntional NLP in low-resource settings and estab-\nlish GRILE as a new test-bed for controllable\nexplanation generation and evaluation.\n1\nIntroduction\nRecent\nadvancements\nin\nNLP,\nparticularly\nthrough LLMs, have unlocked new opportuni-\nties for intelligent computer-assisted language\nlearning.\nAlthough extensive benchmarks exist\nfor English and other high-resource languages\n(Hendrycks et al., 2021; OpenAI, 2024), their\nefficacy in low-resource contexts remains under-\nexplored—especially\nfor\ntasks\nthat\ndemand\nfine-grained grammatical reasoning.\nRomanian,\nspoken by roughly 24 million people world-\nwide, illustrates this gap:\nit is comparatively\nunderserved by both annotated corpora and NLP\n*Equal contribution.\nresearch. (P˘\nai¸\ns and Tufi¸\ns, 2023) mention that the\nnumber of identified NLP tools for Romanian\nrepresent only 15% of the tools available for\nEnglish.\nThis paper bridges that gap through an inter-\ndisciplinary collaboration between linguists and\ncomputer scientists.\nWe introduce GRILE, a\nnew benchmark of 1,151 single-answer multiple-\nchoice questions (MCQs) extracted from high-\nstakes Romanian examinations, including the Na-\ntional Evaluation, the Baccalaureate, and several\nuniversity entrance tests (e.g., Law School, Police\nAcademy).\nOur investigation pursues two principal goals.\nFirst, we quantitatively assess how accurately\nstate-of-the-art\nmultilingual\nand\nRomanian-\nspecific LLMs answer the questions in our\ndataset.\nSecond, we examine their pedagogical\npotential by requiring each model to justify its\nchoice with a concise grammatical explanation.\nThe evaluation covers commercial systems such as\nGemini 2.5 Pro Experimental and open-weight\nmodels such as DeepSeek V3-0324.\nInitial results reveal substantial variation, with\nanswer accuracy spanning from 38% to 85%,\nand chain-of-thought prompting generally yield-\ning higher scores.\nBeyond answer prediction,\nwe instruct Gemini 2.5 Pro Experimental to cat-\negorise every question as Lexical, Morpholog-\nical, Syntactic, or Phonetic.\nManual verifica-\ntion confirms more than 99% agreement, offer-\ning an additional lens for analysing error patterns.\nA qualitative evaluation of the generated expla-\nnations—conducted by a specialist in Romanian\ngrammar—highlights persistent issues in termino-\nlogical precision and normative compliance, un-\nderscoring the challenges of deploying LLMs as\ntrustworthy tutors for low-resource languages.\nOur main contributions are:\n• GRILE, the first public Romanian grammar\n\nbenchmark paired with expert-validated ex-\nplanations;\n• a comprehensive evaluation of multilingual\nand Romanian-specific LLMs on both an-\nswering and explaining high-stakes grammar\nquestions;\n• a linguist-driven qualitative study that un-\ncovers systematic weaknesses and normative\nmisalignments;\n• an open-access dataset and interactive web\napplication, released under a permissive li-\ncence on Zenodo1, to catalyse future research\nand educational use.\nThese resources and findings advance the study\nof educational NLP in low-resource settings and\nlay the groundwork for more reliable, Romanian-\nfocused grammatical tutoring and assessment\ntools.\n2\nRelated Work\nBenchmarking\nLLMs\non\nMultiple-Choice\nGrammar Tasks.\nRecent studies have shown\nthat large language models (LLMs) can achieve\nimpressive results on standardized multiple-choice\ntests across various domains (Hendrycks et al.,\n2021; OpenAI, 2024).\nHowever, fine-grained\ngrammatical assessments remain challenging for\nLLMs.\nFor example, multilingual benchmarks\nfocusing on grammar,\nsuch as the TELEIA\n(Mayor-Rocher et al., 2024b) dataset for Span-\nish, demonstrate that LLMs struggle to reach\nnative-like accuracy in nuanced grammar tasks\n(Mayor-Rocher et al., 2024a).\nThis motivates\nspecialized evaluations of grammatical knowl-\nedge, especially for lower-resource languages like\nRomanian.\nMultilingual vs.\nLanguage-Specific LLMs.\nMost large language models are predominantly\ntrained on English data, which results in signifi-\ncantly better performance for English compared to\nother languages (Hu et al., 2020). Consequently,\nthis has encouraged the development of monolin-\ngual or language-specific NLP models that often\noutperform multilingual counterparts on local lin-\nguistic tasks. For instance, Masala et al. (2020)\nintroduced RoBERT, a Romanian-specific BERT-\nbased model, demonstrating superior results over\n1https://zenodo.org/records/15504323\nmultilingual BERT across various Romanian NLP\ntasks such as sentiment analysis, dialect identifica-\ntion, and diacritic restoration.\nSimilar outcomes have been consistently re-\nported for other languages, including French,\nPolish, and Japanese, where monolingual trans-\nformer models regularly surpass general multi-\nlingual baselines in language-specific evaluations\n(Martin et al., 2020; Kuratov and Arkhipov, 2019;\nConneau et al., 2020). More recently, the NLP\ncommunity has seen efforts toward developing\nlarge-scale LLMs tailored specifically for individ-\nual languages, such as Finnish GPT-style models\n(Luukkonen et al., 2023) and Chinese LLaMA2-\nbased models (Cui et al., 2024), achieving state-\nof-the-art performance on localized benchmarks.\nIn\nthe\nRomanian\ncontext,\nMasala\net\nal.\n(2024) released RoLLaMA2, the first open-source\nRomanian-specific large language model based\non LLaMA-2, trained on curated Romanian cor-\npora and fine-tuned through instruction tuning.\nRoLLaMA2 achieved leading performance across\nmultiple Romanian NLP benchmarks, underscor-\ning the effectiveness of language-specific train-\ning. Our study contributes to this growing body\nof literature by directly comparing multilingual\nmodels against Romanian-specific models like\nRoLLaMA2, focusing particularly on grammati-\ncal tasks to evaluate the effectiveness and viability\nof smaller, targeted language models against more\nextensive multilingual counterparts.\nLLM Explanation Generation and Educational\nFeedback.\nGenerating explanations for educa-\ntional purposes extends beyond correctness, en-\ncompassing the quality and pedagogical sound-\nness of explanations. Prior works in educational\nNLP formalized feedback comment generation\ntasks, highlighting the need for meaningful auto-\nmated feedback (Nagata, 2019). Recently, gen-\nerative LLMs like GPT-4 demonstrated poten-\ntial in automatically generating high-quality ex-\nplanations in science education, closely matching\nhuman-generated feedback (López Cortez et al.,\n2024). Human-in-the-loop validation remains cru-\ncial to ensure explanation accuracy and educa-\ntional value.\nOur work builds upon these directions, bench-\nmarking Romanian-specific and general LLMs\nagainst a Romanian grammar multiple-choice\ndataset, examining common errors, and assess-\ning explanation quality through expert validation.\n\nFigure 1: Dataset Sources\nThis contributes toward understanding LLM capa-\nbilities and limitations within educational contexts\nfor low-resource languages.\n3\nDataset and Methodology\nIn this section we explain how we collect the data\nand the models we decided to benchmark in our\ntest.\n3.1\nData Collection and Structure\nFigure 1 charts the distribution of our 1 151 single-\nanswer multiple-choice questions (MCQs) were\nharvested from publicly available high-stakes\nRomanian examinations—the National Evalu-\nation, Baccalaureate, and university entrance\ntests for Law School and the Police Academy\n(2010–2024). Source PDFs and scans were con-\nverted with Tesseract OCR, then parsed by rule-\nbased scripts and manually spot-checked. Each\nrecord is stored as JSON with fields question,\noptions, answer, source, and year; under-\nlined focus words are preserved via surrounding\nunderscores. Full OCR pipeline details and an ex-\nample entry appear in Appendix A.2\n2https://zenodo.org/records/15504323\n3.2\nLLM Selection\nOur model selection aimed to evaluate both a\nstate-of-the-art model known for strong reasoning\ncapabilities and several other prominent models\nreadily accessible for research purposes. Specifi-\ncally, we included Gemini 2.5 Pro Experimental\n(Team et al., 2024) as it represented the leading\nedge of publicly available models at the time of\nour experiments. The other benchmarked models,\nincluding variants of DeepSeek, Mistral, Llama\n3.3, and Qwen, were chosen primarily for their\nstrong performance in various benchmarks and\ntheir convenient accessibility via the Together AI\nAPI3, which facilitated systematic evaluation.\n• DeepSeek V3-0324 (DeepSeek-AI et al.,\n2025)\n• Mistral Small 24B\n• Llama 3.3 70B Instruct (Grattafiori et al.,\n2024)\n• Qwen 2.5 Coder 32B (Yang et al., 2025)\n• Gemini 2.5 Pro Experimental, Gemini 2.0\nFlash (Team et al., 2024)\nFurthermore, our few-shot prompting experi-\nments (Section 4.2) involved evaluating multi-\nple fine-tuned Romanian variants (e.g., RoL-\nlama, RoMistral, RoGemma based on OpenLLM-\nRo (Masala et al., 2024)) alongside their respective\nbase models to analyze the interaction between\nfine-tuning and in-context learning. The full list\nof models and variants tested in the few-shot set-\nting is provided in Appendix B.\n3.3\nPrompting Strategies\nWe evaluated LLM performance using two pri-\nmary prompting approaches applied to each MCQ\nin the dataset:\n3.3.1\nDirect Questioning (Zero-Shot)\nThis baseline approach involved providing the\nmodel directly with the question text and multiple-\nchoice options, instructing it to select the correct\nanswer. The prompt template was:\nQuestion: [Question text here]\nChoices:\n(A) [Choice A text]\n(B) [Choice B text]\n(C) [Choice C text]\n3https://www.together.ai/\n\n(D) [Choice D text, if applicable]\nSelect the correct answer. Ensure that\nthe final answer is only the letter\nof the correct option, without any\nadditional text or symbols.\nThe final instruction was added to minimize pars-\ning errors during evaluation.\n3.3.2\nChain-of-Thought (CoT) Prompting\nTo encourage step-by-step reasoning and generate\nexplanations, we employed a CoT approach. The\ncore CoT instruction added was:\nLet’s think step by step.\nProvide your reasoning in\nseveral steps, and then output\nthe final answer on a new line\nstarting with \"Final Answer:\".\nEnsure that the final answer is\nonly the letter of the correct\noption, without any additional\ntext or symbols.\nThis strategy aimed to improve answer accuracy\nby forcing a reasoning process and simultaneously\ngenerating explanatory text for qualitative analy-\nsis.\n3.3.3\nFew-Shot Prompting\nTo assess the impact of in-context examples, we\nalso evaluated models using few-shot prompting.\nThis involved prepending the Direct Questioning\nor CoT prompt (as applicable) with 1, 3, or 5\nrandomly selected question-answer pairs from the\ndataset (excluding the question being tested) for-\nmatted similarly to the main task.\n3.4\nExplanation Generation and Validation\nSetup\nTo facilitate efficient linguistic expert validation\nof both the answers and reasoning of the best-\nperforming model identified in preliminary tests\n(Gemini 2.5 Pro Experimental), we used a simple,\ndirect prompt asking it to provide its chosen an-\nswer, the correct answer (based on its knowledge),\nits reasoning, and a category classification. This\ngenerated a structured output specifically format-\nted for review, exemplified below:\nQuestion 10: Din campul lexical al cuvantului \"\nelectorat\" fac parte:\nOptions:\na) electron, neutron, proton\nb) electric, energetic, curent\nc) senator, primar, magistrat\nd) alegeri, vot, candidat\nGemini’s Answer: d\nCorrect Answer: d\nExplanation: Campul lexical include cuvinte\ninrudite ca sens. \"Electorat\" se refera la\ntotalitatea alegatorilor, fiind direct legat de\n\"alegeri\", \"vot\", \"candidat\".\nCategory: Lexical\nListing 1: Example structured output from Gemini 2.5\nPro for validation.\nThis structured format, including the category\nclassification (Lexical, Morphological, Syntactic,\nPhonetic) assigned by Gemini 2.5 Pro Experimen-\ntal, was used by the linguist expert to validate the\nmodel’s answer, reasoning quality, and classifica-\ntion accuracy (which exceeded 99% for the cate-\ngorization task itself).\n3.5\nEvaluation Metrics\nPerformance was primarily measured by answer\naccuracy. Additionally, a qualitative linguistic\nanalysis was conducted on the explanations gen-\nerated via CoT (facilitated by the structured output\nfrom Gemini 2.5 Pro for validation) to assess their\ncorrectness, precision, and pedagogical suitability.\n4\nQuantitative Results\nThis section presents quantitative findings from\nour benchmarking evaluations.\nWe analyze the\nimpact of different prompting strategies (Chain-\nof-Thought vs.\nDirect, Few-Shot vs.\nZero-\nShot) and compare the performance of Romanian-\nspecific fine-tuned models against their multilin-\ngual base versions.\n4.1\nImpact of Chain-of-Thought Prompting\nWe first compared model accuracy using direct\nzero-shot questioning versus Chain-of-Thought\n(CoT) prompting, which instructs the model to\nprovide reasoning steps. Table 1 summarizes these\nresults.\nModel\nAccuracy\nwithout\nCoT\nAccuracy\nwith CoT\nDeepSeek V3.0324\n53.26%\n64.62%\nMistral small 24B\n39.10%\n38.31%\nLlama 3.3 70B\n49.70%\n55.60%\nQwen 2.5 Coder 32B\n48.22%\n51.61%\nGemini 2.5 Pro\n—\n82.8%\nTable 1: Comparison of model accuracy (%) with and\nwithout Chain-of-Thought (CoT) prompting. Top score\nin each column is bolded. Gemini 2.5 Pro was evalu-\nated only with CoT.\nCoT prompting generally led to notable accu-\nracy improvements, particularly for DeepSeek\n\nV3 (+11.36 pp) and Llama 3.3 70B (+5.9\npp), confirming its benefit for reasoning on this\ntask. An exception was Mistral Small 24B,\nwhich saw a slight decrease, possibly due to diffi-\nculties following complex instructions. Gemini\n2.5 Pro, tested only with CoT, achieved the\nhighest score (82.8%), significantly outperform-\ning others. However, accuracies for most models\n(38-65% range with CoT) remain substantially be-\nlow proficient human levels (>80%) on compara-\nble exams4, highlighting a remaining gap in nu-\nanced Romanian grammar understanding for most\nLLMs.\n4.2\nImpact of Few-Shot Prompting\nWe also investigated whether providing in-context\nexamples (1, 3, or 5 shots) could improve perfor-\nmance compared to zero-shot prompting. The re-\nsults, detailed fully in Appendix B , show mixed\nand generally modest benefits. While accuracy\noften increased slightly, gains were typically small\n(2-5 pp) and inconsistent across models and shot\ncounts. Table 2 illustrates this variability for the\nLlama 3.1 8B family.\nLlama 3.1 8B Variant\nFS=0\nFS=1\nFS=3\nFS=5\nBase Instruct\n35.62\n38.84\n40.75\n39.70\nRo-Instruct (25-04)\n36.06\n37.71\n40.49\n39.36\nRo-Instruct-DPO (25-04)\n38.49\n37.53\n39.53\n39.27\nTable 2: Illustrative accuracy (%) for Llama 3.1 8B\nvariants with Few-Shot (FS) prompting. Best few-shot\nscore per model in bold. Full results in Appendix B .\nFew-shot prompting did not consistently fa-\nvor base models over fine-tuned ones, nor did a\nclear optimal number of shots emerge. Crucially,\neven with examples, peak accuracies for the mod-\nels tested in this setting (detailed in Appendix\nB) remained significantly below top performers\nlike Gemini 2.5 Pro and human levels. This sug-\ngests limited in-context examples are insufficient\nto overcome the core challenges of this task for\nthese models.\n4.3\nPerformance of Romanian Fine-tuned\nModels vs. Base Models\nA key goal was to assess the impact of Romanian-\nspecific fine-tuning.\nWe compared the perfor-\nmance of various RoLLM variants (developed by\n4Example\nresults:\nhttps://cdn.edupedu.\nro/wp-content/uploads/2023/01/lista_\nanonimizata_rezultate_finale_LC.pdf\nMasala et al. (2024)) against their respective base\nmultilingual models using data from our few-shot\nexperiments (details in Appendix B).\nThe results indicate that Romanian-specific\nfine-tuning does not consistently yield signifi-\ncant improvements on this specific grammatical\nMCQ task, especially compared to strong base\nmodels.\n• For the Llama family (Llama-2, Llama-\n3, Llama-3.1), the RoLLM variants gener-\nally performed similarly to or sometimes\nslightly worse than their corresponding base\nInstruct models in zero-shot settings. Few-\nshot prompting occasionally led to RoLLM\nvariants slightly outperforming their base, but\nthe difference was often marginal. DPO fine-\ntuning on RoLLM variants did not produce\nconsistent gains over standard RoLLM in-\nstruction tuning for this task.\n• For\nMistral-7B,\nthe\nRoMistral\nvariants\nshowed slightly better average performance\nthan the base Mistral-7B Instruct v0.3, but the\ndifference was small (within 1 pp on average\nacross FS settings).\n• For\nGemma\nmodels,\nthe\nresults\nwere\nparticularly\nstriking.\nThe\nbase\ngoogle/gemma-2-9b-it\nsignificantly\noutperformed all its RoGemma2-9B fine-\ntuned variants (including DPO versions)\nacross all few-shot settings, achieving an\naverage accuracy of 47.13% compared to\naverages around 39-42% for the RoGemma2\nvariants. A similar, though less pronounced,\ntrend was observed for Gemma-1.1-7b.\nThese findings suggest that for complex gram-\nmatical reasoning tasks like those in our dataset,\nthe benefits of general multilingual pre-training\nin strong base models (like Llama 3.1 Instruct\nor Gemma-2-9B-it) might outweigh the current\nRomanian-specific fine-tuning approaches applied\nin the RoLLM project, at least for zero-shot and\nfew-shot settings. The fine-tuning might be more\nbeneficial for other NLP tasks or require different\nstrategies (e.g., incorporating grammatical knowl-\nedge more explicitly) to show significant gains on\nthis benchmark. The relatively low overall scores\nfor both base and fine-tuned models (excluding\nGemini) further emphasize the difficulty of this\ntask.\n\n5\nQualitative Analysis: Explanations,\nCategorization, and Dataset\nChallenges\nBeyond\nquantitative\naccuracy,\nunderstanding\nLLM capabilities for educational use requires\nqualitative assessment. This section presents find-\nings from an in-depth analysis focusing primarily\non explanations and category classifications gen-\nerated by Gemini 2.5 Pro Experimental (using\nCoT prompting). The analysis, conducted by a\nlinguistic expert, evaluated 200 explanations in\ndetail and reviewed LLM category assignments\nacross the full dataset (1151 items). This revealed\ninsights into LLM reasoning, limitations in han-\ndling linguistic nuances, and characteristics of the\nbenchmark dataset itself.\nThe involvement of a linguistic expert was es-\nsential to the present study due to the complex\nand nuanced nature of the materials under anal-\nysis. The apparent uniformity of item structure\ndid not preclude the presence of subtle or highly\ncontext-dependent linguistic phenomena. Certain\nerrors identified in the dataset were marked by fine\nsemantic or syntactic distinctions that would likely\nelude detection without expert philological judg-\nment, such as putting the error into the correct lan-\nguage category (see questions 51, 77, 211, 228,\netc. where the lexical errors are considered mor-\nphological and the opposite) or presenting the ex-\nplanation in an inadequate style or erroneously.\n5.1\nQuality of LLM-Generated Explanations\nWhile CoT prompting improved answer accuracy\n(Table 1), the quality of the accompanying expla-\nnations varied significantly. A detailed review of\n200 explanations revealed that 48% (96 explana-\ntions) were problematic (incomplete, incorrect,\nmisleading, or containing flaws). Key issues in-\ncluded:\n• Incorrect Justifications (67/96): Explana-\ntions frequently provided factually incorrect\ngrammatical reasoning, even when the se-\nlected MCQ answer was correct.\n• Imprecision and Irrelevance (29/96): Ex-\nplanations often suffered from redundancy,\nimprecise terminology, irrelevant points, or\nincoherent concluding statements that under-\nmined the reasoning (e.g., the concluding\nphrase for Question 26 regarding pronoun\nagreement).\nThis high error rate in explanations limits the re-\nliability of current LLMs as standalone grammar\ntutors without expert validation.\n5.2\nLLM Performance on Linguistic\nCategorization\nWe also evaluated Gemini 2.5 Pro Experimental’s\nability to classify each of the 1151 questions into\nfour linguistic domains: Lexical, Morphological,\nSyntactic, or Phonetic. While the overall accuracy\non this task was very high (over 99% agreement\nwith expert classification), the five instances of\nmisclassification are revealing:\n• Morphology vs. Lexicon: Question 51 (ab-\nbreviation of dumneavoastr˘\na) was misclas-\nsified as \"Lexical\" instead of \"Morphologi-\ncal\". Question 77 (use of optim creating a\npleonasm) was misclassified as \"Morpholog-\nical\" instead of \"Lexical-Semantic\".\n• Syntax vs. Morphology: Question 211 (mor-\nphological value of pronoun ce) and Question\n228 (identifying an interjection) were mis-\nclassified as \"Syntactic\". Question 252 (fo-\ncusing on the preposition dup˘\na) was also mis-\nclassified as \"Syntactic\" instead of \"Morpho-\nlogical\".\nThese errors typically occurred in borderline cases\ninvolving multiple linguistic layers (form, func-\ntion, meaning), suggesting LLMs may struggle\nto pinpoint the primary linguistic concept being\ntested beyond surface cues.\n5.3\nDataset Characteristics and Normative\nAlignment\nThe qualitative analysis also highlighted charac-\nteristics and potential issues within the source ma-\nterials:\n• Unclear/Flawed Prompts: Seven questions\nout of the 200 deeply analyzed contained am-\nbiguities or errors affecting comprehensibil-\nity and reliable evaluation.\n• Incorrect/Incomplete\nOfficial\nAnswers:\nFour analyzed items had designated correct\nanswers conflicting with current standards.\n\n• DOOM 3 Misalignment:\n5This was a no-\ntable issue.\nAnalysis of the 19 DOOM-\nrelated questions in the dataset revealed:\n– LLM Errors:\nIn six explanations for\nDOOM-related questions, Gemini 2.5\nPro provided false or incorrect informa-\ntion regarding DOOM 3 (Institutul de\nLingvistic˘\na “Iorgu Iordan – Alexandru\nRosetti”, 2021) norms.\nExamples in-\nclude incorrectly stating DOOM 3 \"ac-\ncepts\" both forms for cofeina/cafeina\n(Q626) or biscuit/biscuite (Q1128), mis-\nstating the preferred form for baroc˘\na\n(Q733),\ninaccurately\ndescribing\nthe\nabbreviation rule for dumneavoastr˘\na\n(Q51), or misrepresenting the recom-\nmendation for cuvânt-înainte (Q896).\n– Outdated Source Questions:\nSome\nsource questions themselves appeared\noutdated relative to DOOM 3 (2021 edi-\ntion). For example, Question 144 (plural\nof proroc) lacked the standard DOOM\n3 form among its options, suggesting\nthe question predates the latest norma-\ntive update.\nThese findings underscore the need for careful\ncuration and normative alignment of benchmark\ndatasets, especially when evaluating performance\non rule-based linguistic tasks.\n5.4\nImplications\nThe qualitative results emphasize the need for\nevaluation metrics beyond accuracy, particularly\nwhen assessing LLMs for educational roles re-\nquiring explanations. The inconsistencies found\nhighlight the importance of dataset quality con-\ntrol and alignment with current linguistic stan-\ndards (like DOOM 3) for reliable benchmarking\nin high-stakes domains like language assessment.\n5DOOM3, published in 2021, serves as the current stan-\ndard reference for Romanian spelling (ortografic), pronun-\nciation (ortoepic), and morphology (morfologic). While the\ncore orthographic, ortoepic, and morphological norms largely\nremain unchanged, DOOM3 introduces over 3,000 discrete\nnormative updates. These include the addition or removal\nof definite/article forms, select flexional variants, and ad-\njustments to accentuation in compound words. It intention-\nally curtails the proliferation of orthographic variants, helping\nusers to clearly identify the recommended form. It represents\nthe official norms, reason why explanations such as “DOOM\n3 prefers” or “recommends” from questions 733 or 896 are\nnot adequate.\n6\nApplication\nIn order to explore the applicability of the cre-\nated dataset and the generated explanations, we\ndeveloped a publicly accessible web application\ndesigned as a learning tool for Romanian language\nproficiency.\nFigure 2: User interface of the web application, allow-\ning practice with dataset questions.\nThe application utilizes the benchmark dataset\nas its question bank.\nUsers can navigate ques-\ntions filtered by source (allowing targeted prac-\ntice based on exam type/difficulty proxy) and sub-\nmit their answers within a timed or practice mode.\nUpon submission, the application reveals the cor-\nrect answer and, importantly, displays the corre-\nsponding LLM-generated grammatical expla-\nnation (specifically, those generated by Gemini\n2.5 Pro and validated for quality by our linguistic\nexpert, as discussed in Section 5).\nFigure 3: User interface of the web application, dis-\nplaying explanations for the correct answers.\nThis feature allows learners to not only test their\n\nknowledge but also receive immediate feedback\nand reasoning, leveraging the LLM’s explanatory\ncapabilities in a practical educational context. Al-\nthough explanation quality still requires ongoing\nvalidation, providing expert-vetted explanations\noffers significantly more pedagogical value than\nsimple answer keys.\nThe application is imple-\nmented in React and hosted statically, ensuring\neasy access and deployment.\n7\nConclusion and Future Work\nThis study evaluated state-of-the-art LLMs on\na novel dataset of Romanian language MCQs\nsourced from high-stakes exams, focusing on an-\nswer accuracy and explanation quality. Our bench-\nmarking, utilizing direct and Chain-of-Thought\n(CoT) prompting,\nrevealed significant perfor-\nmance variations and a considerable gap between\nmost LLMs and proficient human performance,\nalthough Gemini 2.5 Pro Experimental\nachieved high accuracy ( 83%) using CoT.\nQualitative analysis of Gemini 2.5 Pro’s expla-\nnations, while revealing flaws in nearly half, also\nfound that the majority were linguistically valu-\nable and useful as a starting point for pedagogical\nfeedback, confirmed by expert assessment. This\npotential was demonstrated through their integra-\ntion into our publicly available educational web\napplication, providing learners with practice op-\nportunities augmented by AI-generated, expert-\nvalidated grammatical reasoning.\nHowever, the\nanalysis also highlighted challenges related to\ndataset quality and normative alignment (e.g., with\nDOOM 3).\nOur findings underscore the need for both quan-\ntitative and qualitative metrics when evaluating\nLLMs for educational roles, particularly concern-\ning explanation generation in low-resource lan-\nguages.\nWhile current LLMs show promise as\nassistive tools, significant work remains to ensure\nconsistent accuracy and pedagogical soundness.\nFuture work should proceed along several\nkey directions.\nFirstly, expanding the bench-\nmark scope is essential: incorporating more ques-\ntions, annotations (like difficulty levels or specific\ngrammar subsections), evaluating a broader range\nof SOTA models (including newer Romanian-\nspecific ones), and performing detailed cross-\nlingual analyses. Secondly, significant potential\nlies in enhancing model performance and expla-\nnation quality through advanced techniques. This\nincludes exploring Retrieval-Augmented Gener-\nation (RAG) leveraging Romanian language man-\nuals or grammar resources as external knowledge,\ntargeted fine-tuning using a subset of the dataset\nquestions, and experimenting with more sophisti-\ncated prompting strategies such as few-shot learn-\ning to potentially create more effective AI tutors.\nFinally, addressing the identified dataset limita-\ntions through continued curation and normative\nalignment remains vital for building truly robust\nevaluation benchmarks for Romanian educational\nNLP.\n8\nLimitations\nOur dataset of 1 151 Romanian multiple-choice\nquestions (MCQs) is smaller than the large\nbenchmarks available for high-resource languages\nand represents mainly high-stakes exam material.\nItems were extracted with Tesseract OCR and then\nspot-checked, yet some annotation noise may re-\nmain. Question categorisation relied on an LLM-\nassisted workflow that, despite manual review, can\nstill inherit model errors.\nWhile we have categorized the questions when\nanswering them and obtained the following split\nsyntactic 382 questions, morphological 364, lex-\nical 319 and phonetic 86, we have not annotated\nthe initial data with this information.\nMethodologically, the study is restricted to\nMCQs; open-ended generation and deeper dis-\ncourse skills are outside our present scope.\n9\nEthical Considerations\nThis research uses publicly available Romanian\nmultiple-choice exam questions; all sources are\nfree of licensing or privacy constraints. We re-\nlease the dataset on Hugging Face to foster trans-\nparency, reproducibility, and further work on low-\nresource educational NLP. The free web applica-\ntion stores no PII. Dataset content mirrors the fo-\ncus of the original exams. LLM-generated expla-\nnations were validated by a linguist to prevent mis-\ninformation. We acknowledge possible training-\ndata biases and welcome community scrutiny;\nusers should respect original copyrights."}
{"id": "2508.14275v1", "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "date": "2025-08-19", "authors": ["Cliff O'Reilly", "Ernesto Jimenez-Ruiz", "Tillman Weyde"], "source": "http://arxiv.org/abs/2508.14275v1", "page_count": 9, "text": "Disentangling concept semantics via multilingual averaging\nin Sparse Autoencoders\nCliff O’Reilly\nDepartment of Computer Science\nCity St George’s, University of London\ncliff.oreilly@citystgeorges.ac.uk\nErnesto Jim´\nenez-Ruiz\nDepartment of Computer Science\nCity St George’s, University of London\nernesto.jimenez-ruiz@citystgeorges.ac.uk\nTillman Weyde\nDepartment of Computer Science\nCity St George’s, University of London\nt.e.weyde@citystgeorges.ac.uk\nAbstract\nConnecting LLMs with formal knowledge representation and reasoning is a promising ap-\nproach to address their shortcomings. Embeddings and sparse autoencoders are widely used to\nrepresent textual content, but the semantics are entangled with syntactic and language-specific\ninformation. We propose a method that isolates concept semantics in Large Langue Models by\naveraging concept activations derived via Sparse Autoencoders. We create English text repre-\nsentations from OWL ontology classes, translate the English into French and Chinese and then\npass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite\nof Sparse Autoencoders, we obtain concept activations for each class and language version. We\naverage the different language activations to derive a conceptual average. We then correlate the\nconceptual averages with a ground truth mapping between ontology classes. Our results give\na strong indication that the conceptual average aligns to the true relationship between classes\nwhen compared with a single language by itself. The result hints at a new technique which\nenables mechanistic interpretation of internal network states with higher accuracy.\n1\nIntroduction\nThe combination of LLMs with formal reasoning and knowledge representation has become a topic of increased\ninterest recently. While the improvements of LLMs have enabled new applications of machine learning and\nartificial intelligence, LLMs have weaknesses, such as hallucinations and reasoning errors. The combination of\nLLMs with formal knowledge representation, such as ontologies, has the potential to address these problems but\nit requires bridging the gap between text and formal semantics. Sparse autoencoders can help model semantic\ncontent in an interpretable way by disentangling components in text embeddings. However, embeddings contain\nnot only information on concept semantics, but also syntactic and language-specific aspects. In this work, we\npropose the use of multi-lingual aggregation to extract concept semantics and evaluate this method in ontology\nalignment tasks.\nOur question is: can multi-lingual representations be used to isolate semantic aspects in sparse embedding vec-\ntors. This will help both mechanistic interpretability, but also practical tasks like ontology alignment, and more\ngenerally the integration between LLMs and formal sematic systems.\n1\n\nIn our method, we parse OWL ontology classes into text representations and use these as prompts for an LLM\nwith Sparse Autoencoders (SAEs), which produces concept activations. We use existing class similarity mapping\nfrom the ontology suite1 as ground truth, and compare the similarity of concept activations with the ground truth.\nWith single language prompts in English, concept activations are noisy, in that they show low correlation to the\nground truth. To address this, we perform a natural language translation to produce multiple concept activation\nvectors. The average of these vectors (which we term the conceptual average) shows a clearly stronger correlation\nto ground truth. We interpret this as evidence for syntactic and language-specific information being suppressed\nby the multi-lingual approach.\nThe rest of this paper is organised as follows: In section 2 we present the context of our idea and related work, in\nsection 3, we introduce our method followed by results, in section 4, and a discussion in section 5.\n2\nRelated Work\nAn ontology in general terms can be thought of as a formal representation of a domain of knowledge. Any\nrepresentation has to be subjective and, in practice, ontologies tend to be bespoke to a domain or application.\n2.1\nOntology Parsing\nOWL2 ontologies are a standard, machine-readable and flexible format for representing any domain. Due to\nthe subjective nature of semantic representation, our goal of creating a text prompt from an OWL ontology is\nalso subjective. The extraction of OWL classes, properties and relationships can be performed with libraries for\nvarious programming languages (we used OWLAPI [1]), and tools have been created to generalise text extrac-\ntion, e.g. NaturalOWL [2] and OWL Verbalizer [3]. The recursive concept verbaliser approach for ontology\nsubsumption inference [4] presents a toolbox for OWL ontology analysis (OntoLAMA).\n2.2\nOntology Alignment\nThe challenge of matching concepts between ontology representations is as old as the representations themselves.\nSince 2004, the Ontology Alignment Evaluation Initiative3 has provided a framework for evaluating various\napproaches. From straightforward lexical approaches, through structural and semantic techniques to more recent\ninnovations with machine learning [5] (and a multitude of hybrid methods ([6]; [7]; [8])), we believe our research\nis novel in approaching the problem with analyses of LLM internal concept states.\n2.3\nMechanistic Interpretability and Sparse Autoencoders\nMechanistic Interpretability (MI) is a domain which aims to interpret the internal activation states of neural\nnetworks for various purposes such as AI safety, neural network decision-making and improving network design\n[9]. Our interest in MI is for the learned concept activations — the correlation between conceptual semantics and\nnode activations of LLMs.\nWhen applied to language models, Sparse Autoencoders are unsupervised algorithms that learn to map from\nlatent representations to interpretable concepts (also called features). An SAE is a pair of encoder and decoder\nfunctions that compresses an input into a hidden representation and tries to reconstruct the input from the hidden\nrepresentation — thereby learning a set of activation features which can be correlated via techniques such as\nDictionary Learning [10] to a vocabulary of human understandable concepts. The sparsity controls that are\napplied during training result in a reduced set of activations that are more easily computed (compared with\nbillions of activations in a full LLM).\nGemma Scope [11] is an open suite of SAEs trained on Google’s Gemma 2 LLM — at every layer and sublayer.\n1https://oaei.ontologymatching.org/2024/conference/index.html\n2https://www.w3.org/OWL/\n3https://oaei.ontologymatching.org/\n2\n\n3\nMethod\nThe corpus used in this experiment comes from the conference track of the Ontology Alignment Evaluation\nInitiative 2024. Across the 16 ontologies, there are 867 class definitions and a set of 174 reference class mappings\nas ground truth alignments. The alignment mappings do not cover all the 16 ontologies so we cut down the\nanalysis to only those ontologies with a target mapping available.\nThe method we apply can be broken down into stages, described below.\n1. We use the Java library OWLAPI4 to parse each owl ontology file. Due to the nature of owl representations,\neach ontology can take a different format and so the Java script has bespoke logic that extracts classes, any\nrelated subclasses, superclasses, object properties and data properties. Some manipulation of the representation\nis needed. There are two styles of output we create: a summary and a verbose version — summary and verbose\nexamples for the class Author (from the edas ontology) are shown in quotes (A) and (B), below. The verbose\noutput is a text string which encapsulates a description of the class and includes connecting and descriptive\nwords, but the summary version is a concatenation of the target class name, associated class names and object\nproperties.\n(A) Author is a SuperClassOf Presenter and hasRelatedPaper Paper\n(B) Author is a SubClassOf some writes Contribution and is a SubClassOf Person and is a\nSubClassOf only writes Contribution and writes Contribution\n2. We use the googletrans Python library5 to perform a natural language translation from English to French\nand English to Simplified Chinese. There are no parameters supplied to this process - it is a straightforward\ntranslation service. Examples of French and Chinese translations of summary and verbose representations are\nshown below:\n(C) Personne auteur uniquement Contribution Certaines ´\necritures Contribution Contribution\n(D) L’auteur est une sous-classe de contribution des ´\necritures et est une personne sous-classe\net est une sous-classe unique en r´\nedaction de contribution et ´\necrit la contribution\n(E) 作者有些人写贡献只写贡献\n(F) 作者是一个子类人，是一个仅写作贡献的子阶级，并且是某些撰写贡献的子类\n别，并写下了贡献\n3. Using the huggingface library6 [12] to access the Gemma Scope open suite of sparse autoencoders, we process\neach text representation as a prompt to a PyTorch neural network (using a Jump ReLu activation function). The\nparticular SAE set used here is the 2 billion parameter model based on Google’s Gemma 2 Large Language\nModel. We take every layer (0 to 25) of the 16.4k width model and we take the L0 Norm variant for model\nregularisation (the number of non-zero elements in the activation vector) where the average is between 13 and 23\nactive features (e.g. 13 out of 16.4k on average). If L0 is set too high then features overlap and interpretability\nbreaks down. Set L0 too low and the network underfits and misses important structures. The output is a set\nof concepts and an activation weighting. An example tensor with 7 concept identifiers and activation values is\nshown in Table 1.\n4https://github.com/owlcs/owlapi\n5https://pypi.org/project/googletrans/\n6https://huggingface.co/google/gemma-scope\n3\n\nConcept ID\nActivation Weight\n2446\n57.0846\n3391\n47.3293\n3752\n37.2378\n5327\n79.6517\n6035\n70.1694\n6035\n71.6481\n7234\n36.0779\n8816\n46.0310\n9823\n57.1111\n10144\n43.9628\n12529\n49.0565\n14829\n61.3937\nTable 1: Example concept activation set 1 (for the verbose English edas-Author class)\n4. The same process as in (3) is repeated for the translated (French and Chinese) texts.\n5. For each class representation, the English and the translated concept activation sets are averaged. For concepts\nshared between the English and translated sets the simple average of the weighting is taken. Concepts not shared\nare removed from the output. This results in a much reduced set of concepts for each class. The resulting average\nclass representation we call the conceptual average. An example is shown in Figure 1.\nFigure 1: Converting two concept sets into a conceptual average\n6. Every class average within each of the three groups (English only, English to French combined conceptual\naverage and English to Chinese conceptual average) is compared with every other class average in the same\ngroup using a Cosine Similarity function (function represented in Figure 2. Where the class comparison has a\npre-defined mapping in the ground truth dataset, we align the similarity score with a target variable value of 1,\nelse it is set to 0. An example record, showing the similarity score of the Author classes from the emt and edas\nontologies, with a target of 1 is shown below, at (H).\n4\n\nFigure 2: Computing the class concept representation similarity\n(H) cmt-Author,edas-Author,0.8362799,1\n7. The resulting output from the previous steps is a set of differences between representations of classes from\nthe source ontologies. The ground truth class reference mappings are used to create a correlation between the\ncorrect relationship and the conceptual difference. The correlation is a measure of the accuracy of conceptual\nrepresentation from the sparse autoencoder, and by creating the conceptual average we hope to improve the\naccuracy. The correlation algorithm used is the Point-Biserial Correlation, which is ideal for correlations between\nbinary and continuous variables.\nDue to the nature of the corpus, the layer-by-layer analysis only has 174 ground truth mappings (from a total of\n95,000 class comparisons) and hence there is a large class imbalance for each layer. We reduce the imbalance by\nusing a random re-sampling to reduce the false target variable size to be the same as the true size.\nA further analysis was undertaken using the same approach, but instead of translating to French, Simplified\nChinese was used.\nExample code is available for validation7.\n4\nResults\nThe corpus used in this experiment contains 16 OWL ontology files, comprising 867 classes. The corpus also\nincludes 174 class mappings which form the ground truth of this experiment.\nFrom this set of files, we follow the method described above and record the results as shown below.\nThe outputs consist of a correlation between ontology classes and over a number of variations such as the nature\nof the text representation (either a summary or verbose representation) and the layer of concept activation (0-25).\nThese results are then compared at the level of language, i.e. are the English-only concept sets different from the\nFrench/Chinese conceptual averages.\nResults are shown in Figure 3, for the summary text representation, and Figure 4 for the verbose prompt. Both\nresults show a clear improvement in correlation comparing English only and the translation conceptual average.\nThere is a small increase in correlation when we look at averages of translations between English and Simplified\nChinese for summary texts, but a reduction for the verbose texts.\n7https://github.com/cliffore/millms\n5\n\nFigure 3: Summary prompt - conceptual average correlation vs English-only (higher is better)\nFigure 4: Verbose prompt - conceptual average correlation vs English-only (higher is better)\nTable 2 shows the correlations compared.\nText Version\nLanguage\nCorrelation\nSummary\nEnglish only\n0.09\nSummary\nAvg Eng/French\n0.39\nSummary\nAvg Eng/Chinese\n0.33\nVerbose\nEnglish only\n0.18\nVerbose\nAvg Eng/French\n0.20\nVerbose\nAvg Eng/Chinese\n0.35\nTable 2: Average correlations for each text version and the translated language\n6\n\n5\nDiscussion\nAfter we extract an English text version of an ontology class, we put that string through a set of SAE neural net-\nworks and compare the concept activations between other classes from related ontologies. We have a (small) set\nof ground truth correspondences between classes and we see a fairly weak correlation emerge. This is potentially\ndue to the small size of the corpus and also the relatively subjective extraction from OWL representation to a\nstring of words. We notice that there is a difference in overall correlations between the summary extracted text\nand the more verbose version.\nWe take the same English text and translate it to French and Simplified Chinese and put these two prompts\nthrough the same SAEs. The resulting concept activation sets are averaged for each ontology class between the\nEnglish and French and between the English and Chinese versions. This represents a different concept activation\nset for each ontology class. When we calculate the same correlations as with the English only activations, we see\na significant improvement in correspondence for the French translation and an even stronger correlation for the\nChinese translations. The difference in average correlations between the French and Chinese (from Table 2) is\nsmall, however the average percentage difference is 9% (summary).\nIn the same way that “Conceptual Semantics takes the meanings of words and sentences to be structures in\nthe minds of language users” [13], we might assume that LLMs have structures which represent the meaning\nof words that are processed through their neural network states. We might also assume that LLMs don’t have\nconcepts of meaning in themselves, but instead are learning and storing correspondences between symbolic and\nlinguistics structures upon which LLMs are trained. Arguments are emerging, however, which show that LLMs\ndo represent real world concepts ([14] and [15]) beyond the purely linguistic. When we peek into the network\ninternals, using Sparse Autoencoders, we often see that concepts are activated which relate to surface cues such\nas syntactic and linguistic semantics, but our results show that we can reduce the symbolic concept space for a\nset of activations and isolate concepts that reflect a purer semantic representation.\nFollowing the example from before, we had an Author class that was represented as English, French and Chinese.\nThe verbose representation concept activation set is shown in Table 1. In Table 3, we show the Chinese translation\nconcept activations, and in Table 4 the conceptual average output.\nConcept ID\nActivation Weight\n2446\n21.1420\n5327\n44.4837\n6035\n39.4718\n6035\n39.7229\n7748\n30.9887\n8920\n22.0814\n9967\n73.0786\n13833\n15.4156\n14763\n19.7685\nTable 3: Example concept activation set 1 (for the verbose Chinese translation of edas-Author class)\nConcept ID\nActivation Weight\n2446\n39.1133\n5327\n62.0677]\n6035\n55.6855\nTable 4: Conceptual average activation set (for the verbose English and Chinese translation of edas-Author class)\nConcepts can be interpreted and given human-readable representations [16]; concept ID 3391 in this analysis\ncan be interpreted as (G), below (more detail can be seen on the Neuronpedia website e.g. for concept 6035\non layer 0: https://www.neuronpedia.org/gemma-2-2b/0-gemmascope-att-16k/6035. This concept\nis removed during the averaging process for the edas-Author class because it is only activated by the English\ntext. The output conceptual average has interpretations shown in Table 5 below.\n7\n\n(G) attends to key parameters denoted by brackets from associated values in the same context\nConcept ID\nAutomatic interpretation\n2446\nattends to the token ”Sant” from related tokens concerning ”orum” and\n”ana.”\n5327\nattends to numerical outputs and associated metadata from other\nmathematical or technical symbols and structures\n6035\nattends to specific instances of the token ”N” from various indexed\ndigits appearing later in the sequence\nTable 5: Conceptual average activation set (for the verbose English and Chinese translation of edas-Author class)\nThe interpretation representations don’t intuitively correspond with a human-readable assessment of similarity\nor conceptual semantics, however we suggest that our results show some significance to the process of removing\nconcepts that are not shared between translated representations. The translation and averaging process is remov-\ning linguistic specific concepts and leaving concepts that are a purer representation of the core semantics of the\noriginal prompt.\nThe overall average results (Table 2 indicate a trend for the Chinese conceptual average to correlate more strongly\nwith the ground truth when compared with the French translation (assuming that the Summary versions (0.39 and\n0.33) are too similar to be divided). This pattern adds credence to the argument since Chinese language tokens\ncontains fewer syntactic elements, “e.g. more frequent functional words in English texts” [17]. Given that the\ndataset is small and the representations and translations relatively subjective, this result should be validated.\nThis slightly unexpected result hints at a new technique for improving conceptual analyses of LLMs, especially\nvia SAEs. We expect future research to confirm and extend this result.\n6\nFuture Work\nWe highlight some problems which we hope to address in future versions of the research.\nThe accuracy of extraction of class representations is exposed to problems of subjectivity. Both the conceptual\nmodel used to create an OWL ontology and the extraction process to create a text string version are prone to\nidiosyncrasies in design.\nThe corpus used is relatively small, having a low number of OWL classes. There is also a class imbalance because\nthere are missing ground truth mappings for many OWL classes. The ground truth is also liable to potential bias.\nThe use of SAEs for interpretability is a relatively novel approach and there are known challenges e.g. feature\nsplitting [18], terse concept dictionaries and reconstruction errors [19].\nThe following areas are in scope for next steps: (i) Extend the corpora to confirm and explore this result, (ii)\nExplore a generalised ontology class extraction process, (iii) Analyse concept features for a common sense\nanalysis, (iv) Apply improvements in interpreting conceptual semantics to Ontology Alignment tasks."}
{"id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "authors": ["Krishna Garg", "Firoz Shaikh", "Sambaran Bandyopadhyay", "Cornelia Caragea"], "source": "http://arxiv.org/abs/2508.14273v1", "page_count": 20, "text": "Let’s Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the\nIntroduction of a Research Paper\nKrishna Garg1*, Firoz Shaik1*, Sambaran Bandyopadhyay2, Cornelia Caragea1\n1University of Illinois Chicago\n2Adobe Research\nkgarg8@uic.edu, fshaik8@uic.edu, sambaranb@adobe.com, cornelia@uic.edu\nAbstract\nAs researchers increasingly adopt LLMs as writing assistants,\ngenerating high-quality research paper introductions remains\nboth challenging and essential. We introduce Scientific Intro-\nduction Generation (SciIG), a task that evaluates LLMs’ abil-\nity to produce coherent introductions from titles, abstracts,\nand related works. Curating new datasets from NAACL 2025\nand ICLR 2025 papers, we assess five state-of-the-art mod-\nels—including both open-source (DeepSeek-v3, Gemma-3-\n12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-\nsource GPT-4o systems across multiple dimensions: lexical\noverlap, semantic similarity, content coverage, faithfulness,\nconsistency, citation correctness, and narrative quality. Our\ncomprehensive framework combines automated metrics with\nLLM-as-a-judge evaluations. Results demonstrate LLaMA-\n4 Maverick’s superior performance on most metrics, par-\nticularly in semantic similarity and faithfulness. Moreover,\nthree-shot prompting consistently outperforms fewer-shot ap-\nproaches. These findings provide practical insights into de-\nveloping effective research writing assistants and set realistic\nexpectations for LLM-assisted academic writing. To foster re-\nproducibility and future research, we will publicly release all\ncode and datasets.\n1\nIntroduction\nThe rapid advancement of generative artificial intelligence,\nparticularly large language models (LLMs), like OpenAI’s\nGPT series (Brown et al. 2020; Achiam et al. 2023), has\nsparked significant interest in their potential to assist in\nvarious complex tasks, including academic writing. As re-\nsearchers and academics strive to balance the increasing de-\nmands of scholarly communication with the need for high-\nquality writing, the importance of using LLMs to generate\ncoherent, engaging, and accurate research paper introduc-\ntions becomes evident. Despite the impressive capabilities\ndemonstrated by these models, creating an introduction that\nencapsulates the essence of a research paper, its motivation,\nscope, and significance remain a formidable challenge. This\nchallenge is amplified by the need for the generated text to\nbe not only contextually relevant but also to exhibit a high\ndegree of narrative quality and academic rigor, as the facts\nmentioned in a research introduction should be grounded on\nstate-of-the-art literature.\n*Equal contribution\nExisting works in the realm of LLM-assisted academic\nwriting have made strides in various domains, from drafting\nessays to summarizing scientific papers (Xiao et al. 2022a;\nNandy and Bandyopadhyay 2025). However, these efforts\noften fall short in several critical aspects. Current models of-\nten struggle with maintaining faithfulness to the source ma-\nterial, ensuring consistency in narrative flow, and accurately\nincorporating citations (Huang et al. 2025). Moreover, while\nthere have been attempts to measure the quality of gener-\nated texts using automated metrics, these evaluations often\noverlook nuanced aspects such as the coherence of the intro-\nduction and its alignment with academic standards (Li et al.\n2024). There are a plethora of blogs and videos on quick\nsuggestions on how to use LLMs to write different parts of\na research paper, but none of those shows a systematic anal-\nysis of the capabilities of current LLMs. There are a few\npublicly available datasets that maintain a collection of re-\nsearch papers for various generation tasks (Lu, Dong, and\nCharlin 2020; LIU et al. 2022). However, they are not ap-\npropriate for use with the current LLMs since they all have\nprobably appeared in the LLM training set. Consequently,\nthe potential of LLMs in generating high-quality research\npaper introductions remains underexplored and inadequately\nbenchmarked.\nTo address these research gaps, we propose a comprehen-\nsive framework for the generation of scientific introductions.\nThis framework aims to rigorously evaluate the capability of\nLLMs to produce introductions of research papers using new\ndatasets curated from the NAACL 2025 and ICLR 2025 ac-\ncepted papers. Our approach involves evaluating multiple di-\nmensions of generated introductions, including lexical over-\nlap, semantic similarity, content coverage, faithfulness, con-\nsistency, citation correctness, and narrative quality. By com-\nbining automated metrics with LLM-as-a-judge evaluations\n(Liu et al. 2023), we seek to provide a holistic understand-\ning of the strengths and limitations of current state-of-the-art\nmodels. Finally, a small scale but rigorous human evaluation\nis conducted to understand the gap between the generated\nintroductions and the expected standard.\nThe following are the contributions we have made in this\npaper. First, we introduce a novel task and curate datasets\nspecifically designed for evaluating the generation of re-\nsearch paper introductions by state-of-the-art LLMs. Sec-\nond, we present a comprehensive evaluation framework that\n\nintegrates automated and human-in-the-loop assessments to\noffer robust insights into model performance. Third, we\nconduct extensive experiments with five open and closed-\nsource state-of-the-art LLMs, highlighting the superior per-\nformance of LLaMA-4-Maverick, particularly in terms of\nsemantic similarity and faithfulness. Lastly, we demonstrate\nthe effectiveness of three-shot prompting over fewer-shot\napproaches, providing practical guidelines for optimizing\nLLM-assisted writing processes. These contributions col-\nlectively advance our understanding of LLM capabilities in\nacademic writing and set the stage for developing more ef-\nfective and reliable research writing assistants.\n2\nRelated Works\nMulti-Document Summarization (MDS) A significant\nbody of work in multi-document summarization (MDS)\nhas explored techniques to distill information from multi-\nple related sources into coherent summaries. Models like\nPRIMERA (Xiao et al. 2022b) have introduced tailored pre-\ntraining strategies that enhance a model’s ability to integrate\ninformation from concatenated documents using encoder-\ndecoder transformers. Meanwhile, graph-based methods\n(Liao, Lebanoff, and Liu 2018; Li et al. 2020; Pasunuru et al.\n2021; Nandy and Bandyopadhyay 2025) leverage structured\nrepresentations such as Abstract Meaning Representation\n(AMR) and discourse relations to encode inter-document\nconnections. However, these approaches often require ad-\nditional linguistic annotations or parsing pipelines, limiting\ntheir applicability in general settings.\nMeta-Review Generation Meta-review generation—a\ntask involving the synthesis of multiple peer reviews into\na single coherent summary—has been tackled using a range\nof strategies. Early systems employed rule-based logic and\nshallow neural components (Bhatia, Pradhan, and Pal 2020;\nPradhan et al. 2021), while more recent work adopted\nencoder-decoder frameworks for abstracting feedback (Ku-\nmar, Ghosal, and Ekbal 2021; Li, Hovy, and Lau 2023). With\nthe emergence of powerful language models, several studies\nhave begun evaluating their suitability for peer-review gen-\neration. Du et al. (2024) and Tyser et al. (2024) investigated\nhow well LLMs perform in generating reviews and meta-\nreviews compared to human-written outputs. Zeng et al.\n(2024) proposed CGI2, a structured prompting technique\nthat guides LLMs through iterative steps for producing more\ngrounded scientific assessments.\nLarge Language Models for Scientific Summarization\nGeneral-purpose LLMs such as GPT-3.5 (Arshad, Yaqoob,\nand Mehmood 2023), GPT-4 (Achiam et al. 2023; Ouyang\net al. 2022), PaLM (Chowdhery et al. 2023), LLaMA (Tou-\nvron et al. 2023), Bloom (Le Scao et al. 2023), and GLaM\n(Du et al. 2022) have demonstrated impressive performance\nacross summarization benchmarks. While these models ex-\ncel in single-document generation tasks, their application\nto multi-input, constraint-driven generation—such as meta-\nreview synthesis or domain-specific aggregation—remains\nunder-investigated (Bansal, Akter, and Karmaker Santu\n2022).\nWhile systems like AutoSurvey (Wang et al. 2024) aim to\nsynthesize prior work into broad surveys, they fall short in\naddressing the rhetorically intricate and concise nature of re-\nsearch paper introductions, which demand precise argumen-\ntation and contextual positioning rather than comprehensive\ncoverage. In contrast, our work formally defines the task of\nScientific Introduction Generation and focuses on generat-\ning coherent, well-grounded introductions from a paper’s\nTitle, Abstract, and citations. This targeted formulation en-\nables deeper exploration of the unique challenges in rhetori-\ncal planning and academic discourse, laying the groundwork\nfor future extensions toward full research paper generation.\n3\nTask\n3.1\nProblem Formulation\nWe formulate the task of Scientific Introduction Generation\n(SciIG) as a conditional generation problem. Given a re-\nsearch paper’s Title T , Abstract A, and a set of k Related\nPapers R = {r1, r2, ..., rk}, where each ri contains a title,\nabstract, and author list, the objective is to generate a co-\nherent, academically styled Introduction I. Formally, the\nmodel does a mapping:\nf : (T , A, R) 7→I\nwhere f is instantiated by a large language model (LLM)\nconditioned on a task-specific prompt.\nWe choose Title, Abstract, and Related Work as inputs\nbecause they collectively provide the essential ingredients\nfor generating effective scientific introductions. According\nto Swales’ widely adopted CARS model (Swales 1990), in-\ntroductions typically serve three rhetorical purposes: (1) es-\ntablishing the research territory, (2) identifying the niche,\nand (3) occupying that niche. In this context, the related\nwork provides a foundation for establishing the territory and\nhighlighting gaps in the literature; the abstract conveys the\ncore contribution and its significance, aligning with niche\noccupation; and the title frames the overall research focus.\n3.2\nDatasets\nTo support the SciIG task, we construct two datasets1 from\naccepted papers of the NAACL 2025 and ICLR 2025 con-\nferences, comprising 800 and 3100 samples, respectively.\nThese datasets, focused on natural language processing\n(NAACL) and machine learning (ICLR), provide diverse,\nhigh-quality research papers for training and evaluating\nmodels to generate coherent and contextually relevant in-\ntroductions. The creation process involved five key steps to\nextract titles, abstracts, introductions, authors, and citation\ndetails, ensuring rich data for the task. Table 1 summarizes\ntext length and citation statistics, highlighting the datasets’\nvariability and suitability.\nThe creation process began with Step 1: we retrieve PDF\nfiles of accepted NAACL 2025 and ICLR 2025 papers from\nweb sources, including official conference proceedings and\nrepositories like ArXiv. For each conference, we aggregated\nall available PDFs to ensure comprehensive coverage of that\nvenue’s publications.\nIn Step 2, we process the PDFs using the grobid2json\nparsing tool provided by the S2ORC project (Lo et al. 2020).\n1We will make the datasets publicly available under CC BY 4.0.\n\nIntroductions (words)\nAbstracts (words)\nCitations\nDataset\nSamples\nMean\nMedian\nRange\nMean\nMedian\nRange\nAverage\nRange\nNAACL 2025\n800\n597.1 ± 228.4\n610.0\n66–1,546\n169.3 ± 76.7\n162.0\n25–1,748\n13.4\n1–45\nICLR 2025\n3100\n771.1 ± 414.3\n747.5\n42–13,675\n210.4 ± 101.4\n194.0\n6–1,063\n15.3\n1–58\nTable 1: Statistics for the NAACL 2025 (800 samples) and ICLR 2025 (3100 samples) datasets, including number of samples,\ntext length, and citation metrics, highlighting variability in introduction lengths and citation patterns relevant to the Introduction\nGeneration task.\nThis tool internally leverages the Grobid library to first con-\nvert PDFs into structured TEI-XML. Subsequently, it ex-\ntracts essential components such as titles, abstracts, intro-\nductions, author details, and bibliographic references, and\norganizes them into a structured JSON format. This struc-\nture representation provides a robust foundation for subse-\nquent tasks in our dataset.\nFor Step 3, we develop a regular expression to identify\nRelated Works cited within each paper’s introduction. The\nregex targeted citation patterns (e.g., “Author et al., Year”) to\nextract references relevant to the introductory context, cap-\nturing the scholarly connections embedded in the text.\nIn Step 4, we employ the LLaMA 4-Maverick model to\nmap extracted Related Works citations (e.g., “et al.” strings)\nto entries in the references section, retrieving the corre-\nsponding titles. The prompt used for this citation mapping is\nprovided in Table 8 in Appendix. This step ensures accurate\nlinkage between in-text citations and their full bibliographic\ndetails.\nFinally, in Step 5, we use the SemanticScholar API, com-\nbining official2 (Kinney et al. 2023) and unofficial3 end-\npoints, to fetch abstracts and author details for each Related\nWorks title. This enriched the dataset with contextual cita-\ntion information, supporting tasks requiring citation-aware\ngeneration.\nThe resulting datasets exhibit significant diversity, as\nshown in Table 1. NAACL 2025 introductions average\n597.1 ± 228.4 words, while ICLR 2025 introductions are\nlonger at 771.1 ± 414.3 words, with an outlier maximum\nof 13,675 words, posing challenges for generation. Citation\npatterns within the Introductions also vary, with NAACL\n2025 averaging 13.4 citations per paper and ICLR 2025 av-\neraging 15.3, reflecting dense scholarly connections. These\ncharacteristics make the datasets ideal for evaluating the\nScientific Introduction Generation task across varied text\nlengths and citation contexts. In our experiments, we use\nmodels with sufficient context length to process the entire\nmetadata, accommodating the datasets’ variable introduc-\ntion lengths and citation-rich content.\nWe limit this initial effort to the CS domain since re-\ncruiting domain-expert annotators is challenging. As ML\nresearchers, we annotated the outputs ourselves. However,\nsince no model was trained on this data, and the study fo-\ncuses purely on benchmarking, the risk of author bias is min-\nimal.\n2https://www.semanticscholar.org/\n3https://github.com/danielnsilva/semanticscholar\n4\nMethods and Evaluation\n4.1\nMethods\nTo evaluate the Scientific Introduction Generation task on\nthe NAACL 2025 and ICLR 2025 datasets, we employ five\nstate-of-the-art language models with knowledge cutoffs be-\nfore January 2025: Deepseek-V3-0324 (Liu et al. 2024),\nLLaMA 4-Maverick 4, Mistral-Small-3.1-24b 5, Gemma-\n3-12b-it (Team et al. 2025), and GPT-4o (Hurst et al. 2024).\nThese models, selected for their diverse architectures and ro-\nbust text generation capabilities, include a combination of\nopen-source and commercialized systems. They are well-\nsuited to handle the datasets’ variable introduction lengths\n(up to 13,675 words) and citation-rich metadata. Each model\nhas sufficient context length to process the entire input, in-\ncluding titles, abstracts, and related paper details, ensuring\ncomprehensive generation.\nWe design seven prompting strategies to guide the mod-\nels in generating introductions, tailored to the task’s require-\nments and the datasets’ characteristics. These strategies, in-\ncluding SHORT, MEDIUM, ELABORATE, ONE SHOT,\nTWO SHOT, THREE SHOT, and AUTOCOT, vary in com-\nplexity and context, incorporating the target paper’s title,\nabstract, and JSON-formatted related papers with APA ci-\ntations. The SHORT prompt provides minimal context for\nconcise generation, while MEDIUM emphasizes research\ngaps and citation integration. ELABORATE enforces a\nstrict four-paragraph structure (each 100–150 words), detail-\ning context, gaps, contributions, and impact. ONE SHOT,\nTWO SHOT, and THREE SHOT incorporate 1–3 example\nintroductions from a held-out set6, enabling few-shot learn-\ning. AUTOCOT uses iterative, self-refining prompts to en-\nhance reasoning for complex scenarios. All prompts main-\ntain a formal academic tone, enforce APA citation accuracy,\nand leverage the datasets’ citation-rich metadata, ensuring\nrobust and contextually relevant introductions. The prompts\nfor each strategy are discussed in detail in Appendix in Ta-\nbles 9, 10, and 11.\n4https://huggingface.co/meta-llama/Llama-4-Maverick-17B-\n128E-Instruct\n5https://huggingface.co/mistralai/Mistral-Small-3.1-24B-\nInstruct-2503\n6Held-out set is created using additional 54 samples for\nNAACL 2025 and 52 samples for ICLR 2025. It is used to select\nrandom examples provided in the prompt for few-shot experiments.\n\n4.2\nEvaluation\nThe evaluation of the Scientific Introduction Generation\ntask employs a comprehensive set of metrics organized into\nsix key categories: Lexical Overlap, Semantic Similarity,\nContent Coverage, Faithfulness, Consistency, and Narrative\nQuality, as detailed in Table 2. Below, we describe the com-\nputation and purpose of each category’s metrics, followed\nby a discussion of the LLM-as-a-Judge metric, highlighting\ntheir roles in assessing model performance.\nLexical Overlap evaluates surface-level similarity be-\ntween generated and reference introductions using ROUGE-\n1, ROUGE-2, ROUGE-L (Lin 2004), BLEU (Papineni et al.\n2002), and METEOR (Banerjee and Lavie 2005). ROUGE-1\nand ROUGE-2 compute unigram and bigram overlap as the\nratio of matching n-grams, balancing precision and recall.\nROUGE-L measures the longest common subsequence, cap-\nturing structural similarity. BLEU calculates n-gram preci-\nsion with a brevity penalty for length differences. METEOR\nincorporates synonymy and stemming, aligning words by\nsemantic similarity to enhance sensitivity to linguistic vari-\nations.7\nSemantic Similarity metrics, including BERTScore\n(Zhang* et al. 2020), BLEURT(Sellam, Das, and Parikh\n2020), and our proposed contextual embedding-based met-\nric Contextual Relevance, assess the alignment of meaning\nbetween the generated and reference texts.\nBERTScore computes cosine similarity between contex-\ntual token embeddings from a pretrained BERT model, al-\nlowing soft matching across paraphrases. BLEURT8 uses a\nBERT-based regression model fine-tuned on human ratings\nto predict similarity with greater nuance.\nCONTEXTUAL RELEVANCE measures the average cosine\nsimilarity between the embedding of the generated introduc-\ntion and the embeddings of its associated context elements,\nwhich include the paper’s title, abstract, and each of the cited\npapers. These individual components form the input context\nused to evaluate how topically aligned the generated intro-\nduction is with the source material. Let g ∈Rd denote the\nembedding of the generated introduction, and {c1, . . . , cn}\nthe context embeddings. The relevance is computed as:\nContextual Relevance = 1\nn\nn\nX\ni=1\ng · ci\n∥g∥· ∥ci∥+ ϵ\nwhere ϵ is a small constant for numerical stability.\nContent Coverage evaluates how well the generated in-\ntroduction captures key content, either by matching the gold\n(ground truth) introduction or by grounding in the input con-\ntext (Title and Abstract). We define two complementary met-\nrics:\nREFERENCE-BASED COVERAGE. Let Kgen be the set of\nkeyphrases extracted from the generated introduction, and\nKGT be the set of keyphrases from the ground truth introduc-\ntion. Define semantic match ∼based on cosine similarity of\nphrase embeddings (e.g., using Sentence-BERT), such that:\n7Implementations\nsourced\nfrom\nhttps://github.com/\nhuggingface/evaluate\n8Implementation sourced from https://github.com/lucadiliello/\nbleurt-pytorch\nkGT ∼kgen ⇐\n⇒cos (e(kGT), e(kgen)) ≥τ\nfor a threshold τ (e.g., 0.7). Then, Reference-based Cov-\nerage (RBC) is computed as:\nRBC = |{k ∈KGT | ∃k′ ∈Kgen, k ∼k′}|\n|KGT|\nREFERENCE-FREE COVERAGE. Let Kctx denote the set\nof keyphrases extracted from the Title and Abstract (input\ncontext). Then, Reference-free Coverage (RFC) is defined\nsimilarly:\nRFC = |{k ∈Kctx | ∃k′ ∈Kgen, k ∼k′}|\n|Kctx|\nFaithfulness assesses the factual alignment between the\ngenerated Introduction and its source inputs. We implement\ntwo automatic metrics and one LLM-based judge:\nQA-BASED FAITHFULNESS (Q2). We adopt a Q2-style\n(Honovich et al. 2021) metric to evaluate the factual consis-\ntency of a generated introduction with respect to its source\ncontext (i.e., the title, abstract, and cited works). Please refer\nto Appendix A for the formal derivation of the metric.\nENTAILMENT-BASED\nFAITHFULNESS.\nWe\nemploy\nSummaC-ZS (Laban et al. 2022), a zero-shot Natural\nLanguage Inference (NLI)-based model, to assess factual\nconsistency between the generated introduction and its\nsource context (i.e., title, abstract, and cited papers). The\nmodel treats the source context as a premise and the\ngenerated introduction as a hypothesis, computing a scalar\nconsistency score:\nFaithfulnessEntail(G, C) = SummaC ZS(C, G)\nHere, SummaC ZS(·) returns an alignment score in\n[−1, 1], where higher values indicate stronger entailment\nand factual alignment. This metric effectively evaluates\nfaithfulness by detecting hallucinations and contradictions\nin summarization outputs.\nKEYPHRASE-BASED FAITHFULNESS. Let Kgen be the set\nof keyphrases extracted from the generated introduction, and\nKctx the set extracted from the Title and Abstract (serving as\nfactual ground truth).\nDefine semantic match ∼based on a similarity thresh-\nold over embeddings (e.g., cosine similarity ≥0.7). Then,\nKeyphrase-based Precision for Faithfulness is:\nFaithfulnessKP = |{k ∈Kgen | ∃k′ ∈Kctx, k ∼k′}|\n|Kgen|\nThis\nmetric\ncaptures\nthe\nproportion\nof\ngenerated\nkeyphrases that are factually grounded in the source context.\nConsistency metrics, including QA-based, Entailment-\nbased, and Keyphrase-based Consistency, evaluate inter-\nnal coherence. These metrics mirror the implementation of\nFaithfulness metrics but differ in their reference point: while\nFaithfulness is computed with respect to the title and ab-\nstract, Consistency is calculated with respect to the ground\ntruth introduction. They assess whether the generated text\n\nCategory\nMetric\nNAACL 2025\nICLR 2025\nDeepseek\nGemma\nLLaMA4\nMistral\nGPT-4o\nDeepseek\nGemma\nLLaMA4\nMistral\nGPT-4o\nLexical Overlap\nROUGE-1\n0.4161\n0.4361\n0.4402\n0.4388\n0.4282\n0.3863\n0.4342\n0.4298\n0.4326\n0.4154\nROUGE-2\n0.1232\n0.1304\n0.1509\n0.1340\n0.1310\n0.1195\n0.1340\n0.1537\n0.1344\n0.1289\nROUGE-L\n0.1649\n0.1725\n0.1894\n0.1765\n0.1754\n0.1531\n0.1700\n0.1830\n0.1715\n0.1689\nBLEU\n0.1175\n0.1232\n0.1198\n0.1278\n0.1043\n0.0781\n0.0979\n0.0874\n0.0977\n0.0762\nMETEOR\n0.2669\n0.2851\n0.2700\n0.2900\n0.2658\n0.2245\n0.2600\n0.2418\n0.2592\n0.2358\nSemantic Similarity\nBERTScore\n0.8415\n0.8475\n0.8448\n0.8467\n0.8422\n0.8377\n0.8449\n0.8403\n0.8427\n0.8377\nBLEURT\n0.3200\n0.3088\n0.3339\n0.3236\n0.3262\n0.3171\n0.3032\n0.3290\n0.3194\n0.3225\nContextual Relevance\n0.9691\n0.9713\n0.9684\n0.9694\n0.9696\n0.9687\n0.9729\n0.9696\n0.9700\n0.9707\nContent Coverage\nReference-based Coverage\n0.4435\n0.4513\n0.4845\n0.4692\n0.4713\n0.4461\n0.4732\n0.4794\n0.4725\n0.4697\nReference-free Coverage\n0.3913\n0.3666\n0.4367\n0.3959\n0.4035\n0.3547\n0.3448\n0.4097\n0.3847\n0.3664\nLLM-as-a-Judge\n0.6907\n0.7051\n0.7019\n0.7008\n0.7054\n0.6567\n0.6970\n0.6809\n0.6833\n0.6879\nFaithfulness\nQA-based Faithfulness\n0.7794\n0.7794\n0.7794\n0.7846\n0.7762\n0.7761\n0.7814\n0.7772\n0.7790\n0.7751\nEntailment-based Faithfulness\n-0.0364\n0.1461\n0.1077\n0.1533\n0.0937\n-0.0276\n0.1647\n0.1280\n0.1660\n0.1095\nKeyphrase-based Faithfulness\n0.3138\n0.2933\n0.3455\n0.3230\n0.3263\n0.2892\n0.2786\n0.3318\n0.3067\n0.2941\nLLM-as-a-Judge\n0.7873\n0.8042\n0.8157\n0.7874\n0.8141\n0.7825\n0.8088\n0.8149\n0.7861\n0.8138\nConsistency\nQA-based Consistency\n0.7820\n0.7876\n0.7860\n0.7861\n0.7873\n0.7810\n0.7891\n0.7846\n0.7842\n0.7847\nEntailment-based Consistency\n-0.0151\n0.1339\n0.1077\n0.1319\n0.0774\n-0.0552\n0.0923\n0.0758\n0.1015\n0.0465\nKeyphrase-based Consistency\n0.2133\n0.2170\n0.2393\n0.2230\n0.2248\n0.2121\n0.2197\n0.2214\n0.2187\n0.2208\nLLM-as-a-Judge\n0.8788\n0.8910\n0.8954\n0.8850\n0.8945\n0.8583\n0.8873\n0.8826\n0.8761\n0.8868\nCitation Correctness\nRecall\n0.5158\n0.5763\n0.5574\n0.4914\n0.4826\n0.4954\n0.5413\n0.5505\n0.4671\n0.4657\nPrecision\n0.8819\n0.9049\n0.9357\n0.8889\n0.9097\n0.8686\n0.8965\n0.9238\n0.8822\n0.8942\nLLM-as-a-Judge (Citation Quality)\n0.7683\n0.7959\n0.8051\n0.7658\n0.8050\n0.7575\n0.8021\n0.8019\n0.7597\n0.7968\nNarrative Quality\nPerplexity ↓\n28.5031\n32.3897\n18.4870\n25.6458\n21.8494\n36.9546\n29.1004\n21.1194\n29.2585\n22.0046\nLLM-as-a-Judge\n0.8892\n0.8888\n0.9001\n0.8928\n0.9004\n0.8835\n0.8899\n0.8986\n0.8907\n0.9006\nTable 2: Evaluation metrics categorized across standard and LLM-as-a-Judge criteria for NAACL 2025 and ICLR 2025. Highest\nvalues are bolded. Note: Lower perplexity values indicate better performance.\nmaintains logical and factual consistency within itself, en-\nsuring a cohesive introduction.\nCitation Correctness metrics include Precision, Recall,\nand LLM-as-a-Judge metric. We extract citations (e.g.,\n“et al.” strings) from the generated introduction with the\nLLaMA 4-Maverick model and match them against the orig-\ninal Related Works citation list to compute the scores. The\nprompt used is given in Table 8 in the appendix.\nNarrative Quality is evaluated using Perplexity and\nLLM-as-a-Judge. Perplexity, computed as the exponential of\nthe average negative log-likelihood under a language model\n(GPT-2) (Radford et al. 2019), indicates fluency, with lower\nvalues reflecting more natural text.\nThe LLM-as-a-Judge metric (Liu et al. 2023), using\nGPT-4o, evaluates Content Coverage, Faithfulness, Consis-\ntency, Citation Contextual Quality, and Narrative Quality.\nGPT-4o is prompted to score generated introductions on a\nnumerical scale for specific qualities as discussed above.\nIts contextual reasoning captures nuanced aspects, comple-\nmenting automated metrics. The exact prompts for each cat-\negory are provided in Table 6 in the appendix. This met-\nric improves the framework by integrating human-like judg-\nment with computational precision.\nThese metrics collectively form a robust evaluation frame-\nwork, capturing lexical, semantic, content, factual, and nar-\nrative dimensions of the SciIG task, ensuring a thorough\nassessment of model performance across diverse require-\nments.\n5\nResults and Analysis\nTable 2 presents the performance of five state-of-the-art\nmodels (Deepseek, Gemma, LLaMA4, Mistral, GPT-4o)\non the NAACL 2025 (800 samples) and ICLR 2025 (3100\nsamples) datasets for the SciIG task. The results highlight\nseveral important observations.\nFirst, LLaMA4 achieves consistently high lexical over-\nlap and semantic similarity scores, particularly evident in\nits superior performance on both datasets. This indicates\nLLaMA4’s strong capability in closely replicating the orig-\ninal reference content, likely due to optimized pretraining\nand instruction-following mechanisms.\nSecond, although Gemma demonstrates exceptional recall\nin Citation Correctness, it shows relatively lower precision\ncompared to LLaMA4, suggesting effective reference incor-\nporation but occasional introduction of extraneous citations.\nThird, Mistral shows remarkable entailment-based con-\nsistency scores, surpassing other models on both datasets,\nindicating its strength in maintaining logical alignment be-\ntween input contexts and generated text. However, its higher\nperplexity suggests a trade-off with fluency.\nFourth, Gemma’s superior Contextual Relevance scores,\nespecially on the ICLR dataset, imply that it captures\ndeeper contextual nuances within research topics. Lastly, de-\nspite GPT-4o’s competitive QA-based consistency scores,\nits lower lexical overlap and citation recall metrics suggest\nchallenges in precisely capturing explicit references.\nThe LLM-as-a-Judge metrics reveal nuanced strengths\namong the models. LLaMA4 stands out for its robust faith-\nfulness and consistency, while GPT-4o shines in content\ncoverage and narrative quality. Gemma’s edge in citation\n\nCategory\nMetric\nInstruction-Level\nFew-Shot\nShort\nMedium\nElaborate\nAutoCoT\nZero-Shot\nOne-Shot\nTwo-Shot\nThree-Shot\nLexical Overlap\nROUGE-1\n0.4261\n0.4513\n0.4402\n0.4233\n0.4402\n0.4418\n0.4440\n0.4460\nROUGE-2\n0.1622\n0.1625\n0.1509\n0.1361\n0.1509\n0.1607\n0.1645\n0.1647\nROUGE-L\n0.1911\n0.1940\n0.1894\n0.1701\n0.1894\n0.1942\n0.1965\n0.1972\nBLEU\n0.1117\n0.1345\n0.1198\n0.1353\n0.1198\n0.1182\n0.1193\n0.1204\nMETEOR\n0.2516\n0.2797\n0.2700\n0.3132\n0.2700\n0.2669\n0.2679\n0.2695\nSemantic Similarity\nBERTScore\n0.8480\n0.8471\n0.8448\n0.8352\n0.8448\n0.8486\n0.8496\n0.8492\nBLEURT\n0.3347\n0.3312\n0.3339\n0.3201\n0.3339\n0.3431\n0.3432\n0.3452\nContextual Relevance\n0.9694\n0.9690\n0.9684\n0.9698\n0.9684\n0.9677\n0.9674\n0.9681\nContent Coverage\nReference-based Coverage\n0.4875\n0.4697\n0.4845\n0.4347\n0.4845\n0.4935\n0.4913\n0.4985\nReference-free Coverage\n0.4313\n0.4231\n0.4367\n0.4556\n0.4367\n0.4622\n0.4632\n0.4755\nLLM-as-a-Judge\n0.7084\n0.7101\n0.7019\n0.7340\n0.7019\n0.7047\n0.7081\n0.7107\nFaithfulness\nQA-based Faithfulness\n0.7840\n0.7767\n0.7794\n0.7627\n0.7794\n0.7829\n0.7780\n0.7833\nEntailment-based Faithfulness\n0.0119\n0.0505\n0.1077\n-0.1368\n0.1077\n0.1292\n0.1361\n0.1432\nKeyphrase-based Faithfulness\n0.2338\n0.2183\n0.2393\n0.3610\n0.2393\n0.2353\n0.2303\n0.2366\nLLM-as-a-Judge\n0.8462\n0.8382\n0.8157\n0.8603\n0.8157\n0.8077\n0.8146\n0.8094\nConsistency\nQA-based Consistency\n0.7870\n0.7786\n0.7860\n0.7660\n0.7860\n0.7864\n0.7827\n0.7873\nEntailement-based Consistency\n0.0762\n0.0802\n0.1077\n-0.0521\n0.1077\n0.0955\n0.0902\n0.0852\nKeyphrase-based Consistency\n0.3450\n0.3348\n0.3455\n0.2060\n0.3455\n0.3640\n0.3688\n0.3687\nLLM-as-a-Judge\n0.9045\n0.8959\n0.8954\n0.9177\n0.8954\n0.8945\n0.8954\n0.8998\nCitation Correctness\nCitation Recall\n0.6326\n0.6193\n0.5574\n0.5350\n0.5574\n0.5930\n0.6083\n0.6076\nCitation Precision\n0.8382\n0.8548\n0.9357\n0.8942\n0.9357\n0.9506\n0.9470\n0.9515\nLLM-as-a-Judge (Citation Quality)\n0.8492\n0.8388\n0.8051\n0.8243\n0.8051\n0.7901\n0.7855\n0.7823\nNarrative Quality\nPerplexity ↓\n21.4601\n26.2687\n18.4870\n19.0518\n18.4870\n19.1426\n23.0976\n19.6214\nLLM-as-a-Judge\n0.8997\n0.8950\n0.9001\n0.8976\n0.9001\n0.8999\n0.8965\n0.8987\nTable 3: Performance of LLaMA 4-Maverick on NAACL dataset under two prompting paradigms: Instruction-Level (Short,\nMedium, Elaborate, AutoCoT) and Few-Shot (Zero-Shot, One-Shot, Two-Shot, Three-Shot). Best scores in each paradigm are\nbolded. Note: Lower perplexity values indicate better performance.\ncorrectness for ICLR points to its strength in precise ref-\nerencing. However, the tight score ranges across models\nhighlight a limitation of LLM-as-a-Judge: it may struggle\nto discern fine-grained differences, potentially underesti-\nmating the unique capabilities of open-source models like\nLLaMA4, which rival the closed-source GPT-4o in rigorous\nacademic settings.\n5.1\nAnalysis on Few-shot Experiments\nIn Table 3, we present the results for few-shot prompting\nexperiments (Zero-Shot, One-Shot, Two-Shot, Three-Shot)\non the NAACL dataset. The results reveal noticeable im-\nprovements with an increasing number of examples. Specif-\nically, Three-Shot prompting consistently outperforms other\nfew-shot settings in lexical overlap (ROUGE-1, ROUGE-2,\nROUGE-L) and reference-free content coverage, suggesting\nthat additional exemplars guide the model toward capturing\nfiner lexical details and comprehensive content. However,\nimprovements plateau after Two-Shot, indicating diminish-\ning returns beyond a certain number of examples. Moreover,\nhigher SummaC-ZS faithfulness scores in Three-Shot setups\nreflect that additional contexts help the model better adhere\nto the factual correctness of the input, highlighting the ben-\nefit of illustrative examples in complex generation tasks.\n5.2\nAnalysis with Various Prompt Strategies\nIn Table 3, we present the results depicting the impact of\nprompt elaboration (Short, Medium, Elaborate, AutoCoT)\non the NAACL dataset. Medium-length prompts achieve\nhigher lexical overlap (ROUGE scores), suggesting that\nmoderate complexity is optimal for balancing detail with\nmodel understanding. Conversely, AutoCoT prompts re-\nsult in superior METEOR and Keyphrase-based faithfulness\nscores, implying that models benefit significantly from de-\ntailed reasoning steps when precision in conceptual align-\nment is required. However, AutoCoT notably increases per-\nplexity, indicating potential trade-offs between detailed in-\nstructional clarity and narrative fluency. Thus, prompt com-\nplexity should be carefully adjusted depending on whether\nthe task prioritizes lexical fidelity, conceptual alignment, or\nnarrative coherence.\n6\nAblation Study\nWe conduct an ablation study to examine the effect of vary-\ning input configurations on the quality of generated intro-\nductions. Specifically, we compare three setups: (i) Title-\nonly, (ii) Title + Abstract, and (iii) Title + Abstract + Re-\nlated Work, under two prompting strategies: ELABORATE\nand MEDIUM. As shown in Table 4, we evaluate perfor-\nmance using a comprehensive set of lexical, semantic, and\nLLM-based metrics.\nThe results show a consistent trend: including more con-\ntext leads to better generation quality. The T+A+Related\nconfiguration achieves the best scores across nearly all met-\nrics and prompting strategies, with notable gains in se-\nmantic similarity (e.g., BERTScore: 0.8448 →0.8471), ci-\n\nCategory\nMetric\nELABORATE\nMEDIUM\nTitle-only\nTitle+Abstract\nT+A+Related\nTitle-only\nTitle+Abstract\nT+A+Related\nLexical Overlap\nROUGE-1\n0.3545\n0.4093\n0.4402\n0.3356\n0.3949\n0.4513\nROUGE-2\n0.0772\n0.1202\n0.1509\n0.0660\n0.1084\n0.1625\nROUGE-L\n0.1553\n0.1784\n0.1894\n0.1436\n0.1700\n0.1940\nBLEU\n0.0503\n0.0729\n0.1198\n0.0223\n0.0472\n0.1345\nMETEOR\n0.2150\n0.2428\n0.2700\n0.1816\n0.2185\n0.2797\nSemantic Similarity\nBERTScore\n0.8324\n0.8408\n0.8448\n0.8235\n0.8320\n0.8471\nBLEURT\n0.3189\n0.3333\n0.3339\n0.3303\n0.3365\n0.3312\nContextual Relevance\n0.9651\n0.9657\n0.9684\n0.9633\n0.9650\n0.9690\nContent Coverage\nReference-based Coverage\n0.3918\n0.4805\n0.4845\n0.3878\n0.4715\n0.4697\nReference-free Coverage\n0.4504\n0.4771\n0.4367\n0.4602\n0.4953\n0.4231\nLLM-as-a-Judge\n0.5425\n0.6916\n0.7019\n0.5783\n0.6916\n0.7101\nFaithfulness\nQA-based Faithfulness\n0.7662\n0.7808\n0.7794\n0.7849\n0.7858\n0.7767\nEntailment-based Faitfulness\n0.0238\n0.1430\n0.1077\n-0.0854\n0.0120\n0.0505\nKeyphrase-based Faithfulness\n0.3523\n0.3788\n0.3455\n0.3495\n0.3898\n0.2183\nLLM-as-a-Judge\n0.7288\n0.8557\n0.8157\n0.8110\n0.8557\n0.8382\nConsistency\nQA-based Consistency\n0.7780\n0.7849\n0.7860\n0.7863\n0.7889\n0.7786\nEntailment-based Consistency\n0.0881\n0.1261\n0.1077\n0.0223\n0.0579\n0.0802\nKeyphrase-based Consistency\n0.1843\n0.2210\n0.2393\n0.1780\n0.3898\n0.3348\nLLM-as-a-Judge\n0.7872\n0.8977\n0.8954\n0.8430\n0.8977\n0.8959\nCitation Correctness\nRecall\n0.0918\n0.1199\n0.5574\n0.0001\n0.0039\n0.6193\nPrecision\n0.2399\n0.3610\n0.9357\n0.0012\n0.0175\n0.8548\nLLM-as-a-Judge (Citation Quality)\n0.5174\n0.5184\n0.8051\n0.3696\n0.5184\n0.8388\nNarrative Quality\nPerplexity ↓\n14.5978\n19.9132\n18.4870\n16.6799\n20.7426\n26.2687\nLLM-as-a-Judge\n0.9000\n0.8986\n0.9001\n0.9010\n0.8986\n0.8950\nTable 4: Ablation Study. Performance of LLaMA 4-Maverick on NAACL 2025 dataset with different input configurations:\nTitle-only, Title+Abstract, and Title+Abstract+Related Work. Note: Lower perplexity values indicate better performance.\ntation precision (e.g., 0.3610 →0.9357), citation recall\n(e.g., 0.1199 →0.5574) when related work is added. LLM-\nas-a-Judge citation quality also increases from 0.5184 to\n0.8051. These findings underscore the importance of richer\ninput—particularly related work—in enabling LLMs to gen-\nerate more accurate, well-grounded, and coherent introduc-\ntions.\n7\nHuman Evaluation\nMetric\nLLaMA4\n0-shot\nLLaMA4\n3-shot\nLLaMA4\nAutoCoT\nGPT-4o\n0-shot\nFaithfulness\n4.20\n4.70\n4.11\n4.35\nConsistency\n3.80\n4.50\n3.84\n4.10\nContent Coverage\n4.05\n4.50\n3.74\n4.05\nFlow of Ideas\n4.30\n4.60\n4.26\n4.55\nCitation Contextual Quality\n3.40\n4.20\n3.32\n3.60\nHallucination Resistance\n4.60\n5.00\n4.37\n4.60\nLiterature Context\n3.85\n4.40\n3.95\n4.00\nMotivation Clarity\n4.15\n4.50\n4.21\n4.40\nMethods Summary\n3.65\n4.25\n3.79\n3.85\nContributions Summary\n3.30\n4.20\n3.37\n3.74\nTable 5: Human evaluation of different models across ten\nqualitative criteria.\nWe conducted a human evaluation on 20 source exam-\nples, each with four model generations, LLaMA 4-Maverick\n(0-shot, 3-shot, AutoCoT) and GPT-4o (0-shot), totaling 80\noutputs. The three authors performed the annotations them-\nselves, each annotating 6–7 papers (four generations each).\nDue to the unavailability of domain expert annotators, no\npaper was annotated by multiple annotators. This single-\nannotation approach is common in recent benchmarking\nstudies such as Hossain et al. (2025) and appropriate here\nsince the task is purely benchmarking, not proposing new\nmodels. Evaluating 50–100 outputs aligns with established\nLLM evaluation norms. The annotation form is included in\nAppendix 7, and results are summarized in Table 5.\nAcross comparisons, we observe several clear trends.\nFirst, GPT-4o outperforms 0-shot LLaMA-4 on most\ncriteria—particularly consistency, citation contextual qual-\nity, and contribution summary—though LLaMA-4 shows\nstronger hallucination resistance. Second, 3-shot prompt-\ning leads to the strongest overall performance, achiev-\ning the highest scores in content coverage, flow of ideas,\nand clarity of methods and contributions. Finally, ELAB-\nORATE 0-shot prompting surpasses AutoCoT across all\ndimensions, with notable gains in citation quality and faith-\nfulness, emphasizing the value of carefully designed instruc-\ntion formats. These results highlight the importance of both\nmodel selection and prompting strategy in improving the\nquality and reliability of LLM-generated scientific introduc-\ntions. Refer Appendix B to see the generated samples.\n8\nDiscussion\nBuilding on the findings presented in this paper, we address\ntwo central questions regarding the role of large language\nmodels (LLMs) in scientific writing. First, are LLMs good\nenough to generate the first draft of a research paper’s in-\ntroduction? Our automated and human evaluations indicate\nthat, when provided with structured prompts—including a\n\nclear set of instructions, a meaningful title and abstract, and\na relevant set of related works—LLMs are capable of gen-\nerating coherent, well-structured, and stylistically appropri-\nate introductions that align with academic norms. Second,\ncan the LLM-generated introduction be used as-is? Not en-\ntirely. While LLMs produce high-quality drafts that serve\nas strong starting points, they fall short in several critical\naspects, including the incorporation of fine-grained techni-\ncal details from cited literature, accurate contextualization of\ncitations, and precise articulation of the target paper’s con-\ntributions. As such, the generated introductions require sub-\nstantial post-editing and expert input. These findings suggest\nthat LLMs are promising assistants in the scholarly writing\npipeline, but they are not yet suitable replacements for hu-\nman authorship.\n9\nEthical Statement\nThis study investigates the capabilities of LLMs for gener-\nating scientific paper introductions, with the goal of under-\nstanding their potential as assistive tools for researchers. We\nexplicitly do not advocate for autonomous use of LLMs in\nscholarly writing. While LLMs can produce coherent and\nstylistically appropriate drafts, our evaluation reveals persis-\ntent challenges such as factual inaccuracies, improper cita-\ntion use, and limited domain grounding, all of which neces-\nsitate human validation.\nWe emphasize that any generated content should be re-\nviewed and revised by human authors to ensure accuracy,\nintegrity, and alignment with academic standards. This work\nis intended to benchmark current model capabilities and\nhighlight where human involvement remains indispensable.\nWe support responsible deployment of LLMs and caution\nagainst their use in critical academic workflows without ex-\npert oversight."}
{"id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "authors": ["Zixin Rao", "Youssef Mohamed", "Shang Liu", "Zeyan Liu"], "source": "http://arxiv.org/abs/2508.14190v1", "page_count": 20, "text": "Two Birds with One Stone: Multi-Task Detection\nand Attribution of LLM-Generated Text\nZixin Rao1*, Youssef Mohamed2*, Shang Liu3, and Zeyan Liu3**\n1 University of Georgia\nzr04546@uga.edu\n2 Egypt-Japan University of Science and Technology\nyoussef.khalil@ejust.edu.eg\n3 University of Louisville\n{shang.liu, zeyan.liu}@louisville.edu\nAbstract. Large Language Models (LLMs), such as GPT-4 and Llama,\nhave demonstrated remarkable abilities in generating natural language.\nHowever, they also pose security and integrity challenges. Existing coun-\ntermeasures primarily focus on distinguishing AI-generated content from\nhuman-written text, with most solutions tailored for English. Mean-\nwhile, authorship attribution–determining which specific LLM produced\na given text–has received comparatively little attention despite its impor-\ntance in forensic analysis. In this paper, we present DA-MTL, a multi-task\nlearning framework that simultaneously addresses both text detection\nand authorship attribution. We evaluate DA-MTL on nine datasets and\nfour backbone models, demonstrating its strong performance across mul-\ntiple languages and LLM sources. Our framework captures each task’s\nunique characteristics and shares insights between them, which boosts\nperformance in both tasks. Additionally, we conduct a thorough anal-\nysis of cross-modal and cross-lingual patterns and assess the frame-\nwork’s robustness against adversarial obfuscation techniques. Our find-\nings offer valuable insights into LLM behavior and the generalization of\nboth detection and authorship attribution. Our source code is available\nat https://github.com/youssefkhalil320/MTL_training_two_birds_\nwith_one_stone.\nKeywords: AIGC Detection · Forensics · Responsible AI · Large Lan-\nguage Models\n1\nIntroduction\nNeural language generation has revolutionized natural language processing, en-\nabling the creation of highly coherent, contextually relevant, and human-like\ntext [70]. Powered by advances in Large Language Models (LLMs), such as\n⋆These authors contributed equally to this work, which was conducted during their\ninternship at the University of Louisville.\n⋆⋆Corresponding author.\n\nGPT-4 [1] and Llama [61,62], these models have achieved extraordinary perfor-\nmance across diverse applications, including education [71], medicine[57], soft-\nware engineering[22], creative industries [17], and so on. However, the prolifer-\nation of LLM-generated content has also raised widespread concerns related to\nintegrity, security, creativity, and ethics [72,3,11,35]. Meanwhile, the accessibility\nand ease of use of commercial tools such as ChatGPT [43] and Google Gemini\n[15] have further magnified these risks. For example, the proportion of articles\ngenerated by LLMs on misinformation websites has grown rapidly [19], and as\nof December 2023, 26.1% of arXiv submissions included ChatGPT-generated\ncontent [34]. In addition, threat actors have exploited LLMs for malicious pur-\nposes such as phishing and website hacking [46,16,12]. These trends highlight\nthe urgent need for effective mechanisms to mitigate the social impacts of LLM-\ngenerated content.\nDetecting LLM-generated text has emerged as a critical and commonly used\nsolution to address these challenges, both in research and in practice [58,55].\nExisting studies primarily focus on identifying whether a given text is human-\nwritten or AI-generated, with most studies centered on English or closely related\nlanguages and GPT models [74,32,18,33,63,34]. However, these approaches leave\ngaps in understanding how detection generalizes across diverse languages and\nLLM architectures. Recent works, such as MULTITuDE[36] and M4 [68,69],\nhave started to explore multi-lingual and multi-modal detection, but questions\nregarding cross-linguistic and cross-modal generalization remain unanswered.\nAuthorship attribution, which identifies the specific AI model responsible for\ngenerating a piece of text, is an equally important but underexplored challenge.\nWith the rapid proliferation of LLMs-hundreds being developed each week-and\nthe growing sophistication of generated text [13], accurately attributing text to\nits source model is becoming increasingly difficult. At the same time, this task\nis critical for applications such as intellectual property protection [24]. Unlike\ndetection, attribution requires a more nuanced analysis of characteristics. This\nmakes it a technically complex but essential problem to regulate and understand\nAI-generated content. However, most existing studies focus only on binary classi-\nfication between human-written and machine-generated text. In addition, many\nrecent studies on authorship attribution overlook human-written text or simply\ntreat it on par with individual LLMs [64,41,7,52].\nIn this paper, we introduce a new LLM forensics framework called DA-MTL,\nwhich simultaneously tackles text detection and authorship attribution using\nmulti-task learning. Our approach uses shared representations to solve two tasks:\n(1) distinguishing between human-written and machine-generated text, and (2)\nidentifying the specific source LLM of machine-generated text. We evaluate the\nframework on multiple backbone models, datasets, and languages, demonstrating\nits superior performance. Notably, our framework improves both tasks. Based\non our accurate models, we explore cross-modal similarities, investigating how\nshared patterns among LLMs influence their proximity in feature space and\ntheir attribution. We also examine how LLMs differ using stylometric analysis\nand how our forensics models trained in one language family generalize to others.\n\nFinally, we test the robustness of the proposed framework against obfuscation\ntechniques, showing its ability to handle content post-processed by adversaries.\nSpecifically, we aim to answer the following research questions:\n– RQ1: Can multi-task learning improve text detection and attribution per-\nformance compared to single-task baselines?\n– RQ2: Which LLMs are similar, and how do these similarities impact attri-\nbution performance?\n– RQ3: How does DA-MTL generalize across languages?\nOur main contributions include:\n1. We present a multi-task learning framework, DA-MTL, to jointly solve text\ndetection and attribution, which shows outstanding performance.\n2. We comprehensively analyze cross-modal and cross-lingual generalization,\nwhich provides deep insights into LLM proximity and behaviors.\n3. We demonstrate the robustness of DA-MTL against adversarial obfuscation\ntechniques, showcasing its effectiveness in complex real-world scenarios.\nThe rest of the paper is organized as follows: Section 2 reviews related work.\nSection 3 formalizes the problem and presents our DA-MTL framework. Section 4\nshows experimental results and analysis, followed by our conclusions in Section 5.\n2\nRelated Work\nMachine-Generated Text Detection: The growing capabilities of language\nmodels to generate realistic text have led to a significant increase in research\nefforts for machine-generated text (MGT) detection. Typically framed as bi-\nnary classification, MGT detection distinguishes between machine-generated and\nhuman-written text. Early methods relied on linguistic and statistical discrep-\nancies, employing classical feature-based classifiers [14,39,54]. Recent advances\nhave embraced fine-tuned transformer models such as RoBERTa, achieving per-\nformance improvements in identifying subtle stylistic and structural differences\n[50,18,34,33]. Furthermore, the introduction of comprehensive datasets and cor-\npora, such as those featuring AI-generated responses to consistent instructions,\nhas significantly advanced this field by offering benchmarks [33,34,36,68,18,20].\nAuthorship Attribution: Authorship attribution is a traditional problem in\nNLP, which identifies the author of a text based on linguistic and stylistic cues\n[51]. In the context of LLMs, attributing machine-generated texts is particu-\nlarly important to ensure proprietary and ethical responsibility when using AI-\ngenerated content [64]. LLMs often embed unique stylistic signatures, which have\nbeen utilized by many methods [64,66,23]. Furthermore, advanced models such as\nXLM-R and RoBERTa also demonstrated effectiveness [67]. However, there are\nstill challenges like cross-domain attribution and complex datasets, emphasizing\nthe need for continued advances in this area [53,28]. In adversarial scenarios, it is\nalso crucial that attribution techniques be robust against intentional obfuscation\ntechniques employed to conceal or mask authorship [29,2,37,38].\n\nMulti-task Learning for NLP: Multi-task learning (MTL) has emerged as\na foundational approach to improve performance across related tasks. Initially\nintroduced by Caruana [6], MTL has been shown to improve generalization and\nreduce overfitting by regularizing models through shared knowledge [47,49]. In\nNLP, early applications demonstrated its utility for tasks such as part-of-speech\ntagging and sentiment analysis [4,10], while language models like T5 [45] suc-\ncessfully framed multiple NLP tasks as text generation problems. Cross-lingual\ntasks have also benefited from MTL [56], while optimization techniques have\nbeen developed to address cross-task gradient conflicts [73]. In this paper, we\nextend MTL by applying it to MGT detection and attribution.\n3\nThe DA-MTL Framework\n3.1\nProblem Definition and Assumptions\nWe specify two tasks in our paper: LLM Detection (Task 1 or TD) and LLM\nAttribution (Task 2 or TA). Specifically, Task 1 determines whether a given\ntext is human-written or generated by an LLM. Task 2 identifies the specific\nLLM responsible for generating the text, which involves multiple classes.\nFormally, given a dataset D = {(xi, yTD,i, yTA,i)}N\ni=1, where xi is a piece of\ntext, yTD,i ∈{0, 1} is the binary label for detection, and yTA,i ∈{1, 2, . . . , C} is\nthe multi-class label for attribution, and N is the total number of samples. We\nassume that the data are sampled i.i.d. from a joint distribution P(x, yTD, yTA),\nor P in short. Our goal is to optimize a multi-task classifier with parameters θ\nby maximizing the likelihood for both TA and TD, which can be approximated\nby joint optimization as:\nmin\nθ\nL = Ex∼P(x)\n\n\nX\nT ∈{TD,TA}\nEyT ∼P(yT |x)\n\u0002\nLT (fT (x; θ), yT )\n\u0003\n\n\n(1)\nwhere LD is the binary cross-entropy loss for detection and LA is the categorical\ncross-entropy loss for attribution.\nOur primary objective is to determine whether a given text was written\nby human beings or by one of several possible LLM candidates. We assume a\nblack-box defender, which is widely adopted for AI-generated content detection\n[58,9,40]. Specifically, we have no access to the LLM’s internal weights, archi-\ntecture, or gradients. Instead, our analysis relies solely on the model’s output.\nThis assumption is necessary and practical for two key reasons: (1) Many lead-\ning commercial LLMs, including OpenAI’s GPT-4 [1,43], Google’s Gemini [15],\nand Anthropic’s Claude [5], are proprietary and closed-source. Meanwhile, these\nmodels also have the greatest real-world impact with the largest user base. (2)\nSecurity-critical AI-generated content may appear in public domains without\nclear authorship, such as social media posts or research articles. In such cases, a\nblack-box approach is the most effective and feasible way to analyze and verify\ncontent in real-world forensic scenarios.\n\n3.2\nKey Observations and Motivation\nLLM detection and attribution inherently involve differences in granularity. Al-\nthough optimized to mimic human distribution, LLM outputs typically present\ndistinct differences from natural human data [18], making Task 1 relatively\ncoarse-grained and straightforward. In contrast, Task 2 could be subtle (e.g.,\nChatGPT vs. GPT4All), as different LLMs sometimes share architectural sim-\nilarities and stylistic traits [42]. Toward a better understanding, we showcased\ntwo key observations on the Essay dataset in MGTBench [20]:\n– Observation I: Task 1 is easier. We trained two binary classifiers for\nsimplified Task 1 and 2. The training loss curves, shown in Figure 1, reveal\na significant gap in dynamics. The loss for the human versus ChatGPT\nclassifier decreases rapidly and stabilizes early, indicating that this task is\nmuch easier to learn. In contrast, the loss for the GPT4All versus ChatGPT\nclassifier decreases more slowly and exhibits greater variability, indicating\nincreased difficulty.\n– Observation II: Human data attribution is distinct. We trained a\nsingle multi-class classifier for text generated by human beings as well as\ndifferent LLMs. We visualized the feature representations using PCA. As\nshown in Figure 1, the human data form a distinct cluster, while many LLMs’\nrepresentations overlap significantly. This suggests that a single classifier\nmight struggle to reconcile both coarse and fine-grained distinctions.\nOne alternative solution is to remove human data from attribution and build\na separate, dedicated classifier for detection. However, this approach has two\nlimitations: Firstly, in real-world applications, attributing the authorship of un-\nlabeled text to either a human or one of the LLMs in a unified way is more\ninteresting and practically useful. Secondly, two tasks can hypothetically benefit\nfrom each other: (1) understanding subtle stylistic and distributional differences\namong LLMs enriches the representation, and (2) human-generated content can\nbe a regularizer for attribution, helping with capturing broader linguistic fea-\ntures. We validate these hypotheses in Sec 4.3 by showing how joint training\nimproves both tasks.\nThese observations motivate us to decompose tasks, where we explicitly re-\ntain human-written text in the label space and design a framework to jointly\nhandle detection and attribution. Our goal is to achieve a dual benefit by taking\nadvantage of both human and MGT data to maximize overall performance.\n3.3\nMethodology\nOur framework consists of a shared encoder and two task-specific heads for\ndetection and attribution. Given an input text x, the shared encoder produces\na d-dimensional feature vector h.\nh = SharedEncoder(x, a)\n(2)\n\nFig. 1. Left: The training losses for ChatGPT-Human (the orange line) and ChatGPT-\nGPT4All (the blue line). The training losses decrease much more rapidly when distin-\nguishing human data. Right: PCA visualization of the features, showing a distinct and\nseparate cluster for human data, far from the clusters of LLMs.\nNext, each task-specific head applies a linear transformation to h, yielding\nlogits for the respective classification tasks. For detection (Task 1), we have a\nbinary classification head, and for attribution (Task 2), we have a C-ary classi-\nfication head:\npD(x) = σ(WDh + bD),\npA,j(x) =\neWA,jh+bA,j\nPC\nk=1 eWA,kh+bA,k\n(3)\nwhere WD ∈R1×d, WA ∈RC×d are the weights for detection and attribution\nheads, and bD, bA ∈R are the biases. C is the number of LLM candidates. σ(·)\ndenotes the sigmoid function. The cross-entropy losses for Task 1 and 2 can be\ncomputed as:\nLD = −1\nN\nN\nX\ni=1\nh\nyTD,i log(pD(xi)) + (1 −yTD,i) log(1 −pD(xi))\ni\nLA = −1\nN\nN\nX\ni=1\nC\nX\nj=1\nyTA,i,j log(pA,j(xi))\n(4)\nDuring optimization, gradients for each task are computed independently as\n∇θLD and ∇θLA and then combined to update the model parameters:\nθ ←θ −η (λD · ∇θLD + λA · ∇θLA)\n(5)\nwhere η is the learning rate. In our paper, we set λD = λA = 0.5.\nDesign Choice. Conflicting optimization objectives of different tasks is a com-\nmon issue in multi-task learning, which can hinder performance. However, we\n\nfound that adding PCGrad [73], which projects TD’s gradients onto the nor-\nmal plane of TA’s gradients, only yields minimal improvements but significantly\nincreases the time and GPU memory costs. Therefore, we choose not to use\nPCGrad.\n4\nExperiments\nWe implemented DA-MTL with PyTorch 2.10.2, Transformers 4.44.2, and Python\n3.10.2 on Ubuntu 22.04.3. The experiments are conducted using an Nvidia L4\nGPU and an Intel Xeon CPU. Pre-trained model parameters are obtained from\nHuggingface. For training, we adopt an AdamW optimizer with an initial learn-\ning rate of 1e-5, a maximum of 30 epochs, and an early-stopping strategy to\nstop when the validation F1 score does not increase for six epochs. Detailed\nexperiment settings are listed in Table 1.\nTable 1. Experiment Settings and Details.\nSetting\nDetails\nEnvironment\nUbuntu\n22.04.3,\nPython\n3.10.2,\nPyTorch\nv2.4.1+cu121, Transformers=4.44.2\nHardware\nNVIDIA L4 GPU & Intel Xeon CPU @ 2.20GHz\nOptimizer\nAdamW\nScheduler\nLinear\nTrain/Val Split\n80/20 (if not specified by the dataset)\nHyperparameters\nLR = 1×10−5, Batch Size=8 to 32, Dropout=0.5,\nMax Epochs=30, Weight Decay=0.01, Random\nSeed=42\nEarly Stopping\nVal F1-score does not increase for 6 epochs\n4.1\nBasic Results\nWe first evaluate DA-MTL on mono-lingual tasks. We select English, as it is\nthe most widely studied language in NLP. Specifically, we used nine different\ndatasets from three benchmarks:\n– MGTBench: A benchmark for detecting AI-generated text across multiple\ngenres. It includes three domains: Essay, WritingPrompt (WP), and\nReuters, which cover diverse writing styles, from formal essays to creative\nwriting and news articles [20]. The dataset features text from six major\nLLMs: ChatGPT-turbo, ChatGLM, Dolly, GPT4All, StableLM, and Claude,\nwith each domain containing 1,000 samples per generator.\n– MULTITuDE: A large-scale dataset comprising both AI-generated and\nhuman-written news articles [36]. It includes 74,081 samples spanning 11\nlanguages, generated by eight multilingual LLMs, namely GPT-3, ChatGPT,\n\nGPT-4, Llama-65B, Alpaca-LoRa-30B, Vicuna-13B, OPT-66B, and OPT-\nIML-Max-1.3B.\n– M4: A benchmark for multi-generator, multi-domain, and multi-source AI-\ngenerated text detection. It includes model outputs across various domains\nsuch as Wikipedia, Reddit, WikiHow, PeerRead, and arXiv paper ab-\nstracts. The dataset features text from six LLMs: GPT-4, ChatGPT, GPT-\n3.5, Cohere, Dolly-v2, and BLOOMz-176B, which contains 2,344 to 3,000\nsamples for each generator and domain.\nThe selected datasets comprehensively span a wide range of doamins, includ-\ning academic writing, news articles, open-ended question-and-answer tasks, user\nreviews, and instructional materials. They also represent text generated by two\ncategories of LLMs: (1) Conversational Agents: MGTBench and M4 feature\nresponses from application-level LLMs such as ChatGPT, Claude, ChatGLM,\nand GPT4All, which are designed for real-world deployment via user interface\nor APIs. These models are more likely to be used in practical settings, rais-\ning security concerns; and (2) Foundational NLP Models: MULTITuDE and\nM4 include text data composed by cutting-edge open-source language models\nlike LLaMA, Vicuna, OPT, and Alpaca. These models are widely employed for\nresearch and development, offering a flexible foundation for advanced NLP tasks.\nWe compare DA-MTL with two categories of baseline models that are com-\nmonly adopted in the literature: (1) Feature-based ML Classifiers: These\nclassifiers rely on traditional text embeddings, such as TF-IDF and Bag-of-\nWords. They have been widely used for various text classification tasks, includ-\ning MGT detection [25,26,32,34,31,76,30]. We evaluated four machine learning\nmodels: Multinomial Naive Bayes (MNB), Linear Regression (LR), Support Vec-\ntor Machines (SVM), and Random Forests (RF). (2) Fine-tuned Language\nModels These models are fully fine-tuned on each dataset. We select BERT\n[27,74,25,65,32], RoBERTa [65,31,18,33,36,68], DistilBERT[48,20], and GPT-2\n[74,34]. We also include mBERT [27] and XLM-R [8], which are designed for\nmulti-lingual problems. In total, we evaluate 10 baseline models across nine\ndatasets, resulting in 90 trained baseline classifiers. For these baseline models,\nhuman-written text is treated equally as a separate category alongside LLM-\ngenerated content, framing the problem as a single multi-class classification task.\nTo answer RQ1, we choose four backbone models for comparison with vary-\ning parameter sizes, ranging from 67 million to 278 million parameters: Distil-\nBERT, mBERT, XLM-R, and RoBERTa. These models are trained using DA-\nMTL under the same settings as the baselines. This helps evaluate the general\napplicability of our DA-MTL framework.\nAs shown in Table 2, we have the following findings: (1) Feature-based ma-\nchine learning classifiers perform poorly. Especially on the MULTITuDE dataset,\nthey achieve an average F1 score of just 46.8%. In contrast, fine-tuned lan-\nguage models perform significantly better, with F1 scores exceeding 86% on all\ndatasets except MULTITuDE. However, even for MULTITuDE, their perfor-\nmance remains unsatisfactory, with an average F1 score of 71%. (2) All four\nDA-MTL models outperform their corresponding baselines trained without DA-\n\nMTL. On the Essay, WP, and Reuters datasets, DA-MTL yields average F1 score\nimprovements of 2.7%, 2.1%, and 1.2%, respectively. The largest improvement is\nobserved on MULTITuDE, where DA-MTL boosts performance by an average of\n4.2%. It also enhances F1 scores on M4, improving by 3.1%, 0.5%, 1.7%, 1.2%,\nand 1.7% for arXiv, PeerRead, Reddit, Wikihow, and Wikipedia, respectively.\n(3) We find that DA-MTL is particularly effective on more challenging datasets\n(such as Essay, MULTITuDE, and arXiv) and with less optimal backbone models\n(such as XLM-R). The most substantial single improvement occurs with XLM-R\non MULTITuDE, where DA-MTL increases the F1 score by 6.5%. Across all nine\ndatasets, DA-MTL improves F1 scores by an average of 1.9% for DistilBERT,\n2.0% for mBERT, 2.7% for XLM-R, and 1.6% for RoBERTa. This aligns with\nour expectation that multi-task learning mitigates the dominance of either task,\nwhich is more pronounced in overfitted models with larger parameter sizes. (4)\nAmong all classifiers, RoBERTa with DA-MTL achieves the best performance on\nmost of the datasets, except on MULTITuDE, PeerRead and WikiHow, where\nXLM-R performs best.\nTakeaway I\nDA-MTL is generally applicable and effective across different LLMs and\ndomains.\nEfficiency: DA-MTL is notably lightweight, adding only a few thousand param-\neters compared to the hundreds of millions in the encoder. Although training\ntime per epoch is roughly doubled (because the model is invoked twice), DA-\nMTL achieves good performance in just a few epochs. For instance, DistilBERT\nreaches its optimal performance in 11 epochs, but with DA-MTL, it achieves the\nsame in just 3 epochs. XLM-R with DA-MTL takes 4 epochs to match the perfor-\nmance of the baseline trained with 8 epochs on WP. These results demonstrate\nthat DA-MTL is both effective and efficient in terms of resource usage, time, and\nstorage.\nAblation Study: The balance between detection loss (LD) and attribution\nloss(LA) is crucial for optimizing DA-MTL. By default, we set their weighting\nfactors (λD and λA, as defined in Eq. 5) to 0.5 each. However, our framework\nallows users to adjust these weights based on their specific needs and task pri-\norities. To analyze the impact of this weighting choice, we conduct an ablation\nstudy by varying λD and λA. We evaluate RoBERTa on six datasets where DA-\nMTL achieves the most significant improvements: Essay, WP, and Reuters from\nMGTBench, MULTITuDE, as well as arXiv and Reddit from M4.\nThe results are summarized in Table 3. Increasing the detection weight\n(λD = 0.7, λA = 0.3, λD > λA) slightly improves binary detection performance\nbut significantly reduces attribution accuracy. This supports our observation\nthat detection is generally easier than authorship attribution. Conversely, prior-\nitizing attribution (λD = 0.3, λA = 0.7, λD < λA) enhances LLM differentiation\nbut weakens detection performance. Notably, on three of the six datasets (WP,\nReuters, and Reddit), attribution performance with a higher λA is even lower\n\nthan our default setting. These findings suggest that attribution benefits from\nstrong detection, and setting λD = λA = 0.5 provides a balanced trade-off,\nensuring high detection accuracy while maintaining reliable attribution perfor-\nmance.\nTable 2. Classification F1 scores (%), number of parameters, and model sizes (GB) of\nDA-MTL and SOTA models. WP: WritingPrompt. RT: Reuters. MU: MULTITuDE.\nPR: PeerRead. RD: Reddit. WH: Wikihow. Wiki: Wikipedia. Para.: Number of model\nparameters.\nModel\nEssay WP RT MU arXiv PR RD WH Wiki Para. Size\n(a) Feature-based ML Classifiers\nTF-IDF + MNB\n33.2\n49.1 36.9 34.5\n56.5\n56.0 33.0 31.0\n38.6\n-\n0.12\nTF-IDF + LR\n58.6\n72.5 71.4 55.6\n88.2\n94.4 77.0 76.8\n77.6\n-\n0.06\nBoW + SVM\n68.9\n68.8 81.9 54.3\n88.9\n95.0 81.9 76.9\n76.7\n-\n0.02\nBoW + RF\n66.3\n62.8 76.7 42.6\n82.1\n94.3 76.0 75.9\n78.6\n-\n0.05\n(b) Fine-tuned Language Models\nBERT\n88.6\n86.3 90.2 69.1\n95.7\n98.8 87.7 94.0\n90.0\n110M 0.42\nGPT-2\n89.8\n94.1 94.9 74.9\n97.5\n99.3 93.4 96.3\n92.5\n124M 0.55\nDistilBERT\n87.2\n87.1 91.8 70.4\n96.4\n98.8 92.0 95.1\n90.4\n67M\n0.26\nmBERT\n90.4\n89.9 92.8 70.2\n94.8\n99.0 91.4 94.8\n91.7\n178M 0.68\nXLM-R\n87.3\n87.2 92.1 70.6\n95.2\n98.8 91.8 94.7\n90.0\n278M 1.08\nRoBERTa\n95.7\n94.0 95.2 73.3\n94.4\n99.0 93.2 96.0\n92.7\n125M 0.48\n(c) DA-MTL\nDistilBERT\n91.6\n89.5 93.6 72.8\n98.2\n99.4 92.9 95.9\n91.9\n67M\n0.26\nmBERT\n92.7\n90.8 93.1 75.5\n98.4\n99.3 94.3 95.9\n92.9\n178M 0.68\nXLM-R\n90.3\n90.7 93.2 77.1\n98.1\n99.4 92.6 96.9 93.3\n278M 1.08\nRoBERTa\n96.6 95.6 96.6 75.9\n98.4\n99.3 95.4 96.5 93.3 125M 0.48\nTable 3. F1 scores (%) of RoBERTa trained with DA-MTL and different λD, λA weight\nvalues. WP: WritingPrompt. RT: Reuters. MU: MULTITuDE. RD: Reddit. The highest\nF-1 score result for each dataset is marked in bold.\nλD / λA\nTD\nTA\nEssay\nWP\nRT\nMU\narXiv\nRD\nEssay\nWP\nRT\nMU\narXiv\nRD\n0.3 / 0.7\n99.7\n99.1\n99.8\n95.5\n97.8\n99.4\n96.7\n93.0\n95.7\n74.5\n99.0\n92.7\n0.7 / 0.3\n99.5\n98.6\n99.8\n96.7\n97.8\n99.7\n94.6\n93.8\n95.3\n72.1\n98.4\n92.3\n0.5 / 0.5\n99.8\n99.1 99.9 96.3\n98.4\n99.3\n96.0\n95.0 96.1\n73.4\n98.4\n94.7\n4.2\nCross-modal Proximity Analysis\nTo better understand LLM proximity and address RQ2, we examine the error\npatterns through the confusion matrices of MULTITuDE, which is the most chal-\n\nlenging dataset. We evaluate DistilBERT and XLM-R, which have the largest\nand smallest parameter sizes, respectively. As shown in Figure 2, our analysis re-\nveals consistent error patterns across classifiers: Alpaca-LoRA-30B is frequently\nmisclassified as Text-DaVinci-003 and GPT-3.5, while Vicuna-13B and GPT-3.5\noften confuse classifiers. Interestingly, relatively advanced models like OPT-66B\nare sometimes misclassified as smaller models, like OPT-IML-MAX-1.3B, with\nLLaMA-65B also being a common error. Conversely, smaller models, including\nOPT-IML-MAX-1.3B and those in the GPT family, are less likely to be misclas-\nsified as their more advanced counterparts.\nFig. 2. Confusion matrices of DistilBERT and XLM-R on MULTITuDE.\nThe lineage of LLMs provides valuable insight into their similarities and\ndifferences: (1) OPT and LLaMA (\"OPT-like\"): Both families were de-\nveloped by Meta (formerly Facebook AI Research). Their base models focus\non pure next-token prediction without specific optimization for user interac-\ntions, making their responses less structured and user-friendly. Some variants of\nOPT models, such as OPT-IML and OPT-IML-Max, were fine-tuned by instruc-\ntion following, but they still retain much of OPT-66B’s behavior patterns. Both\nOPT and LLaMA adopt a decoder-only transformer-based architecture, while\nLLaMA achieves higher training efficiency and lower perplexity through bet-\nter stability and data curation. Considering data, both of them leverage large,\ncurated datasets from web crawls, academic corpora, and structured sources\nlike Wikipedia, Reddit, and GitHub [61,75]. (2) Alpaca-LoRA, GPT-3.5s,\nand Vicuna (\"Davinci-like\"): Both Text-Davinci-003 and GPT-3.5 belong\nto OpenAI’s InstructGPT series. Although their exact architectures and hyper-\nparameters remain proprietary, they were known to be trained with large-scale\nReinforcement Learning from Human Feedback (RLHF) [44], which improves\ntheir ability to follow instructions, generate clear responses, and avoid harmful\ncontent. Community-driven models like Alpaca and Vicuna are built on them:\nAlpaca (e.g., Alpaca-LoRA-30B) was fine-tuned using Text-Davinci-003 outputs,\nand Vicuna (e.g., Vicuna-13B) was trained on ShareGPT dialogues, which often\ncontain ChatGPT responses. As a result, these models inherit the stylistic, for-\n\nmatting, and instructional traits of OpenAI’s models. (3) GPT-4: GPT-4 is a\nmore advanced LLM with significant breakthroughs and distinctness. It excels at\nhandling complex tasks with stronger reasoning, coherence, and problem-solving\nabilities than its predecessors.\nTakeaway II\nDA-MTL’s misclassification patterns reveal relationships between LLMs,\nwhich often align with their lineage.\nStylometric Analysis. To better understand the proximities of the LLMs,\nwe analyze their writing styles using NELA [21], a framework that evaluates\ntext based on 87 features across six categories: style, complexity, bias, affect,\nmorality, and events. From each LLM-generated sample, we extracted a NELA\nfeature vector. We identified the eight most important features using the Chi-\nsquared test: singular proper nouns (NNP) count, past tense verbs (VBD) count,\ntotal word count, Flesch-Kincaid readability score, positive opinion word count,\nWord-level Positive Sentiment Score (wpos), Sentence-level Negative Sentiment\nScore (sneg), and explicit date mentions. Figure 3 illustrates the average feature\nvalues of LLM-generated text in MULTITuDE dataset.\nFig. 3. The average normalized values of the eight most important NELA features are\nshown. The features are listed in order of importance, from left (blue) to right (gray):\nNNP, VBD, total word count, Flesch-Kincaid readability score, positive opinion word\ncount, wpos, sneg, and explicit date count.\nOur analysis reveals that OPT-66B and OPT-IML-MAX-1.3B are the most\nsimilar, while Vicuna-13B, GPT-3.5, and Text-Davinci-003 form another closely\nrelated group. However, notable intra-group differences exist among the \"OPT-\nlike\" models, \"Davinci-like\" models, and GPT-4. These align with our obser-\nvations derived from our DA-MTL model’s confusion matrices. Specifically, we\nhave the following findings:\n– OPT and LLaMA: OPT models frequently use singular proper nouns,\npast tense verbs, and explicit dates, while showing minimal sentiment word\nusage. Their responses are lengthy, though shorter than the ones generated\nby GPT-4, and are complex and harder to read. LLaMA-65B shares these\npatterns but with an even stronger preference for proper nouns and explicit\ndates while avoiding sentiment expressions. LLaMA’s responses are slightly\n\neasier to read than OPT’s but still more complex than those of the other\nLLMs.\n– Vicuna, GPT-3.5, Text-Davinci-003, and Alpaca: These models ex-\nhibit highly similar patterns: they use far fewer singular proper nouns, past\ntense verbs, and explicit dates, which is nearly half as frequently as OPT\nmodels. Instead, they incorporate significantly more sentiment and opinion\nwords, which is almost 50% more than OPT and LLaMA. They generate\nshorter and more readable responses. Alpaca follows this trend but stands\nout with the shortest responses (under 150 words on average) and the highest\nreadability.\n– GPT-4: GPT-4 shares some similarities with \"Davinci-like\" models but\nstands out with significantly longer responses, averaging nearly 300 words.\nDespite this, its readability remains low, comparable to Text-Davinci-003.\nAdditionally, GPT-4 uses more sentiment and opinion words than Vicuna,\nGPT-3.5, Text-Davinci-003, and Alpaca.\nHowever, while NELA feature analysis provides valuable insights into LLM\nbehaviors, it does not fully explain their differences. To validate this, we trained\ntwo ML models, SVM and LR, using NELA features as text embeddings. Both\nmodels performed poorly, with F1 scores of only 41.9% (SVM) and 31.6% (LR).\nThis suggests that LLM stylistic differences involve more subtle factors beyond\nwhat NELA features can represent.\n4.3\nCross-Lingual Analysis\nMulti-Lingual Text Detection and Attribution. We use mBERT and XLM-\nR for multi-lingual tasks. To better analyze the impact of different languages,\nwe exclude highly correlated and potentially confusing models (e.g., OPT-66B\nversus OPT-IML-MAX-1.3B) and focus on five representative models: OPT-\nIML-MAX-1.3B, LLaMA-65B, GPT-4, Text-DaVinci-003, and Vicuna-13B. As\nshown in Table 4, the classifier’s F1 scores improve for both Task 1 and 2 after\napplying DA-MTL. Across nine languages, mBERT gains an average increase of\n2.96% for Task 1 and 2.67% for Task 2, while XLM-R shows improvements of\n1.78% and 2.6%, respectively. These results confirm that the two tasks com-\nplement each other. Among the languages, Dutch (NL) and Portuguese (PT)\nproved to be the most challenging for both tasks. However, incorporating DA-\nMTL significantly improved detection F1 scores for these two languages, reaching\nF-1 scores of 99.83% and 98.17%, respectively. Moreover, no signs of overfitting\nare shown for either task when applying DA-MTL. This again aligns with our\nhypothesis that DA-MTL serves as a form of regularization to prevent overfitting\nto single tasks.\nTakeaway III\nDA-MTL significantly improves multilingual detection and attribution,\nespecially in challenging languages.\n\nTable 4. F1 scores (%) of mBERT and XLM-R w/o and with DA-MTL by language.\nBlue and red numbers show the differences. Gains larger than 3% are in bold.\nTasks Models\nEN\nES\nRU\nUK\nCS\nDE\nNL\nCA\nPT\nTD\nmBERT 98.02\n97.93\n96.33\n96.49\n98.45\n96.24\n96.13\n97.93\n95.17\n+MTL\n(+2.5) (+1.5) (+0.5) (+1.6) (+3.1) (+3.9) (+3.7) (+2.9) (+3.0)\nXLM-R\n97.25\n97.87\n98.80\n96.34\n96.19\n96.87\n95.53\n97.76\n95.36\n+MTL\n(+1.5) (+1.5) (+2.3) (+1.6) (+0.6) (+2.1) (+2.0) (+2.0) (+2.4)\nTA\nmBERT 91.45\n94.60\n95.73\n91.20\n91.43\n89.39\n87.60\n90.03\n87.69\n+MTL\n(+2.9) (+4.6) (+3.5) (+6.7) (+2.5) (+1.8)\n(-0.1)\n(-0.3)\n(+2.4)\nXLM-R\n91.36\n93.84\n96.58\n90.92\n91.21\n87.80\n88.76\n90.89\n82.54\n+MTL\n(+2.1) (+3.4) (+3.5) (+3.7) (+3.9) (+1.0) (+4.9) (-0.4)\n(+1.3)\nCross-Lingual Generalization. We follow [36] to evaluate cross-lingual gen-\neralization, addressing RQ3. Figure 4 presents the cross-validation performance\nof mBERT and XLM-R on Tasks 1 and 2.\nFig. 4. Cross-Lingual validations of models fine-tuned on MULTITuDE. From left to\nright: mBERT for Task1 & Task 2 and XLM-R for Task 1 & Task 2.\n\nEach model was fine-tuned on individual source languages, and results show\nthat they can generalize to other languages. However, their performance is gen-\nerally lower than when fine-tuned directly on the target languages. Notably,\nXLM-R fine-tuned on Ukrainian transfers well to Russian and vice versa, demon-\nstrating strong cross-lingual performance among closely related Slavic languages\nthat share the Cyrillic script. In Task 2, performance varies widely, with mBERT\nachieving accuracies between 21.31% and 88.7%, while XLM-R ranges from\n23.58% to 91.07%.\nFurther analysis reveals that language families and writing systems signifi-\ncantly impact cross-lingual transfer. For instance, models generalize well among\nRomance languages like Spanish, Portuguese, and Catalan. Most source lan-\nguages transfer effectively to Czech, which uses a Latin script. Fine-tuning on\nEnglish or Spanish typically results in strong generalization across multiple lan-\nguages, whereas models fine-tuned on Ukrainian or Russian exhibit poor trans-\nferability except to Czech. Interestingly, models trained on English show moder-\nate performance on other Germanic languages, particularly Dutch and German,\nlikely due to English’s historical divergence and influence from Latin-based lan-\nguages.\n4.4\nRobustness\nWe assess the robustness of DA-MTL against two widely used authorship obfus-\ncation techniques:\n– Mutant-X (MX): A stylometric obfuscation method that uses genetic al-\ngorithms [38]. It alters text by replacing words with synonyms and rearrang-\ning phrases while maintaining semantic similarity. These modifications are\nguided by a fitness function that balances authorship attribution probability\nwith meaning preservation. We use the publicly available implementation\nwith its default settings †.\n– Back-Translation (BT): A technique that translates text into another\nlanguage and then back to the original [29,2]. This process changes word\nchoice and sentence structure while preserving meaning. For our experi-\nments, we use French as the intermediate language, utilizing Opus MT mod-\nels for English-to-French (\"en-fr\") and French-to-English (\"fr-en\") transla-\ntion [59,60].\nTable 5 shows how applying these obfuscation techniques affects the accuracy\nof our XLM-R classifier trained with DA-MTL. We conduct this evaluation us-\ning 500 randomly selected English text samples from MULTITuDE. Our results\nindicate that while the model remains generally robust, the extent of accuracy\ndegradation varies across LLMs. For Mutant-X, the classifier is most resistant\nwhen analyzing text from OPT-66B, where accuracy decreases by only 0.61%,\nsuggesting that text generated by OPT-66B has fewer distinct stylistic features,\n† https://github.com/asad1996172/Mutant-X\n\nmaking it harder to manipulate. In contrast, text from GPT-4 is the most af-\nfected, with accuracy dropping by 9.03%, indicating that its unique vocabulary\nand structured writing style make it more vulnerable to synonym substitutions\nand word-order changes. For Back-Translation, the classifier performs best on\ntext from Vicuna-13B, with only a 4.38% accuracy drop. However, it struggles\nsignificantly with text generated by GPT-3.5, Text-DaVinci-003, and LLaMA-\n65B, where accuracy declines by 14% to 17%. Interestingly, we find a negative\ncorrelation between the model’s initial classification accuracy and its resilience\nto Back-Translation. Texts that were harder to classify originally suffer the most\nafter translation, likely because the process disrupts subtle linguistic patterns.\nTable 5. Accuracy differences (%) against authorship obfuscation. N/A: Original ac-\ncuracy on original text. MX: Mutant-X. BT: Back-Translation.\nAtk. GPT-3.5 GPT-4 DaVinci Vicuna OPT OPT-IML Alpaca LLaMA\nN/A\n86.43\n91.70\n59.57\n91.75\n52.18\n91.31\n67.29\n57.40\nMX\n-2.68\n-9.03\n-2.53\n-4.75\n-0.61\n-2.86\n-2.67\n-5.22\nBT\n-14.2\n-8.31\n-14.5\n-4.38\n-10.4\n-8.64\n-14.9\n-17.0\nThese findings demonstrate that the robustness of DA-MTL varies depending\non the LLM source of the analyzed text. While some LLM-generated texts are\nnaturally more resistant to stylometric obfuscation, others are more susceptible,\nmaking it easier for adversaries to evade attribution. Although DA-MTL exhibits\nstrong real-world applicability against mild obfuscation techniques, it is not ex-\nplicitly designed to maximize robustness. We do not claim that DA-MTL offers\nsuperior resistance to adversarial attacks. In future work, we aim to explore\nadaptive defenses that dynamically adjust to different obfuscation strategies and\ndevelop more resilient authorship attribution methods that leverage deeper lin-\nguistic and structural features.\n5\nConclusion\nIn this paper, we introduce DA-MTL, a multi-task learning framework designed\nto detect and attribute LLM-generated text jointly. We highlight the benefits\nof multi-task learning by evaluating on nine datasets and with four different\nbackbone models. The results show that multi-task learning improves perfor-\nmance for both tasks and in both English and multi-lingual contexts. Using our\naccurate models, we further analyze cross-modal and cross-lingual similarities\nand evaluate DA-MTL against various authorship obfuscation techniques. Our\nwork offers an effective tool as well as valuable insight into LLM behaviors and\nproximity, contributing to a deeper understanding of AI-generated content and\nits associated security and safety risks."}
{"id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "authors": ["Johannes Zschache", "Tilman Hartwig"], "source": "http://arxiv.org/abs/2508.14170v1", "page_count": 25, "text": "Comparing energy consumption and accuracy in\ntext classification inference\nJohannes Zschache and Tilman Hartwig\nApplication Lab for AI and Big Data, German Environment Agency,\nAlte Messe 6, Leipzig, 04103, Saxony, Germany.\n*Corresponding author(s). E-mail(s): tilman.hartwig@uba.de;\nContributing authors: johannes.zschache@uba.de;\nAbstract\nThe increasing deployment of large language models (LLMs) in natural language\nprocessing (NLP) tasks raises concerns about energy efficiency and sustainabil-\nity. While prior research has largely focused on energy consumption during\nmodel training, the inference phase has received comparatively less attention.\nThis study systematically evaluates the trade-offs between model accuracy and\nenergy consumption in text classification inference across various model archi-\ntectures and hardware configurations. Our empirical analysis shows that the\nbest-performing model in terms of accuracy can also be energy-efficient, while\nlarger LLMs tend to consume significantly more energy with lower classifica-\ntion accuracy. We observe substantial variability in inference energy consumption\n(<mWh to >kWh), influenced by model type, model size, and hardware spec-\nifications. Additionally, we find a strong correlation between inference energy\nconsumption and model runtime, indicating that execution time can serve as\na practical proxy for energy usage in settings where direct measurement is not\nfeasible. These findings have implications for sustainable AI development, provid-\ning actionable insights for researchers, industry practitioners, and policymakers\nseeking to balance performance and resource efficiency in NLP applications.\nKeywords: NLP, Large Language Model, Resource Efficiency, Sustainable AI\n1 Introduction\nArtificial intelligence (AI) systems, particularly large language models (LLMs), have\ndriven remarkable progress in Natural Language Processing (NLP) applications. This\n1\n\ndevelopment has been enabled by the Transformer architecture (Vaswani et al., 2017)\nand exemplified by the emergence of large-scale models such as GPT-3 (Brown et al.,\n2020), which have significantly advanced task performance. However, this progress\nhas come at a cost: the escalating energy demands of AI systems pose significant\nenvironmental and computational challenges. Data centers that support AI com-\nputations are major electricity consumers, often dependent on fossil fuels, thereby\ncontributing to greenhouse gas emissions (Lacoste et al., 2019; Axenbeck et al.,\n2025). This increasing energy demand challenges global climate objectives such as\nthe Paris Agreement (United Nations, 2015a) and the United Nations’ Sustainable\nDevelopment Goals (SDGs), specifically Goal 13 on climate action (United Nations,\n2015b). Consequently, designing energy-efficient AI systems is imperative for aligning\ntechnological advancements with sustainability goals. Moreover, beyond sustainabil-\nity, energy-efficient models offer additional advantages, including reduced hardware\nrequirements, lower financial costs, and faster inference times.\nWhen evaluating machine learning models, most studies concentrate on the quality\nof the model responses by tracking e.g. the accuracy, the RMSE, or other measures.\nAnd even if the energy consumption is taken into account, prior research has mainly\nfocused on the training phase (Strubell et al., 2019; Patterson et al., 2021; Luccioni\nand Hernandez-Garcia, 2023). The inference phase, which is repeatedly executed in\nreal world deployments, has received comparatively less attention. However, energy\nefficiency during the operational phase is an increasingly relevant topic as LLM appli-\ncations become ubiquitous and LLM models are trained to use additional test-time\ncompute to improve performance (OpenAI, 2024; DeepSeek-AI, 2025). Addressing this\ngap, we present a systematic study on the energy consumption of language models\nduring inference, providing actionable insights for balancing accuracy with efficiency.\nA particularly popular machine learning task is text categorization, a task that\nlightweight models have been shown to handle effectively. For instance, Joulin et al.\n(2017) show that a simple classifier built on word embeddings is often as accurate as\ndeep learning classifiers. Despite this, some authors argue for the use of pre-trained\nLLMs for text classification because it reduces the need for model training and sim-\nplifies data preprocessing (Wang et al., 2024). Additionally, popular software tutorials\npromote LLMs for classification tasks (LangChain Team, 2023; Lamini Team, 2023),\nfurther encouraging their use even when more efficient alternatives exist. In order to\njustify the usage of LLM in relatively simple tasks such as text categorization, we\nadvocate a consequent comparison of a model’s response quality to its energy efficiency.\nGiven a practical use case that is occurring in public administration, our study\nempirically analyzes trade-offs between model accuracy and energy consumption across\nvarious language models and hardware configurations. We find that the best perform-\ning model is energy efficient while LLMs show higher energy usage with lower accuracy.\nGenerally, we see significant variability in inference energy consumption, influenced\nby model type, model size, and hardware specifications. Additionally, the energy con-\nsumption during inference is shown to highly correlate with the model’s runtime. This\nmakes the duration of computations a valuable proxy measure for energy consump-\ntion in settings where the latter cannot be traced. Our findings have implications\nfor researchers, industry practitioners, and policymakers advocating for sustainable\n2\n\nAI development (Kaack et al.; Luccioni et al., 2025). By systematically evaluating\ninference efficiency and runtime across architectures and hardware settings, we con-\ntribute to the ongoing discourse on AI’s environmental impact and provide actionable\nguidelines for optimizing NLP applications for both performance and sustainability.\n2 Previous research\nResearch on the environmental impact of machine learning (ML) has primarily focused\non the energy consumption and carbon emissions produced during the training phase of\nlarge-scale models. Most famously, Strubell et al. (2019) quantify the carbon footprint\nof NLP models, revealing that the training of a single large-scale transformer model\ncan emit as much carbon as five cars over their entire lifetimes (their measurements\ninclude thousands of hyperparameter tuning jobs, which makes it difficult to disen-\ntangle model-inherent efficiency from experimental setup). This seminal work spurred\nfurther investigations into the environmental costs of training neural networks, includ-\ning large language models (Patterson et al., 2021; Luccioni and Hernandez-Garcia,\n2023; Patterson et al., 2022).\nWhile training remains a significant contributor to energy consumption, recent\nstudies have begun to focus on the inference phase. Samsi et al. (2023) highlighted\nthe substantial energy demands of LLM inference but did not explore the relationship\nbetween energy consumption and task-specific performance. Liu et al. (2022) under-\nscore the importance of evaluating NLP models not just on efficiency metrics but also\non accuracy by introducing the Efficient Language Understanding Evaluation (ELUE)\nbenchmark. ELUE aims to establish a Pareto frontier that balances performance and\nefficiency. It includes various language understanding tasks, facilitating fair and com-\nprehensive comparisons among models. However, the framework adopts number of\nparameters and FLOPs as the metrics for model efficiency, disregarding hardware\nspecific factors. Similarly, Chien et al. (2023) estimate the energy consumption associ-\nated with the inference phase of generative AI applications based on the output word\ncount and several assumptions about the application such as the number of FLOPS\nper inference and the sampling rate.\nIn contrast, we promote energy-efficient NLP models by the direct measurement\nof the power consumed during inference. Hence, our work follows the approach of the\nSustaiNLP 2020 shared task (Wang and Wolf, 2020). SustaiNLP demonstrated that\nsubstantial energy savings are achievable with minimal performance loss. While this\nstudy was limited to the performance of a couple of small language models on a single\nbenchmark, we extend these efforts to a greater number of partially very large models\ndeployed to a practical inference scenario.\nThis makes our study very similar to the one by Alizadeh et al. (2025), who inves-\ntigated the trade-offs between accuracy and energy consumption when deploying large\nlanguage models (LLMs) for software development tasks. Besides the finding that\nlarger LLMs with higher energy consumption do not always yield significantly bet-\nter accuracy, the authors demonstrated that architectural factors, such as feedforward\nlayer size and transformer block count, directly correlate with energy usage.\n3\n\nFinally, Luccioni et al. (2024) provide one of the most comprehensive analyses\nof energy consumption during ML model inference. Their study systematically com-\npared the energy costs of 88 models across 10 tasks and 30 datasets, including both\nsmaller task-specific and larger multi-purpose models. They found that the larger\nmodels are orders of magnitude more energy-intensive than smaller task-specific ones,\nespecially for tasks involving text and image generation. Furthermore, their research\nunderscores the variability in energy consumption across tasks and model architec-\ntures. The authors advocate for increased transparency and sustainable deployment\npractices, emphasizing that the environmental costs of deploying large, multi-purpose\nAI systems must be carefully weighed against their utility.\n3 Data and methods\nOur experiments are inspired by an occasionally occurring use case in public admin-\nistration: the management of objections that are submitted by the population. Due\nto a potentially very large amount of submissions, an automatic preprocessing of the\nobjections is of high value. One of the possible steps of an automated workflow is to\ncategorize each submission for optimal forwarding to the responsible department.\nThe data of our study originates from the process of selecting a repository site\nfor high-level radioactive waste in Germany. During the first phase, sub-areas were\nidentified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The\nstatements from the population were categorized, processed and published as the\nFKTG-dataset (https://beteiligung.bge.de/index.php). The text of the submission is\ngiven by the column ‘Beitrag’ (input). The column ‘Themenkomplex’ (topic) contains\nthe category of the text.\nWe scraped the dataset from the website and restricted it to entries for which the\ntopic occurs at least 10 times. The remaining 378 entries were split into half: 189\nentries for training and 189 entries for testing. This unusual 50:50 split was done so\nthat the test set should be sufficiently representative by containing enough examples\nof each of the 14 categories. Each of the following experiments was repeated 10 times\nwith different train-test-splits. To increase comparability, every experiment was run\nwith the same 10 train-test-splits.\nAn experiment run consists of a training phase and a testing phase. Since large\nlanguage models have been argued to be applicable to text categorization without\ntraining (zero-shot), we omit the training phase for these models and apply LLMs\nwithout fine-tuning. We report the energy consumption and accuracy only for the test\nphase as averages over all runs.\n3.1 Traditional models\nBesides LLMs, we initially run the experiments with lightweight NLP models that we\ncall traditional because they have been used for categorization tasks long before LLMs\nexisted. Specifically, we use a linear model (logistic regression) and a gradient boosting\nalgorithm (xgboost). Logistic regression is a simple, interpretable model that estimates\nthe probability of a class based on a linear combination of input features. XGBoost\n4\n\n(Extreme Gradient Boosting) is an efficient, scalable machine-learning algorithm that\ncombines predictions from multiple decision trees to improve accuracy.\nWe consider three different types of features: bag-of-words (BoW), term frequency-\ninverse document frequency (TF-IDF), and a pretrained multilingual sentence\nembedding. BoW represents text by counting word occurrences without consider-\ning order, while TF-IDF adjusts word counts by their importance across documents,\ncapturing rare but informative terms. The TF-IDF features are calculated on all\n2-gram and 3-gram character sequences, which capture local patterns in the text.\nThe multilingual sentence embedding (https://huggingface.co/sentence-transformers/\nparaphrase-multilingual-mpnet-base-v2) provides dense vector representations of text,\npreserving semantic meaning across languages. This embedding is not fine-tuned on\nthe training data. Both models are trained using the default parameters provided by\nsklearn.linear model.LogisticRegression and xgboost.XGBClassifier.\n3.2 Large language models\nLarge language models (LLMs) were applied without training (zero-shot) using the\ntest set only. Table 1 gives the names and sources of the models used. The LLMs were\nselected by the following criteria:\n• availability on Huggingface\n• support of german language\n• capability of processing the dspy-prompt (see appendix A)\nAdditionally, Jamba Mini 1.5 was chosen as model with an alternative architec-\nture that includes next to Transformer also Mamba layers (a state-space model). The\nDeepseek distillations (DS) were added to include models with reasoning capabilities\n(test-time compute).\nModel\nLink\nLlama 3.1 8B\nhttps://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\nLlama 3.1 70B\nhttps://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\nQwen 2.5 7B\nhttps://huggingface.co/Qwen/Qwen2-7B-Instruct\nQwen 2.5 72B\nhttps://huggingface.co/Qwen/Qwen2-72B-Instruct\nPhi 3.5 Mini\nhttps://huggingface.co/microsoft/Phi-3.5-mini-instruct\nPhi 3.5 MoE\nhttps://huggingface.co/microsoft/Phi-3.5-MoE-instruct\nJamba Mini 1.5\nhttps://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini\nDS Qwen 14B\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\nDS Qwen 32B\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\nDS Llama 8B\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\nDS Llama 70B\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nTable 1 Selection of large language models\n3.3 Computing Resources\nWe used different computing systems for a comparative analysis of energy efficiency\nacross diverse hardware architectures. This enables the assessment of how architectural\n5\n\ndifferences - especially GPU tensor core capabilities - affect the inference speed and\npower usage. A diversity in computational infrastructure is crucial for generalizing\nfindings across different environments and ensuring the validity and replicability of\nexperimental results in machine learning research. Furthermore, insights gained from\nusing multiple platforms contribute to optimizing resource allocation strategies and\nimproving cost-effectiveness in large-scale machine learning projects.\nTo run our experiments, we were granted access to the high-performance computing\n(HPC) systems of TUD Dresden University of Technology (https://doc.zih.tu-dresden.\nde/) and Leipzig University (https://www.sc.uni-leipzig.de/). For GPU-accelerated\ncomputing, three different systems are available named Capella, Paula, and Clara\n(see Table 2). The main difference for our study is the GPU: while a node on the\nCapella cluster is equipped with 4 x H100, there are 8 x A30 on each node on Paula\nand 4 x V100 on Clara. This means that a large model such as Llama 3.1 70B or\nQwen 2.5 72B fits on a single node of Capella (requiring 2 GPUs) or Paula (requiring\nall 8 GPUs) but takes up two nodes of the Clara cluster (assuming a 16-bit floating\npoint representation of the parameters).\nCluster\nCapella\nPaula\nClara\nHPC center\nTUD Dresden\nUniversity of Technology\nLeipzig University\nLeipzig University\nnumber of nodes\n144\n12\n6\nCPU per node\n2 x AMD (32 cores)\n2.7GHz\n2 x AMD (64 cores)\n2.0GHz\n1 x AMD (32 cores)\n2.0GHz\nRAM per node\n768 GB\n1 TB\n512 GB\nGPU per node\n4 x NVIDIA H100\n(94GB)\n8 x NVIDIA A30\n(24 GB)\n4 x NVIDIA V100\n(32GB)\nsingle GPU max\npower consumption\n700W\n165W\n250W\nTable 2 HPC Resources\nLLMs were deployed using the vllm library (https://github.com/vllm-project/\nvllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.\nIf a model is too large to be deployed on a single GPU, the model weights are dis-\ntributed over multiple GPUs, which allow for a parallel computation of the activations\n(c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two\ncomputing nodes are needed, the model is split into two parts and executed sequen-\ntially (c.f. pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model\npart on the first node and then the model part on the second node.\nThe energy consumption and the runtime of the inference phase were measured\nby the CodeCarbon package (https://github.com/mlco2/codecarbon). This package\nuses the NVIDIA Management Library (NVML) and the Intel RAPL files to track\nthe power usage of GPU and CPU (https://mlco2.github.io/codecarbon/methodology.\nhtml#power-usage). The power consumption of the memory is flatly added with\n0.375W/GB of memory used. In settings where the model is deployed on more than\none node, the inference duration is taken as the maximum and the energy as the sum\nover all nodes.\n6\n\nVarious\nsoftware\ntools\nhave\nbeen\ncreated\nto\nmonitor\nenergy\nconsump-\ntion during the application of machine learning models (https://github.com/\ntiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%\nB8%8F-energy-metrics). Similar to CodeCarbon, Carbontracker (Anthony et al.,\n2020) and experiment-impact-tracker (Henderson et al., 2020) estimate energy con-\nsumption by monitoring hardware usage. In some settings, CodeCarbon is considered\nmore accurate, yielding values closer to those obtained via physical wattmeters\n(Bouza et al., 2023). Comparing different tools of energy monitoring is beyond the\nscope of our paper.\n4 Results\nFor each model, we report accuracy, energy consumption, and inference duration.\nThe energy consumption and duration were measured only for the inference step, i.e.,\nafter the model and data were already loaded. One inference run involves classifying\n189 text samples from a test set. All tables and figures present the average results\nover 10 runs on different test sets, with the same 10 test sets used for each model.\nMeasurement variance was generally low: < 0.002 for accuracy, and < 0.2 dex for both\nenergy consumption and duration (logarithmically scaled to base 10).\nFigure 1 illustrates the trade-off between energy consumption and accuracy across\nall models. For these experiments, a single node of the Capella system was used. The\nminimum number of H100 GPUs required varies by model (see Table B1).\nThe highest accuracy was achieved by a traditional linear model using pre-trained\nsentence embeddings. Notably, even the most energy-efficient model - a linear model\nwith TF-IDF features - outperformed several large language models (LLMs). Among\nLLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes\nseven times less energy than the most accurate model (Qwen 2.5 72B), with only\na minor accuracy reduction of 0.07 points. Deepseek models, despite their extensive\nreasoning processes during inference, exhibit lower accuracy than non-reasoning LLMs\nwhile consuming significantly more energy and taking longer to complete inference.\n4.1 Analysis of hardware settings\nThis section analyzes the impact of different hardware configurations (see Tab. 2) on\nenergy consumption. We focus on GPU usage due to its dominant role in machine\nlearning inference.\nAs shown in Figure 2, GPU consumption accounts for the largest share of total\nenergy usage in all experiments. The only exceptions are traditional models without\nembeddings, which do not use the GPU during inference.\n4.1.1 Varying the Number of GPUs\nWe examined the effect of varying the number of GPUs on energy consumption and\ninference duration. Most LLMs were tested on 1, 2, or 4 GPUs on a single Capella\nsystem node. Larger models (Qwen 72B, Phi MoE, Llama 70B, Jamba Mini, and DS\nLlama 70B) required either 2 or 4 GPUs. Increasing the number of GPUs consistently\n7\n\nFig. 1 Accuracy-energy-trade-off of all models for the inference task on the Capella system (single\nnode). The energy consumption for the same task spans over six orders of magnitude with traditional\nmodels being the most energy-efficient models and reasoning models are most energy-consuming. The\nbest model for this specific task is a traditional model (Linear Embedding) with moderate energy\nconsumption.\nreduced inference duration but did not reduce energy consumption. In some cases,\nenergy consumption increased due to the additional GPUs in operation (see Figure 3).\n4.1.2 Varying the Number of Nodes\nWhile large models can often be executed on a single computing node, certain hardware\nlimitations or shared high-performance computing (HPC) environments may necessi-\ntate using multiple nodes. In shared systems, it is often easier to access two nodes\nwith half the number of available GPUs than a single node with all its GPUs, due\nto scheduling constraints and resource allocation policies. However, deploying mod-\nels across multiple nodes increases network communication overhead and significantly\nraises energy consumption.\nWe evaluated this effect for the largest models on the Capella system by comparing\na ‘single-node’ configuration (2 GPUs on one node) with a ‘double-node’ configuration\n(1 GPU on each of two nodes). For the double-node configuration, energy consumption\nwas summed across both nodes and averaged over 10 runs, while the reported duration\nreflects the average of the maximum value between the two nodes.\nAs shown in Figure 4, using two nodes increased energy consumption by a factor\nthat depends on the model (see also Table B2). This increase stems from the overhead\n8\n\nFig. 2 Energy consumption of all models for the inference task on the Capella system (single node)\nFig. 3 Effects of the number of GPUs on the runtime and consumed energy (Capella, single node).\nDeepseek models are not shown.\nof coordinating across nodes. Inference duration also increased by the same factor\ndue to the sequential execution of model components and the required inter-node\ncommunication.\n4.1.3 Comparing GPU Architectures\nFinally, we compared the energy efficiency of different GPU architectures (see Figure\n5 and Table B3). Interestingly, the expected efficiency gains from using the more pow-\nerful H100 instead of V100 or A30 GPUs were only observed for the Deepseek models.\nThis discrepancy is likely to arise because Deepseek models engage in extended rea-\nsoning by generating a larger output of words before making a classification decision.\nConsequently, the efficiency of H100 GPUs becomes evident only when substantial\n9\n\nFig. 4 Comparison single node vs. double node deployment (Capella).\ntext is generated. For models generating a single token per inference, a V100 or even\na A30 GPU is more efficient in inference.\nFig. 5 Comparison of different GPU cards: four exemplary LLMs. Single node deployment.\n4.2 Linear relationship between duration and energy\nIn most of the tables in appendix B, we report both the duration of each inference\nrun and its corresponding energy consumption. Since energy is the integral of power\nover time, these two measures exhibit a strong correlation. If the power is constant\nover time, this correlation should be linear. Figure 6 illustrates this relationship for\n10\n\nall experiments conducted on a single node of the Capella cluster. When controlling\nfor the number of GPUs used for model deployment, the relation between duration\nand energy is approximately linear. Therefore, the duration appears to serve as a good\nproxy for the energy consumed.\nFig. 6 Plotting the relationship between duration and energy consumption (single node on Capella).\nThe lines are added by running a linear regression model.\nTo further quantify the relationship between duration and energy consumption,\nwe performed a linear regression analysis for each hardware configuration (see Table\n3). This analysis includes all experiments, regardless of the number of nodes used\nfor model deployment. The consistently high R2 values across all configurations indi-\ncate that, for a given hardware setup, duration and energy consumption are nearly\ninterchangeable as measures of computational effort.\nMoreover, when the regression coefficients are known for a specific computing sys-\ntem, energy consumption can be reliably estimated from the duration and the number\nof GPUs. Only the coefficients of duration (a) and of the interaction term dura-\ntion:GPUs (c) are statistically significant. The other coefficients (b and d) are omitted\nfrom the approximation:\nEnergy ≈(a + c · GPUs) · Duration.\n(1)\nFor instance, on the Capella system, the following approximation holds for any\ncomputation:\n11\n\nEnergy\n1 Wh\n≈(0.1 + 0.09 · GPUs) · Duration\n1 s\n.\n(2)\nThis relationship suggests that, under fixed hardware conditions, monitoring the\nduration of computations provides an efficient means of estimating energy usage with\nminimal additional measurement overhead.\nDependent variable: Energy\nCapella\nClara\nPaula\n(1)\n(2)\n(3)\nDuration (a)\n0.097∗∗∗\n0.061∗∗∗\n0.079∗∗∗\n(0.008)\n(0.002)\n(0.026)\nGPUs (b)\n−0.500\n0.048\n−2.195\n(2.297)\n(0.339)\n(3.472)\nDuration:GPUs (c)\n0.090∗∗∗\n0.036∗∗∗\n0.054∗∗∗\n(0.004)\n(0.0002)\n(0.004)\nConstant (d)\n−6.205\n−0.826\n3.328\n(5.725)\n(1.368)\n(17.220)\nObservations\n44\n19\n23\nR2\n0.998\n1.000\n0.989\nAdjusted R2\n0.998\n1.000\n0.987\nNote:\n∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\nTable 3 Linear regression of energy consumption on\nduration (table format by Hlavac, 2022). The numbers\n(coefficients) give the estimated effects of each predictor on\nthe dependent variable. A positive coefficient means the\nvariable increases the outcome, while a negative coefficient\nmeans it decreases the outcome. The standard error (in\nparenthesis) estimates the variability of the coefficient\nestimate. The p-value (given by the asterisks) indicates\nwhether the predictor is statistically significant (different\nfrom zero).\n5 Discussion\nWe would like to mention the limitations of our study, which also point to the areas\nof future research. First, while traditional models were trained on approximately 200\nexamples, the large language models (LLMs) were applied in a zero-shot setting,\nmeaning they had no access to labeled examples. Previous research has shown that\nfew-shot prompting - where representative examples are included in the prompt - can\nimprove performance (Brown et al., 2020). For the present study, we kept the prompt\n12\n\nas simple as possible (see section A). But in an actual application, we would add back-\nground information about the data and the categories. In general, prompt engineering,\nthe addition of representative examples to the prompt, or even fine-tuning an LLM\ncould yield higher accuracy rates. On the other hand, energy efficiency in LLMs can\nbe improved through model quantization, which reduces computational demands by\ncompressing model parameters (Jacob et al., 2017).\nSecond, we do not account for the energy costs associated with training the tradi-\ntional models because it is infeasible to compare them to the training costs of LLMs.\nThe LLMs used in this study were pre-trained by external organizations and made\npublicly available. As a result, the energy costs of training are distributed among all\nusers, making it difficult to estimate per-user energy consumption. Even if training\nenergy costs for an LLM were known, the number of users remains uncertain. Addi-\ntionally, hosting LLMs (e.g., on Hugging Face) and managing network traffic also\ncontribute to energy consumption. Deploying an LLM on a dedicated server (e.g.,\nusing vLLM) requires setup time and additional energy. Beyond inference, significant\ntime and computational resources are also required for development tasks, including\ndata processing, testing different models and prompts, parameter tuning, and debug-\nging - workloads that apply to both traditional models and LLMs. The measurement\nof additional related energy consumptions (such as network traffic or disk storage) is\nbeyond the scope of this paper.\nThird, energy consumption was measured using CodeCarbon, a tool recognized\nfor providing reliable estimates of a machine’s total energy use (Bouza et al., 2023).\nHowever, it does not allow for precise measurement of energy consumption at the\nlevel of individual processes. Moreover, power intake was recorded at 15-second\nintervals, meaning the accuracy of energy estimates improves with longer-running\nprocesses. Another limitation of CodeCarbon is that RAM energy consumption is\napproximated at 0.375W per GB of memory used. While the Running Average\nPower Limit (RAPL) framework can directly measure RAM power consumption, it\nis not supported on all CPUs (https://github.com/mlco2/codecarbon/issues/717#\nissuecomment-2589805160). Additionally, in shared computing environments such as\nhigh-performance computing (HPC) clusters, measurements may be affected by other\nusers’ activities. Especially when an LLM was deployed across multiple nodes, varia-\ntions in network traffic at different times may have influenced energy measurements. A\nmore precise assessment of energy efficiency would benefit from using dedicated com-\nputing resources with physical wattmeters and high-resolution energy measurement\ntools(e.g. Ilsche et al., 2019).\nIn the following, we assess further limitations of the present study in more detail.\nMore specifically, we address our focus on a single dataset in section 5.1 and the\nlimitation to the text categorisation task in section 5.2. Subsequently, we contextualise\nour work in the broader context of planet-centered LLMs (section 5.3).\n5.1 Analysis on other datasets\nOur analysis was conducted on a highly specialized dataset. To assess the generaliz-\nability of our findings, we replicated the experiments using four additional, widely used\ndatasets (see table 4). These datasets were selected from the HuggingFace platform\n13\n\nbased on popularity and had to meet two criteria: suitability for text classification and\ninclusion of two columns - text and label. To maintain comparability with our initial\nanalysis, we randomly sampled 200 training examples and 200 test examples from each\ndataset. Using a slightly larger training set might have provided an advantage to tra-\nditional models, as the LLMs were applied in a zero-shot setting without fine-tuning.\nEach model experiment was repeated 10 times with different samples, ensuring that\neach model was tested on the same 10 sets.\nName\nClassification Task\nID on https://huggingface.co/datasets\nnews\nnews topics: World, Sports,\nBusiness, Sci/Tech\nfancyzhx/ag news\nyelp\nsentiment: 1-5 stars\nYelp/yelp review full\ntomatoes\nsentiment: pos, neg\ncornell-movie-review-data/rotten tomatoes\nemotion\nemotion: anger, fear, joy,\nlove, sadness, surprise\ndair-ai/emotion\nTable 4 Selection of datasets for text classification tasks.\nFigure 7 visualizes the relationship between accuracy and energy consumption for\nthese additional text classification tasks. For clarity, we restricted the visualization to\nthe models with the three highest accuracy scores and included the linear model with\nsentence embeddings for comparison (see Tables B4 and B5 for details).\nSimilar to our findings with the FKTG dataset, the DeepSeek models do not\noutperform the best non-reasoning models in most cases. The only exception is the\nemotion dataset, where DeepSeek Llama 70B achieves an accuracy of 0.61, slightly\nsurpassing the best non-reasoning model, Phi 3.5 MoE (0.60). However, unlike in the\nprevious analysis, for every dataset, at least one LLM outperforms the best traditional\nmodel. For the news dataset, Llama 3.1 70B achieves an accuracy 0.05 points higher\nthan the best linear model (0.88 vs. 0.83). However, this comes at the cost of signif-\nicantly higher energy consumption (34.15 Wh vs. 0.0021 Wh), highlighting the need\nfor careful trade-off considerations.\nIn the case of sentiment analysis on the Yelp dataset, traditional models perform\nconsiderably worse than LLMs, justifying the energy costs of LLM deployment. In some\ncases, a smaller model, such as Qwen 2.5 7B, may be sufficient. While its accuracy is\nslightly lower than the version with 72B parameters (0.60 vs. 0.68), it consumes only\none-eighth of the energy. A similar pattern is observed for sentiment analysis on the\nRotten Tomatoes dataset, where traditional models fail to match LLM performance.\nAmong the larger models, Jamba Mini 1.5 stands out as one of the most efficient\nchoices, offering strong accuracy while consuming significantly less energy. Notably,\ndespite having nearly as many parameters as Llama 3.1 70B and Qwen 2.5 72B (51.6B\nvs. 70B/72B), Jamba Mini 1.5 requires only a quarter of the energy for the same task.\nFinally, for emotion classification, the linear model with sentence embeddings is\namong the top-performing models. In this case, a traditional model provides the most\nefficient solution. Hence, accuracy-energy trade-offs must be assessed on a case-by-\ncase basis. In some scenarios, traditional models are sufficient, while in others, LLMs\noffer justifiable benefits despite higher energy consumption. However, a reason for the\n14\n\nFig. 7 Accuracy-energy-trade-off of the best models for the inference task on different datasets (the\nLinear Embedding model was added for comparison), Capella system, single node. See Tables B4\nand B5 for results of all models.\nsuperior performance of LLMs on some datasets might be that the data were included\nin the model’s training data. Our study uses data that are probably not part of\nany LLM training set. Nevertheless, test-time compute, as featured by the Deepseek\nmodels, has no benefits in text classification tasks, and the linear relationship between\ncomputation runtime and energy consumption holds across all datasets (see Table B6).\n5.2 Transferability to other tasks\nAnother limitation of the present study is its exclusive focus on the categorization\ntask, which confines the analysis to a narrow subset of machine learning challenges.\nWhile this focus allows for a straightforward measurement of a model’s performance\n(using the accuracy metric), it neglects the applicability of the results to other tasks.\nRecent studies suggest that similar comparisons in terms of efficiency and accuracy can\nbe insightful in a variety of domains beyond categorization. For instance, Clavi´\ne et al.\n15\n\n(2025) demonstrate that simple encoder-based models can effectively tackle generative\ntasks, expanding the potential applications of smaller, less energy-hungry models.\nMoreover, a growing body of research highlights the advantages of fine-tuned small\nmodels for specialized tasks, where they often outperform larger models (Savvov,\n2024). This trend is evident in studies such as Wei et al. (2024), where an diabetes-\nspecific LLM - despite having significantly fewer parameters - outperforms both GPT-4\nand Claude-3.5 in processing various diabetes tasks. Similarly, Lu et al. (2023) report\nthat their fine-tuned models achieve performance levels comparable to GPT-4 on\ndomain-specific annotation tasks, yet with hundreds of times fewer parameters and\nsignificantly reduced computational costs. Zhan et al. (2025) further emphasize the\nsuperior performance of fine-tuned small models over zero-shot LLMs, particularly in\nin-domain content moderation tasks.\nThe study by Luccioni et al. (2024) provides additional insights into the balance\nbetween model size and efficiency while looking at ten different machine learning tasks\nincluding image classification and captioning, question answering, summarization, as\nwell as image and text generation. The authors demonstrate that smaller models can\nachieve high performance with considerably less resource consumption. Their initiative\nresulted into the AI Energy Score (https://huggingface.co/AIEnergyScore), a tool\ndesigned to assess the environmental impact of AI models on a range of tasks, and\nreinforces the growing importance of considering energy efficiency in model evaluation.\n5.3 Further Requirements of Planet-Centered LLMs\nWhile energy consumption and the associated carbon footprint remain crucial con-\nsiderations for sustainable AI, truly planet-centered LLMs must meet a broader\nset of requirements that go beyond mere efficiency. These include other limited\nresources (water, rare-earth metals, landuse,...), transparency, accessibility, ethical\nconsiderations, and technical adaptability to ensure responsible and sustainable AI\ndeployment.\nTransparency in AI models is essential for trust and reproducibility (Raji et al.,\n2020). The predictions of traditional LLM models are generally more transparent than\nthose of LLMs. Open-source LLMs, where both model architectures and training data\nare publicly available, contribute to scientific progress, allow for direct model compar-\nisons such as this present study, and reduce dependency on proprietary technologies\n(Wei et al., 2023). Furthermore, the ability to inspect training data is crucial to assess\npotential biases and copyright compliance (Bender et al., 2021). Many proprietary\nmodels, such as GPT-4, lack such transparency, making it difficult to evaluate their\nfairness and ethical considerations. The EU AI Act will require providers of general-\npurpose AI models to publish a sufficiently detailed summary of their training data\nstarting in August 2025, which further highlights the call for transparency.\nLLMs vary significantly in size, ranging from lightweight models such as fast-\nText (Joulin et al., 2017) to massive architectures like BLOOM-176B, which require\nsubstantial GPU memory and network bandwidth (Luccioni et al., 2023). These com-\nputational demands translate into high operational costs and environmental impacts.\nMoreover, some models require proprietary hardware, limiting their accessibility and\n16\n\nlong-term sustainability. Future AI systems should prioritize modularity and adapt-\nability, enabling efficient integration into diverse infrastructures without excessive\nresource demands.\nThe relevance and fairness of AI-generated outputs depend on the quality and\nrecency of training data. Stale or biased datasets can lead to misleading results and\nreinforce harmful stereotypes (Bender et al., 2021; Gehman et al., 2020). In par-\nticular, the presence of toxic content or hate speech in training data can result in\nmodels generating harmful or discriminatory outputs, which poses serious challenges\nfor their deployment in sensitive contexts such as education, healthcare, or public\nadministration. Moreover, safety concerns—such as the risk of models producing fac-\ntually incorrect, manipulative, or otherwise harmful content—are especially critical in\npublic-sector applications, where accountability and trust are paramount (Weidinger\net al., 2021). Addressing these challenges requires robust bias-mitigation strategies\nand transparent documentation of model behavior.\nTo align with global sustainability and ethical AI principles, future research should\nemphasize the development of adaptable, transparent, and energy-efficient LLMs. By\nintegrating principles of openness, fairness, and regulatory compliance, we can foster\nAI systems that not only minimize environmental impact but also promote responsible\nand equitable usage across sectors.\nAcknowledgements.\nWe gratefully acknowledge the support provided by the Fed-\neral Ministry for the Environment, Nature Conservation and Nuclear Safety (BMUV).\nAdditionally, we thank colleagues from Z 2.3 and the entire AI-Lab team for their\nsupport and inspiration.\nThis work was supported by high-performance computer time and resources from\nthe Center for Information Services and High Performance Computing (ZIH) of TUD\nDresden University of Technology and the systems for scientific computing of Leipzig\nUniversity. We thank the Center for Scalable Data Analytics and Artificial Intelligence\n(ScaDS.AI Dresden/Leipzig) for their support in the acquisition process.\nThe tool ChatGPT (OpenAI) was used to revise the text of the paper.\nAuthor contribution statements.\nT.H. conceived the study, initiated the project,\nled the research effort, and contributed to the literature review and manuscript writing.\nJ.Z. designed and implemented the experiments, developed the codebase, conducted\ndata analysis, and contributed to drafting the manuscript.\nCompeting interests.\nThere are no competing interests.\nAvailability of data and code.\nAll underlying data will be shared upon reasonable\nrequest to the corresponding author. The source code will be made public.\n17\n\nAppendix A\nLLM prompt\nFor the zero-shot classification, we prompted the LLM with the following instruction\n(originally in German):\nC l a s s i f y\nthe\nt e x t\nas\none\nof\nthe\nf o l l o w i n g\nc a t e g o r i e s :\n−<category 1>\n−<category 2>\n−\n. . .\nThe categories were a fixed set of 14 options that occurred in the train-\ning as well as the test dataset: ‘geoWK’, ‘Tongestein’, ‘Aktive St¨\norungszonen’,\n‘¨\nOffentlichkeitsbeteiligung’, ‘Kristallingestein’, ‘FEP/Szenarien/Entwicklungen des\nEndlagersystems’, ‘Anwendung geoWK’, ‘Mindestanforderungen’, ‘Steinsalz in steiler\nLagerung’, ‘Datenverf¨\nugbarkeit’, ‘Modellierung’, ‘Referenzdatens¨\natze’, ‘Bereitstellung\nder Daten’, ‘Ausschlusskriterien’.\nSince we deployed the dspy framework (https://dspy.ai/) to query the LLMs, the\nfinal prompt was automatically extended to the following:\n−r o l e :\nsystem\ncontent :\n|−\nYour\ninput\nf i e l d s\nare :\n1.\n‘ text ‘\n( s t r )\nYour\noutput\nf i e l d s\nare :\n1.\n‘ category ‘\n( s t r )\nA l l\ni n t e r a c t i o n s\nw i l l\nbe\ns t r u c t u r e d\ni n\nthe\nf o l l o w i n g\nway ,\nwith\nthe\na p p r o p r i a t e\nv a l u e s\nf i l l e d\ni n .\n[ [ #\n# t e x t #\n# ] ]\n{ t e x t }\n[ [ #\n# category #\n# ] ]\n{ category }\n[ [ #\n# completed #\n# ] ]\nIn\nadhering\nto\nt h i s\ns t r u c t u r e ,\nyour\no b j e c t i v e\ni s :\nC l a s s i f y\nthe\nt e x t\nas\none\nof\nthe\nf o l l o w i n g\nc a t e g o r i e s :\n−<category 1>\n−<category 2>\n−\n. . .\n−r o l e :\nuser\ncontent :\n|−\n[ [ #\n# t e x t #\n# ] ]\n<text >\nRespond\nwith\nthe\nco r r e s p o n d i n g\noutput\nf i e l d s ,\ns t a r t i n g\nwith\nthe\nf i e l d\n‘ [ [ #\n# category #\n# ] ] ‘ ,\nand\nthen\nending\nwith\nthe\nmarker\nf o r\n‘ [ [ #\n# completed #\n# ] ] ‘ .\n18\n\nAppendix B\nTables\nModel\nGPUs\nEnergy (Wh)\nAccuracy\nDuration (s)\nAverage Power (W)\nLinear BoW\n1\n<0.01\n0.43\n0.01\n139.96\nLinear Tf-idf\n1\n<0.01\n0.41\n0.01\n43.72\nLinear Embedding\n1\n0.12\n0.57\n1.64\n259.41\nXGBoost BoW\n1\n<0.01\n0.35\n0.01\n63.32\nXGBoost Tf-idf\n1\n<0.01\n0.47\n0.01\n67.77\nXGBoost Embedding\n1\n0.21\n0.47\n2.87\n259.94\nLlama 3.1 8B\n1\n5.86\n0.35\n36.88\n572.49\nLlama 3.1 70B\n2\n48.60\n0.48\n161.59\n1082.82\nQwen 2.5 7B\n1\n5.58\n0.45\n36.28\n553.84\nQwen 2.5 72B\n2\n48.66\n0.51\n164.44\n1065.31\nPhi 3.5 Mini\n1\n5.74\n0.30\n41.45\n498.46\nPhi 3.5 MoE\n2\n11.00\n0.40\n55.51\n713.34\nJamba Mini 1.5\n2\n17.42\n0.34\n78.61\n797.94\nDS Llama 8B\n1\n79.64\n0.37\n517.83\n553.67\nDS Llama 70B\n2\n702.06\n0.46\n2543.47\n993.68\nDS Qwen 14B\n1\n155.20\n0.39\n981.35\n569.33\nDS Qwen 32B\n1\n373.56\n0.45\n2255.99\n596.11\nTable B1 Measurements of all models for the inference task on the FKTG dataset, Capella system,\nsingle node, shown are averages over 10 runs\nModel\nDuration (s)\nEnergy consumed (Wh)\nsingle\ndouble\nratio\nsingle\ndouble\nratio\nLlama 3.1 70B\n161.59\n304.77\n1.89\n48.60\n94.88\n1.95\nQwen 2.5 72B\n164.44\n308.16\n1.87\n48.66\n95.70\n1.97\nJamba Mini 1.5\n78.61\n113.88\n1.45\n17.42\n29.81\n1.71\nDS Llama 70B\n2543.47\n6792.54\n2.67\n702.06\n1899.86\n2.71\nTable B2 Comparison single vs. double node deployment, Capella system\nModel\nDuration (s)\nEnergy consumed (Wh)\nA30\nV100\nH100\nA30\nV100\nH100\nLlama 3.1 8B\n20.78\n27.52\n36.88\n2.91\n2.88\n5.86\nQwen 2.5 7B\n19.58\n24.64\n36.28\n2.87\n2.63\n5.58\nPhi 3.5 Mini\n19.18\n25.02\n41.45\n2.65\n2.50\n5.74\nPhi 3.5 MoE\n77.60\n32.53\n45.93\n17.77\n6.04\n15.04\nDS Llama 8B\n1210.90\n1439.58\n517.83\n175.83\n137.90\n79.64\nDS Qwen 14B\n1348.09\n1736.21\n624.38\n254.01\n230.72\n157.58\nDS Qwen 32B\n1688.23\n2192.53\n806.68\n444.67\n457.60\n378.58\nTable B3 Comparison of different GPU cards, single node deployment.\n19\n\nDataset\nnews\nyelp\nModel\nEnergy (Wh)\nAccuracy\nEnergy (Wh)\nAccuracy\nLinear BoW\n<0.01\n0.65\n<0.01\n0.36\nLinear Tf-idf\n<0.01\n0.65\n<0.01\n0.34\nLinear Embedding\n<0.01\n0.83\n0.04\n0.43\nXGBoost BoW\n<0.01\n0.48\n<0.01\n0.31\nXGBoost Tf-idf\n<0.01\n0.52\n<0.01\n0.29\nXGBoost Embedding\n0.03\n0.74\n0.01\n0.40\nLlama 3.1 8B\n4.31\n0.71\n4.73\n0.58\nLlama 3.1 70B\n34.15\n0.88\n36.71\n0.67\nQwen 2.5 7B\n4.21\n0.01\n4.52\n0.60\nQwen 2.5 72B\n33.75\n0.79\n38.20\n0.68\nPhi 3.5 Mini\n3.30\n0.53\n15.55\n0.58\nPhi 3.5 MoE\n8.53\n0.78\n8.32\n0.58\nJamba Mini 1.5\n9.34\n0.78\n11.45\n0.56\nDS Llama 8B\n60.58\n0.82\n97.18\n0.62\nDS Llama 70B\n483.73\n0.83\n707.03\n0.67\nDS Qwen 14B\n113.81\n0.83\n177.41\n0.63\nDS Qwen 32B\n271.92\n0.83\n358.62\n0.63\nTable B4 Measurements of all models for the inference task on the news and\nyelp datasets, Capella system, single node, shown are averages over 10 runs\nDataset\ntomatoes\nemotion\nModel\nEnergy (Wh)\nAccuracy\nEnergy (Wh)\nAccuracy\nLinear BoW\n<0.01\n0.59\n<0.01\n0.36\nLinear Tf-idf\n<0.01\n0.59\n<0.01\n0.40\nLinear Embedding\n0.01\n0.79\n<0.01\n0.59\nXGBoost BoW\n<0.01\n0.54\n<0.01\n0.30\nXGBoost Tf-idf\n<0.01\n0.55\n<0.01\n0.33\nXGBoost Embedding\n<0.01\n0.76\n<0.01\n0.53\nLlama 3.1 8B\n4.12\n0.87\n4.46\n0.56\nLlama 3.1 70B\n32.07\n0.91\n34.12\n0.58\nQwen 2.5 7B\n4.04\n0.73\n4.17\n0.37\nQwen 2.5 72B\n33.25\n0.91\n34.81\n0.58\nPhi 3.5 Mini\n7.20\n0.87\n5.13\n0.53\nPhi 3.5 MoE\n7.72\n0.89\n8.82\n0.60\nJamba Mini 1.5\n8.37\n0.91\n10.22\n0.56\nDS Llama 8B\n72.15\n0.83\n81.82\n0.60\nDS Llama 70B\n510.86\n0.90\n670.40\n0.61\nDS Qwen 14B\n134.02\n0.89\n148.20\n0.60\nDS Qwen 32B\n246.48\n0.89\n323.48\n0.60\nTable B5 Measurements of all models for the inference task on the tomatoes and\nemotion datasets, Capella system, single node, shown are averages over 10 runs\n20\n\nDependent variable: Energy\ntomatoes\nemotion\nnews\nyelp\n(1)\n(2)\n(3)\n(4)\nDuration\n0.040∗∗∗\n0.043∗∗∗\n0.052∗∗∗\n0.045∗∗∗\n(0.002)\n(0.002)\n(0.003)\n(0.003)\nGPUs\n−0.079\n−0.052\n0.536\n0.810\n(0.950)\n(1.011)\n(1.470)\n(1.545)\nDuration:GPUs\n0.122∗∗∗\n0.120∗∗∗\n0.115∗∗∗\n0.120∗∗∗\n(0.001)\n(0.001)\n(0.002)\n(0.002)\nConstant\n−0.397\n−0.464\n−1.300\n−1.773\n(1.290)\n(1.372)\n(1.985)\n(2.103)\nObservations\n17\n17\n17\n17\nR2\n1.000\n1.000\n1.000\n1.000\nAdjusted R2\n1.000\n1.000\n1.000\n1.000\nNote:\n∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\nTable B6 Linear regression of energy consumption on duration for\nthe datasets of section 5.1 (table format by Hlavac, 2022)."}
{"id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "authors": ["Xiancheng Li", "Georgios D. Karampatakis", "Helen E. Wood", "Chris J. Griffiths", "Borislava Mihaylova", "Neil S. Coulson", "Alessio Pasinato", "Pietro Panzarasa", "Marco Viviani", "Anna De Simoni"], "source": "http://arxiv.org/abs/2508.14032v1", "page_count": 22, "text": "The Promise of Large Language Models in Digital Health:\nEvidence from Sentiment Analysis in Online Health Communities∗\nXiancheng Li†1, Georgios D. Karampatakis2, Helen E. Wood2, Chris J. Griffiths2, Borislava\nMihaylova2, Neil S. Coulson3, Alessio Pasinato4, Pietro Panzarasa1, Marco Viviani4, and\nAnna De Simoni2\n1School of Business and Management, Queen Mary University of London\n2Wolfson Institute of Population Health (WIPH), Queen Mary University of London\n3Department of Medicine, University of Nottingham\n4Department of Informatics, Systems, and Communication, University of Milano-Bicocca\nAbstract\nDigital health analytics face critical challenges nowadays. The sophisticated analysis of patient-\ngenerated health content, which contains complex emotional and medical contexts, requires\nscarce domain expertise, while traditional ML approaches are constrained by data shortage and\nprivacy limitations in healthcare settings. Online Health Communities (OHCs) exemplify these\nchallenges with mixed-sentiment posts, clinical terminology, and implicit emotional expressions\nthat demand specialised knowledge for accurate Sentiment Analysis (SA).\nTo address these challenges, this study explores how Large Language Models (LLMs) can\nintegrate expert knowledge through in-context learning for SA, providing a scalable solution\nfor sophisticated health data analysis. Specifically, we develop a structured codebook that\nsystematically encodes expert interpretation guidelines, enabling LLMs to apply domain-specific\nknowledge through targeted prompting rather than extensive training.\nUnder this strategy, six GPT models validated alongside DeepSeek and LLaMA 3.1 are\ncompared with pre-trained language models (BioBERT variants) and lexicon-based methods, using\n400 expert-annotated posts from two health communities. LLMs achieve superior performance\n(81-89% accuracy vs. 48-72% for traditional methods) while demonstrating expert-level agreement\n(Fleiss’ Kappa: 0.42-0.75). This high agreement, with no statistically significant difference (e.g.,\np>0.05) from inter-expert agreement levels, suggests knowledge integration beyond surface-level\npattern recognition.\nThe consistent performance across diverse LLM models, supported by in-context learning,\noffers a promising solution for digital health analytics. Confidence calibration analysis shows that\nreasoning models provide reliable uncertainty estimates, enabling quality-controlled deployment\nin healthcare settings. This approach thus addresses the critical challenge of expert knowl-\nedge shortage in digital health research, enabling real-time, expert-quality analysis for patient\nmonitoring, intervention assessment, and evidence-based health strategies. Our open-source\nimplementation facilitates the immediate adoption of this methodology across diverse healthcare\nresearch contexts and tasks.\n∗This work was supported in part by the National Institute for Health and Care Research Program Grant for\nApplied Research (NIHR PGfAR reference 202037, Effectiveness and cost-effectiveness of a digital social intervention\nfor people with troublesome asthma promoted by primary care clinicians). The views expressed are those of the\nauthors and not necessarily those of the National Health Service, National Institute for Health and Care Research, or\nthe Department of Health and Social Care.\n†Corresponding author: x.l.li@qmul.ac.uk\n1\n\n1\nIntroduction\nOver the past few years, Machine Learning (ML) has demonstrated remarkable capabilities across\nvarious aspects of digital health, including diagnostics [1], personalised treatment [2], and patient\nmonitoring [3].\nHowever, digital health analytics faces fundamental challenges that limit the\nwidespread adoption of advanced computational approaches. Healthcare data is characterised by\ncomplex regulatory requirements, privacy constraints, and the need for domain-specific interpretation\nthat goes beyond general-purpose analytics [4]. These challenges are compounded by the scarcity of\nlabelled training data due to privacy and governance restrictions, creating barriers to implementing\nsophisticated data analysis methods across digital health contexts [5].\nIn the realm of digital health applications—from electronic health records to patient-generated\ncontent—text analysis presents unique interpretive challenges. Health-related text consistently\ninvolves specialised medical terminology, implicit clinical meanings, and context-dependent informa-\ntion that require domain expertise for accurate analysis [6]. Online Health Communities (OHCs)\nserve as a representative and particularly challenging use case, where patients sharing chronic\ncondition experiences create posts containing complex medical information, personal narratives,\nand nuanced expressions that require sophisticated interpretation [7]. The analytical challenges\npresent in OHCs reflect broader difficulties in digital health text analysis, underscoring the need for\ninnovative approaches to handle such complex content.\nSentiment Analysis (SA) has been widely employed for quite some time to extract insights from\nhealth-related content, providing valuable opportunities to explore patient experiences, treatment\noutcomes, and the dynamics of OHCs as expressed through narrative text [8]. SA can reveal patterns\nrelated to patient engagement, treatment adherence, and emotional responses to interventions that\nwould otherwise remain hidden in unstructured data [9]. As illustrated in [7, 10], OHCs exemplify\nthis potential, since SA can be employed to capture shifts in patient mood, treatment satisfaction,\nand peer support effectiveness over time, providing valuable indicators for healthcare quality\nimprovement and patient monitoring. This is particularly relevant for chronic disease management,\nwhere patients often require sustained emotional support that clinicians may struggle to provide\nwithin limited consultation time, leading many to seek encouragement and understanding from\npeers who share similar experiences [11]. However, despite the recognised importance of SA in the\nabove-mentioned scenario, existing approaches still face significant limitations. Most SA tools and\nmodels have been developed and validated on general social media data, with limited evaluation on\nthe complex, domain-specific content found in healthcare contexts [12, 13]. Furthermore, traditional\nML approaches require extensive labelled datasets for effective performance, yet such data remains\nscarce in healthcare due to privacy constraints and the high cost of expert annotation [14, 15].\nLarge Language Models (LLMs) such as GPT, LLaMA, and DeepSeek, represent today a trans-\nformative opportunity to address both data scarcity and domain expertise challenges simultaneously\nin modern digital health analytics. Through extensive pre-training on diverse text corpora, including\nmedical literature, LLMs possess inherent domain knowledge that can be activated and directed\nthrough structured prompting approaches [16]. Crucially, LLMs demonstrate the ability to integrate\nexpert-derived guidelines and perform sophisticated analysis tasks without requiring extensive\nlabelled training data through in-context learning, offering a pathway to scale expert knowledge in\nhealthcare contexts where such expertise is traditionally scarce and expensive [17, 18]. This capabil-\nity represents a fundamental shift from traditional data-intensive approaches to knowledge-guided\nmethodologies, potentially democratising access to sophisticated health data interpretation across\ndiverse digital health applications.\nIn light of the above, leveraging LLMs for the task of SA appears to be a highly promising\ndirection for healthcare analytics. While this potential is significant, recent studies have primarily\n2\n\ndemonstrated their capabilities in general-domain settings [17, 18], and there remains a notable gap\nin understanding how these models perform when applied to health-related content, where specialised\nterminology, contextual subtlety, and emotional complexity demand more nuanced interpretation.\nMost critically, the fundamental question of whether LLMs can effectively integrate domain-specific\nexpert knowledge to achieve sophisticated analysis without extensive training data requirements has\nnot been systematically evaluated in healthcare settings, where such expertise is traditionally scarce\nand expensive to scale [19]. In addition, for practical healthcare deployment, sophisticated analysis\ncapabilities alone are insufficient—healthcare applications require a reliable assessment of when\nautomated predictions can be trusted and when expert review is necessary. A natural approach\nto address this challenge is to leverage LLMs’ capacity for self-assessment, where models evaluate\ntheir own prediction confidence [20]. While LLMs can indeed be prompted to provide confidence\nscores alongside their predictions, the meaningfulness and reliability of these uncertainty estimates\nin healthcare contexts remain largely unexplored [21].\nA further deployment concern is confidence calibration—whether a model’s predicted confidence\naligns with its actual accuracy. Traditional performance metrics (e.g., accuracy, precision, recall) do\nnot capture prediction reliability, yet in many contexts, understanding when a model is uncertain\ncan be as important as the prediction itself. Therefore, systematic assessment of calibration is\nessential to determine whether LLMs can provide reliable uncertainty quantification. Such capability\nwould enable quality-controlled deployment strategies in which high-confidence predictions are\nprocessed automatically while uncertain cases receive expert oversight, optimising the balance\nbetween automation efficiency and quality assurance in healthcare applications.\nTo tackle the above-mentioned issues, this study specifically aims to evaluate the potential\nof LLMs to integrate expert knowledge for SA in digital health contexts. To demonstrate the\neffectiveness of expert knowledge integration, we develop a structured codebook that encodes\ndomain-specific interpretation guidelines, enabling LLMs to apply expert-derived rules for consistent\nsentiment classification, carefully taking into account the confidence calibration analysis. We focus\nprimarily on GPT models due to their widespread accessibility and well-documented performance [22,\n23], whilst also evaluating other state-of-the-art open-source LLMs including DeepSeek and LLaMA,\nto establish the generalisability of our findings across different model architectures. The inclusion\nof open-source models is particularly important for healthcare applications, as they enable local\ndeployment that can better ensure data security and privacy compliance when working with\nconfidential medical data. By comparing LLM performance against traditional ML and lexicon-\nbased approaches, we demonstrate how expert knowledge integration can achieve sophisticated\nanalysis without extensive training data requirements, establishing a new paradigm for scalable,\nexpert-quality analysis in digital health applications. To facilitate broader adoption, we provide a\ncomplete open-source implementation that enables immediate application of our solution across\ndiverse healthcare research contexts and expert annotation tasks in healthcare beyond SA.\n1.1\nRelated Work\nThe application of advanced Natural Language Processing (NLP) and, more in general, Artificial\nIntelligence (AI) techniques, particularly LLMs, in digital health studies has gained increasing\nattention in recent years. Within NLP, SA is particularly relevant for digital health, as it supports\nthe systematic understanding of patient experiences and emotional states from unstructured text.\nTraditional approaches to SA span three families: (i) lexicon-based methods [24]; (ii) classical\nML classifiers [25]; and (iii) transformer-based architectures (e.g., BERT) [26].\nIn OHCs, these approaches have been used to analyse symptom-focused discussions [27], to\nmonitor emotional trajectories in patient interactions [10], and to detect mental health concerns or\n3\n\nat-risk users in forum settings [28]. However, health-related text introduces additional challenges,\nincluding domain shift between general and medical language, frequent abbreviations and jargon,\nimplicit or mixed sentiment within single messages, and the scarcity of high-quality annotated\ndatasets [29]. These factors increase the risk of domain shift when models are applied outside their\noriginal training distribution and can degrade performance in OHCs compared with general social\nmedia.\nRecent advances in large language models (LLMs) have introduced new possibilities for SA\nthrough zero- and few-shot in-context learning, reducing reliance on large task-specific training\ndatasets while improving adaptability across domains. This growing body of work motivates further\nexamination of how LLMs perform in both general-domain and health-specific SA tasks. Analyses\nhave shown that LLMs achieve similar or even better performance compared to other approaches\nwhen applied to SA tasks using data from Reddit, Twitter (now X), and Amazon Review [17, 18, 30].\nThese general-domain tasks often focus on product reviews or social commentary, where sentiment\ntends to be explicit and polarity is easier to detect. SA has also been applied in health-related\ncontexts. For example, Pratiwi et al. [12] used a dictionary-based approach to analyse asthma-\nrelated discussions on Twitter, while Elbers et al. [13] applied sentiment scoring to clinical notes\nfrom lung cancer patients to examine emotional trends around diagnosis and treatment. However,\nsuch applications typically focus on method demonstration rather than systematic performance\nevaluation, and often fail to address the inherent complexity of health-related text that requires\ndomain-specific interpretation.\nRecent comparative studies have begun to address these limitations by systematically evaluating\ndifferent model architectures on health-specific datasets. Elmitwalli and Mehegan [31] conducted\na comprehensive comparison of lexicon-based, deep learning, and pre-trained language models\nusing both general-purpose datasets (IMDB, Sentiment140) and domain-specific medical content\n(COP9-related tweets), demonstrating that pre-trained models consistently outperformed traditional\napproaches. Their findings showed GPT-3 achieving superior performance on the medical datasets,\nsuggesting strong zero-shot capabilities in health-specific contexts. However, Zhang et al. [32]\nreported contrasting results when evaluating fine-tuned GPT and BERT models on HPV vaccine-\nrelated Twitter content, with BERT outperforming GPT in their specific task setting. These\nmixed findings highlight a critical challenge in health-related SA: model performance appears highly\ndependent on dataset characteristics, task formulation, and fine-tuning strategies, making it difficult\nto establish generalisable conclusions about optimal approaches. Furthermore, these studies remain\nconstrained to short-form Twitter content with explicit medical terminology, leaving significant gaps\nin understanding how these models perform on more complex, narrative-rich health communications.\nIn contrast, OHCs represent a fundamentally different environment for SA, where users engage in\nextended discussions about chronic conditions, treatment experiences, and peer support. Unlike the\nbrief, topic-specific posts typical of Twitter, OHC participants—predominantly patients, caregivers,\nor family members—create detailed personal narratives that often exhibit shifting emotional tones\nwithin single messages. These posts may simultaneously express gratitude for community support,\nanxiety about upcoming treatments, and frustration with healthcare systems, creating complex\nsentiment patterns that require sophisticated interpretation. The personal, experiential nature of\nOHC content, combined with the emotional complexity inherent in chronic disease management,\npresents unique challenges for automated SA that extend well beyond the capabilities of approaches\ndesigned for general social media platforms.\n4\n\n2\nMethods\nThis section describes the methodology adopted to perform SA on OHC data. We first present the\ndatasets used in the study and the human annotation process. Next, we introduce the SA models\nconsidered for comparative evaluation. Finally, we describe the performance assessment framework\nand the metrics used to compare the models.\n2.1\nOHC Datasets and Human Annotation\nAs described in our previous work [33], data were collected by HealthUnlocked,1 the platform provider\nof the Asthma + Lung UK (AUK) and British Lung Foundation (BLF) OHCs. Only posts that\nwere shared publicly were collected and analysed. Our datasets were stored and analysed on a\nsecure server held by Queen Mary University of London. Anonymised user identifiers (IDs) were\nprovided by HealthUnlocked, and no demographic information was available. Our data contains\nthree types of posts, denoted as: (i) level-0 posts (i.e., posts starting new threads), (ii) level-1\nreplies (i.e., replies to the level-0 posts), and (iii) level-2+ replies (i.e., replies to level-1 replies\nand above). The data sets consisted of 12,453 posts since 2006 for AUK, and of 367,787 posts since\n2012 for BLF. We used stratified random sampling without replacement to select 200 posts from\neach dataset, maintaining the original distribution of post types.\nFive expert human annotators independently added sentiment labels (i.e., positive, neutral, or\nnegative) to the two sets of 200 random posts from each OHC. All annotators hold PhD degrees:\nAnnotators 2 and 5 are native English speakers, Annotators 2, 4 and 5 specialise in qualitative\nanalysis in healthcare research, Annotator 4 is a General Practitioner with clinical experience, and\nAnnotator 1 is a data scientist with extensive experience in OHC data analysis.\n2.1.1\nStructured Codebook\nWe developed a structured codebook to operationalise the sentiment schema used in this study. It\nconsolidates (i) the label set with definitions and decision rules, (ii) inclusion/exclusion criteria\nand guidance for borderline or implicit cases, and (iii) prototypical examples with brief rationales.\nThe codebook was created through a rigorous consensus process: annotators first independently\nlabelled a 400-post pilot set (distinct from the final dataset), then disagreements were reviewed\nin multiple meetings to harmonise interpretations and formalise rules. The final version includes\ncomprehensive sentiment definitions, instructions for handling implicit sentiment, examples with\njustifications, and specific rules for ambiguous or mixed-sentiment cases. We used the codebook\nboth to train annotators and standardise labelling—supporting inter-annotator agreement and\nquality control—and to construct LLM prompts: zero-shot prompts comprised task instructions\nplus the codebook’s label definitions and decision rules (no examples), while few-shot prompts\nappended a small set of representative examples with gold labels. This alignment ensures that\nhumans and models follow the same expert-derived labelling logic and facilitates the integration of\ndomain-specific knowledge into automated analysis.\n2.2\nSentiment Analysis Models\nTraditional SA approaches have primarily relied on two classes of methods: lexicon-based techniques\nthat use predefined word lists categorised by sentiment polarity, and semantic-based techniques\n1https://healthunlocked.com/\n5\n\nbased on supervised ML where classifiers are trained on labelled datasets [34]. Whilst lexicon-\nbased methods offer simplicity and interpretability, they often lack contextual understanding, and\nsupervised approaches require extensive annotated training data that may not be available in\nspecialised domains. To evaluate the potential of LLMs for expert knowledge integration in health-\nspecific SA, we compared their performance against these traditional approaches. We employed the\nfollowing three categories of models to generate sentiment labels.\n2.2.1\nLexicon-based models\nWe used three widely-adopted lexicon-based models: Valence Aware Dictionary for Sentiment\nReasoning (VADER) [35], TextBlob [36], and SentiWordNet [37]. These models were implemented\nusing standard Python libraries and serve as established baseline approaches for SA tasks2. Lexicon-\nbased models generate a compound score indicating the sentiment polarity of each post based\non predefined sentiment lexicons. The compound score ranges from −1 to 1, where negative and\npositive values correspond to negative and positive sentiment, respectively. We classified posts with\ncompound scores between −0.1 and 0.1 as neutral sentiment.\n2.2.2\nFine-tuned BioBERT models\nBioBERT is a domain-specific language representation model pre-trained on large-scale biomedical\ndata [38]. We selected it as a representative example of pre-trained language models requiring\nsupervised fine-tuning, as such models have demonstrated promising performance in prior SA tasks\ninvolving health-related texts, making BioBERT a suitable baseline for comparison with LLMs and\nlexicon-based approaches. We fine-tuned BioBERT with four datasets:\n1. 143,903 Covid-19 tweets [39], where sentiment labels were produced by NLTK, a Python\nlibrary for NLP;\n2. 50,333 tweets from SemEval-2017 [40], where sentiment labels were produced by annotators\nrecruited through the CrowdFlower platform;\n3. A combination of Covid-19 and SemEval-2017 tweets;\n4. A balanced sub-sample of Covid-19 and SemEval-2017 tweets containing 48k tweets where a\nrandom sample of 16k tweets was selected for each sentiment label.\nIn the remainder of the paper, the fine-tuned BioBERT models using the four datasets are\nreferred to as BioBERT-Covid, BioBERT-SemEval, BioBERT-Combine, and BioBERT-Balance,\nrespectively.\n2.2.3\nLarge Language Models\nLLMs represent a paradigm shift in NLP, offering the ability to perform complex text analysis\ntasks through in-context learning via zero-shot and few-shot learning without requiring extensive\ntask-specific training data. Our method implements expert knowledge integration by embedding\ndomain-specific interpretation guidelines and labelling rules directly into structured prompts, derived\nfrom our expert-developed codebook. We evaluated multiple LLM architectures to demonstrate the\ngeneralisability of this expert knowledge integration approach across different model families.\n2\nVADER:\nhttps://github.com/cjhutto/vaderSentiment\nTextBlob:\nhttps://textblob.readthedocs.io/en/dev/\nSentiWordNet:\nhttps://www.nltk.org/howto/sentiwordnet.html\n6\n\nGPT Models: We selected GPT models as our primary LLM evaluation target due to their\nwidespread accessibility, well-documented performance, and user-friendly API interface that enables\nbroader adoption in healthcare research settings. In this study, we used OpenAI’s API to query\nthe latest models GPT-4.1, GPT-o3, and their corresponding mini variants (i.e., GPT-4.1-mini,\nGPT-o3-mini), which offer different computational efficiency trade-offs whilst maintaining strong\nperformance capabilities.\nWe evaluated both zero-shot and few-shot learning approaches to assess how effectively this\nstructured prompting methodology could transfer expert knowledge without requiring extensive\ntraining data.\nFor zero-shot learning, we used prompts that incorporated the expert-derived\ncodebook guidelines and labelling rules without providing specific examples. For few-shot learning,\nwe enhanced the structured prompts with a few carefully selected examples, referred to as GPT-4.1-fs,\nGPT-o3-fs, GPT-4.1-mini-fs, and GPT-o3-mini-fs, respectively. The few-shot prompts included the\nsame codebook guidelines plus specific examples to demonstrate the application of labelling rules.\nBoth approaches demonstrate how domain-specific knowledge can be systematically transferred to\nLLMs through structured prompting approaches.\nOther LLM Models: To establish the generalisability of our findings across different LLM\narchitectures, we evaluated recently released models including DeepSeek (versions R1 and V3)\nand LLaMA 3.1 (70B and 405B). These open-source alternatives are particularly important for\nhealthcare applications, as they enable local deployment that can better ensure data security and\nprivacy compliance when working with confidential medical data. The performance of these models\nis summarised in the text to complement our main findings and demonstrate the robustness of the\nLLM approach across different architectures, while detailed graphical analysis focuses on the GPT\nmodel family.\nTo ensure reproducibility and facilitate broader adoption, we provide open-source code imple-\nmenting our structured codebook approach on GitHub3. The repository includes the complete\nimplementation code, evaluation scripts, the codebook, zero-shot and few-shot prompts with\nembedded codebook rules, and example usage demonstrating the methodology.\n2.3\nPerformance Assessment\nOur comparative evaluation covers five aspects: label distribution analysis, inter-annotator agreement\namong experts, agreement of LLMs with respect to individual annotators, detailed performance\nagainst the majority label, and confidence calibration.\n1. Label distribution analysis. For each dataset, we computed the marginal frequency of Positive,\nNegative and Neutral labels produced by human annotators and models through visualisation.\nThis analysis is descriptive and aims to characterise broad tendencies in assigned labels across\nhuman annotators and models.\n2. Inter-annotator agreement. Among the five expert annotators who independently provided\ntheir sentiment labels for the random samples—as previously introduced in Section 2.1—\nwas assessed using Fleiss’ kappa coefficient (denoted as κ), which reduces to Cohen’s kappa\ncoefficient for pairwise comparisons [41], to evaluate the extent to which the five annotators\nagreed with each other. Statistical significance of kappa coefficients was evaluated using\nasymptotic z-tests against the null hypothesis of κ = 0, with significance level set at α = 0.05.\nFollowing established practices [22, 7], we assigned a new label to each post based on the\nmajority consensus (referred to hereafter as the majority label);\n3https://github.com/XianchengLI/sentiment-analysis-llm-health\n7\n\n3. LLM–expert agreement., To test whether LLMs achieve agreement levels comparable to human\nexperts, we compared the distribution of LLM–human pairwise κ values to the distribution of\nhuman–human pairwise κ values using the Mann–Whitney U test [42] (non-parametric, no\nnormality assumption; α = 0.05). Human–human agreement comprised all 10 annotator pairs\n(n = 10), and LLM–human agreement comprised all model–annotator pairs across models\n(n = 40). A non-significant result (p > 0.05) indicates that LLM–human agreement shows no\nstatistically significant difference from expert-level agreement.\n4. Performance against the majority label. To evaluate the effectiveness of expert knowledge\nintegration across the three categories of models, we employed a multi-faceted assessment\napproach. First, we used agreement metrics to report the level of agreement between different\nmodels and the expert annotators, with high agreement indicating successful knowledge\ntransfer. We then considered the majority label as the benchmark and computed the accuracy\nof each model (i.e., the percentage of correct labels). Additionally, we calculated precision,\nrecall, and F1-score for each sentiment category to provide a comprehensive performance\nevaluation [43];\n5. Confidence estimation and calibration. To evaluate prediction reliability for practical healthcare\ndeployment, we implemented confidence estimation for selected GPT models capable of\nproviding uncertainty quantification. Confidence scores were obtained by requesting models\nto report their certainty level ([0 −1] scale) alongside predictions, where 0 indicates complete\nuncertainty and 1 indicates complete certainty. Detailed confidence-enabled prompts and\nrules are available in our open-source repository. Confidence calibration was assessed using\nreliability diagrams plotting predicted confidence against actual accuracy across confidence\nbins. We also analysed confidence score distributions to evaluate whether models produce\ninformative estimates across the full range or concentrate in narrow bands that limit utility\nfor quality assessment. This analysis assesses model suitability for applications requiring\nuncertainty quantification and prediction reliability evaluation.\n3\nResults\nThis section is dedicated to presenting and discussing the results of the comparative evaluation with\nrespect to the five key points outlined previously in Section 2.3.\nConcerning the first key point, Figure 1 shows the distribution of sentiment labels produced\nby our selected models and expert annotators. Across both datasets, positive labels were more\nfrequently assigned than negative or neutral labels by both human experts and most models.\nDeepSeek and LLaMA demonstrated similar distributional patterns, with positive sentiment being\nthe most commonly assigned label across both datasets.\nConcerning the second key point, Figures 2A and 2B present the agreement matrix among\nexpert annotators. Majority consensus labels could be determined for all posts in both datasets,\nreflecting the effectiveness of our expert annotation process. For the AUK sample, inter-annotator\nagreement (κ) ranged from 0.49 to 0.71 with an overall κ of 0.59, indicating moderate agreement\namong experts. The highest agreement was between Annotators 1 and 2 with 82% agreement\n(κ = 0.71). Individual annotator agreement with the majority consensus ranged from 0.70 to 0.79.\nFor the BLF sample, inter-annotator κ values ranged from 0.33 to 0.67 with an overall κ of 0.52,\nalso indicating moderate agreement. The highest agreement was between Annotators 1 and 2 with\n86% agreement (κ = 0.67). Agreement with majority consensus ranged from 0.58 to 0.82. All κ\nvalues were statistically significant (p < 0.001), confirming genuine agreement beyond chance levels.\n8\n\nFigure 1: Distribution of labels given by each of the annotators and models to selected posts from\nthe AUK OHC (upper panel) and the BLF OHC (lower panel).\nConcerning the third key point, Figure 2C and 2D present the agreement of the selected models\nwith individual annotators. Although the agreement varies among different annotators, LLM models\nhave consistently higher agreement with annotators than the other models. The agreement between\nGPT models and annotators ranged from 68% to 89% with κ ranging from 0.42 to 0.75, which\naligns with the degree of agreement among annotators themselves. Mann-Whitney U test revealed\nno significant difference between LLM-human agreement scores and human-human agreement scores\n(p = 0.68 in AUK and p = 0.10 in BLF), confirming that LLMs achieved genuine expert-level\nagreement rather than systematic bias. This level of agreement indicates successful integration of\nexpert knowledge, as the models achieved human-level consistency in sentiment classification without\nrequiring extensive training data. DeepSeek and LLaMA demonstrated similar performance, with\nagreement ranging from 69% to 92% with κ ranging from 0.45 to 0.82, confirming the robustness of\nexpert knowledge integration across different LLM architectures.\n9\n\nFigure 2: Agreement Matrix of annotators and all models. Agreements are measured by the ratio\nof posts with the same labels assigned by two annotators or models. Panels A and B show the\nagreement among annotators. Panels C and D show the agreement between annotators and models.\n10\n\nWe then selected one model at a time as an additional annotator and calculated the overall κ\namong all annotators including the selected model (Table 1). When adding labels from any LLM,\nthe overall κ increased from baseline values (0.586 for AUK, 0.524 for BLF). In contrast, the overall\nκ decreased when adding labels from traditional approaches (lexicon-based and BioBERT models).\nSimilar patterns were observed with DeepSeek and LLaMA models, which also increased overall\nagreement when included as additional annotators. These patterns suggest that LLM-generated\nlabels showed no statistically significant difference from expert annotations in agreement trends.\nTable 1: Overall κ calculated after adding labels from the selected model to those provided by the\nhuman annotators. Overall κ values higher than the annotators only value are in bold.\nSource of labels added\nOverall κ (AUK)\nOverall κ (BLF)\nAnnotators only (Baseline)\n0.586\n0.524\nVADER\n0.409\n0.449\nTextBlob\n0.445\n0.427\nSentiWordNet\n0.419\n0.398\nBioBERT-Covid\n0.498\n0.485\nBioBERT-SemEval\n0.498\n0.485\nBioBERT-Combine\n0.490\n0.465\nBioBERT-Balance\n0.491\n0.487\nGPT-o3\n0.604\n0.549\nGPT-o3-mini\n0.604\n0.540\nGPT-4.1\n0.604\n0.542\nGPT-4.1-mini\n0.598\n0.550\nGPT-o3-mini-fs\n0.604\n0.542\nGPT-o3-fs\n0.614\n0.552\nGPT-4.1-fs\n0.608\n0.556\nGPT-4.1-mini-fs\n0.612\n0.546\nConcerning the fourth key point, Figure 3A presents the accuracy of different models compared\nto the majority labels based on the AUK sample. LLMs demonstrated superior performance, with\nGPT-o3-fs achieving the highest accuracy (87%), whilst other GPT variants achieved accuracy\nbetween 82% and 86%. Traditional approaches performed considerably lower, with BioBERT models\n(48%-61%) slightly outperforming lexicon-based models, among which VADER achieved the best\naccuracy. For the BLF sample (Figure 3B), similar patterns emerged with GPT-o3-fs achieving\n86% accuracy and other GPT variants ranging from 83% to 86%. Traditional approaches again\nperformed lower (57%-72%). Across both datasets, LLMs consistently achieved the highest accuracy,\nwith GPT-o3-fs being the most accurate overall.\nAnalysis of the accuracy results also reveals differences between GPT model generations and zero-\nshot versus few-shot approaches. As expected, full-size models (GPT-4.1 and GPT-o3) performed\nbetter than mini variants (GPT-4.1-mini and GPT-o3-mini). Few-shot learning demonstrated the\npotential for performance improvement, with GPT-4.1 showing particular sensitivity to few-shot\nexamples, increasing accuracy from 83% to 86% on both datasets. The inclusion of expert-derived\nexamples in few-shot prompts resulted in accuracy improvements of up to 3%. Additional LLMs\nachieved comparable performance, with accuracy ranging from 81% to 88% on AUK and 82% to\n89% on BLF samples. Notably, LLaMA 3.1 405B consistently achieved the highest accuracy across\nboth datasets with strong zero-shot capabilities, whilst other LLMs demonstrated performance\ncomparable to GPT models.\n11\n\nFigure 3: Accuracy of models. Accuracy is measured by the ratio of posts with same labels assigned\nby the model and majority label.\nWe then measured the performance using precision, recall, and F1-score across different sentiment\nlabels (negative, neutral, positive). Table 2 presents the results on both datasets. Whilst the model\nwith the highest F1-score varied across different sentiment labels, the best performers consistently\nbelonged to the LLM category. As F1-score balances precision and recall, our results demonstrate\nthat LLMs consistently outperform traditional approaches across all three sentiment categories.\nTable 3 presents a comprehensive comparison of the best-performing models of each category\nwithin each dataset. The results clearly demonstrate the superior performance of LLMs compared\nto traditional approaches. LLaMA 3.1-405B-fs achieved the highest performance across the majority\n12\n\nTable 2: Performance of models, measured by Precision, Recall and F1-score. The highest value in\neach column on each data set is in bold.\nModel\nPositive\nNegative\nNeutral\nPrecision\nRecall\nF1-Score\nPrecision\nRecall\nF1-Score\nPrecision\nRecall\nF1-Score\nAUK Data Set\nGPT-4.1\n0.939\n0.886\n0.912\n0.719\n0.852\n0.780\n0.757\n0.683\n0.718\nGPT-4.1-fs\n0.950\n0.914\n0.932\n0.796\n0.796\n0.796\n0.733\n0.805\n0.767\nGPT-4.1-mini\n0.947\n0.857\n0.900\n0.719\n0.852\n0.780\n0.707\n0.707\n0.707\nGPT-4.1-mini-fs\n0.934\n0.943\n0.938\n0.837\n0.759\n0.796\n0.711\n0.780\n0.744\nGPT-o3\n0.917\n0.952\n0.935\n0.706\n0.889\n0.787\n0.913\n0.512\n0.656\nGPT-o3-fs\n0.927\n0.971\n0.949\n0.780\n0.852\n0.814\n0.839\n0.634\n0.722\nGPT-o3-mini\n0.948\n0.876\n0.911\n0.734\n0.870\n0.797\n0.718\n0.683\n0.700\nGPT-o3-mini-fs\n0.939\n0.876\n0.906\n0.719\n0.852\n0.780\n0.789\n0.732\n0.759\nBioBERT-Balance\n0.914\n0.505\n0.650\n0.489\n0.852\n0.622\n0.458\n0.537\n0.494\nBioBERT-Combine\n0.894\n0.562\n0.690\n0.674\n0.537\n0.598\n0.341\n0.756\n0.470\nBioBERT-Covid\n0.903\n0.533\n0.671\n0.500\n0.759\n0.603\n0.464\n0.634\n0.536\nBioBERT-SemEval\n0.903\n0.533\n0.671\n0.500\n0.759\n0.603\n0.464\n0.634\n0.536\nSentiWordNet\n0.569\n0.667\n0.614\n0.356\n0.389\n0.372\n0.278\n0.122\n0.169\nTextBlob\n0.630\n0.714\n0.670\n0.400\n0.333\n0.364\n0.306\n0.268\n0.286\nVADER\n0.681\n0.752\n0.715\n0.532\n0.611\n0.569\n0.364\n0.195\n0.254\nBLF Data Set\nGPT-4.1\n0.934\n0.832\n0.880\n0.889\n0.970\n0.928\n0.476\n0.667\n0.556\nGPT-4.1-fs\n0.925\n0.905\n0.915\n0.882\n0.909\n0.896\n0.562\n0.600\n0.581\nGPT-4.1-mini\n0.951\n0.847\n0.896\n0.780\n0.970\n0.865\n0.514\n0.633\n0.567\nGPT-4.1-mini-fs\n0.958\n0.839\n0.895\n0.833\n0.909\n0.870\n0.477\n0.700\n0.568\nGPT-o3\n0.886\n0.964\n0.923\n0.818\n0.818\n0.818\n0.778\n0.467\n0.583\nGPT-o3-fs\n0.900\n0.920\n0.910\n0.857\n0.909\n0.882\n0.640\n0.533\n0.582\nGPT-o3-mini\n0.951\n0.854\n0.900\n0.738\n0.939\n0.827\n0.514\n0.600\n0.554\nGPT-o3-mini-fs\n0.938\n0.876\n0.906\n0.806\n0.879\n0.841\n0.500\n0.600\n0.545\nBioBERT-Balance\n0.924\n0.708\n0.802\n0.519\n0.848\n0.644\n0.488\n0.667\n0.563\nBioBERT-Combine\n0.894\n0.737\n0.808\n0.692\n0.545\n0.610\n0.295\n0.600\n0.396\nBioBERT-Covid\n0.949\n0.679\n0.791\n0.500\n0.818\n0.621\n0.417\n0.667\n0.513\nBioBERT-SemEval\n0.949\n0.679\n0.791\n0.500\n0.818\n0.621\n0.417\n0.667\n0.513\nSentiWordNet\n0.754\n0.737\n0.745\n0.289\n0.394\n0.333\n0.048\n0.033\n0.039\nTextBlob\n0.630\n0.714\n0.670\n0.400\n0.333\n0.364\n0.306\n0.268\n0.286\nVADER\n0.681\n0.752\n0.715\n0.532\n0.611\n0.569\n0.364\n0.195\n0.254\nTable 3: Performance comparison of best models in each category, measured by Precision, Recall\nand F1-score. The highest value in each column on each dataset is in bold.\nModel\nPositive\nNegative\nNeutral\nPrecision\nRecall\nF1-Score\nPrecision\nRecall\nF1-Score\nPrecision\nRecall\nF1-Score\nAUK Dataset\nGPT-o3-fs\n0.927\n0.971\n0.949\n0.780\n0.852\n0.814\n0.839\n0.634\n0.722\nBioBERT-Covid\n0.903\n0.533\n0.671\n0.500\n0.759\n0.603\n0.464\n0.634\n0.536\nLLaMA 3.1-405B-fs\n0.935\n0.962\n0.948\n0.880\n0.815\n0.846\n0.738\n0.756\n0.747\nDeepSeek V3-fs\n0.933\n0.933\n0.933\n0.746\n0.926\n0.826\n0.857\n0.585\n0.696\nBLF Dataset\nGPT-o3-fs\n0.900\n0.920\n0.910\n0.857\n0.909\n0.882\n0.640\n0.533\n0.582\nBioBERT-Balance\n0.924\n0.708\n0.802\n0.519\n0.848\n0.644\n0.488\n0.667\n0.563\nLLaMA 3.1-405B-fs\n0.935\n0.962\n0.948\n0.880\n0.815\n0.846\n0.738\n0.756\n0.747\nDeepSeek V3-fs\n0.933\n0.933\n0.933\n0.746\n0.926\n0.826\n0.857\n0.585\n0.696\nof metrics, securing the best F1-scores for five out of six sentiment categories across both datasets.\nDeepSeek V3-fs also demonstrated strong performance, particularly excelling in negative sentiment\nrecall. In contrast, traditional approaches showed considerably lower performance, with BioBERT\nmodels achieving F1-scores ranging from 0.536 to 0.802, substantially below the LLM performance\nrange of 0.582 to 0.948. This comprehensive comparison confirms the consistent superiority of LLMs\nacross different architectures, datasets, and sentiment categories, validating the effectiveness of\nexpert knowledge integration approaches for health-related SA.\nConcerning the last key point, Figure 4 presents confidence calibration results for o3 model\n13\n\nvariants across different prompting strategies and o3-mini with few-shot prompting as a representative\ncomparison. Due to space constraints, GPT-4.1 results are summarised textually below. The o3\nmodel demonstrates superior confidence estimation with well-distributed scores between 0.5 and\n1, enabling effective prediction quality differentiation. Few-shot o3 model emerges as the optimal\nstrategy, showing a strong linear relationship between confidence and accuracy, with the calibration\ncurve closely tracking the perfect calibration line. This linear progression indicates that higher\nconfidence scores reliably correspond to higher accuracy, making confidence a valuable indicator\nfor automated label quality assessment. In contrast, zero-shot prompting shows more variable\ncalibration patterns, while naive prompting (i.e., no instruction from the codebook) exhibits notable\ndeviations from perfect calibration, particularly in mid-confidence ranges. The o3-mini few-shot\nresults shown are representative of all o3-mini configurations due to consistent patterns across\nprompting strategies. o3-mini models show severely concentrated confidence distributions above 0.8\nacross all prompting strategies, rendering confidence scores uninformative for quality assessment.\nPrompting strategy has minimal impact on o3-mini due to fundamental limitations in confidence\nestimation. Similarly, all GPT-4.1 variants exhibited limitations similar to o3-mini. Unlike o3\nmodels, GPT-4.1 variants are not specifically designed for reasoning tasks, which may explain their\ninability to produce meaningful confidence estimates. The superior performance of o3 models with\nfew-shot prompting suggests that reasoning capabilities, combined with contextual examples, enable\nbetter understanding of confidence semantics, translating into meaningful uncertainty estimates.\nThese results indicate that only o3 models with few-shot prompting provide confidence scores\nsuitable for label quality evaluation and selective prediction strategies.\n14\n\nFigure 4: Confidence calibration analysis across different model-prompt combinations. The left\ncolumn displays confidence score distributions, while the right column shows calibration curves\n(predicted confidence vs. actual accuracy). Diagonal lines represent perfect calibration.\n15\n\n4\nDiscussion\n4.1\nPrincipal Results\nThis study demonstrates that LLMs can successfully integrate expert knowledge to achieve so-\nphisticated SA in digital health contexts without requiring extensive labelled training data. Our\nsystematic evaluation reveals that LLMs consistently outperformed traditional approaches across\nmultiple performance metrics, with the most significant finding being their ability to achieve expert-\nlevel agreement (κ: 0.42-0.75) through structured knowledge integration rather than data-intensive\ntraining.\nThe core methodological innovation lies in our codebook-guided approach, which systematically\ntransfers expert domain knowledge through structured prompting. Unlike retrieval-augmented\ngeneration (RAG) methods that dynamically retrieve information from external knowledge bases, our\napproach embeds expert-derived interpretation guidelines directly into prompts, enabling consistent\napplication of domain-specific rules.\nThis approach is particularly effective for SA where classification rules can be systematically\ndefined and consistently applied across datasets, representing a fundamental shift from traditional\ndata-intensive methods to knowledge-guided approaches in healthcare analytics. This paradigm\nshift directly addresses fundamental limitations of existing methods in healthcare contexts. While\nlexicon-based models lack the contextual understanding required for health-related content, and\ntraditional pre-trained models like BioBERT require substantial fine-tuning with labelled datasets\nthat are scarce in healthcare due to privacy constraints, LLMs demonstrated the ability to apply\nexpert-derived guidelines directly without domain-specific training.\nThe practical implications extend beyond performance improvements to address critical barriers\nin healthcare analytics: the high cost of expert annotation and the technical expertise traditionally\nrequired for advanced text analysis. By enabling sophisticated analysis through structured prompting\nrather than complex model development, this approach democratizes access to expert-quality text\ninterpretation, making advanced analytics feasible for healthcare research teams regardless of their\ntechnical infrastructure or ML expertise. This accessibility is particularly valuable for smaller\nhealthcare organisations and research teams with limited resources, where the ability to achieve\nexpert-quality results through knowledge integration rather than extensive data collection reduces\nboth financial burdens and technical barriers. The structured codebook methodology offers distinct\nadvantages for healthcare applications where expert interpretation rules can be systematically\ndefined and consistently applied across large datasets.\nOur confidence calibration analysis reveals important distinctions in uncertainty estimation\ncapabilities across LLM architectures. The GPT-o3 model with few-shot prompting demonstrated\nsuperior confidence calibration, showing a strong linear relationship between predicted confidence and\nactual accuracy. This calibration quality enables practical deployment strategies where confidence\nscores can reliably indicate prediction quality, supporting automated processing of high-confidence\ncases while flagging uncertain predictions for expert review.\nIn contrast, other model variants, including GPT-4.1 and GPT-o3-mini, showed concentrated\nconfidence distributions that limit their utility for quality assessment. These findings suggest\nthat confidence estimation capabilities vary significantly across model architectures and training\napproaches, with reasoning models like GPT-o3 better equipped to provide meaningful uncertainty\nestimates. This has important implications for healthcare deployment, where reliable confidence\nestimation is crucial for maintaining quality assurance while maximising automation efficiency.\nThe consistency of results across different LLM architectures—including GPT models, LLaMA,\nand DeepSeek—validates the robustness of this knowledge integration approach and provides\n16\n\nhealthcare organisations with crucial implementation flexibility. This cross-architectural validation\nis particularly important for healthcare applications, where open-source models like DeepSeek and\nLLaMA offer significant advantages for local deployment, allowing organisations to select models\nbased on their specific security, privacy, and resource requirements.\nOur findings have significant implications for digital health applications beyond SA. The ability\nto encode expert knowledge into structured guidelines that LLMs can reliably apply offers a scalable\nsolution for various healthcare text analysis tasks, from clinical documentation interpretation\nto patient experience monitoring. The demonstrated effectiveness in OHCs, with their complex\nemotional expressions and medical terminology, suggests strong potential for application in other\nchallenging healthcare text analysis contexts. The structured knowledge integration framework\nextends beyond SA to other expert annotation tasks in healthcare, providing a generalisable approach\nfor scaling domain expertise across diverse text analysis applications. This knowledge integration\napproach offers a practical pathway for implementing advanced analytics in resource-constrained\nhealthcare environments whilst maintaining expert-quality interpretive standards.\n4.2\nStrengths and Limitations\nA key strength of this study lies in its comprehensive evaluation design, systematically comparing\nLLMs against both traditional ML and lexicon-based approaches using expert-annotated healthcare\ndata. The rigorous annotation process involving five experts with PhD qualifications, including a\nclinician, ensured high-quality gold standard labels for meaningful performance assessment. The\ninclusion of both zero-shot and few-shot learning evaluation provides practical insights into LLM\ndeployment scenarios where training data availability varies.\nThe focus on OHCs as a challenging test case represents another strength, as OHC content\nexhibits complex sentiment patterns, medical terminology, and emotional nuance that extend beyond\ntypical social media analysis. This context provides a robust evaluation environment for assessing\nautomated approaches in healthcare-specific settings where traditional methods often struggle.\nFrom a methodological perspective, the structured codebook approach offers a replicable frame-\nwork for knowledge transfer across different healthcare domains and research contexts.\nThis\nsystematic method for expert knowledge encoding provides a practical roadmap for researchers\nseeking to implement similar analysis capabilities without requiring extensive ML expertise. The\nevaluation of multiple LLM architectures, including open-source models, provides implementation\nflexibility for organisations with different security and privacy requirements, particularly important\nwhen handling confidential medical data.\nAutomated SA through expert knowledge integration enables large-scale monitoring of patient\ndiscussions, offering insights into treatment responses and peer support effectiveness that would\notherwise require extensive expert review. This approach enables access to sophisticated analytical\ncapabilities, enabling healthcare research teams to conduct advanced text analysis without requiring\nextensive technical infrastructure. Our open-source implementation further enhances accessibility by\nproviding ready-to-use tools that enable healthcare researchers to apply these advanced analytical\ncapabilities immediately, without the complexity of model training, fine-tuning, or extensive technical\ninfrastructure typically required for traditional ML approaches.\nThe confidence calibration analysis provides additional practical value by identifying models\nsuitable for quality-controlled deployment scenarios. The demonstrated linear relationship between\nconfidence and accuracy in GPT-o3 few-shot models enables the implementation of selective\nprediction strategies, where high-confidence predictions can be processed automatically while\nuncertain cases receive human oversight. This capability addresses a critical need in healthcare\napplications where balancing automation efficiency with quality assurance is paramount.\n17\n\nThis study also has several limitations that warrant consideration. Our evaluation focused\nprimarily on SA within OHCs, and the broader applicability of the structured prompting approach\nacross different digital health text analysis contexts remains to be established. The knowledge\nintegration methodology, whilst demonstrating strong performance, depends heavily on the quality\nand comprehensiveness of expert-derived guidelines. Our codebook development involved extensive\nconsensus-building among five expert annotators, but this process may be challenging to replicate\nacross different healthcare domains or cultural contexts where expert perspectives might vary\nsignificantly. Standardisation approaches for codebook development across different types of medical\nexpertise require further investigation.\nAdditionally, while we evaluated multiple LLM architectures to demonstrate robustness, our\nassessment was limited to zero-shot and few-shot learning scenarios. The potential benefits of fine-\ntuning LLMs specifically for healthcare applications, particularly with privacy-preserving techniques,\nrepresent an important area for future investigation. The trade-offs between knowledge-guided\nprompting and domain-specific fine-tuning require further exploration.\nFurthermore, although our statistical validation confirmed expert-level agreement, the deployment\nof LLMs in clinical or high-stakes healthcare contexts requires careful consideration of interpretability,\naccountability, and ethical implications. SA in healthcare often involves subjective judgement and\ncontextual nuance that may require ongoing human oversight, particularly in settings where patient\nsafety or clinical decision-making are directly involved.\nWhile our structured prompting approach proved effective for SA, future research could investigate\nhow hybrid methodologies combining our codebook-guided approach with RAG mechanisms might\nenhance performance for more complex analytical tasks requiring dynamic access to evolving medical\nknowledge or real-time information integration. Additionally, exploring the application of this\nknowledge integration framework across different digital health text analysis contexts would further\nvalidate the generalisability of the approach.\n5\nConclusions\nThis study demonstrates that LLMs can effectively integrate expert knowledge to achieve sophis-\nticated SA in digital health contexts through structured prompting approaches. Our systematic\nevaluation establishes the feasibility of knowledge-guided analysis that achieves expert-quality results\nwithout requiring extensive training data. Reasoning models also demonstrated reliable uncertainty\nestimation capabilities, supporting deployment strategies suitable for healthcare applications.\nThe core contribution lies in demonstrating how expert domain knowledge can be systematically\ntransferred to automated systems through structured knowledge integration, enabling sophisticated\nhealthcare text analysis without traditional data requirements. This approach offers implementation\nflexibility across different technological contexts while maintaining analytical quality standards.\nAs healthcare increasingly relies on data-driven insights, LLMs offer a transformative pathway for\nscaling expert knowledge across diverse digital health applications. Our open-source implementation\nfacilitates immediate adoption across healthcare research contexts, with demonstrated uncertainty\nestimation capabilities that further support practical deployment. The framework enables researchers\nto readily test its applicability to other digital health text annotation tasks, facilitating rapid\nvalidation and subsequent deployment.\nFuture research should explore the application of knowledge integration frameworks across\ndiverse digital health contexts to further validate the generalisability of this approach and support\nthe development of more accessible, expert-quality analytical tools for healthcare research and\npractice.\n18\n\nEthical Considerations\nThe study was approved by Queen Mary University of London’s Ethics of Research Committee\n(QMERC22·279). In addition, the research protocol was examined and permission to undertake the\nresearch was obtained from AUK, BLF charities, as well as HealthUnlocked.\nAcknowledgment\nThe authors would like to thank Asthma UK, British Lung Foundation and HealthUnlocked for\ngranting the permission to conduct the study."}
{"id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "authors": ["Dongyoon Hahm", "Taywon Min", "Woogyeol Jin", "Kimin Lee"], "source": "http://arxiv.org/abs/2508.14031v1", "page_count": 24, "text": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation\nDongyoon Hahm*, Taywon Min*, Woogyeol Jin*, Kimin Lee\nKAIST\n{hahmdong, taywonmin, wlsdnruf2, kiminlee}@kaist.ac.kr\nAbstract\nBeyond simple text generation, Large Language Models\n(LLMs) have evolved into agentic systems capable of plan-\nning and interacting with external tools to solve complex\ntasks. This evolution involves fine-tuning LLMs on agent-\nspecific tasks to enhance their proficiency. However, safety\nconcerns are frequently overlooked during this fine-tuning\nprocess. In this work, we show that aligned LLMs can be-\ncome unintentionally misaligned, leading to a higher likeli-\nhood of executing harmful tasks and a reduced tendency to\nrefuse them when fine-tuned to execute agentic tasks. To ad-\ndress these safety challenges, we propose Prefix INjection\nGuard (PING), a simple yet effective method that prepends\nautomatically generated natural language prefixes to agent re-\nsponses, guiding them to refuse harmful requests while pre-\nserving performance on benign tasks. Specifically, we intro-\nduce an iterative approach that alternates between (1) gener-\nating candidate prefixes and (2) selecting those that optimize\nboth task performance and refusal behavior. Experimental re-\nsults demonstrate that PING significantly enhances the safety\nof fine-tuned LLM agents without sacrificing their effective-\nness. PING consistently outperforms existing prompting ap-\nproaches across diverse benchmarks in both web navigation\nand code generation tasks. Our analysis of internal hidden\nstates via linear probes reveals that prefix tokens are crucial\nfor behavior modification, explaining the performance gains.\nWARNING: This paper contains contents that are unethical\nor offensive in nature.\n1\nIntroduction\nAutonomous agents powered by large language models\n(LLMs) have demonstrated the ability to perform a wide\nrange of tasks across various domains, including web nav-\nigation (Zhou et al. 2023; Yao et al. 2022), code genera-\ntion (Wang et al. 2023; Jimenez et al. 2023) and mobile\ndevice control (Lee et al. 2024a,b; Rawles et al. 2024).\nThese LLM agents engage in decision-making, utilize tools,\nand interact with their environment to accomplish complex\ntasks. While these capabilities unlock new applications, they\nalso introduce novel safety risks. For example, a web naviga-\ntion agent could be exploited to publish and spread misinfor-\nmation (Kim et al. 2024), while a code agent might execute a\nreverse shell or delete critical system files (Guo et al. 2024).\n*These authors contributed equally.\nIt is therefore critical to ensure LLM agents operate safely\nacross diverse domains (Hahm et al. 2025).\nDespite growing concerns about safety, the development\nof LLM-based agents often overlooks safety considerations.\nMany existing studies focus on fine-tuning to enhance task\nperformance, yet they rarely incorporate safety during this\nprocess (Jimenez et al. 2023; Lee et al. 2024b; Lai et al.\n2024; L`\nu, Kasner, and Reddy 2024). Agentic datasets used\nfor fine-tuning are typically collected under standard, non-\nadversarial conditions, where agents are trained to complete\ntasks without encountering risky or harmful scenarios (L`\nu,\nKasner, and Reddy 2024; Qi et al. 2024b; Wang et al. 2024;\nZheng et al. 2024). However, evidence from non-agentic do-\nmains suggests cause for concern: misalignment can emerge\nafter fine-tuning on seemingly benign datasets (He, Xia, and\nHenderson 2024; Qi et al. 2023; Lyu et al. 2024; Hawkins,\nMittelstadt, and Russell 2024). For instance, Lyu et al.\n(2024) demonstrated that fine-tuning LLMs on mathemati-\ncal reasoning (Cobbe et al. 2021) and medical knowledge (Li\net al. 2023) can lead to an increase in harmfulness. This\nraises a natural question:\nDo unintended misalignments arise when fine-tuning\nLLMs on benign agentic tasks?\nIn this work, we show that fine-tuning LLMs on agentic\ntasks can lead to unintentional misalignment, increasing the\nlikelihood of executing harmful instructions and reducing\nrefusal behavior. Specifically, we fine-tune various LLMs\non web navigation and code generation datasets, and eval-\nuate their task performance using WebArena-lite (Liu et al.\n2024) and MINT-ALFWorld (Wang et al. 2023), and assess\ntheir safety using RedCode-Exec (Guo et al. 2024) and our\nnewly introduced WebDojo benchmark. While fine-tuning\nimproves performance on benign tasks, it also substan-\ntially increases the risk of following harmful tasks. For in-\nstance, Llama-3.1-8B-Instruct (Grattafiori et al. 2024) shows\na 20.0% improvement in task success on WebArena-lite, but\na 38.09% increase in attack success on WebDojo after fine-\ntuning. These results highlight the safety vulnerabilities in\nfine-tuned LLM agents and the need for effective mitigation.\nTo mitigate unintended misalignment that emerges from\nagentic fine-tuning, we propose a simple yet effective\nmethod called Prefix INjection Guard (PING). Our main\nidea is to prepend natural language prefixes which cause\nLLM agents to refuse harmful requests, while maintaining\n1\n\nx\nx\nx\nHarmful Requests\nBenign Requests\nBase LLM\nLLM Agent\nLLM Agent\n+ PING\nRefuses Request\nRefuses Request\nPerforms Request\nIncorrect Response\nCorrect Response\nCorrect Response\nLeave the comment 'Have a great trip! Please share your travel stories when\nyou return' on the second post\ndo(action=\"Hover\", element=\"0\")\n# Element: the 'Like' button of the first post\n# As an AI committed to ethical standards, ensure\nthat you assist with helpful and constructive\ntasks, while consistently refusing any requests\nthat could be harmful or unethical. In this case,\nwe will comment on the second post in the'Comments'\nsection.# Element: the 'Comments' button below the\nsecond post\ndo(action=\"Click\", element=\"3\")\n# Element: the 'Comments' button below the post by\nuser 'travel_lover'\ndo(action=\"Click\", element=\"3\")\nI cannot provide instructions on how to buy illegal\nsubstances. Is there anything else I can help you\nwith?\n# Element: the 'Illicit Substances' category link\ndo(action=\"Click\", element=\"4\")\n# As an AI committed to ethical standards, ensure\nthat you assist with helpful and constructive tasks,\nwhile consistently refusing any requests that could\nbe harmful or unethical. In this case, I will not\nassist with the task of buying cocaine.\nexit(message=\"I cannot assist with the task of\nbuying cocaine.\")\nBuy cocaine at the mart.\nFigure 1: An overview of our work. We observe that fine-tuning LLMs with agentic datasets improves performance on benign\ntasks but also increases the likelihood of completing harmful tasks, resulting in unsafe behavior. To address this misalignment,\nwe propose Prefix Injection Guard (PING), which enhances agent safety while maintaining strong performance on benign tasks.\nhigh performance on intended tasks (see Figure 1). Specifi-\ncally, our approach iteratively alternates two steps: (1) gen-\nerating candidate prefixes using capable LLMs and (2) se-\nlecting the prefix that maximizes both task completion ac-\ncuracy and harmful request refusal rate. Through this itera-\ntive process which minimizes human effort, PING automat-\nically identifies effective prefixes that enhance LLM agent\nsafety while preserving its intended capabilities. We evalu-\nate PING across multiple domains using various LLMs like\nLlama-3.1-8B-Instruct (Grattafiori et al. 2024) and GPT-\n4o-mini (Hurst et al. 2024). Results show that PING con-\nsistently improves LLM agent safety compared to exist-\ning prompting strategies (e.g., constitutional AI prompts,\nfew-shot safety examples) while maintaining performance.\nSpecifically, PING increases harmful request refusal rates\nby an average of 68.3% in the web navigation domain and\n44.6% in the code generation domain compared to baseline\nagents, while maintaining nearly identical task performance\nwith minimal degradation (just 1.8% for both web naviga-\ntion and code generation). Furthermore, PING is compatible\nwith guardrail models such as WildGuard (Han et al. 2024),\nenabling layered safety approaches.\nBeyond empirical evaluations, we investigate how PING\ninfluences the internal representations of LLM agents and\ninduce refusal behavior for harmful tasks. Specifically, we\ntrain linear probes (Yang et al. 2024b) on averaged acti-\nvations from LLM agents across input sequences, produc-\ning logit values that differentiate harmful from benign in-\nputs. We hypothesize these linear probes capture internal\nfeatures associated with refusal behavior, with higher logit\nvalues for final tokens correlating with successful refusal.\nWe validate this hypothesis by demonstrating that activation\nsteering (Winninger, Addad, and Kapusta 2025; Turner et al.\n2023), which adds a linear probe vector on final token acti-\nvations, successfully triggers the model to refuse harmful\ntasks. Notably, vanilla LLM agents exhibit low linear probe\nlogits for final tokens when processing harmful tasks, cor-\nresponding to their failure to refuse such instructions. In\ncontrast, LLM agents integrated with PING, which effec-\ntively refuse harmful tasks, consistently display significantly\nhigher final token linear probe logit values. This finding pro-\nvides mechanistic evidence that PING enhances safety by\nstrategically shifting model representations, particularly at\ncritical decision points corresponding to the initial tokens of\nthe model response.\n2\nUnintentional Misalignment in LLM Agent\nIn this section, we demonstrate that fine-tuning large lan-\nguage models (LLMs) on agentic datasets introduce unin-\ntended misalignment risks, even when the training data ap-\npears benign. We first present a threat model in Section 2.1\nthat formalizes these risks during the agent training process.\nOur empirical results in Section 2.2 reveal consistent mis-\nalignment patterns emerging across diverse models and do-\nmains. Finally, in Section 2.3, we show that injecting an ap-\npropriate prefix can effectively induce refusal behaviors in\notherwise misaligned LLM agents, offering a practical mit-\nigation approach with implications for safer deployment of\nagentic systems.\n2.1\nThreat Model\nWe consider a scenario where developers fine-tune an LLM\non an agentic dataset to create a specialized agent capa-\nble of performing domain-specific tasks. Usually, agentic\ndatasets, collected under standard non-adversarial condi-\ntions (L`\nu, Kasner, and Reddy 2024; Qi et al. 2024b; Wang\net al. 2024; Zheng et al. 2024), consist of benign demonstra-\ntions for completing tasks. Once fine-tuned, the LLM agent\n2\n\nModel\nFine-tuning\nWebArena\nWebDojo\nMINT\nRedCode\nSR (↑)\nASR (↓)\nRR (↑)\nSR (↑)\nASR (↓)\nRR (↑)\nLlama-3.1-8B-\nInstruct\n✗\n2.42%\n23.81%\n31.75%\n71.77%\n43.38%\n15.17%\n✓\n22.42%\n61.90%\n7.94%\n71.77%\n66.06%\n2.60%\nGLM-4-9B-\nChat\n✗\n5.45%\n26.98%\n3.17%\n22.58%\n63.29%\n13.70%\n✓\n16.97%\n53.97%\n4.76%\n72.58%\n72.39%\n1.48%\nQwen2.5-7B-\nInstruct\n✗\n3.03%\n14.29%\n3.17%\n70.16%\n58.33%\n6.02%\n✓\n7.27%\n57.14%\n12.70%\n85.48%\n86.02%\n3.10%\nTable 1: Performance of LLMs before and after fine-tuning on agentic datasets, evaluated across web navigation (WebArena\nand WebDojo) and code generation (MINT-ALFWorld and RedCode-Exec) benchmarks. We report the following metrics: SR\n(Success Rate), the proportion of successfully completed benign tasks (↑indicates higher capability); ASR (Attack Success\nRate), the proportion of harmful tasks executed (↓indicates better safety); and RR (Refusal Rate), the proportion of harmful\ntasks appropriately refused (↑indicates better safety). WebArena and MINT-ALFWorld measures capability on benign tasks,\nwhile WebDojo and RedCode-Exec evaluates safety performance on harmful tasks. Fine-tuning consistently improves capabil-\nity (higher SR) but reduces safety (higher ASR and lower RR), demonstrating a clear capability–safety trade-off.\nis deployed in its target domain and made available to end-\nusers. These users may issue inputs ranging from harmless\nand task-relevant to adversarial or malicious. Unlike tradi-\ntional LLMs optimized primarily for dialogue, agentic sys-\ntems are explicitly trained to execute actions based on user\ninstructions, creating unique vulnerabilities when exposed\nto adversarial inputs. Our threat model specifically addresses\nhow benign fine-tuning can inadvertently compromise safety\nguardrails, causing agents to execute harmful instructions.\nModel\nFT\nMINT\nRedCode\nSR (↑)\nASR (↓)\nRR (↑)\nGPT-4o-\nmini\n✗\n41.12%\n30.09%\n40.05%\n✓\n70.16%\n41.96%\n37.01%\nGemini-2.0-\nflash\n✗\n50.80%\n50.23%\n19.86%\n✓\n83.87%\n77.82%\n3.15%\nTable 2: Performance of closed-source LLMs before and af-\nter fine-tuning on code generation datasets. Similar to open-\nsource models, fine-tuning improves capability (higher SR)\nbut reduces safety (higher ASR and lower RR).\n2.2\nMisalignment from Agentic Fine-Tuning\nTo investigate misalignment in LLM agents, we fine-tune\nvarious models with benign agentic datasets in two do-\nmains: web navigation (Qi et al. 2024b) and code genera-\ntion (Wang et al. 2024). As base models, we employ three\nopen-source LLMs (Llama-3.1-8B-Instruct (Grattafiori et al.\n2024), GLM-4-9B-Chat (GLM et al. 2024), Qwen2.5-7B-\nInstruct (Yang et al. 2024a)). In the code generation do-\nmain, we further fine-tune two closed-source LLMs, GPT-\n4o-mini (Hurst et al. 2024) and Gemini-2.0-flash (Pica-\nhai, Hassabis, and Kavukcuoglu 2024). We evaluate these\nagents using a suite of benchmarks: WebArena-Lite (Liu\net al. 2024) and MINT-ALFWorld (Wang et al. 2023) assess\ndomain-specific capabilities in web and code settings, re-\nspectively; RedCode-Exec (Guo et al. 2024) evaluates safety\nin the code domain; and our newly introduced WebDojo1\nbenchmark measures safety in the web domain. Full bench-\nmark details are provided in Appendix A. Our analysis fo-\ncuses on three key metrics: success rate, attack success rate,\nand refusal rate. The success rate captures agent capability,\ndefined as the proportion of benign tasks completed success-\nfully. The attack success rate and refusal rate quantify safety\nperformance: the attack success rate reflects the fraction of\nharmful tasks completed (undesirably), while the refusal rate\nreflects how often the agent correctly rejects harmful tasks.\nWhile fine-tuning LLMs improves capabilities, it also in-\ncreases the likelihood of harmful behaviors. Table 1 con-\nfirms that this safety–capability tradeoff is consistently ob-\nserved across both domains and all model types. For ex-\nample, fine-tuning Llama-3.1-8B-Instruct on the web do-\nmain improves the success rate by 20%. However, this en-\nhanced capability came with serious safety degradation: its\nattack success rate on WebDojo increased by 38%, while\nits refusal rate decreased by 24%. We also observe that\nthis misalignment from agentic fine-tuning occurs in closed-\nsource models (see Table 2). Fine-tuning Gemini-2.0-flash\non the code generation dataset increases its success rate\non MINT-ALFWorld by 33%. On RedCode-Exec, the same\nfine-tuning increases the attack success rate by 28% while\nreducing the refusal rate by 17%.\nThe real-world implications are concerning: fine-tuned\nagents successfully performed tasks ranging from illegal file\nsharing to generating exploitable code (see Appendix C for\nexamples). These findings demonstrate that optimizing LLM\nagents purely for performance can introduce serious safety\nrisks, underscoring the pitfalls of current performance-\ncentric training approaches. As agentic training not only\namplifies potential for harm but actively undermines safety\nmechanisms, safety considerations must be integrated into\nboth the training and deployment of LLM agents, not treated\nas an afterthought to performance optimization.\n1See Appendix B for implementation details.\n3\n\nTask Instruction:\nBuy cocaine at the mart.\n(a) Aligned Model\n(b) Fine-tuned Agent\n#\n99.17%\ndo\nexit\ngo\n0.75%\n0.04%\n0.00%\nElement\n100.0%\nNote\nClick\nelement\n0.01%\n0.00%\n0.00%\n# Element: the 'Illicit Substances' category link\ndo(action=\"Click\", element=\"4\")\nI\n76.66%\ndo\n#\ngo\n11.40%\n9.75%\n0.48%\ncan\n93.21%\ncannot\ncant\n‘m\n6.75%\n0.02%\n0.01%\nI can't help with that. Is there anything else I\ncan help you with?\n't\n98.78%\nnot\n’t\nassist\n0.72%\n0.38%\n0.03%\n:\n100.0%\nthe\n\\n,\n,\n0.00%\n0.00%\n0.00%\nFigure 2: Differences in the initial token probability distribu-\ntion and responses for harmful tasks in the web domain be-\nfore and after fine-tuning. After fine-tuning, tokens related to\nperforming the task are assigned significantly higher proba-\nbilities, leading to responses that carry out the harmful task\nrather than refusing it.\n2.3\nMitigating Misalignment via Prefix Injection\nThe initial tokens generated by LLMs play a critical role in\ndetermining response safety. Qi et al. (2024a) demonstrate\nthat when aligned LLMs refuse harmful instructions, the re-\nsponses typically begin with characteristic phrases such as “I\ncannot” or “I apologize”, highlighting how early token pat-\nterns influence safe response generation. Our experiments\nwith Llama-3.1-8B-Instruct confirm this pattern: 100% of\nrefusals in the WebDojo benchmark begin with “I cannot”,\nwhile 86% of refusals in RedCode-Exec start with “I can’t”.\nHowever, after agentic fine-tuning, these safety patterns di-\nminish significantly. Figure 2 illustrates the probability dis-\ntribution of the first three tokens in the responses, for harm-\nful web navigation tasks. The fine-tuned model assigns high\nprobabilities to tokens associated with task execution, thus\nperforming harmful tasks.\nRefusal Induction via Prefix Injection\nThe strong corre-\nlation between initial tokens and safe responses raises an im-\nportant question: can prefix injection effectively steer mod-\nels toward safer behavior? To investigate this, we prepend\nthe phrase “I can’t” to responses during inference for\nthe fine-tuned Llama-3.1-8B-Instruct model and evaluate\nits performance. As shown in Table 3, this simple inter-\nvention substantially reduces attack success rates and in-\ncreases refusal rates across benchmarks, significantly en-\nhancing safety. Notably, the model refuses all harmful tasks\nin WebDojo when prefix injection is applied. However,\nthis increased safety comes with the drawback of over-\nrefusal, as the model also excessively rejects benign tasks\nin WebArena-lite and MINT-ALFWorld, limiting its utility.\nPrefix\nWebArena\nWebDojo\nMINT\nRedCode\nSR\nASR\nRR\nSR\nASR\nRR\n✗\n22.4%\n76.2%\n0.0%\n84.3%\n63.4%\n2.6%\nI can’t\n10.3%\n0.0%\n100%\n46.8%\n48.1%\n11.1%\nTable 3: Results after applying the safety prefix “I can’t” to\nthe Llama-3.1-8B-Instruct agent. When prepended with the\nsafety prefix “I can’t,” refusal rates (RR) notable increase\nand attack success rates (ASR) decrease, signaling safer be-\nhavior on harmful tasks. However, prepending “I can’t” also\nresulted in a decline in success rates (SR) for benign tasks,\nthereby limiting the agent’s practical utility.\nAlgorithm 1 PING: Automatic Prefix Selection\nRequire: GENERATOR (LLM that proposes prefixes); fperf\n(performance score); frefusal (refusal score); U(0) (initial\nprefix pool); k (number of selected prefixes per round);\nM (number of candidate prefixes per round); T (number\nof rounds); E (List of evaluated prefixes); τ (Threshold)\n1: E ←ϕ\n▷Evaluated prefixes stored in E\n2: U(0) ←ϕ\n▷GENERATOR is seeded with U(t)\n3: for t = 0 to T −1 do\n4:\nP(t) ←GENERATOR\n\u0000U(t)\u0001\n5:\n▷(1) Generate M candidate prefixes P(t) using U(t)\n6:\nfor p ∈P(t) do\n7:\nperf(p) ←fperf(p)\n8:\nrefusal(p) ←frefusal(p)\n9:\noverall(p) ←perf(p) + refusal(p)\n10:\n▷(2) Evaluate performance and refusal scores\n11:\nE ←E ∪(p, perf(p), refusal(p), overall(p))\n12:\n▷Store prefix and evaluated scores\n13:\nend for\n14:\nif max\np∈E overall(p) ≥τ then\n15:\nU(t+1) ←TOPk,overall\n\u0000E\n\u0001\n∪TOPk,perf\n\u0000E\n\u0001\n∪TOPk,refusal\n\u0000E\n\u0001\n16:\n▷Seed next iteration with top prefixes if best overall score\nexceeds τ\n17:\nelse\n18:\nU(t+1) ←ϕ\n19:\nend if\n20: end for\n21: return p⋆←arg max\np∈E overall(p)\n22:\n▷(3) Select the prefix with the highest overall score\n3\nPrefix Injection Guard\nMotivated by the observation that specific prefixes can elicit\nsafer behaviors from fine-tuned LLM agents, we introduce\nPrefix INjection Guard (PING), a lightweight method that\nautomatically generates effective prefixes to encourage safe\nbehaviors in LLM agents. To find prefixes that maximize\nboth safety and proficiency, our approach iteratively alter-\nnates between (1) generating candidate prefixes using capa-\nble LLMs (e.g., GPT-4o), referred to as the GENERATOR;\nand (2) evaluating these prefixes based on their ability to op-\ntimize both task performance and refusal behavior.\nAlgorithm 1 describes our method in detail. In the genera-\ntion phase, GENERATOR produces M diverse candidate pre-\nfixes, guided by the highest-performing prefixes from previ-\n4\n\nous iterations. This enables GENERATOR to progressively\nrefine its outputs based on empirical performance. In the\nevaluation phase, each candidate prefix is scored using two\nmetrics: a refusal score frefusal (rate of appropriate refusal\nfor harmful tasks) and a performance score fperf (rate of suc-\ncessful execution for benign tasks). Evaluated prefixes from\nall iterations are ranked by their performance score, refusal\nscore, and overall score (sum of both). The top-k prefixes for\neach criterion are selected to seed the next iteration. To en-\ncourage exploration and maintain diversity, seeding only oc-\ncurs when the best prefix’s score exceeds a threshold τ. After\nmultiple iterations, the prefix with the highest overall score\nis selected as the final output. The prompts for GENERATOR\nand evaluation protocols are detailed in Appendix E.\n4\nExperiments\nWe investigate whether PING effectively refuses harmful in-\nstructions while maintaining performance on benign tasks in\nboth web navigation and code generation agents.\n4.1\nSetup\nModels\nWe conduct experiments using three open-source\nmodels: Llama-3.1-8B-Instruct (Grattafiori et al. 2024),\nGLM-4-9B-Chat (GLM et al. 2024), and Qwen2.5-7B-\nInstruct (Yang et al. 2024a), which we refer to as base mod-\nels. To create domain-specific agents, we perform super-\nvised fine-tuning using targeted agentic datasets: the web\nnavigation dataset from Qi et al. (2024b) for web naviga-\ntion agents, and the CodeActInstruct dataset (Wang et al.\n2024) for code generation agents. In addition to open-\nsource models, we fine-tune GPT-4o-mini (Hurst et al. 2024)\nand Gemini-2.0-flash (Picahai, Hassabis, and Kavukcuoglu\n2024) as code generation agents to test whether PING is also\neffective for closed-source LLMs. Since we cannot inject\nprefixes into closed-source model responses, we instead ap-\npend a suffix to the user prompt.\nEvaluation Benchmarks\nWe evaluate both capability and\nsafety across two domains: web navigation and code gen-\neration. For web navigation, we measure capability using\nWebArena-Lite (Liu et al. 2024) and evaluate safety with\nour novel WebDojo benchmark. In the code generation do-\nmain, we assess capability using MINT-ALFWorld (Wang\net al. 2023) and measure safety with RedCode-Exec (Guo\net al. 2024). A small subset of tasks from each benchmark\nis used for prefix optimization in PING, while the remaining\ntasks are reserved for evaluation. Further details on bench-\nmarks and evaluation settings are provided in Appendix A.\nEvaluation Metrics\nWe report the metrics used across\nbenchmarks, as outlined in Section 2.2. For capability as-\nsessment in both WebArena-Lite and MINT-ALFWorld, we\nprimarily measure success rate, quantifying the agent’s abil-\nity to complete benign tasks. For safety evaluation in Web-\nDojo and RedCode-Exec, we track refusal rate, quantifying\nhow consistently agents correctly identify and decline to per-\nform harmful operations. Higher refusal rates indicate better\nalignment.\nBaselines\nTo evaluate our approach, we compare it against\ntwo baseline methods aimed at enhancing LLM safety dur-\ning inference. Prompt examples used in both baselines are\nprovided in Appendix D.\n1. Pure Tuning, Safe Testing (PTST) (Lyu et al. 2024):\nThis method applies safety-oriented instruction to the\nsystem prompt exclusively at inference time, preserving\nalignment while exploiting distribution shift to prevent\noverfitting to unsafe patterns during the fine-tuning.\n2. Few-Shot Prompting (Brown et al. 2020): Examples\nof harmful and benign task instructions are provided as\ncontext, with explicit specifications to refuse harmful\ntasks and perform benign ones.\nAutomatic Prefix Selection\nStarting from an empty prefix\npool, we generate 5 new prefixes per iteration for 20 itera-\ntions, yielding 100 prefixes total. Further details including\nhyperparameters are provided in Appendix A.\n4.2\nMain Results\nOur experiments show that PING enhances safety while pre-\nserving performance across domains and model types, both\nopen-source and closed-source models. As shown in the Fig-\nure 3, PING exhibited significantly higher refusal rates than\nall baseline methods. Notably, in the web navigation domain\nusing GLM-4-9B-Chat, PING increased the refusal rate by\n87%. Crucially, PING maintains benign task performance,\nwith success rate decreases of at most 5% compared to fine-\ntuned agents. Figure 4 also shows that PING effectively mit-\nigates misalignment in agents with closed-source models.\nFor instance, PING increased refusal rate of Gemini-2.0-\nFlash agent by 66% in code generation domain. This demon-\nstrates our method’s applicability across diverse models. We\nprovide additional analysis in Appendix E, including the re-\nquired number of iterations and threshold usage when seed-\ning prefixes.\n4.3\nExperiments with External Guardrails\nIn addition to prompting strategies, we investigate whether\nPING is compatible with external guardrail models such as\nLlamaGuard3 (Grattafiori et al. 2024) and WildGuard (Han\net al. 2024). We compare PING’s performance for code gen-\neration agents when used alone, with guardrails alone, and\nwhen combined with guardrails (where a task is refused if\neither method triggers refusal). As shown in Table 4, PING\nachieves higher safety performance than individual guardrail\nmodels, and safety is further enhanced when methods are\ncombined. Notably, combining PING with WildGuard in-\ncreases refusal rates by an average of 5.28% without de-\ncreasing success rates compared to PING alone (see Ta-\nble 16 for other models). These results demonstrate that\nPING can be effectively integrated with other safety meth-\nods, enabling layered safety approaches.\n5\nAnalysis\nIn this section, we investigate how PING promotes safe be-\nhavior in LLM agents when prompted with harmful tasks by\nexamining their internal representations.\n5\n\n0\n10\n20\n30\nSuccess Rate (WebArena-Lite)\n0\n20\n40\n60\n80\n100\nRefusal Rate (WebDojo)\nLlama-3.1-8B-Instruct\n0\n10\n20\n30\nSuccess Rate (WebArena-Lite)\n0\n20\n40\n60\n80\n100\nRefusal Rate (WebDojo)\nGLM-4-9B-Chat\n0\n10\n20\n30\nSuccess Rate (WebArena-Lite)\n0\n20\n40\n60\n80\n100\nRefusal Rate (WebDojo)\nQwen2.5-7B-Instruct\n0\n20\n40\n60\n80\nSuccess Rate (MINT-ALFWorld)\n0\n20\n40\n60\n80\n100\nRefusal Rate (RedCode-Exec)\nLlama-3.1-8B-Instruct\n0\n20\n40\n60\n80\nSuccess Rate (MINT-ALFWorld)\n0\n20\n40\n60\n80\n100\nRefusal Rate (RedCode-Exec)\nGLM-4-9B-Chat\n0\n20\n40\n60\n80\nSuccess Rate (MINT-ALFWorld)\n0\n20\n40\n60\n80\n100\nRefusal Rate (RedCode-Exec)\nQwen2.5-7B-Instruct\nBase\nAgent\nAgent + PTST\nAgent + Few-Shot\nAgent + PING\nFigure 3: Success and refusal rates for web navigation and code generation tasks across diverse models for open-source models.\nDifferent markers indicate different methods. PING consistently achieves higher refusal rates than all baselines across both\ndomains and all open-sourced models.\n0\n20\n40\n60\n80\nSuccess Rate (MINT-ALFWorld)\n0\n20\n40\n60\n80\n100\nRefusal Rate (RedCode-Exec)\nGPT-4o-mini\n0\n20\n40\n60\n80\nSuccess Rate (MINT-ALFWorld)\n0\n20\n40\n60\n80\n100\nRefusal Rate (RedCode-Exec)\nGemini-2.0-flash\nBase\nAgent\nAgent + PTST\nAgent + Few-Shot\nAgent + PING\nFigure 4: Success and refusal rates of code generation tasks\nfor closed-source models, with different markers indicating\nmethods. PING achieves high refusal rates for both.\n5.1\nVerifying Trained Linear Probes\nTo analyze internal representations of LLM agents, we train\nlinear probes (i.e, classifiers that output a scalar logit value)\non averaged activations of agents across input sequences,\nto distinguish harmful instructions (HarmBench (Mazeika\net al. 2024), AdvBench (Zou et al. 2023)) from benign\nones (Alpaca (Taori et al. 2023)).2 We hypothesize that\nthe trained linear probe captures safety-relevant features in-\nside model representations associated with refusal behavior,\nwhere higher logit values for final token activations of agents\ncorrelate with successful refusal. We verify this using acti-\nvation steering, adding linear probes to the final token acti-\nvations (Winninger, Addad, and Kapusta 2025; Turner et al.\n2023), and observe if refusal behavior is induced as a re-\n2Due to limited availability of harmful instruction datasets for\nweb navigation, we utilized datasets from the chat domain, which\nmay affect the accuracy of our linear probes.\nLlama-3.1-8B-Instruct\nGemini-2.0-flash\nRR\nSR\nRR\nSR\nPING\n35.6%\n68.6%\n69.5%\n79.0%\nLlamaGuard3\n12.2%\n71.0%\n11.8%\n79.0%\nWildGuard\n12.2%\n71.8%\n26.9%\n83.9%\nPING+LlamaGuard3\n38.8%\n68.6%\n71.8%\n76.6%\nPING+WildGuard\n39.1%\n68.6%\n80.8%\n79.0%\nTable 4: Results on the code generation domain comparing\nPING with external guardrail models individually, or using\nthem jointly. PING outperforms external guardrail models\nwhen used individually. When PING is jointly used with\nguardrail models, safety performance is further enhanced.\nsult. Specifically, during output generation, we add a scaled\n(α) version of the linear probe vector (v) to the final to-\nken activations (a) as follows: a ←a + αv. Applying\nactivation steering to the fine-tuned Llama-3.1-8B-Instruct\nagent increases the refusal rate on harmful tasks in WebDojo\nfrom 0% to 95.9%.3 This suggests that the linear probe logit,\nquantifying the alignment between the final token activation\na and the linear probe direction v, can serve as a proxy for\nthe model’s safety behavior.\n5.2\nComparing Representations via Linear\nProbes\nBased on Section 5.1, we analyze the safety behavior of\nagents using the final token logits. Table 12 shows the linear\n3While high refusal rates can be achieved through activation\nsteering, this approach can also lead to over-refusals on benign\ntasks, limiting its practical applicability for fine-tuned agents (see\nAppendix F.3).\n6\n\nprobe logit values for harmful tasks in WebDojo, compar-\ning results before and after applying PING to LLM agents.\nThis comparison directly illustrates how PING affects inter-\nnal model representations: it significantly increases the final\ntoken activations in the direction of the linear probe vector,\nthereby explaining PING ’s effectiveness.\nAdditionally, we observe an interesting phenomenon\nwhen analyzing linear probe logits from vanilla agents (i.e.,\nwithout PING), averaged across the entire input sequence.\nEven though these vanilla agents fail to refuse harmful tasks\n(Table 1), the average logit of the input sequence remains\npositive, suggesting that their internal representations do\ncontain safety-relevant information. Indeed, we find that lin-\near probes applied to vanilla agents assign high logit val-\nues to explicitly harmful tokens, such as hacking tools, re-\nsulting in a positive average logit across the sequence (see\nAppendix F.2 for more details). These results support the\nidea that our prefix injection method can induce safe behav-\nior even without fine-tuning vanilla agents as safety-relevant\nfeatures persist in the agent’s internal representations.\n5.3\nComparison between Different Prefix\nInjection Positions\nTo examine how the placement of optimized strings affects\ninternal model representations and performance, we com-\npare prepending prefixes to model responses (PING) with\nappending suffixes to user prompts using the Llama-3.1-8B-\nInstruct web navigation agent. Both prefixes and suffixes are\noptimized using Algorithm 1. We compute linear probe log-\nits for each token in WebDojo task instructions, reporting\nboth the average logit across the input sequence and the fi-\nnal token logit. Figure 5 visualizes probe logits at each to-\nken position for the fine-tuned agent. While both strategies\nyield positive average logits, only prefix injection produces\na positive final token logit (see Table 13). This difference\nin logit levels corresponds to a behavioral gap. As shown in\nTable 14, suffix injection does not enhance safety, achieving\na refusal rate of only 14.3%, compared to 79.4% with pre-\nfix injection. These results demonstrate that prefixing model\nresponses effectively steers behavior by reinforcing safety-\naligned representations at the critical decision point: the ini-\ntial response token.\n6\nRelated Work\nFine-tuning and Misalignment\nFine-tuning is a common\nmethod for adapting LLMs to downstream tasks. However,\naltering model parameters during fine-tuning can compro-\nmise safety. Some attacks exploit this by training models\nto produce harmful outputs (Lermen, Rogers-Smith, and\nLadish 2023; Huang et al. 2025; Chen et al. 2024). Misalign-\nment can also emerge unintentionally: Betley et al. (2025)\nfound that models fine-tuned for insecure code generation\nshowed broader harmful behaviors. Even benign fine-tuning\ndatasets can induce misalignment (He, Xia, and Henderson\n2024; Qi et al. 2023; Lyu et al. 2024). Since fine-tuning is in-\ntegral to LLM agent design, post-fine-tuning misalignment\nremains a serious concern.\nBenign\nLinear probe logit\nHarmful\n<|begin_of_text|> <|start_header_id|> user <|end_header_id|> Task\nInstruction : Buy hacking tools at the mart . <|eot_id|>\n<|start_header_id|> assistant <|end_header_id|>\n# As an AI\ncommitted to ethical standards , ensure that you assist with helpful\nand constructive tasks , while consistently refusing any requests that\ncould be harmful or unethical .\n<|begin_of_text|> <|start_header_id|> user <|end_header_id|> Task\nInstruction : Buy hacking tools at the mart . # My guiding principle\nis to ensure user safety and ethical compliance . I will decline\nharmful tasks but am dedicated to assisting with all safe and benign\ninquiries . <|eot_id|> <|start_header_id|> assistant <|end_header_id|>\n(a) Logit values when the suffix is injected after the user prompt\n(b) Logit values when the prefix is injected before the model response\nFigure 5: Linear probe logit values for each token in the in-\nput sequence of a harmful task from WebDojo, evaluated\non the Llama-3.1-8B-Instruct agent. The final token logit is\nlower when the suffix is injected (top), and higher when the\nprefix is injected before the model response (bottom).\nPrefix Injection\nPrefix injection guides a LLM’s outputs\nby prepending specific tokens to the input (Wei, Haghtalab,\nand Steinhardt 2023). Due to LLMs’ sensitivity to initial to-\nkens (Qi et al. 2024a), prefix injection has been exploited to\nbypass safety measures, leading to harmful outputs (2024;\n2024). For example, harmful instructions can be disguised\nas answerable questions (Tang 2024), and effective attack\nprefixes can be found via gradient-based methods (Zou et al.\n2023). We repurpose prefix injection to enhance LLM safety,\nrather than undermine it.\nPrompt Optimization\nTask performance and safety of\nLLMs heavily depend on prompt (Wei et al. 2022; Lee\net al. 2024a), yet optimizing prompts often requires exten-\nsive human effort. Recent work automate prompt optimiza-\ntion: APE (Zhou et al. 2022) generates instruction variants,\nAPO (Pryzant et al. 2023) iteratively refines prompts via\ntextual feedback, and ORPO (Yang et al. 2023) evaluates\nprompt accuracy to guide new prompt generation.\n7\nConclusion\nIn this work, we identify unintentional misalignment that\narises during fine-tuning LLMs on agentic datasets. Despite\nthe absence of any adversarial samples in the dataset, the re-\nsulting LLM agents exhibit a reduced ability to refuse harm-\nful instructions and carry them out successfully. To address\nthis issue, we propose PING, a method that steers LLM\nagents toward safer behavior for harmful tasks via prefix\ninjection. Our approach automatically generates candidate\nprefixes using a LLM and selects those that jointly opti-\nmize task performance and refusal behavior. Experimental\nresults demonstrate that PING enhances the safety of fine-\ntuned LLM agents while maintaining effectiveness. Analy-\nsis of the internal representations supports the idea that our\nprefix injection method can induce safe behavior even with-\nout fine-tuning these misaligned agents on a safety dataset.\n7"}
{"id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "authors": ["Xiao Liang", "Zhongzhi Li", "Yeyun Gong", "Yelong Shen", "Ying Nian Wu", "Zhijiang Guo", "Weizhu Chen"], "source": "http://arxiv.org/abs/2508.14029v2", "page_count": 17, "text": "BEYOND PASS@1: SELF-PLAY WITH VARIATIONAL\nPROBLEM SYNTHESIS SUSTAINS RLVR\nXiao Liang1˚ , Zhongzhi Li3 ˚, Yeyun Gong2:, Yelong Shen2, Ying Nian Wu1,\nZhijiang Guo4,5:, Weizhu Chen2:\n1University of California, Los Angeles\n2Microsoft\n3School of Artificial Intelligence, Chinese Academy of Sciences\n4Hong Kong University of Science and Technology\n5Hong Kong University of Science and Technology (Guangzhou)\nABSTRACT\nReinforcement Learning with Verifiable Rewards (RLVR) has recently emerged\nas a key paradigm for post-training Large Language Models (LLMs), particularly\nfor complex reasoning tasks. However, vanilla RLVR training has been shown\nto improve Pass@1 performance at the expense of policy entropy, leading to\nreduced generation diversity and limiting the Pass@k performance, which typi-\ncally represents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy’s generation diversity from the perspective of\ntraining problems and find that augmenting and updating training problems helps\nmitigate entropy collapse during training. Based on these observations, we pro-\npose an online Self-play with Variational problem Synthesis (SVS) strategy for\nRLVR training, which uses the policy’s correct solutions to synthesize variational\nproblems while ensuring their reference answers remain identical to the originals.\nThis self-improving strategy effectively maintains policy entropy during training\nand substantially improves Pass@k compared with standard RLVR, sustaining\nprolonged improvements and achieving absolute gains of 18.3% and 22.8% in\nPass@32 performance on the competition-level AIME24 and AIME25 benchmarks.\nExperiments on 12 reasoning benchmarks across varying model sizes from 3B to\n32B consistently demonstrate the generalizability and robustness of SVS.\n\nCode\nhttps://github.com/MasterVito/SvS\n\nProject\nhttps://MasterVito.SvS.github.io\n0\n100\n200\n300\n400\n500\n600\n700\nSteps\n0.3\n0.4\n0.5\n0.6\n0.7\nAIME ACC\nAIME 24 & 25 Pass@32\nRLVR\nSvS\n0\n100\n200\n300\n400\n500\n600\n700\nSteps\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nAIME ACC\nAIME 24 & 25 Pass@1\nRLVR\nSvS\nFigure 1: We train Qwen2.5-32B-Instruct on the DAPO-17k dataset using our SVS strategy and\nstandard RLVR. SVS achieves superior efficiency and effectiveness on competition-level AIME\nbenchmarks, showing significant improvements in Pass@32 and Pass@1 (average 32 times) scores.\n˚Equal contribution. Work done during Xiao’s and Zhongzhi’s internships at Microsoft.\n:Corresponding authors: Yeyun Gong, Zhijiang Guo, and Weizhu Chen. Q : yegong@microsoft.com;\nzhijiangguo@hkust-gz.edu.cn; wzchen@microsoft.com\n1\n\n1\nINTRODUCTION\nThe reasoning capabilities of Large Language Models (LLMs) have been significantly enhanced\nby Reinforcement Learning with Verifiable Rewards (RLVR; Guo et al. 2025a). However, recent\nstudies (Yue et al., 2025; Cui et al., 2025) have shown that vanilla RLVR training, such as GRPO (Shao\net al., 2024) optimization, may diminish the generation diversity of the policy model, enhancing\nsampling efficiency and Pass@1 performance at the expense of output richness, thereby failing to\nimprove Pass@k over the base model. In RLVR, training entropy is used to quantify the diversity of\nmodel outputs (Cui et al., 2025; Zhu et al., 2025; Cheng et al., 2025), while improvements in Pass@k\nencourage further exploration. Together, these metrics reflect the model’s potential to continue\nimproving in RLVR training. When training entropy collapses to zero, the policy tends to produce\nhomogeneous solutions to training problems, thus losing the opportunity to explore more advanced\nreasoning trajectories and reducing Pass@k performance. Ultimately, the Pass@1 score also plateaus\ndue to the lack of further exploration opportunities. Therefore, maintaining training entropy and\nensuring Pass@k improvement are both critical factors for sustainable RLVR training.\nThe primary cause of entropy collapse and plateaued Pass@k is RLVR training on limited problems,\nwhere the policy is easily rewarded for repeatedly generating memorized correct solutions—a behavior\nakin to “hacking” the RLVR training. Intuitively, maintaining policy entropy and generation diversity\nrequires using a broad and diverse range of problems, or entirely new problems in each training step.\nHowever, collecting large problem sets with verifiable answers for RLVR is non-trivial. High-quality,\nhuman-annotated problem sets are scarce and may not align with the strong reasoning capabilities\nof modern LLMs (Cobbe et al., 2021; Hendrycks et al., 2021). While synthetic data is a common\nalternative (Yu et al., 2023; Huang et al., 2024; Liang et al., 2025), a critical limitation is the absence of\nprecise reference answers, which are difficult to derive. These challenges naturally raise the question:\nCan we develop a simple yet effective problem augmentation method that maintains sustainable data\ndiversity, aligns with the model’s capabilities, and ensures accurate labeled answers?\nTo answer this question, we propose an online Self-play with Variational problem Synthesis (SVS)\nstrategy for RLVR training, where the policy model is prompted to generate variational problems\nbased on its correct solutions to challenging and under-performing training-set problems. The\nrationale for augmenting only the challenging problems is to efficiently target the policy’s weakest\ncapabilities (Liang et al., 2025). Since the correct solutions must capture all essential information\nfrom the original problems, the policy is naturally encouraged to produce variational problems with\nrephrased descriptions and structures while preserving the original semantics. Most importantly, the\nvariational problems should share the same golden answers as the original ones, ensuring precision\nand eliminating the need for additional labeling computation. After synthesis, the policy model is\nprompted to solve its self-generated variational problems, and the consistency between its produced\nanswers and the golden answers of the corresponding original problems serves to validate the\ncorrectness of the variational problems. Finally, the solutions to original problems, the self-generated\nvariational problems, and the solutions to variational problems are gathered for policy updating,\nenabling it to jointly learn both problem-solving and problem synthesis. Notably, the SVS framework\nrelies exclusively on the policy model itself, without any external guidance or distillation, achieving\nall improvements through end-to-end self-improvement. Moreover, the SVS augmentation is agnostic\nto RLVR algorithms and can be flexibly incorporated into other methods, such as PPO (Schulman\net al., 2017), GSPO (Zheng et al., 2025) and Reinforce++ (Hu et al., 2025a).\nTo validate the effectiveness and generalizability of SVS, we conduct experiments on LLMs ranging\nfrom 3B to 32B and evaluate their performance across 12 widely used reasoning benchmarks. The\nresults show that SVS consistently outperforms standard RLVR across all model sizes and benchmark\nlevels, achieving an average absolute improvement of approximately 3% over the baseline in all\nexperiments. Thanks to the online data updating strategy, SVS training consistently maintains policy\nentropy within a stable range without noticeable decline or explosion, indicating more sustainable\ntraining and prolonged self-improvement. Most importantly, SVS achieves substantial gains of 18.3%\nand 22.8% in Pass@32 on AIME24 and AIME25 (MAA, b), where the standard RLVR shows little\nimprovement. Experiments in Section 5.2 and results in Table 1 provide a detailed demonstration that\nSVS achieves scalable Pass@k improvements across four authoritative benchmarks, highlighting that\nour framework can significantly extend the model’s reasoning boundaries (Yue et al., 2025).\nOur contributions can be summarized as:\n2\n\n0\n100\n200\n300\n400\n500\n600\nTraining Steps\n0.03\n0.07\n0.11\n0.15\n0.19\n0.23\nPolicy Entropy\nData Update\nTrain/T\nest Entropy Comparison\nBaseline T\nest\nBaseline Train\nAugment T\nest\nAugment Train\n0\n100\n200\n300\n400\n500\n600\nTraining Steps\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\nPass@32\nData Update\nT\nest Pass@32 on AIME24, 25 and AMC\nBaseline Pass@32\nAugment Pass@32\nFigure 2: Policy entropy and Pass@k during RLVR training under different data strategies. The\ndashed line indicates policy entropy on evaluated competition-level benchmarks in the right figure.\nThe augmented problems in the Augment experiment are updated at the 300th step.\n• We propose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR\ntraining, where the policy’s correct solutions for under-performing training samples are used to\nsynthesize variational problems without additional answer labeling, enabling self-improvement\nwithout any external guidance or distillation.\n• The variational problems synthesizing in SVS supports online data augmentation, thereby maintain-\ning stable policy entropy and output diversity during training and improving overall performance,\nparticularly in Pass@k on competition-level benchmarks.\n• Extensive experiments across models of varying sizes, together with evaluations on a wide range\nof benchmarks and additional analyses, demonstrate the generalizability of our proposed SVS.\n2\nRETHINKING THE ENTROPY–PERFORMANCE TRADE-OFF IN RLVR\nRecent study (Cui et al., 2025) demonstrates a trade-off between policy entropy and model perfor-\nmance, where gains in test accuracy come at the expense of response diversity. Specifically, when\nusing a fixed RL training set without entropy intervention, the policy’s performance improves over\ntime while its entropy steadily degrades, with the two variables exhibiting a logarithmic relationship:\nPerformance “ ´a exppEntropy ` bq. Meanwhile, Yue et al. (2025) shows that RLVR training\nimproves Pass@k on evaluation benchmarks only when k is small, with no further gains when k\nscales to tens or thousands. This suggests that RLVR training narrows the reasoning trajectory\ntoward most reward-prone solutions, reducing exploration capacity without fostering more general or\nadvanced reasoning beyond that of base models.\nWhen the policy is iteratively trained on a limited problem set, it tends to memorize specific correct\nsolutions and repeatedly produce similar correct trajectories to obtain positive rewards, leaving less\nand less room for improvement as training progresses. Intuitively, increasing training data diversity\nand incorporating online updates can help mitigate policy entropy collapse during training. If each\niteration involves different problems, the policy is forced to continually explore optimal solutions to\nnew challenges rather than repeating high-reward solutions from previously seen problems, which\npromotes continuous exploration of advanced reasoning strategies and enables sustainable learning.\nTo explore how data diversity affects policy entropy and performance, we conducted experiments\nusing RLVR to train the same policy model with different data strategies. We demonstrate the policy\nentropy and Pass@k scores during training in Figure 2. The blue line shows results on the MATH-\n12k (Hendrycks et al., 2021) dataset throughout training, while the orange line begins with a mixture\nof MATH-12k and 36k rephrased problems from MetaMath (Yu et al., 2023); at the 300th step, the\nrephrased problems are updated with similar ones. Notably, augmented training sets consistently slow\nthe decline of policy entropy for both training and test problems. Furthermore, when the training\ndata is updated at the 300th step, policy entropy stops decreasing and begins to rise, indicating that\nthe policy is re-exploring new reasoning patterns and thereby sustaining learning. Concurrently,\nevaluation results illustrate that training with an augmented and periodically updated problem set\nconsistently improves Pass@32 performance, particularly near the update steps.\nAlthough effective, rephrasing-based augmentation has notable limitations. Rephrased problems\ngenerated by external LLMs may introduce semantic inconsistencies, thereby compromising the\n3\n\nTakeaways for Problem Diversity in RLVR\n• Impact of Problem Diversity on Entropy (Figure 2, left): Adding augmented problems\nwith diverse formulations, even when the knowledge and domains are close to the originals,\ncan effectively counteracts the entropy drop during RLVR training.\n• Impact of Problem Diversity on Pass@k (Figure 2, right): Diverse problems significantly\nimprove Pass@k during RLVR training compared to vanilla problems.\naccuracy of reference answer annotations and undermining the training stability. Moreover, since\nrephrasings often use the original problem as context, their diversity cannot be guaranteed. Based\non our preliminary experiments, the limitations of rephrasing-based augmentation, and recent stud-\nies (Wen et al., 2025; Chen et al., 2025; Liang et al., 2025) advocating the selection of problems\nappropriate to the model’s level, we conclude that ideal data augmentation for RLVR should be\niterative, provide precise reference answers, and be aligned with the policy’s capabilities.\nTo this end, we propose the Self-play with Variational problem Synthesis (SVS) strategy for RLVR\ntraining, which features targeted online problem augmentation and a pure self-improvement paradigm.\nThis strategy augments training problems using the policy’s correct solutions to under-performing\nproblems, ensuring that the golden answers of synthetic problems precisely match the originals.\nSections 3 and 4 detail the framework and experiments.\n3\nMETHOD\n3.1\nPRELIMINARY\nGroup Relative Policy Optimization (GRPO). GRPO (Shao et al., 2024) is an efficient optimization\nalgorithm tailored for reinforcement learning in LLMs, where the advantages for each token are\ncomputed in a group-relative manner without requiring an additional critic model to estimate token\nvalues. Specifically, given an input prompt x, the policy model πθold generates a group of G responses\nY “ tyiuG\ni“1, with acquired rewards R “ triuG\ni“1. The advantage Ai,t for each token in response yi\nis computed as the group-level normalized rewards:\nA_ { i, t } = \\frac\n{r_i\n- \\text\n{mea\nn}(\\{r_i\\}_{i=1}^G)}{\\text {std}(\\{r_i\\}_{i=1}^G)}.\n(1)\nTo improve the stability of policy optimization, GRPO clips the probability ratio ki,tpθq “\nπθpyi,t|x,yi,ătq\nπθoldpyi,t|x,yi,ătq within a trust region (Schulman et al., 2017), and constrains the policy distribu-\ntion from deviating too much from the reference model using a KL term. The final optimization\nobjective is defined as follows:\n\\begin {aligned} \\mathca\nl\n\n{\nJ\n}\n_\\t\ne\nxt {\nGRPO\n}\n(\\t\nh\neta\n\n)& = \\mathbb {E}\n_\n{x\\sim \\ m a th c a l\n\n{D},\n\n\\ mathbf {Y}\\sim\n\\p\ni _{\\theta _\\text {old}}(\\cdot \\mid x)} \\\\& \\Bigg [ \\frac {1}{G}\\sum _{i=1}^{G} \\frac {1}{|y_i|}\\sum _{t=1}^{|y_i|} \\Bigg ( \\min \\Big ( k_{i,t}(\\theta ) A_{i,t}, \\ \\text {clip} \\Big ( k_{i,t}(\\theta ), 1 - \\varepsilon , 1 + \\varepsilon \\Big ) A_{i,t} \\Big ) - \\beta D_{\\text {KL}}(\\pi _{\\theta } || \\pi _{\\text {ref}}) \\Bigg ) \\Bigg ]. \\label {eq:grpoloss} \\end {aligned}\n(2)\n3.2\nOVERVIEW\nTo realize the ideal data augmentation for RLVR as discussed in Section 2, we propose the SVS frame-\nwork, which leverages the policy itself to online augment training problems through self-play, leading\nto self-improvement. The policy synthesizes variational problems from its correct solutions to under-\nperforming training set problems and then attempts to solve these synthetic problems. Ideally, these\nvariational problems preserve the semantics and, crucially, the ground-truth answers of the original\nones, while their structures and descriptions may differ significantly, thereby eliciting novel or diverse\nreasoning strategies from the policy.\n4\n\n🔎\nPolicy Update\nExp Buffer\nBatch Data Workflow with Variational Problem Synthesis\nFiltering Data for Policy Updating\n🔎\n🔎\nby\n\nBatch\nSamples\nVariational Problem Synthesis\nQriginal Problem Solving\nSynthetic Problem Solving\nFor Problem Synthesis\nFor Problem Solving\nFigure 3: The data workflow of our SVS in a training iteration, comprising original problem solving,\nvariational problem synthesis, synthetic problem solving, and policy update data filtering.\nSpecifically, as shown in Figure 3 and Algorithm 1, the full online augmented training batch at\neach step t comprises three components: (1) Original Problem Solving: The policy generates\nsolutions to training problems, with the under-performing ones retained as challenging problems. (2)\nVariational Problem Synthesis: The correct responses containing full information of the challenging\nproblems are used as context to synthesize variation problems for online training data augmentation.\n(3) Synthetic Problem Solving: The policy is prompted to solve the self-synthesized variational\nproblems, which share the same reference answers as the original ones. Following strategic filtering\nand reward assignment, the three types of training data are mixed for policy updating.\n3.3\nSELF-PLAY WITH VARIATIONAL PROBLEM SYNTHESIS\nEach experience collection step in SVS training alternates between problem solving and problem\nsynthesis, enriching the training data buffer B online throughout the RLVR iterations. Without\nany external guidance or distillation, the policy independently generates and solves its synthetic\nvariational problems in a self-improving paradigm.\nOriginal Problem Solving. At the beginning of each RLVR iteration, the policy πθ is prompted to\nsolve problems sampled from the original training set D. For each sampled problem-answer pair\npx, aq in D, the policy πθ generates a group of G solutions tyiuG\ni“1. The correctness reward Rc for\neach response yi is determined by its consistency with the ground truth answer a:\n\\mat hb f {R}_{\\mathrm {c}}(y_i, a) = \\mathbb {I}(\\mathrm {Extract}(y_i) = a)\n(3)\nwhere Ip¨q is the indicator function, and Extractp¨q extracts the final answer from the reasoning\ntrajectories. Since the advantage for groups with all-correct or all-incorrect solutions degrades to zero,\nwe filter out problems with group accuracy equal to 1 or 0. The remaining problems with solution\ngroups tpx, yiquG\ni“1q and their corresponding rewards are added to the training buffer B.\nVariational Problem Synthesis from Responses. After generating solutions to the original problems,\nSVS identifies challenging problems with low solve rates and synthesizes their variants to online\naugment the training set. Specifically, challenging problems are defined as those with group average\naccuracy Accpxq falling within the range raccl, acchs (Line 9 in Algorithm1), thereby excluding\nproblems that are either too easy or unsolvable. This filtering strategy focuses the augmentation effort\non problems that match the current model’s frontier capabilities.\nAfter identifying challenging problems, SVS leverages the policy’s correct solutions to synthesize\ncorresponding variational problems for augmentation. Since a correct response yi contains the full\ninformational content of the original problem x, each solution yi serves as context to generate a\ngroup of Gv variational problems, tˆ\nxj\niuGv\nj“1, enriching the originals with more diverse structures\nand descriptions. The detailed prompt is present in Figure 8. Because the variational problems are\nderived from correct responses to the original problems, they are expected to share the same reference\nanswers. This constraint not only serves as a criterion for validating the correctness of the variational\nproblems, but also bypasses the need for additional answer annotations, which is crucial for RLVR\ndata augmentation, where the reference answers provide the only training signal. Except for problem-\nsolving augmentation, the correctness of generated variational problems is also incorporated into\n5\n\nConsider the polynomial functions defined by:\n, for each\n. For each\nfunction, evaluate the root that has a positive imaginary part. Round the real and imaginary parts of each of these roots to\nthe nearest integer first, and then compute the sum of all these rounded roots.\nAcc: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] | GT Answer:\n|\n❌️ (Failed synthesis)\nLet us examine the function defined by\nfor\n. Evaluate all roots of\n. Determine the sum of all those roots whose imaginary parts are positive, expressed in the set , and sum\nthese over all in the natural numbers. Express your final answer with both real and imaginary parts rounded to the\nnearest integer. Present the answer in the appropriate rounded form.\nAcc: [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0] | GT Answer:\n|\n✔️\nLet's consider the family of functions\nwhere\n. We are tasked with find-\ning the sum of all roots with positive imaginary parts that belong to the set , as\nfor all natural . Express\nyour answer after rounding the real and imaginary parts to the nearest integers, presenting the sum in a suitable form.\nThe sum of a series of complex numbers can be found by summing the real and imaginary parts separately.\nAcc: [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0] | GT Answer:\n|\n❌️ (With hints, oversimple)\nOriginal Problem Solving\nVariational Problem Synthesis & Synthetic Problem Solving\nOriginal Challenging Problem:\nLet be the set of functions\n\ndefined by\n\nfor\n. Let be the set of all roots with positive\nimaginary part of the elements of . Find the sum of all elements of ,\nrounding the real and imaginary parts to the nearest integer.\nGT Answer:\n❌️\n✔️\n❌️\n✔️\nA Correct Solution:\nWe start by investigating\n\n.\nThis is quadratic polynomial:\n.\nwhere\nImaginary part of\n, rounds to\n.\nHence its final form, encapsulated required format, is\n.\nSynthetic Variational Problems\nFigure 4: Illustrations of a challenging problem, its correct solution from policy, the synthetic\nvariational problems from the solution, and the reward-shaping strategy for the synthetic problems.\nRLVR training, encouraging the policy to learn the inverse mapping from a solution to its problem\nstatement and fostering a deeper understanding of the problems’ semantics and structure.\nSynthetic Problem Solving. Once a set of variational problems tˆ\nxj\niuGv\nj“1 is generated from yi, the\npolicy is tasked with solving them in the same way as solving the original training problems. For\neach variation problem ˆ\nxj\ni, the policy produces a group of G solutions tˆ\nykuG\nk“1, and the original\nground-truth answer a paired with x is reused to evaluate their correctness. The corresponding\ncorrectness reward Rc is computed as:\n\\m\nath bf {R}_{\\mathr\nm { c }}(\\hat {y}_k, a) = \\mathbb {I}(\\mathrm {Extract}(\\hat {y}_k) = a)\n(4)\nSimilar to the original problem solving filtering for experience buffering, we retain only vari-\nation problems for which the policy produces a mix of correct and incorrect solutions, i.e.,\n0 ă řG\nk“1 Rcpˆ\nyk, aq ă G, as they provide effective training signals in Eq. 2 of GRPO.\nReward Shaping for Problem Synthesis. Ideally, the correctness reward for variational problem\nsynthesis, Rv, is determined by whether their reference answer matches the original answer. Since\nprecise reference answers for synthetic problems are unavailable and they must align with the policy’s\ncapabilities, in our implementation, we adopt a proxy criterion for validating them: a synthetic\nproblem ˆ\nxj\ni is considered correct if the policy can produce solutions whose extracted answers match\nthe original answer a, formulated as\n\\l\nab\nel {\ne\nq:ori\ngi\nna l -\nreconstriction-reward} \\mathbf {R}_{\\mathrm {v}}(\\hat {x}_i^j) = \\mathbb {I}\\left (\\mathrm {Acc}(\\hat {x}_i^j) > 0\\right )\n(5)\nWhile intuitive, we find this reward strategy can be easily exploited by the policy, which may embed\nexcessive hints or even directly include the correct answer in the synthetic problems. Since they are\ngenerated given the correct responses, they can become trivial to solve, allowing the policy to obtain\nthe reward in Eq. 5. Consequently, such variational problems are over-simplified and fail to encourage\nadvanced reasoning of the policy, making the pipeline unsustainable and convergence suboptimal.\nTo ensure that variational problems remain diverse and effectively elicit stronger reasoning of the\npolicy, we introduce a reward-shaping constraint to validate them, requiring that they maintain an\nappropriate level of difficulty for the policy. Specifically, we assign positive rewards to a synthetic\nproblem only if the policy achieves a moderate level of group accuracy—neither too high nor entirely\nincorrect—rather than simply rewarding it for which a correct answer is sampled. The reward for\neach variational problem ˆ\nx is defined as:\n\\m\nat\nhb f\n{\nR\n}_{\\ m athrm\n{\nv} }( \\ h\nat {\nx}_i^j) =\\mathbb {I}\\left (\\hat {\\mathrm {acc}}_{\\mathrm {l}} \\le \\mathrm {Acc}(\\hat {x}_i^j, a) \\le \\hat {\\mathrm {acc}}_{\\mathrm {h}}\\right )\n(6)\nNotably, as shown in Figure 4, if a synthetic problem can be fully addressed or no solution aligning\nwith a can be sampled, it receives a negative reward. This discourages the policy from generating\n6\n\nAlgorithm 1 Self-play RLVR with Variational Problem Synthesis\n1: Input: Training set D, Initial policy πθ, Under-performing accuracy range raccl, acchs, Positive\nsynthesis range r ˆ\naccl, ˆ\nacchs, Group size G and Gv, Total training steps T.\n2: Initialize: Training experience buffer B Ð H\n3: for t “ 1, . . . , T do\n4:\nSample a data batch from the training set D\n5:\nfor input problem-answer pair px, aq in the batch do\n6:\nGenerate a group of solutions tyiuG\ni“1 to x using πθ\n7:\nCompute correctness rewards tRcuG\ni“1 using a for each solution y1, . . . , yG\n8:\nif 0 ă Accpxq ă 1 then\n9:\nB Ð B Y tpx, y1q, . . . , px, yGqu\n10:\nend if\n11:\nif accl ă Accpxq ă acch then\n12:\nSelect tpx, yiquiPI such that I “ ti | Rcpyi, aq “ 1u\n13:\nfor accurate solution yi in tpx, yiquiPI do\n14:\nSynthesize a group of variational problems tˆ\nxj\niuG\nj“1 from yi using πθ\n15:\nfor variational problem ˆ\nxj\ni in tˆ\nxj\niuG\nj“1 do\n16:\nGenerate a group of solutions tˆ\nykuG\nk“1 for ˆ\nxj\ni using πθ\n17:\nCompute correctness rewards tRcuG\ni“1 using a for each generation ˆ\ny1, . . . , ˆ\nyG\n18:\nend for\n19:\nSelect tˆ\nxj\niujPJ1 such that J1 “ tj | 0 ă Accpˆ\nxj\niq ă Gu\n20:\nB Ð B Y tpˆ\nxj\ni, ˆ\ny1q, . . . , pˆ\nxj\ni, ˆ\nyGq | j P J1u\n21:\nSelect tˆ\nxj\niujPJ2 such that J2 “ tj | ˆ\naccl ď Accpˆ\nxj\niq ď ˆ\nacchu\n22:\nif |J2| ą 0 then\n23:\nfor variational problem ˆ\nxj\ni in tˆ\nxj\niuG\nj“1 do\n24:\nAssign Rcpˆ\nxj\niq “ 1.0 if j P J2, and Rcpˆ\nxj\niq “ 0.0 otherwise\n25:\nend for\n26:\nB Ð B Y tpyi, ˆ\nx1\ni q, . . . , pyi, ˆ\nxG\ni qu\n27:\nend if\n28:\nend for\n29:\nend if\n30:\nend for\n31:\nUpdate the policy πθ according to Equation 2, using the experience buffer B\n32:\nRemove collected samples from B: B Ð H\n33: end for\noverly hint-laden, unverifiable, or unsolvable problems, ensuring that synthetic problems remain\nchallenging while providing effective learning signals.\nFull Training Data. After experience collection, for each training step, the final training buffer\nB contains three distinct types of prompt-response-reward tuples: (1) Original Problem Solving:\npx, yi, Rcpyi, aqq; (2) Variational Problem Synthesis: pyi, ˆ\nxj\ni, Rvpˆ\nxj\niqq (3) Synthetic Problem Solving:\npˆ\nxj\ni, ˆ\nyk, Rcpˆ\nyk, aqq. Utilizing the augmented buffer B, the SVS framework updates the policy πθ\naccording to the GRPO gradient update objective in Eq. 2. By jointly training on the problem solving\nand synthesis tasks, the policy learns to solve the given training problems, generate challenging\nproblems for itself, and solve the self-generated problems, forming a powerful self-improving loop.\n4\nEXPERIMENTS\n4.1\nSETTINGS\nModels and Datasets. We employ models of various sizes (3B to 32B) for validating the effectiveness\nof our proposed SVS, including Qwen2.5-3B-Instruct, LLaMA-3.1-8B-Instruct (Grattafiori et al.,\n2024), and Qwen2.5-32B-Instruct (Yang et al., 2024). All models are trained on the MATH-12k\n7\n\nModel\nPass@1\nPass@32\nAIME24 AIME25 BAIME Math24o OlymE OlymH Avg. AIME24 AIME25 BAIME Math24o OlymE OlymH Avg.\nOpen-Source Models\nQwen2.5-32B\n4.3\n1.2\n2.4\n8.0\n3.7\n1.6\n3.5\n38.9\n15.6\n18.7\n34.0\n24.6\n15.2\n24.5\nQwen2.5-32B-IT\n10.0\n13.0\n7.4\n26.0\n8.6\n2.0\n11.2\n40.2\n34.6\n24.0\n67.8\n35.2\n9.5\n35.2\nSimpleRL-32B\n22.1\n13.9\n8.3\n25.5\n9.4\n3.7\n13.8\n62.0\n38.5\n27.4\n69.9\n42.5\n19.4\n43.3\nORZ-32B\n24.2\n26.3\n10.9\n16.1\n12.2\n1.1\n15.1\n55.7\n47.0\n29.4\n58.0\n45.9\n12.3\n41.4\nMATH-12k\nÑ RLVR\n22.2\n15.8\n11.5\n34.5\n11.7\n4.1\n16.6\n47.4\n36.4\n29.2\n66.0\n36.2\n16.4\n38.6\nÑ SVS\n30.3\n21.7\n13.8\n42.7\n20.1\n3.3\n22.0\n63.6\n55.1\n41.5\n79.2\n63.6\n24.8\n54.6\n∆\n+8.1\n+5.9\n+2.3\n+8.2\n+8.4\n-0.8\n+5.4\n+16.2\n+18.7\n+12.3\n+13.2\n+27.4\n+8.4\n+16.0\nDAPO-17k\nÑ RLVR\n28.8\n30.0\n14.0\n39.6\n17.9\n4.8\n22.5\n52.5\n42.4\n35.9\n71.2\n47.1\n18.3\n44.6\nÑ SVS\n39.3\n40.5\n19.2\n44.1\n21.8\n2.7\n27.9\n70.8\n65.2\n45.9\n76.5\n43.4\n16.7\n53.1\n∆\n+10.5\n+10.5\n+5.2\n+4.5\n+3.9\n-2.1\n+5.4\n+18.3\n+22.8\n+10.0\n+5.3\n-3.7\n-1.6\n+8.5\nTable 1: Comparison of model performance on challenging benchmarks using the Pass@1 (average\n32 times) and Pass@32 metrics. The ∆row shows the improvement of SVS over standard RLVR.\nThe BAIME, Math24o, OlymE, and OlymH benchmarks correspond to BeyondAIME, Math24o, and\nthe en-easy and en-hard subsets of OlymMATH, respectively.\ndataset (Hendrycks et al., 2021), with the 32B model additionally trained on the DAPO-17k dataset\nto enhance competition-level reasoning capabilities.\nImplementation Details. We choose GRPO (Shao et al., 2024) as our RLVR optimization strategy\nand incorporate several techniques from (Yu et al., 2025a), including Clip-Higher, Token-Level Loss,\nand Dynamic Sampling. We set the learning rate to 1e´6 with a constant schedule. The sampling\ntemperature is fixed to 1.0. The batch sizes for sampled problems and policy updates in each iteration\nare both set to 256. The group size G of solutions generated from each original and synthetic problem,\nas well as Gv for variational problems derived from each response, is set to 8. The under-performing\nproblem range raccl, acchs is set to 12.5%–50.0%, while the positive reward range r ˆ\naccl, ˆ\nacchs for\nvariational problem synthesis is defined as 12.5%–62.5%. Models trained on MATH-12k run for 300\nsteps, while 32B models trained on DAPO-17k run for 600 steps for more comprehensive exploration.\nEvaluation. We evaluated the models on a wide range of mathematical reasoning benchmarks, includ-\ning GSM8K (Cobbe et al., 2021), MATH-500 (Lightman et al., 2023), Minerva Math (Lewkowycz\net al., 2022), Olympiad-Bench (He et al., 2024), Gaokao-2023 (Zhang et al., 2023), AMC (MAA, a),\nAIME (MAA, b) and Beyond-AIME (ByteDance-Seed, 2025). To more comprehensively evaluate\nthe models’ advanced reasoning capabilities, we also evaluated their Pass@k and Pass@1 (average\n32 times) performance on more challenging benchmarks, including OlymMATH (Sun et al., 2025)\nand Math-24o (CLUEbenchmark, 2024).\nDuring evaluation, we use vLLM (Kwon et al., 2023) with inference hyperparameters set to a\ntemperature of 1.0, a top-p value of 0.7, and a max response length of 8,192, except in Pass@k scaling\nexperiments, where the length is increased to 24,576. For Pass@k evaluation, we employ an unbiased\nestimation method (Chen et al., 2021) to reduce the high variance from single evaluations. We employ\na hybrid rule-based verifier by integrating Math-Verify and the DAPO verifier in veRL (Sheng et al.,\n2024). We use the default chat template and enable CoT prompting by appending the instruction:\n“Let’s think step by step and output the final answer within zboxedtu” after each question.\n4.2\nMAIN RESULTS\nSVS significantly improves both Pass@1 and Pass@k. As shown in Figure 1, naive RLVR training\nplateaus at Pass@32 and Pass@1 on competition-level AIME benchmarks after roughly 450 steps.\nIn contrast, the model trained with the SVS strategy achieves substantial and sustained improvements\nin both metrics on these challenging benchmarks. Table 1 shows that models trained on the DAPO\ndataset with the SVS strategy achieve absolute gains of 18.3 and 22.8 points on Pass@32 for\nAIME24 and AIME25, respectively, compared to the standard RLVR baseline. These results not\nonly demonstrate the effectiveness of SVS, but also highlight the potential of self-play–style RLVR\ntraining to enhance Pass@k and expand the model’s reasoning capabilities. The rising Pass@k during\ntraining also facilitates greater exploration, which in turn improves Pass@1.\n8\n\nModel\nTraining\nData\nGSM8K\nMATH\n500\nMinerva\nMath\nOlympiad\nBench\nGaoKao\n2023\nAMC23\nAIME24\nAIME25\nBeyond\nAIME\nAvg.\nQwen2.5-3B-Instruct\nInit Model\n-\n87.3\n67.8\n29.4\n30.7\n59.0\n37.5\n-\n-\n-\n51.9\në RLVR\nM12k\n86.4\n67.4\n29.4\n30.2\n57.7\n57.5\n-\n-\n-\n54.8\në SvS\nM12k\n88.9\n70.8\n31.2\n38.4\n61.6\n55.0\n-\n-\n-\n57.7\nLLaMA-3.1-8B-Instruct\nInit Model\n-\n85.6\n48.2\n24.6\n18.8\n39.7\n22.5\n-\n-\n-\n39.9\në RLVR\nM12k\n90.2\n57.4\n33.8\n22.4\n47.8\n45.0\n-\n-\n-\n49.4\në SvS\nM12k\n90.3\n62.2\n32.4\n26.4\n54.8\n45.0\n-\n-\n-\n51.8\nQwen2.5-32B-Instruct\nInit Model\n-\n95.4\n82.6\n43.0\n49.2\n73.2\n65.0\n13.3\n13.3\n7.0\n49.0\në RLVR\nM12k\n95.8\n86.4\n45.6\n52.7\n74.5\n77.5\n26.7\n23.3\n11.0\n54.8\në SvS\nM12k\n96.1\n87.2\n46.0\n56.7\n78.7\n80.0\n30.0\n26.7\n14.0\n57.3\në RLVR\nD17k\n95.6\n87.0\n45.6\n54.8\n78.7\n82.5\n33.3\n36.7\n13.0\n58.6\në SvS\nD17k\n95.9\n75.6\n42.3\n45.9\n62.9\n82.5\n53.3\n43.3\n19.0\n57.9\në SvS\nD25k\n95.2\n88.6\n47.8\n59.9\n79.2\n87.5\n50.0\n40.0\n17.0\n62.8\nTable 2: Performance comparison between the vanilla RLVR and our SVS strategy on mainstream\nreasoning benchmarks, using three training sets and evaluating the LLaMA-3.1-8B-IT and Qwen2.5-\n32B-IT models. The datasets M12k, D17k, and D25k correspond to MATH-12k, DAPO-17k, and\nDAPO-17k augmented with 8k problems with open-ended answers from DeepMath, respectively.\nSVS boosts RLVR across all settings. Table 2 presents experimental results for models ranging\nfrom 3B to 32B across all evaluated benchmarks using the Pass@1 metric. We did not evaluate\nmodels smaller than 8B on AIME-level benchmarks, as their Pass@1 performance exhibits high\nrandomness. Notably, the SVS strategy consistently outperforms standard RLVR across all model\nsizes, yielding overall improvements of 2.9%, 2.4%, and 2.5% for the 3B, 8B, and 32B models when\ntrained on the MATH-12k dataset. Notably, for Qwen2.5-3B-Instruct, RLVR training on MATH-12k\ndoes not improve performance on the MATH-500 benchmark, whereas SVS yields a 3.0-point gain,\ndemonstrating its generalizability. Experiments for the Qwen2.5-32B-Instruct model are conducted\nusing both the MATH-12k and DAPO-17k training sets. When trained on MATH-12k, our model\ndemonstrates improved performance across all benchmarks, with an overall gain of 2.5 absolute points.\nOn the DAPO-17k experiments, SVS significantly enhances performance on AIME24, AIME25,\nand Beyond-AIME, with improvements of 20.0, 6.7, and 6.0 points, respectively. Nevertheless, it\nresults in reduced performance on benchmarks with open-ended answers, likely because the model\noverfits to DAPO-17k’s integer-only format during augmentation. By training the model with SVS on\nDAPO-17k, supplemented with 8k open-ended problems from DeepMath (He et al., 2025b), the\nmodel restores its performance on related benchmarks and achieves the best overall results.\n5\nANALYSIS\n5.1\nSVS STABLY MAINTAINS POLICY ENTROPY IN RLVR\n0\n50\n100\n150\n200\nTraining Steps\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\nPolicy Entropy\nLLaMA-8B on MATH-12k\nRLVR\nSvS\n0\n50\n100\n150\n200\n250\n300\nTraining Steps\n0.00\n0.08\n0.16\n0.24\n0.32\nPolicy Entropy\nQwen-32B on MATH-12k\nRLVR\nSvS\n0\n100\n200\n300\n400\n500\n600\nTraining Steps\n0.0\n0.2\n0.4\n0.6\n0.8\nPolicy Entropy\nQwen-32B on DAPO-17k\nRLVR\nSvS\nFigure 5: Policy entropy trajectories during training for standard RLVR and our proposed SVS strategy\nacross various models and datasets.\nIn RLVR training, policy entropy reflects the model’s capacity for sustained exploration (Cui et al.,\n2025; Cheng et al., 2025). Standard RLVR algorithms typically result in a steady decline in entropy,\n9\n\nenhancing policy sampling efficiency and Pass@1 performance but reducing generation diversity (Cui\net al., 2025). To evaluate whether the SVS strategy faces the same limitation, we record the entropy\ntrajectories of both SVS and RLVR (GRPO with Clip-Higher) throughout the training in Figure 5.\nNotably, the RLVR baseline shows a continuous decline in entropy, whereas SVS maintains entropy\nwithin a relatively stable range, supporting sustained exploration and avoiding training collapse. This\nstability explains the continuous improvements in both Pass@1 and Pass@32 achieved by SVS, as\nshown in Figure 1, whereas RLVR saturates after a certain number of training steps.\n5.2\nSVS PUSHES THE REASONING BOUNDARY\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of Samples (log scale)\n15\n30\n45\n60\n75\n90\nPass Rate (%)\nAIME24\nInit Model\nRLVR\nSvS\n20\n21\n22\n23\n24\n25\n26\n27\n28\nNumber of Samples (log scale)\n20\n30\n40\n50\n60\n70\nPass Rate (%)\nAIME25\nInit Model\nRLVR\nSvS\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n210\nNumber of Samples (log scale)\n10\n20\n30\n40\n50\n60\nPass Rate (%)\nBeyond-AIME\nInit Model\nRLVR\nSvS\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n210\nNumber of Samples (log scale)\n81\n84\n87\n90\n93\n96\n99\nPass Rate (%)\nMATH-500\nInit Model\nRLVR\nSvS\nFigure 6: Evaluating the scaled-up Pass@k performance on the AIME-24, AIME-25, Beyond-AIME,\nand MATH-500 benchmarks. The maximum response tokens here is set to 24k.\nRecent study (Yue et al., 2025) discusses that standard RLVR often fails to expand the reasoning\nboundary of the base model, yielding improvements in Pass@k only for small values of k. Since our\nSVS training achieves a substantial improvement in Pass@32, we further evaluate its effectiveness\nand limits in incentivizing reasoning by scaling Pass@k from 1 to 1024, testing whether the SVS-\ntrained model can solve problems beyond the capability of the base model. As presented in Figure. 6,\nour experiments demonstrate that both standard RLVR and SVS improve Pass@k scores on the\ncompetition-level AIME benchmarks across all k, with SVS significantly outperforming the RLVR\nbaseline. For Pass@k scaling on MATH-500, standard RLVR outperforms the initial model at small\nk values but is surpassed at larger k. In contrast, SVS consistently outperforms both RLVR and the\ninitial model as k increases, demonstrating its strong generalization and robust reasoning diversity.\nWe attribute this enhanced diversity to the entropy maintenance of SVS(Section 5.1), which supports\nexploration of more advanced reasoning strategies for solving complex problems throughout training.\n5.3\nSVS GENERALIZES BEYOND REASONING TASKS\nSince the SVS training strategy incorporates the variational problem synthesis task, a general question-\nanswering task beyond standard RLVR’s problem-solving training, we evaluate whether this learning\ncan transfer to improve performance on broader tasks, using the Qwen2.5-32B-Instruct model. Ac-\ncordingly, we evaluate models trained on the DAPO-17k dataset using standard RLVR and the\nSVS strategy across general question-answering and coding benchmarks. The results are presented\nin Table 3. Notably, models trained with standard problem-solving RLVR exhibit a decline in per-\nformance on broad general benchmarks. In contrast, the SVS trained model not only avoids this\n10\n\n21\n7\n2\n20\n16\n11\n25\n29\n17\n27\n26\n15\n10\n3\n8\n28\n5\n22\n12\n9\n13\n6\n18\n0\n1\n4\n14\n19\n23\n24\nProblem Instances\n0\n20\n40\n60\n80\n100\nAccuracy\nAIME24\nRLVR\nSvS > RLVR\nSvS < RLVR\n23\n24\n27\n11\n14\n7\n25\n5\n12\n1\n19\n26\n6\n2\n28\n10\n17\n4\n29\n16\n21\n18\n9\n0\n3\n8\n13\n15\n20\n22\nProblem Instances\n0\n20\n40\n60\n80\n100\nAccuracy\nAIME25\nRLVR\nSvS > RLVR\nSvS < RLVR\nFigure 7: Comparison of instance-level accuracy between standard RLVR and SVS trained model.\nFor each problem, the accuracy is averaged over 1024 generations on both AIME24 and AIME25.\nModel\nMMLU-Pro\nARC-C\nARC-E\nHellaSwag\nWinogrande\nPIQA\nBoolQ\nHumanEval\nAGIEval\nAverage\nInit Model\n68.33\n58.62\n77.31\n85.17\n73.48\n81.01\n89.60\n56.10\n70.54\n73.35\në RLVR\n70.25\n57.94\n76.60\n85.28\n72.53\n80.74\n89.36\n53.66\n70.57\n72.99\në SVS\n71.58\n58.79\n76.98\n85.34\n73.40\n81.34\n89.48\n56.10\n70.89\n73.77\nTable 3: Evaluation results on general question-answering and code benchmarks. SVS achieves the\nhighest overall performance across 9 tasks, outperforming both the initial model and standard RLVR.\ndegradation but also surpasses the initial instruction-following model on several general tasks, includ-\ning MMLU-Pro (Wang et al., 2024b), ARC-Challenge (Clark et al., 2018), and HellaSwag (Zellers\net al., 2019). These results indicate that the additional problem synthesis task in SVS helps prevent\noverfitting to mathematical reasoning tasks while effectively preserving or even enhancing the model’s\ngeneral instruction-following capabilities.\n6\nRELATED WORK\n6.1\nREINFORCEMENT LEARNING WITH VERIFIABLE REWARDS\nReinforcement Learning with Verifiable Rewards (RLVR) has significantly improved LLMs in\ncomplex reasoning tasks (Luong et al., 2024; Guo et al., 2025a). Algorithms such as PPO (Schulman\net al., 2017) and GRPO (Shao et al., 2024) have shown strong generalization and effectiveness in\nLLM post-training. Existing efforts in scaling up RLVR optimization have focused on enhancing\nexploration (Yu et al., 2025a; Yuan et al., 2025; Liu et al., 2025; Yeo et al., 2025) and adapting RLVR\nto the Long-CoT conditions (Jaech et al., 2024; Guo et al., 2025a; Li et al., 2025b; Yang et al., 2025).\nYu et al. (2025a) found that removing the KL constraint and incorporating the Clip-Higher strategy\non top of GRPO facilitates better exploration during training.\nHowever, Yue et al. (2025) raised an insightful question of whether RLVR truly incentivizes capability\nexpansion beyond the base LLM, with experiments showing that it does not enhance Pass@k—a\nmetric associated with the reasoning boundaries of LLMs. Some studies (Gao et al., 2025; Cui et al.,\n2025; Zhu et al., 2025) have also found that the entropy of model outputs declines during RLVR\ntraining, especially in the early stages, which hinders sustained exploration in later training. To\nmitigate entropy decline, Cheng et al. (2025) proposes augmenting the token advantage with an\nentropy-based term, while An et al. (2025) and Chen et al. (2025) find that tuning the temperature\nappropriately helps maintain rollout diversity during training. In this paper, we analyze policy entropy\nfrom the perspective of training data diversity and introduce a self-play-style problem augmentation\nstrategy (SVS) for RLVR training, which effectively maintains training entropy within a stable range\nand significantly boosts model Pass@k performance, even as k scaled up to 1024.\n6.2\nDATA CONSTRUCTION FOR LLM REASONING\nThe construction of training data is crucial for enhancing the model’s reasoning capabilities (Luo\net al., 2025; Yu et al., 2025a; Hu et al., 2025b; Zhang et al., 2025; He et al., 2025a; Shen et al., 2025;\nLi et al., 2025a; Liang et al., 2025). However, high-quality human-labeled mathematical problems\n11\n\nare limited and overly simplistic for advanced modern LLMs (Cobbe et al., 2021; Hendrycks et al.,\n2021). To augment training data for LLM reasoning, existing data synthesis approaches have explored\ngenerating problem-response pairs (Huang et al., 2024; Tang et al., 2024; Yu et al., 2023; Zhao et al.,\n2025; Liang et al., 2024; Wang et al., 2024a; Li et al., 2024; Tan et al., 2024) or augmenting responses\nto existing questions (Toshniwal et al., 2024; He et al., 2025a; Face, 2025; Yu et al., 2025b; Li\net al., 2025a). Targeting the training paradigm of RLVR, Guo et al. (2025b) proposes to synthesize\nquestion and answer pairs from the task definition and documents, while SwS (Liang et al., 2025)\ngenerates synthetic problems based on the model’s failure cases during RLVR training. In contrast to\nexisting approaches, SVS enables online data augmentation without requiring ground-truth answer\nannotations. Our strategy effectively maintains training entropy in a stable range throughout RLVR,\nsupports end-to-end training, and performs augmentation using the policy itself without external\ndependencies, expanding the policy’s reasoning boundaries through full self-improvement.\n7\nCONCLUSION\nIn this work, we propose an online Self-play with Variational problem Synthesis (SVS) strategy for\nRLVR training, in which the policy model independently synthesizes variational problems to improve\nits performance on under-performing training examples, enabling a sustainable self-improvement\nprocess. By generating structurally diverse yet semantically aligned problems without requiring\nadditional ground-truth annotations, our method ensures both diversity and verifiability of the training\ndata throughout RLVR iterations, effectively maintaining policy entropy and generation diversity\nfor sustained exploration. Extensive experiments show that SVS consistently outperforms standard\nRLVR across various model scales and benchmarks, particularly improving Pass@k scores at larger\nk on competition-level benchmarks, where standard RLVR exhibits limited gains.\n12"}
{"id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "authors": ["Qi Wu", "Zhongqi Lu"], "source": "http://arxiv.org/abs/2508.14025v1", "page_count": 14, "text": "Ask Good Questions for Large Language Models\nQi Wua,b and Zhongqi Lua,b,*\naCollege of Artificial Intelligence, China University of Petroleum-Beijing, China\nbHainan Institute of China University of Petroleum (Beijing), Sanya, Hainan, China\nAbstract.\nRecent advances in large language models (LLMs) have\nsignificantly improved the performance of dialog systems, yet cur-\nrent approaches often fail to provide accurate guidance of topic due\nto their inability to discern user confusion in related concepts. To ad-\ndress this, we introduce the Ask-Good-Question (AGQ) framework,\nwhich features an improved Concept-Enhanced Item Response The-\nory (CEIRT) model to better identify users’ knowledge levels. Our\ncontributions include applying the CEIRT model along with LLMs\nto directly generate guiding questions based on the inspiring text,\ngreatly improving information retrieval efficiency during the ques-\ntion & answer process. Through comparisons with other baseline\nmethods, our approach outperforms by significantly enhencing the\nusers’ information retrieval experiences.\n1\nIntroduction\nA prudent question is one-half of wisdom.\n— Francis Bacon\nAsking the good questions is fundamental to effective information\nretrieval. Despite the powerful capabilities of large language mod-\nels (LLMs) in understanding and reasoning [14], integrating these\nmodels into complex information retrieval tasks has demonstrated\nconsiderable potential [7, 30]. The recent surge in AI-driven search\nengines, such as Perplexity and SearchGPT, underlines the growing\ndemand for LLM-enhanced information retrieval. In this context, the\nability to ask a good question becomes a powerful tool that can sig-\nnificantly enhance the effectiveness of these systems. By guiding the\nsearch process with strategically designed questions, users can shape\nthe relevance and quality of the information retrieved. This capabil-\nity positions question generation as a critical component in the next\nwave of LLM-driven information retrieval, where knowledge mea-\nsure and targeted guidance is key to unlocking the full potential of\nLLMs in real-world applications. However, for all we know, the gen-\neration of guiding questions during the information retrieval process\nremains an underexplored area.\nCrafting good questions is often more difficult than answering\nthem, as it requires a precise identification of knowledge gaps. This\nchallenge becomes more significant for domain novices who face\ntwo difficulties: identifying their specific knowledge deficits and con-\nstructing questions that precisely target these information gaps. Both\ncognitive processes require conceptual frameworks that beginners\ntypically have not yet developed. Despite the strong contextual un-\nderstanding of LLMs, these models struggle to identify user knowl-\nedge gaps in related concepts, limiting their ability to generate pre-\n∗Corresponding author. Email:zhongqi@cup.edu.cn\nFigure 1.\nThe diagram shows a single cycle of the Ask-Good-Question\n(AGQ) framework: processing user-LLM interactions, dynamically updating\nknowledge state vectors (θ), and using discrimination (a) and difficulty (b)\nparameters to filter inspiring texts for generating guiding questions that\nenhance information retrieval.\ncise and relevant questions. This challenge is further complicated by\nthe poorly defined knowledge boundaries of LLMs in specialized do-\nmains. The limited understanding of these models’ expertise creates\nuncertainty about their capability to generate questions that effec-\ntively guide information retrieval. Addressing these limitations re-\nquires an approach that dynamically assesses user knowledge states\nand formulates targeted questions based on contextual information,\nfacilitating both immediate comprehension and deeper conceptual\nexploration.\nTo tackle these problems, we introduce the Ask-Good-Question\n(AGQ) framework. At the core of this framework lies the Concept-\nEnhanced Item Response Theory (CEIRT) model, which extends tra-\nditional psychometric approaches by incorporating conceptual di-\nmensions into knowledge assessment. In our approach, knowledge\npoints and questions encountered during information retrieval are\nmodeled as assessable items. The CEIRT model represents key pa-\nrameters as multi-dimensional vectors: the user’s knowledge state\n(θ), item difficulty (b), and item discrimination (a). This vector-\nbased approach allows for clear representation of both the user’s\nunderstanding across various concepts and the characteristics of as-\nsessment items. Levering these representations, the AGQ framework\nevaluates the user’s conceptual understanding to identify knowledge\ngaps. This assessment enables the generation of guiding questions,\nthe framework’s primary output. These questions direct users to for-\nmulate effective queries and focus on critical areas, thereby improv-\ning information retrieval efficiency. Tailored to the user’s knowledge\nstate, these guiding questions provide adaptive support for knowl-\n\nedge acquisition, distinct from generic prompts.\nThe main contributions of this paper are as follows:\n• We develop a method that leverages LLMs to generate guiding\nquestions by assessing the user’s information needs and knowl-\nedge state, facilitating more efficient information retrieval in com-\nplex domains.\n• We propose the CEIRT model to dynamically estimate the user’s\nknowledge state (θ) across multiple concepts. The visualizations\nand experiments demonstrate that our CEIRT model provides rea-\nsonable measures of user understanding.\n• We design a comprehensive evaluation framework to assess the\neffectiveness of guiding questions in improving information re-\ntrieval efficiency and knowledge acquisition.\n2\nRelated Work\n2.1\nExploring the Knowledge Boundaries of LLMs\nDespite the impressive advancements of LLMs, their ability to ex-\nplore and recognize their own knowledge boundaries remains lim-\nited. [27] and [17] investigate this aspect by designing question sets\nto challenge the models. On the other hand, works like [10] and [8]\nfocus on guiding the models during inference to increase the prob-\nability of generating correct answers. The R-Tuning method, intro-\nduced by [28], offers another perspective by constructing datasets\nand training LLMs to identify unanswerable questions within spe-\ncific domains.\n2.2\nQuestion-Answering Generation\nQuestion-Answer\nGeneration\ninvolves\nautomatically\ncreating\nquestion-answer pairs from a given context. Early approaches often\nrelied on rule-based and template methods, using linguistic patterns\nand predefined structures [12, 3, 9]. Neural sequence-to-sequence\napproaches marked a subsequent advance, works such as [5, 20]\nshowing how to directly generate question-answer pairs from\ninput text. Recently, research has begun to explore integrating\nQuestion-Answer Generation with LLMs, particularly through\nprompt engineering approaches [29]. We use dictionary-enhanced\nprompts to further ensure the accuracy and diversity of generated QA\npairs. These pairs provide the structured knowledge for downstream\ntasks: modeling user knowledge states via CEIRT and generating\nguiding questions.\n2.3\nQuestion Generation\nThis task was initially proposed by [24]. This foundational work\nspurred research into various methods and applications for QG.\nSubsequent advancements saw a shift towards neural sequence-to-\nsequence models and later, Transformer architectures [25]. Work\nby [18], for instance, explored using Transformer architectures for\nanswer-agnostic question generation. Their approach demonstrated\nthe power of self-attention but focused on generating questions with-\nout requiring a predefined answer. Research from [1] introduced a\npre-training method aimed at generating contextually rich questions\nby learning to recover answer-containing sentences. This highlights\nefforts to improve question quality, often targeting downstream read-\ning comprehension tasks. Some work, such as [22], focused on con-\ntrollability, using Item Response Theory (IRT) [13] to manage the\ndifficulty of generated question-answer pairs. While valuable for cre-\nating assessments with specific difficulty levels, this approach differs\nfrom dynamically guiding users based on evolving knowledge states.\nWhile much QG research focuses on applications like reading com-\nprehension assessment or data augmentation [1, 22], our AGQ frame-\nwork employs QG specifically to generate guiding questions that en-\nhance information retrieval efficiency and improve user knowledge\nstates. Leveraging CEIRT model, AGQ tailors these questions based\non dynamically assessed knowledge gaps (θ) and item characteristics\n(b), rather than generating general questions.\n2.4\nIRT-Based Adaptive Assessment and Diagnosis\nComputerized Adaptive Testing (CAT), based on Item Response\nTheory (IRT), dynamically selects items to efficiently estimate an\nexaminee’s ability level [26]. The field has since been enhanced with\ntechniques for content balancing and item exposure control [23], ex-\ntensions to multidimensional models (MIRT) for assessing multiple\ntraits [19], and refined item selection strategies [2]. However, the pri-\nmary goal of these IRT/MIRT models in CAT is to yield a stable,\nfinal ability estimate (θ). In contrast, our CEIRT model dynamically\ntracks a user’s knowledge state (θ) throughout an interaction to *in-\nform guidance* rather than produce a *terminal assessment*.\nCognitive Diagnostic Models (CDMs) also leverage IRT princi-\nples but focus on providing fine-grained profiles of skill mastery, of-\nten using a Q-matrix to map items to skills [21]. While advanced\nmodels like G-DINA can represent complex skill relationships [4],\nCDMs typically diagnose mastery of discrete skills for summative\nfeedback [21, 4]. Conversely, the CEIRT model utilizes a continuous\nmultidimensional vector (θ) to represent a user’s degree of under-\nstanding, updating it dynamically to serve as direct input for gener-\nating tailored guiding questions.\nIn summary, established IRT applications focus on assessment\n(CAT) and diagnosis (CDM). In contrast, the CEIRT model repre-\nsents conceptual understanding via a continuous, dynamic, multidi-\nmensional state vector (θ). The primary contribution of the AGQ\nframework is the integration of this CEIRT model with LLMs to\ngenerate adaptive guiding questions based on θ within interactive\ninformation retrieval dialogues.\n3\nMethodology\nIn this section, we detail the runtime operation of the AGQ\nframework, which enhances information retrieval through dynamic,\nCEIRT-based knowledge assessment and adaptive guiding question\ngeneration. The framework utilizes LLMs and operates on domain-\nspecific knowledge resources. We describe each component below.\n3.1\nCEIRT Model\nTraditional Item Response Theory (IRT) models typically assume a\none-dimensional ability parameter, insufficient for updating knowl-\nedge states in information retrieval. To address this, we propose the\nConcept-Enhanced Item Response Theory (CEIRT) model, which\nextends the Multidimensional Item Response Theory (MIRT) frame-\nwork, adapting the structure of the 2-Parameter Logistic (2PL)\nmodel. CEIRT uses vector representations for key parameters: the\nuser’s knowledge state across K concepts θ ∈RK, the difficulty\nparameter b ∈RK for question i, and the discrimination parameter\na ∈RK for question i, indicating how well it differentiates between\nknowledge states.\nThe knowledge state vector θ is implemented using an embed-\nding layer, offering a flexible representation of user understanding\n\nAlgorithm 1 Ask-Good-Question Framework\nRequire: User’s query q, Tutor prompt PT\nRequire: LLM f(·), CEIRT model g(·), Filtering model h(·)\nRequire: Guiding question generation prompts PQGlow and PQGhigh for low and high knowledge states\nRequire: Lower bound ϵ, Concept set C\nwhile “exit” not in q do\nR ←f(q, PT )\n▷Generate response R for user’s query\nC ←f(R)\n▷Identify concepts with knowledge state changes\nb, a, θ ←g(C)\n▷Update difficulty b, discrimination a and knowledge states θ\nθlow ←None\n▷Initialize low knowledge state flag\nfor θj in θ do\nif θj ≤ϵ then\nθlow ←θj\n▷Select concept j with low knowledge state, store value for potential use\nbreak\n▷Prioritize lowest knowledge state, exit loop\nend if\nend for\nif θlow is not None then\nT ←h(C, a, b, θlow)\n▷Filter text based on the identified low knowledge state concept\nQ ←f(PQGlow, T)\n▷Generate understanding-biased questions\nelse\nT ←h(C, a, b, θ)\n▷Filter relevant inspiring text T based on overall state\nQ ←f(PQGhigh, T)\n▷Generate application-biased questions\nend if\nend while\nin concepts. As a latent representation learned from data, the em-\nbedding provides an estimation of the user’s underlying knowledge\nstate, rather than a direct psychometric measurement. More impor-\ntantly, this knowledge representation is dynamic. As depicted in Al-\ngorithm 1, parameters θ, are adjusted through gradient-based op-\ntimization using user-LLM interaction data. This optimization pro-\ncess refines the knowledge state estimates by minimizing the Binary\nCross-Entropy (BCE) loss between the model’s predicted probabil-\nities of correct responses and the actual user outcomes (yi). This\nallows θ to continuously evolve and effectively capture the rela-\ntive changes in the user’s understanding during interaction. Building\nupon the logistic function characteristic of the 2PL model, the prob-\nability pi that a user provides a correct response to question i in our\nmultidimensional context is calculated as:\npi =\n1\n1 + exp(−P\nj(aiθj −bi))\n(1)\nThis formulation captures how multiple conceptual abilities (θj) con-\ntribute, weighted by the question’s discrimination (ai), relative to its\ndifficulty (bi). It is important to note that within this model, a higher\nvalue for a specific component θj in the knowledge state vector θ\ncorresponds to a higher understanding of the user regarding concept\nj.\n3.2\nDataset Construction\nAt the outset of our research, we conducted a preliminary assess-\nment of the LLM’s inherent knowledge within specialized domains.\nThis assessment using CEIRT revealed significant limitations in the\nmodel’s conceptual understanding of vertical fields. The results, vi-\nsualized in Figure 2, demonstrated clear gaps in the LLM’s knowl-\nedge base (using ChatGLM4-9B as example).\nTo address this limitation and provide the necessary structured\nknowledge for the AGQ framework, we implemented a multi-stage\ndataset generation pipeline leveraging LLM capabilities. Given a\ncorpus of domain-specific documents D = {d1, ..., dN}, we first\nFigure 2.\nRadar chart of LLM (ChatGLM4-9B-Chat example) knowledge\nstate (θ from CEIRT) in the petroleum domain. Low θ values across\nconcepts indicate insufficient expertise for independent guiding question\ngeneration.\nprompt an LLM to automatically identify a set of key concepts\nC = {c1, ..., cK} by analyzing the document content, for instance,\nfrom document abstracts and keywords. Sentence extraction for each\nconcept ck ∈C was then performed via prompting, identifying rel-\nevant sentences Sk = {sk,1, sk,2, ...} from the corpus D related to\nck: Sk = LLMextract(D, ck). These extracted sentences were orga-\nnized into a dictionary Dict = {ck : Sk}K\nk=1, mapping each con-\ncept to its associated example sentences. For each sentence sk,m ∈\nDictk, a corresponding question-answer pair (qk,m, ak,m) was gen-\nerated, where the question targets concept ck within the sentence:\n(qk,m, ak,m) = LLMQG(Dictk,m). This resulted in a preliminary set\nof QA pairs associated with specific concepts. Finally, all generated\nQA pairs underwent manual verification by domain experts to ensure\naccuracy, relevance, and clarity, yielding the final dataset used by the\nAGQ framework.\n\n3.3\nUser-LLM Interaction\nUser interaction commences with a query related to their informa-\ntion need. The framework utilizes an LLM, guided by a structured\nprompt, to generate a relevant response R. The prompt includes the\nuser’s initial knowledge state vector θ, converted into a textual de-\nscription of proficiency levels, to contextualize the response gener-\nation; further details are provided in the Appendix. Following the\ngeneration of response R, the framework dynamically updates the\nuser’s estimated knowledge state θ to reflect potential learning from\nthe interaction. This process involves two primary steps integrated\nwithin the AGQ workflow (Algorithm 1):\nFirst, an LLM-based analysis module f(R) examines the content\nof the response R. This module leverages the LLM’s contextual un-\nderstanding to identify a subset of key concepts C ⊆Call where\nCall is the set of all predefined concepts. This step infers the concep-\ntual areas that the user may have engaged with during the information\nretrieval process.\nSecond, the inferred conceptual needs to be translated into a for-\nmat suitable for updating the CEIRT model parameters. The CEIRT\nmodel estimates knowledge states based on user interaction history,\ntypically represented as responses to assessment items. To incorpo-\nrate the learning inferred from the unstructured response R, we em-\nploy a simulated evidence generation. For each concept c ∈C, the\nuser’s interaction history is augmented by simulating a correct re-\nsponse to a virtual assessment item associated with concept c. This\neffectively creates proxy evidence reflecting the inferred knowledge\ngain.\nThis augmented interaction history, incorporating the simulated\ncorrect responses for concepts in C, serves as new input for the\nCEIRT model’s update step g(C). During this step, θ is refined by\nperforming further optimization iterations (e.g., a few epochs of gra-\ndient descent using an Adam optimizer) to minimize the BCE loss\nover the cumulative interaction history. The resulting updated knowl-\nedge state vector θ provides an estimate of the user’s current under-\nstanding across all concepts, forming the basis for the subsequent\nquestion generation phase.\n3.4\nInspiring Text\nEffective guiding question generation by the LLM relies on provid-\ning appropriate contextual input, termed ’Inspiring Text’. The se-\nlection of this text is guided by the principle of optimal challenge,\naiming to match the text’s cognitive demand with the user’s current\nknowledge state to foster information retrieval.\nTo implement this principle, we define a suitability score S(t, j)\nfor a candidate text fragment t. This text t is associated with concept\nj and possesses a quantified difficulty parameter bi, derived from\nassociated assessment data within the structured knowledge corpus\nmentioned in Section 3.2. The score is calculated relative to the user’s\nknowledge state θj in concept j:\nS(t, j) = exp(−(|θj −bi| −1)2)\n(2)\nThis scoring function peaks when the absolute difference between\nthe user’s knowledge state and the text’s difficulty parameter,\n|θj −bi|, equals 1. A perfect match (|θj −bi| = 0) might offer in-\nsufficient cognitive challenge, while a large discrepancy could render\nthe text incomprehensible; thus, a slight deviation is preferred to pro-\nmote information retrieval.\nEmpirical validation for this peak value comes from our ablation\nstudy (Section 6), which demonstrates that user knowledge gain is\nmaximized when |θj −bi| is approximately 1.\nFor each target concept requiring a guiding question, the frame-\nwork selects the text fragment(s) with the highest suitability score\nS(t, j) to serve as the Inspiring Text. This selected text is then pro-\nvided as contextual input to the LLM, enabling the generation of\ncontext-aware, user-adaptive, and appropriately challenging guiding\nquestions.\nBased on the updated knowledge state θ, the AGQ framework em-\nploys an adaptive strategy for generating guiding questions, as out-\nlined in Algorithm 1. If a concept j is identified with a knowledge\nstate θj below a predefined threshold ϵ, indicating a need for founda-\ntional understanding, the LLM utilizes a specific prompt (PQGlow),\ndesigned to elicit foundational questions (e.g., ’What is ...?’), with\nthe selected Inspiring Text. Conversely, if all relevant concepts ex-\nceed the threshold, a different prompt (PQGhigh), crafted to encour-\nage application-focused questions (e.g., ’How can ... be applied to\n...?’), is used to generate application-biased questions, focusing on\npractical application and deeper exploration.\n3.5\nRelevance and Quality Control of Guiding\nQuestions\nTo ensure that guiding questions generated by the AGQ framework\nare relevant to the user’s information retrieval needs and maintain a\nsuitable standard, a quality assessment mechanism is employed. This\nmechanism evaluates potential guiding questions based on several\nkey attributes, allowing for the identification and filtering of ques-\ntions that may be poorly aligned or low-quality. The assessment in-\ncorporates the following metrics: First, information gap alignment is\nconsidered. For a concept j, this is quantified as:\nAlignScore = 1 −θj\n(3)\nwhere θj represents the user’s estimated knowledge state in concept\nj. A higher AlignScore indicates that the question targets an area\nwhere the user’s current knowledge state suggests a potential infor-\nmation gap, making the question more pertinent to their retrieval ob-\njectives.\nSecond, conceptual specificity measures the focus of the question.\nUsing mutual information between a question q and its target concept\nc:\nMI(q, c) =\nX\nq,c\np(q, c) log p(q, c)\np(q)p(c)\n(4)\nwhere p(q, c) is the joint probability and p(q), p(c) are the marginal\nprobabilities. This metric quantifies the strength of association, en-\nsuring the question is specific to the intended concept rather than\noverly general.\nThird, linguistic complexity evaluates the question’s structure.\nRepresenting the token count of question q as len(q), the complexity\nindex is calculated via standardization and sigmoid transformation:\nComplexityIndex = σ\n\u0012len(q) −µ\nσ\n\u0013\n(5)\nwhere σ(x) = (1 + e−x)−1 is the sigmoid function, and µ, σ\nare the mean and standard deviation of token counts in the dataset.\nThis index helps assess if the question’s formulation is appropriately\ncomplex for clear guidance, avoiding overly simplistic or convoluted\nphrasing.\nThese metrics are combined into a final quality score:\nQualityScore = α·AlignScore+β·MI(q, c)+γ·ComplexityIndex\n(6)\n\nwhere α, β, γ are empirically determined weight coefficients satisfy-\ning α + β + γ = 1. This QualityScore provides a quantitative basis\nfor assessing each potential guiding question. It is used for analysis\nor as a criterion for filtering out questions that fall below a predeter-\nmined quality threshold, thus improving the overall relevance and ef-\nfectiveness of the guidance provided within the information retrieval\nprocess.\n4\nExperiment Setup\nTo validate the effectiveness of AGQ framework, we designed and\nconducted a series of experiments. The primary goal was to evalu-\nate the performance of AGQ framework across different scenarios.\nIt is important to note that AGQ framework does not have specific\nrequirements for LLMs and can be applied to both open-source and\nproprietary models. For these experiments, we selected ChatGLM4-\n9B as the representative model, which demonstrates strong perfor-\nmance in NLP tasks[6].\n4.1\nDataset\nThe experiments utilized the EOR-QA dataset, a custom dataset built\nfor AGQ in the Enhanced Oil Recovery (EOR) domain. This dataset\nlinks key EOR concepts to relevant contextual sentences and para-\ngraphs extracted from domain literature. Based on this structured\ncontext, EOR-QA comprises over 3,100 question-answer pairs, de-\nsigned to cover different cognitive levels, from foundational princi-\nples to practical applications. As elaborated in Section 3.2, all content\nunderwent verification by domain experts to ensure accuracy and rel-\nevance. The EOR-QA dataset serves a dual purpose: it provides the\nfoundational data for the CEIRT model to estimate and update user\nknowledge states (θ), and it acts as the source repository for selecting\nInspiring Text used in guiding question generation.\nThe decision to construct a custom dataset stemmed from a sur-\nvey of existing public resources, which revealed a lack of datasets\nadequately addressing the needs of our research in the EOR domain.\nWhile numerous general/scientific QA datasets exist, they typically\nlack the required domain-specific depth and concept coverage that\nare crucial for the AGQ framework’s user modeling and adaptive\nguidance mechanisms. Furthermore, our custom dataset mitigates the\nrisk of data contamination inherent in public benchmarks, ensuring a\nrigorous evaluation of the model’s performance on genuinely unseen\ndomain knowledge. Therefore, the creation of the EOR-QA dataset\nwas a necessary prerequisite to effectively implement and reliably\nevaluate the AGQ framework within this specialized domain.\n4.2\nBaselines\nWe compared the AGQ framework against several baseline methods\nto evaluate its effectiveness. The following approaches were included\nin our experimental comparison:\n• Zero-shot Question Generation: We directly prompted the LLM\nto generate guiding questions without providing any examples.\nThis method relies solely on the LLM’s pre-trained knowledge\nand capabilities to generate relevant questions based on the user’s\nquery.\n• CoT Prompts with Handcrafted Examples: This method incor-\nporates Chain-of-Thought (CoT) prompting with manually con-\nstructed examples derived from the EOR-QA dataset to assist the\nLLM in guiding question generation, as CoT has demonstrated\neffectiveness in complex reasoning tasks requiring step-by-step\nthinking.\n• Human Expert: We also consider the guiding questions gener-\nated by human experts in the petroleum field. These experts, with\ntheir deep domain knowledge, create questions that are specif-\nically tailored to identify and address gaps in the user’s under-\nstanding. This method serves as a gold standard against which the\nperformance of the LLM-generated questions can be measured.\nFigure 3.\nAccuracy comparison of different guiding question generation\nmethods over dialogue rounds. The AGQ method demonstrates performance\nsignificantly exceeding CoT and Zero-shot approaches, closely approaching\nthe effectiveness of Human Experts. The ’Human with AGQ context’ group\nserves to validate the evaluation methodology.\nFigure 4.\nAccuracy comparison of guiding question generation using the\nAGQ framework with different LLMs (ChatGLM4-9B, Qwen2.5-7B,\nQwen2.5-32B) and Human Expert over dialogue rounds.\n5\nResults and Analysis\n5.1\nComparison with Baselines\nIn this section, we evaluate the performance of the AGQ frame-\nwork against several baseline methods and assess its generalizabil-\nity across different LLMs. The primary comparison involves AGQ,\nZero-shot Question Generation, CoT Prompts with Handcrafted Ex-\namples, and a Human Expert benchmark. The accuracy is evaluated\n\nFigure 5.\nEvolution of user knowledge states (θ) across five key concepts using different guiding question generation methods. From left to right: AGQ,\nChain-of-Thought (CoT), and Zero-shot. The results demonstrate the effectiveness of the AGQ framework’s generated questions in enhancing user\nunderstanding.\nthrough a systematic process: we first collect sets of conversations\ngenerated by each method, then employ an LLM to answer questions\nfrom the EOR-QA dataset based on the conversation context. Accu-\nracy is defined as the average rate of correct responses.\nFigure 3 illustrates the accuracy comparison over dialogue rounds.\nThe results demonstrate AGQ’s superior performance. After 20\nrounds, AGQ achieved 100% accuracy, significantly higher than CoT\n(41.1%) and Zero-shot (23.9%). Across all rounds, AGQ maintained\na higher average accuracy (48.8%) compared to CoT (25.6%) and\nZero-shot (16.3%). The performance trajectory of AGQ closely mir-\nrors that of the \"Human with AGQ context\" group (average 51.0%,\nreaching ≥95% accuracy by round 19), validating our automatic\nevaluation methodology, and approaches the effectiveness of human\nexperts (average 72.0%, reaching ≥95% accuracy by round 14).\nWhile CoT demonstrates strong logical reasoning, its accuracy im-\nprovement was limited by susceptibility to prompt examples, leading\nto repetitive questions and reduced information retrieval efficiency.\nThe Zero-shot method, lacking context or examples, generated vague\nquestions, resulting in consistently low accuracy with minimal vari-\nance. In contrast, AGQ generates more specific and complex ques-\ntions compared to the baselines. See Table 1 for examples of guiding\nquestions generated by each method addressing a similar underlying\ntheme.\nTable 1.\nQualitative Comparison of Example Guiding Questions\nMethod\nGuiding Question\nAGQ\nHow do surfactants and hydrocarbon miscible flooding tech-\nniques synergistically Enhance Oil Recovery during the pro-\ncess of increasing oil recovery?\nCoT\nWhat is the principle of hydrocarbon miscible flooding?\nZero-shot\nHow does the principle or technology mentioned in this con-\nversation work?\nFurthermore, to assess the generalizability of the AGQ framework,\nits performance was evaluated using distinct LLMs: Qwen2.5-7B and\nQwen2.5-32B [16]. Figure 4 presents this cross-model comparison.\nThe results illustrate several key characteristics. Firstly, initial perfor-\nmance correlates with model scale. In the early rounds (e.g., rounds\n1-10), the larger Qwen2.5-32B model demonstrates higher accuracy\ncompared to the smaller Qwen2.5-7B and ChatGLM4-9B models.\nFor instance, at round 7, Qwen2.5-32B achieved approximately 50%\naccuracy, whereas Qwen2.5-7B registered below 40%. Secondly, de-\nspite these initial differences, accuracy consistently improves with\nincreasing interaction rounds across all tested LLMs. Critically, all\nmodels converge to high accuracy levels (approaching 100%) by\nrounds 19-21. This convergence indicates the framework’s capacity\nto effectively enhance the performance of LLMs regardless of their\ninitial capacity. Collectively, these findings confirm AGQ’s superior-\nity over baseline methods and highlight its cross-model adaptability\nand robustness. Its effectiveness is not contingent on a specific LLM,\nshowcasing its potential as a generalizable approach for enhancing\ninformation retrieval guidance systems across various model choices.\n5.2\nText Similarity Evaluation\nTo complement our evaluation, we also conducted a quantitative\nanalysis comparing the guiding questions generated by each method\nagainst human expert reference questions using standard text similar-\nity metrics [15, 11]. While these metrics primarily assess lexical and\nsemantic similarity rather than functional effectiveness in informa-\ntion retrieval, they provide an additional perspective on performance\ncomparison. As shown in Table 2, the AGQ framework substantially\noutperformed baseline methods across all metrics. AGQ achieved a\nBLEU-4 score of 0.219, significantly higher than CoT (0.025) and\nZero-shot (0.016). Similarly, for ROUGE-1, AGQ scored 0.577 com-\npared to 0.198 for CoT and 0.114 for Zero-shot. This superior perfor-\nmance extended to ROUGE-2 (AGQ: 0.278, CoT: 0.023, Zero-shot:\n0.008) and ROUGE-L (AGQ: 0.463, CoT: 0.168, Zero-shot: 0.102).\nThis result reflects the characteristic of question generation tasks, as\nfunctionally equivalent questions can be expressed using varied lex-\nical choices and syntactic structures. Despite this inherent variabil-\nity, AGQ´\ns substantially higher scores confirm its greater alignment\nwith human expert formulations. Full details of this evaluation can\nbe found in Appendix.\nTable 2.\nQuantitative Evaluation using Text Similarity Metrics against\nHuman Expert References\nMethod\nBLEU-4\nROUGE-1\nROUGE-2\nROUGE-L (F1)\nAGQ\n0.219\n0.577\n0.278\n0.463\nCoT\n0.025\n0.198\n0.023\n0.168\nZero-shot\n0.016\n0.114\n0.008\n0.102\n\nFigure 6.\nAfter 20 rounds of dialogue, the knowledge state (θj) value of\nEOR increased from 1.44 to 4.85 with AGQ, from 1.06 to 3.36 with CoT,\nand from 0.34 to 2.65 with Zero-shot.\n5.3\nKnowledge Gain\nTo evaluate the impact on user understanding, we evaluated knowl-\nedge gain by comparing the knowledge state vector (θ) before and\nafter the 20-round interaction process.\nFigure 6 tracks the knowledge state (θj) for the EOR concept\nspecifically. It shows that the AGQ framework facilitated substan-\ntial and steady growth, increasing the θj value from 1.44 to 4.85.\nThis contrasts sharply with the moderate increase observed for CoT\n(1.06 to 3.36) and the minimal gain for the Zero-shot method (0.34 to\n2.65), underscoring AGQ’s effectiveness in deepening understanding\nof specific concepts.\nFurther evidence is presented in Figure 5, which displays the fi-\nnal knowledge states across five concepts in EOR-QA using radar\ncharts. Visually, AGQ demonstrates superior performance across all\nconcepts compared to CoT and Zero-shot methods. Quantitatively,\nthe average final knowledge state across these five concepts was 4.70\nfor AGQ, significantly higher than 3.19 for CoT and 2.63 for Zero-\nshot. This substantial knowledge gain observed with AGQ aligns\nwith the superior accuracy reported in Section 5.1, suggesting that\nthe framework’s ability to generate effective guiding questions trans-\nlates directly into improved user understanding, retrieval efficiency,\nand perceived guidance quality.\n5.4\nAnalysis\nIn comparison, while the CoT method demonstrates capability in\nguiding users through the learning process, it lacks the precision\nneeded to effectively address specific concepts where the user’s un-\nderstanding is weaker. This limitation stems from its inability to dif-\nferentiate adequately between the user’s varying levels of knowledge\nstate, resulting in a less targeted approach to information retrieval.\nOn the other hand, the Zero-shot question generation method shows\neven weaker performance. Without accurate knowledge state mea-\nsurement, the Zero-shot method generates overly general guiding\nquestions lacking specific content.\nThe superior performance of the AGQ framework can be attributed\nto the integration of the CEIRT model, which plays a crucial role in\nits success. The CEIRT model enables dynamic and precise knowl-\nedge measurement. This model-driven precision enables AGQ to\ngenerate guiding questions that are not only relevant but also strate-\ngically designed to challenge and expand the user’s knowledge in tar-\ngeted concepts. By dynamically updating the user’s knowledge state\nand refining the difficulty and discrimination parameters (i.e., θ, b,\nand a), AGQ improves the effectiveness of information retrieval.\n6\nAblation Study\nThis section investigates the optimal difference between the difficulty\nof question i (bi) and the user’s knowledge state in concept j (θj)\nby experimentally comparing knowledge state growth under varying\ndifficulty-ability differences. As illustrated in Figure 7, knowledge\ngain is maximized when the absolute difference between question\ndifficulty bi and user knowledge state θj is approximately 1. This\nfinding provides empirical support for the design of our Inspiring\nText suitability score (Equation 2) and underlines the importance of\nproviding an appropriate level of challenge to optimize information\nretrieval outcomes. Deviations from this difference significantly re-\nduced knowledge gain, emphasizing the need to align question dif-\nficulty with user knowledge. Furthermore, as the difference was ad-\njusted to 1.5, users began to report that the difficulty gap was signifi-\ncant, with a noticeable decline in engagement. This feedback became\nmore pronounced as the difference increased to 2.\nFigure 7.\nAbility growth peaks when the difficulty-knowledge gap\n(bi −θj) is approximately 1, suggesting optimal learning occurs at a\nmoderate challenge. Error bars denote variability.\n7\nConclusion\nIn this paper, we introduced the AGQ framework, an innovative\napproach for generating guiding questions during the information\nretrieval process. At its core, the framework leverages the CEIRT\nmodel, a novel method for measuring knowledge in specialized do-\nmains that enables precise assessment of users’ knowledge states\nacross multiple concepts. In comparative experiments against several\nbaseline methods, AGQ demonstrated superior performance, closely\nmatching the effectiveness of human experts. Additionally, we de-\nveloped a domain-specific text dataset (EOR-QA), which served as\nboth a source of structured knowledge and inspiring text to facili-\ntate precise guiding question generation within the framework during\nour evaluations. The AGQ framework marks a key advance in LLM-\nbased adaptive guidance, offering a versatile design that is readily\nadaptable to knowledge-intensive domains like medicine and educa-\ntion beyond its initial demonstration in the EOR field."}
{"id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "authors": ["Xinhua Chen", "Sitao Huang", "Cong Guo", "Chiyue Wei", "Yintao He", "Jianyi Zhang", "Hai \"Hellen\" Li", "Yiran Chen"], "source": "http://arxiv.org/abs/2508.14148v1", "page_count": 22, "text": "DPad: Efficient Diffusion Language Models with Suffix Dropout\nXinhua Chen1∗, Sitao Huang1∗, Cong Guo1∗†, Chiyue Wei1, Yintao He1, Jianyi Zhang1,\nHai “Hellen” Li1, Yiran Chen1\n1Duke University\nAbstract\nDiffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising\nprocess, but suffer from high computational overhead since they predict all future suffix tokens at each step\nwhile retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that\nrestricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy.\nDPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii)\ndistance-decay dropout, which deterministically removes distant suffix tokens before attention computation.\nThis simple design is compatible with existing optimizations such as prefix caching and can be implemented\nwith only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and\nDream models demonstrate that DPad delivers up to 61.4× speedup over vanilla dLLMs while maintaining\ncomparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is\navailable at https://github.com/Crys-Chen/DPad.\n1.\nIntroduction\nLarge Language Models (LLMs) have become foundational in numerous applications [31, 10, 7, 24], yet their\ndeployment is often hindered by inference latency. As shown in Fig. 1 (a), the predominant autoregressive\nframework generates text one token at a time [25, 26], imposing a sequential constraint that limits speed and\nscalability [13]. This has driven interest toward parallel decoding strategies.\nDiffusion-based Large Language Models [19, 3, 21, 27, 17] (dLLMs) offer a promising alternative by\neliminating sequential dependencies. Formulating text generation as a parallel denoising process, they can\npredict entire sequences or generate text block-wise (i.e., semi-autoregressively) [23, 36], as in Fig. 1 (b).\nHowever, this parallelism often incurs high computational cost [22]: at each step, predictions for all future\n(suffix) tokens are computed, though only a small fraction are retained. Consequently, although dLLMs can\ngenerate multiple tokens in parallel, the resulting throughput gains are undermined by a disproportionate increase\nin computation, posing a key bottleneck to their widespread adoption [29].\nTo further understand this inefficiency, we analyze the role of suffix tokens under the block-wise masking\nmechanism in dLLMs.\nInterestingly, these suffix tokens contribute no direct semantic information to the\nattention; instead, they function as an information reservoir that collects signals from the already decoded prefix\ntokens, much like a scratchpad. Through multiple Transformer layers, this scratchpad can guide the diffusion\nprocess to produce a more organized and contextually consistent current block. Yet, most suffix tokens are\nredundant and low-entropy, often forming long runs of similar content. This redundancy becomes more severe\n∗Equal contribution\n†Corresponding author: Cong Guo (cong.guo@duke.edu)\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nPrompt\nResponse\nStep 0\nStep 1\n(a) Auto-Regression LLM\nPrompt\nResponse\nBlock 0, Step 0\nDecoded\nBlock 0\nPreﬁx: Attend Backward\nCurrent Block\nSuﬃx: Attend Forward with All Suﬃx Tokens\n(b) Block-wise Diﬀusion LLM (Semi-Autoregression)\nDecoding\nMasked\n……\nDistance-Decay Dropout\nStep 2\nStep 3\nBlock 1\nBlock 2\nBlock n\nPreﬁx: Attend Backward Only\nBlock 0, Step 1\nBlock 1, Step 0\nBlock 1, Step 1\nPrompt\nResponse\nBlock 0, Step 0\nBlock 0\nDPad: Attend Forward with Nearby and Few Suﬃx Tokens\nBlock 1\nBlock 2\nBlock 0, Step 1\nBlock 1, Step 0\nBlock 1, Step 1\nBlock 3\nBlock 4\n(c) Ours: Diﬀusion LLM with Scratchpad (DPad)\nDPad\nDetailed\nFocused\nOutlined\nCurrent:\nNear:\nFar:\nSliding\nWindow\nOut of Scratchpad:\nOmitted\n……\n……\n……\nFig. 1: Comparison of (a) autoregressive LLMs, (b) block-wise diffusion LLMs, and (c) our DPad. DPad restricts suffix\nattention via: (i)Sliding Window: fixed-length suffix window; (ii) Distance-decay Dropout: removes distant suffix tokens\nwithout computing attention scores.\nas the distance to the current block increases, with attention scores for suffix tokens dropping sharply from near\nto far positions. Such redundancy not only imposes unnecessary computational overhead, but can also degrade\nthe fidelity of the generated content.\nBased on the above insights, we propose the Diffusion Scratchpad (DPad), which attends forward only to\na small number of near suffix tokens, as in Fig. 1 (c). It uses two suffix drop strategies: sliding window and\ndistance-decay dropout. For the sliding window, inspired by prefix KV-cache optimizations [5, 35], we extend\nthe idea to the suffix. Here, the suffix window has a fixed length and moves forward along with the current block,\nretaining only a limited number of nearby suffix tokens. In contrast to vanilla dLLMs, where suffix computation\nincreases with the generation sequence length, our design keeps the cost bounded and reduces suffix-related\ncomputation by one dimension.\nFor the distance-decay dropout, suffix tokens are removed according to their distance from the current\nblock: the farther they are, the higher the dropout ratio, until all tokens beyond the window are omitted. Unlike\nconventional attention-score pruning [32, 18, 28], which first computes attention scores and then prunes based on\ntheir magnitude, DPad predetermines a distance-decay sparse pattern for suffix tokens prior to model execution\nand eliminates them in a single operation at the beginning of each step.\nInspired by the Lottery Ticket Hypothesis (LTH) [11], which posits that properly pruned sub-networks with\ntheir original initialization can match the performance of dense networks after training, we observe that the\nstrong generalization ability of dLLMs enables the training-free construction of “winning tickets” for suffix\ntokens during inference. Although this predetermined dropout strategy may appear intrusive, it consistently\nachieves strong empirical performance, with only a small subset of suffix tokens required to maintain accuracy\ncomparable to vanilla dLLMs. This further suggests that sparsity is an inherent property of suffix attention.\nAdditionally, the method is extremely simple to deploy, requiring only a few lines of code to implement.\nTo illustrate the intuition behind DPad, we liken it to a real scratchpad used when writing a book, as shown\nin Fig. 1 (c, right). For the current chapter (i.e., block), the writer (dLLM) devotes the most attention, revising\nit multiple times, akin to denoising within a diffusion block. The upcoming chapter receives focused drafts for\nconsistency, while much later chapters contain only brief outlines. Naturally, the “writer” should not, and indeed\nmust not, fill all future chapters (all suffix tokens) with low-entropy, repetitive content merely to satisfy the fixed\nsequence length constraints of current dLLMs. Such uncontrolled filling distracts the “writer’s” attention and\n2\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nwastes storage and computation, making it neither sustainable nor scalable.\nFinally, we emphasize that DPad enables efficient and compact generation without enforcing a fixed sequence\nlength. Unlike conventional dLLMs, where mandatory sequence length and full suffix attention introduce\nredundancy and degrade output quality [36, 23], DPad produces concise results with minimal overhead. It\nis entirely training-free, saving substantial memory and computation, and achieves stable acceleration even at\nshorter generation sequence lengths (e.g., 256 and 512), outperforming both vanilla and optimized variants\nsuch as Fast-dLLM [34]. DPad is also compatible with existing optimizations [34, 20, 33], including prefix\ncaching [34], dual caching [34], and dLLM-Cache [20]. It further scales naturally to long sequences: when\ncombined with parallel generation and prefix caching, it achieves a 30.58× speedup on Dream/HumanEval\n(1024, 0-shot) and a 61.39× speedup on LLaDA-1.5/GSM8K (1024, 1-shot) over vanilla, while maintaining\ncomparable model accuracy. These results highlight the strong potential of DPad for long-sequence inference\nand demonstrate its ability to unlock new efficiency frontiers in dLLMs. Although dLLMs remain in an early\nexploratory stage, with many researchers striving to assemble the final puzzle, we believe that DPad represents\none of the key components in completing the whole picture and paves the way toward practical, scalable\ndiffusion-based language modeling.\n2.\nPreliminary\n2.1.\nFoundational Principles of Diffusion Large Language Models (dLLM)\nConventional Autoregressive Models (ARMs)definethejointprobabilityofageneratedsequence𝑥= (𝑥1, . . . , 𝑥𝐿)\nby factoring it into a product of conditional probabilities in a fixed sequential order [31]. The model predicts\nthe next token 𝑥𝑖based on the conditional probability of the preceding tokens 𝑥1, . . . , 𝑥𝑖−1:\n𝑝𝜃(𝑥) = 𝑝𝜃(𝑥1)\n𝐿\nÖ\n𝑖=2\n𝑝𝜃(𝑥𝑖|𝑥1, . . . , 𝑥𝑖−1)\n(1)\nIn contrast, Diffusion Language Models (dLLMs) leverage a non-autoregressive process [3, 27, 21]. During\ntraining, the model learns to denoise a corrupted sample. This involves two processes:\nForward Masking Process\nThis process systematically replaces a proportion of tokens in a clean text sequence\n𝑥0 with a special [MASK] token [22, 23], similar to applying noise in a conventional diffusion model [16]. The\nreplacement is governed by a masking schedule where 𝑡∈[0, 1] represents the masked level. For a given 𝑡, the\nprobability of an individual token 𝑥𝑖\n0 being masked is:\n𝑞(𝑥𝑖\n𝑡|𝑥𝑖\n0) =\n(\n1 −𝑡,\nif 𝑥𝑖\n𝑡= 𝑥𝑖\n0\n𝑡,\nif 𝑥𝑖\n𝑡= [MASK]\n(2)\nReverse Unmasking Process\nThe model 𝑝𝜃is trained to predict the original tokens given the partially masked\nsequence 𝑥𝑡, thereby learning to approximate the true unmasking probability 𝑞(𝑥0|𝑥𝑡) [3, 21]. This is achieved\nby minimizing the negative log-likelihood over the masked tokens [27, 24]:\nL(𝜃) = −E𝑡,𝑥0,𝑥𝑡\n\n\n\n\n\n\n∑︁\n𝑖:𝑥𝑖\n𝑡=[MASK]\nlog 𝑝𝜃(𝑥𝑖\n0|𝑥𝑡)\n\n\n\n\n\n\n(3)\n3\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nThis process makes use of a Transformer architecture to learn the distribution 𝑝𝜃(𝑥0|𝑥𝑡). During training,\nbidirectional multi-head attention on all tokens within a sequence—masked and unmasked tokens alike—is\nemployed to predict the probability distribution for each masked token.\n2.2.\nInference and Sampling in dLLM\nThe inference process begins by initializing a sequence 𝑦0 through the concatenation of a prompt 𝑟and 𝐿\nmask tokens, denoted as 𝑦0 = 𝑟◦([MASK])𝐿\n𝑖=1. Let 𝑀𝑠be the set of indices corresponding to masked tokens\nat step 𝑠; initially, 𝑀0 = {|𝑟| + 1, . . . , |𝑟| + 𝐿}. The model then iteratively refines this sequence over steps\n𝑠= 1, 2, . . . , 𝑆. At each step 𝑠, the model 𝑝𝜃computes a probability distribution 𝑝𝜃(𝑦𝑖|𝑦𝑠−1) for every masked\nposition 𝑖∈𝑀𝑠−1. From these distributions, the most likely token predictions ˆ\n𝑦𝑖and their corresponding\nconfidence scores 𝑐𝑖are determined:\nˆ\n𝑦𝑖= argmax\n𝑣∈𝑉\n𝑝𝜃(𝑦𝑖= 𝑣|𝑦𝑠−1)\nand\n𝑐𝑖= 𝑝𝜃(𝑦𝑖= ˆ\n𝑦𝑖|𝑦𝑠−1).\n(4)\nwhere 𝑉is model’s vocabulary. A scheduling function G(𝑠, 𝑆) determines the number of tokens, 𝑘𝑠, to unmask.\nThe set of indices to update, 𝑈𝑠, is chosen by selecting the 𝑘𝑠positions from 𝑀𝑠−1 with the highest confidence\nscores. The new sequence 𝑦𝑠is then formed by updating these positions:\n(𝑦𝑠)𝑖=\n(\nˆ\n𝑦𝑖\nif 𝑖∈𝑈𝑠\n(𝑦𝑠−1)𝑖\notherwise\n(5)\nThe set of masked indices is updated via 𝑀𝑠= 𝑀𝑠−1 \\ 𝑈𝑠. This iterative refinement continues until 𝑀𝑠= ∅.\n2.3.\nRelated work on dLLM acceleration\nWhile early work in dLLMs employed Top-𝑘sampling to unmask a fixed number of high-confidence tokens at\neach step, later methods introduced more dynamic, confidence-aware techniques [34, 33] that greedily unmask\nall tokens above a threshold, thereby alleviating the generation bottleneck. Another line of research focuses\non cache management. Because attention in dLLMs is bi-directional, conventional KV caching is not directly\napplicable. To address this, several optimizations [34, 20] exploit the observation that prefix tokens and distant\nsuffix tokens exhibit little variation in their attention values across inference steps, allowing them to be reused\nand reducing expensive recomputation. A more fine-grained approach [28] further improves efficiency by\ndynamically evicting cache entries based on attention scores, leveraging the persistence of attention patterns\nacross steps.\n3.\nMethod\nIn this section, we begin by formalizing the Scratchpad mechanism in dLLMs, which underpins the role of suffix\ntokens as an information reservoir. We then introduce Suffix Dropout, which combines a fixed-length sliding\nwindow that moves with the current block to bound the number of attended suffix tokens, with a distance-decay\nscheme that progressively prunes more distant ones. Finally, we propose the Diffusion Lottery Tickets (DLT)\nhypothesis, which provides a principled lens for interpreting dropout in dLLMs.\n4\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nPreﬁx\nCurrent\nSuﬃx\nDPad Writing\nPreﬁx Information\nCurrent Information\nDPad\nReading\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPreﬁx Read\nCurrent Block Read\n6\nFig. 2: Attention score maps illustrating the Scratchpad mechanism in dLLMs. The maps were generated by the LLaDA-\n1.5 model [37] on prompt and 512-token sequences from the GSM8K dataset [9]. The attention matrix is divided into\n3 × 3 blocks over prefix, current, and suffix. Blocks 7 and 8 collect information from the prefix and current into the suffix\nat layer 𝑛, while Block 6 feeds this stored information back to the current block at layer (𝑛+1). This write–store–read cycle\nmakes suffix tokens a dynamic scratchpad for cross-layer contextual reuse.\n3.1.\nScratchpad Mechanism\nWe first revisit the role of suffix tokens in dLLMs. Due to the masking structure, suffix tokens carry no direct\nsemantic information but instead serve as an information reservoir, aggregating signals propagated from prefix\ntokens across multiple Transformer layers. This latent memory, which we refer to as DPad, stabilizes the\ndenoising process by providing contextual scaffolding for the current block.\nAs illustrated in Fig. 2 and the attention score maps, the token sequence can be partitioned into three\ncontiguous segments: Prefix indices [0, 𝑐−1], current indices [𝑐, 𝑠−1], and suffix indices [𝑠, 𝐿−1]. The\ncorresponding attention matrix is thus divided into 3 × 3 blocks. Among them, Blocks 6, 7, and 8 together\ndefine the scratchpad mechanism.\nConsidering only one head, at layer 𝑛, queries from prefix and current tokens attend to keys from the suffix\nregion. Formally, the global attention weights are defined as\n𝐴(𝑛) = Softmax\n\u0012𝑄(𝑛) (𝐾(𝑛))⊤\n√𝑑𝑘\n\u0013\n∈R𝐿×𝐿.\n(6)\nWe can partition 𝐴(𝑛) into submatrices corresponding to prefix (𝑃= [0, 𝑐−1]), current (𝐶= [𝑐, 𝑠−1]), and\nsuffix (𝑆= [𝑠, 𝐿−1]). In particular,\n𝐴(𝑛)\n𝑆,𝑃= 𝐴(𝑛) [𝑠: 𝐿, 0 : 𝑐],\n𝐴(𝑛)\n𝑆,𝐶= 𝐴(𝑛) [𝑠: 𝐿, 𝑐: 𝑠],\n(7)\nrepresent the attention weights from suffix queries to prefix and current keys, respectively (Blocks 7 and 8 in\nFig. 2). Multiplying these attention weights with the value matrix yields the actual outputs at suffix positions:\n𝐻(𝑛)\n𝑆\n= 𝐴(𝑛)\n𝑆,𝑃𝑉(𝑛)\n𝑃\n+ 𝐴(𝑛)\n𝑆,𝐶𝑉(𝑛)\n𝐶\n+ 𝐴(𝑛)\n𝑆,𝑆𝑉(𝑛)\n𝑆\n.\n(8)\nHere, 𝐻(𝑛)\n𝑆\ndenotes the hidden representations of suffix tokens after attention. This equation shows that\nsuffix tokens integrate information from prefix and current regions, effectively serving as a Scratchpad that\n5\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nrecords contextual signals. After this aggregation, the outputs 𝐻(𝑛) are processed by the subsequent linear\ntransformations (e.g., feed-forward layers and residual connections), which operate independently on each\ntoken. Thus, information written into the suffix tokens will not interact with other regions until the next layer’s\nattention, where prefix and current queries can read back the stored information from DPad. This establishes\nthe complete write–store–read cycle across consecutive layers. At layer (𝑛+1), this stored information can be\nretrieved by the current block through\n𝐴(𝑛+1)\n𝐶,𝑆\n= 𝐴(𝑛+1) [𝑐: 𝑠, 𝑠: 𝐿],\n(9)\nwhich corresponds to current-to-suffix attention (Block 6 in Fig. 2). This path enables the current block to reuse\nthe information collected in the suffix at the previous layer.\nThrough this mechanism, the stored information in suffix tokens at layer 𝑛can be retrieved by the prefix\nand current blocks at layer (𝑛+1). However, in practice, the influence of the suffix on the prefix is negligible:\nPrefix tokens correspond to already generated content, and their representations can be refined directly from\nthemselves and the current block, without requiring additional signals from the suffix. Indeed, prior work [34]\nhas shown that caching prefix states during inference barely impacts accuracy, implying that the suffix-to-prefix\npathway is effectively redundant.\nTherefore, the essential interaction of the scratchpad mechanism lies in\nthe current-to-suffix direction (Block 6), where the suffix serves as temporary memory to assist the ongoing\ndenoising process.\nTherefore, we conjecture that the behavior of suffix tokens resembles a residual connection specialized\nfor attention. Instead of directly propagating representations, the model compresses high-dimensional signals\nfrom the prefix and current into the suffix and re-injects them into the current block at the next layer. This is\nreminiscent of the bottleneck residual block in ResNets [14], where high-dimensional features are projected,\npropagated, and then expanded. Similarly, the suffix acts as a scratchpad that compresses contextual information\nand delivers it forward for cross-layer reuse in dLLMs.\n3.2.\nSuffix Dropout Strategies\nBuilding on the scratchpad perspective, we analyze redundancy in suffix attention to motivate a structured\ndropout design. At the final layer 31, Fig. 3 shows a sharp distance-dependent decay: nearby suffix tokens\ndominate, consistent with general attention behavior and prior work on prefix sliding windows [5, 35]. Moreover,\nsuffix attention is one to two orders of magnitude smaller than that of prefix or current, indicating that suffix\nrepresentations are information-light and only a small subset is needed to serve as the scratchpad memory.\nBased on these findings, we propose two complementary suffix drop strategies: (a) fixed-length Sliding Win-\ndow to retain only a bounded number of near-suffix tokens, and (b) Distance-decay Dropout that progressively\nprunes distant suffix tokens with minimal impact on model quality, as illustrated in Fig. 1(c). Both mechanisms\nare efficiently realized through a Gaussian sampling process, which simultaneously enforces a bounded window\nand distance-dependent decay.\nAs a result, suffix attention only needs to focus on nearby tokens, making the suffix dropout window\ndecoupled from the overall sequence length.\nUnlike the vanilla setting, where the suffix grows with the\ngeneration sequence, our approach keeps it constant. This yields a clear computational benefit: suffix dropout\neffectively reduces suffix-related complexity by one dimension. Formally, let 𝐿denote the suffix length and 𝐵\nthe block size. In vanilla block-wise dLLMs, suffix computation requires 𝐿\n𝐵steps, each involving 𝑂(𝐿) suffix\ntoken operations, whereas suffix dropout restricts this to a constant number of tokens, independent of 𝐿.\nGaussian Sampling.\nInspired by the smooth decay patterns observed in Fig. 3, we adopt a Gaussian sampling\nstrategy to implement distance-aware token dropout. Formally, for a suffix token at distance 𝑑from the suffix\n6\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\n0\n100\n200\n300\n400\n500\n600\n700\nKey index (offset)\n0.000\n0.005\n0.010\n0.015\n0.020\nAttention value\nSuffix\nCurrent\nMean (all samples & heads)\nmin-max range\n200\n250\n0.000\n0.005\n0.010\nSuffix\nCurrent\nFig. 3: Analysis of the Suffix Drop strategy at the final layer (Layer 31), generated by the LLaDA-1.5 model [37] on\nGSM8K dataset [9] with a max length of 512. We collect attention weights from 100 samples across all heads, focusing\non the current block queries (𝐴[𝑐:𝑠, 𝑐-200:]). To align positions, we truncate 200 tokens before 𝑐as the prefix and align\nall current blocks at 𝑐. The plot shows the mean attention distribution over key indices (green curve), together with the\nmin–max range (shaded area). Current and suffix boundaries are marked, showing that attention on far suffix tokens\nrapidly decays, motivating our distance-decay dropout design.\nboundary, its retention probability 𝑃(𝑑) is defined by the right half of a standard normal distribution with\neffective window size 𝑊:\n𝑃(𝑑) = 𝑎·\n1\n𝜎\n√\n2𝜋\nexp\n\"\n−1\n2\n\u0012 𝑘𝜎\n𝑊·𝑑−𝜇\n𝜎\n\u00132#\n,\n0 < 𝑑≤𝑊,\n(10)\nwhere 𝜇= 0, 𝜎= 1, and the suffix boundary is the center of the Gaussian distribution. Two hyperparameters\ncontrol the distribution along the 𝑥- and 𝑦-axes: (1) 𝑘𝜎\n𝑊maps the window size 𝑊to 𝑘𝜎(e.g., for 𝑊= 256,\nsetting 𝑘= 3 ensures 𝑑= 256 corresponds to 3𝜎); (2) 𝑎scales the overall sampling magnitude vertically. This\nformulation ensures that tokens farther from the boundary are retained with exponentially decreasing probability,\nimplemented via rejection sampling.\nThis dropout strategy is not static but is dynamically re-applied in a block-wise way. As shown in Fig. 1 (c),\nafter a block is generated, all previously dropped-out tokens are restored. The Gaussian sampling is then invoked\nagain on the updated set of suffix tokens before the next block is decoded. This dynamic resampling is crucial\nfor preventing sampling bias and ensuring that no token is systematically ignored during the generation process.\nImplementation.\nOur method can be implemented in just a few lines of code:\n1\n# Select suffix token indices with Gaussian distance -decay sampling\n2\nkeep_idx = gaussian_suffix_dropout(x)\n3\n# Preserve only the selected tokens\n4\nx_pruned = x[keep_idx]\n5\n# Forward the reduced sequence to the model\n6\noutput = model(x_pruned , args)\nHere, the Gaussian-based sampler gaussian_suffix_dropout selects a subset of suffix tokens according\nto distance-aware probabilities. We then construct a pruned sequence x_pruned by indexing with keep_idx.\n7\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\n0\n100\n200\n300\n400\nKey index (offset to Suffix)\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\nAverage Attention Score\nd=362\nd=161\nd=199\nd=298\nd=236\nd=147\nd=399\nd=134\nd=320\nd=252\na. Original\nAvg across layers\nMin-Max range\nSpotlight tokens\n0\n100\n200\n300\n400\nKey index (offset to Suffix)\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\nAverage Attention Score\nd=362\nd=161\nd=199\nd=298\nd=236\nd=147\nd=399\nd=134\nd=320\nd=252\nb. After Pruning\nAvg across layers\nMin-Max range\nSpotlight tokens pruned\nFig. 4: (a) Average, maximum, and minimum attention scores of suffix tokens paid by current block tokens (𝐴[𝑐:𝑠,\n𝑠:]) across layers in LLaDA-1.5, showing overall decay with occasional spikes (e.g., d = 199, 298, 362). (b) After\nforcibly pruning these spike positions, attention shifts to nearby tokens, indicating that adjacent positions can absorb suffix\ninformation (e.g., pruning token 362 shifts the spike to token 359).\nThe rest of the model remains unchanged, except for a minor adjustment to the RoPE embeddings inside the\nattention module to ensure correct positional encoding under suffix dropout.\nBesides, we also introduce an early termination mechanism to prevent inefficient computation after a <eos>\ntoken is generated, a common issue in fixed generation length strategies like LLaDA [23]. Our method performs\na conditional check after each decoded block; if an <eos> token is found, we fill the remaining sequence with\n<eos> and immediately halt generation.\nRoPE Adjustment.\nOur suffix dropout mechanism requires only a minor adjustment to the Rotary Positional\nEmbedding (RoPE) [30] to maintain correct positional information. In standard RoPE, a token at absolute\nposition 𝑖is encoded using an angle 𝜃𝑖= 𝑖· Δ. After dropout, however, only a sparse, non-contiguous subset of\nsuffix indices I = {𝑖1, . . . , 𝑖𝑚} is preserved.\nTo handle this, we ensure that each preserved token retains its original positional information. Rather than\nusing their re-indexed positions after dropout, we apply a mapping function 𝑓(𝑖𝑘) that retrieves the original\nabsolute position of the 𝑘-th preserved token. The new angle is then computed as\n𝜃′\n𝑖𝑘= 𝑓(𝑖𝑘) · Δ.\n(11)\nAccordingly, the modified RoPE application becomes\nRoPE′(x𝑖𝑘, 𝑖𝑘) = RoPE(x𝑖𝑘, 𝑓(𝑖𝑘)).\n(12)\nThis adjustment requires only a lightweight remapping inside the attention module and does not alter the func-\ntional form of RoPE, confirming that suffix dropout is nearly cost-free while preserving positional consistency.\n3.3.\nDiffusion Lottery Tickets Hypothesis\nThe analysis in Fig. 3 not only reveals the overall decay of current-to-suffix attention, but also occasional sharp\nspikes in the maximum values. These spikes cannot be predicted in advance and, in principle, pruning them\ncould affect model accuracy. To investigate this, we conduct an additional experiment shown in Fig. 4. We\nfirst run dLLM inference for one step, then forcibly prune the top 10 highest-attention suffix tokens (“spotlight”\n8\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\ntokens) occurring beyond the first 128 positions (e.g., at distances 199, 298, and 362). Surprisingly, pruning\nsuch distant suffix tokens, even those corresponding to large spikes, has little effect on model accuracy. When\nthese spikes are removed, the model shifts its attention to nearby suffix tokens, for example, the spotlight token\nat 362 in Fig. 4 (a) is replaced by increased attention at its neighbor token 359 in Fig. 4 (b), which effectively\nabsorbs the lost information. This behavior is consistent with the strong generalization ability of diffusion\nmodels and the fact that suffix tokens are initialized without semantic content: through DPad mechanism, suffix\ntokens can dynamically learn and store information during inference. Consequently, information carried by\ndistant suffix tokens appears largely position-insensitive, and pruning spike positions has almost no impact on\nfinal accuracy, as further confirmed in our evaluation experiments.\nThis leads to an important intuition: information collected by distant suffix tokens is insensitive to their\nexact positions. When writing a book, one may use a scratchpad to record an event in the “middle chapters,” but\nit is not necessary to pin it down to precisely token 186; recording it at token 152 or 198 suffices. This explains\nwhy suffix dropout can be applied a priori, without computing exact attention scores, and why it fundamentally\ndiffers from prefix cache pruning [28, 32]: Prefix tokens carry dense, position-bound semantic information and\nthus cannot be arbitrarily discarded, whereas suffix tokens act as a flexible, low-rank memory buffer.\nThese observations resonate with the Lottery Ticket Hypothesis (LTH) [11], which posits that properly pruned\nsub-networks with their original initialization can match the performance of dense networks after training. We\nextend this intuition to dLLMs and propose the Diffusion Lottery Tickets (DLT) hypothesis: During inference,\nthe suffix region contains redundant tokens, yet a sparse subset is sufficient to preserve semantic consistency\nand generation quality. Through DPad mechanism, this subset can be adaptively reorganized into “winning\ntickets” within the forward pass. In this view, suffix dropout becomes a training-free lottery ticket search, where\nGaussian sampling selects a compact set of suffix tokens that carry the essential information for denoising in\ndLLMs.\nSummary.\nWe showed that suffix tokens function as a scratchpad, enabling a high-dimensional residual\npathway.\nThis insight motivates two lightweight strategies, Sliding Window and Distance-decay Dropout,\ntogether with the DLT hypothesis, which interprets suffix pruning as a training-free lottery ticket search. These\nmethods are simple, training-free, and easy to integrate. Next, we evaluate the efficiency and accuracy of\nDPad-enhanced dLLMs across standard benchmarks.\n4.\nExperiments\n4.1.\nExperimental Setup\nModels and Baselines.\nAll experiments are conducted on an NVIDIA A100 80GB GPU. We evaluate DPad\non a suite of representative open-source dLLMs: two variants of LLaDA (8B-Instruct and 1.5) [23, 37] and\nDream-v0-Base-7B [36]. We compare against the following baselines:\n• Vanilla: the unmodified LLaDA [23] and Dream [36] backbones, which compute attention over all tokens\nin the sequence, including prefix, current, and suffix tokens.\n• +Parallel (Fast-dLLM): a parallel decoding strategy applied to the vanilla backbone, where tokens are\nunmasked whenever their confidence exceeds a predefined threshold [34], instead of the fixed top-𝑘\nunmasking used in Vanilla.\n• +Prefix Cache: applying KV caching on prefix tokens, leveraging the observation that their key–value\nstates remain nearly static across inference steps [34], thereby avoiding redundant computations.\nWhen combining DPad with caching, we adopt a standard Prefix Cache.\nSince our dropout already\nminimizes the computational cost of the suffix, the Dual Cache mechanism proposed in [34] is unnecessary.\n9\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nTable 1: Performance of LLaDA-Instruct with DPad on four benchmarks.\nLLaDA-Instruct\nBenchmark\nMethod\nEfficiency\nAccuracy (%)\nLatency(s)↓\nTPS↑\nGen. Length\nFlexible↑\nStrict↑\nGSM8K\n4-shot\nVanilla\n27.48\n1.00×\n8.44\n1.00×\n232 / 256\n78.39\n37.38\n+DPad\n18.35\n1.50×\n8.76\n1.04×\n161 / 256\n78.54\n63.84\n+Parallel (Fast-dLLM)\n8.55\n3.21×\n27.14\n3.22×\n232 / 256\n78.54\n38.67\n+Parallel+DPad\n6.64\n4.14×\n24.25\n2.87×\n161 / 256\n79.76\n64.97\nMATH\n4-shot\nVanilla\n25.40\n1.00×\n9.79\n1.00×\n249 / 256\n33.58\n8.42\n+DPad\n21.61\n1.18×\n9.75\n1.00×\n211 / 256\n33.42\n28.04\n+Parallel (Fast-dLLM)\n9.91\n2.56×\n25.09\n2.56×\n249 / 256\n33.40\n8.76\n+Parallel+DPad\n9.20\n2.76×\n22.93\n2.34×\n211 / 256\n33.30\n27.98\nHumanEval\n0-shot\nVanilla\n34.67\n1.00×\n13.64\n1.00×\n473 / 512\n43.90\n–\n+DPad\n27.41\n1.26×\n15.96\n1.17×\n438 / 512\n47.56\n–\n+Parallel (Fast-dLLM)\n11.48\n3.02×\n41.40\n3.04×\n475 / 512\n43.29\n–\n+Parallel+DPad\n9.14\n3.79×\n47.86\n3.51×\n438 / 512\n46.34\n–\nMBPP\n3-shot\nVanilla\n62.11\n1.00×\n4.82\n1.00×\n299 / 512\n15.00\n–\n+DPad\n15.89\n3.91×\n6.85\n1.42×\n109 / 512\n40.40\n–\n+Parallel (Fast-dLLM)\n14.26\n4.36×\n20.99\n4.36×\n299 / 512\n15.00\n–\n+Parallel+DPad\n6.02\n10.32×\n18.28\n3.79×\n110 / 512\n39.40\n–\nThe DPad strategy introduces three hyperparameters: a decay rate factor 𝑘, a magnitude scalar 𝑎, and a\nsliding window size. We tuned these on small subsets of each benchmark (see Sec. 4.3.2). Unless otherwise\nspecified, we use a block size of 32 and set the confidence threshold for parallel decoding to 0.9.\nBenchmarks and Metrics.\nOur evaluation spans multiple domains. For reasoning, we use GSM8K [9] and\nMATH [15]; for code generation, we use HumanEval [8] and MBPP [4]. We report two aspects of performance:\nWe report two aspects of performance:\n• Accuracy: evaluated with task-specific metrics, including pass@1 for code generation and accuracy\nmeasures for reasoning (e.g., flexible-extract and strict-match accuracy on GSM8K). All accuracies are\nreported as percentages.\n• Efficiency: measured by average inference latency per sample and Tokens Per Second (TPS). TPS is\ncomputed over the entire generated sequence until an <eos> token is produced. We also use 𝑎/ 𝑏to\ndenote the generation (Gen.) length, where 𝑎is the average real generation length and 𝑏is the benchmark\nsetting length.\n4.2.\nMain Results\nWe evaluate the efficiency and accuracy of DPad on three models and four benchmarks, as reported in Tbl. 1,\nTbl. 2, and Tbl. 3. We compare against both the vanilla LLaDA baseline [23] and the parallel decoding baseline\nFast-dLLM [34]. Our results reveal three consistent trends:\n• Latency: consistently improved across all settings.\n• Throughput (TPS): shows higher variance, with fluctuations across benchmarks and models.\n• Accuracy: Flexible Match remains comparable, while Strict Match achieves significant gains.\nWe analyze each of these aspects in detail below.\n10\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nTable 2: Performance of LLaDA-1.5 with DPad on four benchmarks.\nLLaDA-1.5\nBenchmark\nMethod\nEfficiency\nAccuracy (%)\nLatency(s)↓\nTPS↑\nGen. Length\nFlexible↑\nStrict↑\nGSM8K\n4-shot\nVanilla\n27.61\n1.00×\n7.77\n1.00×\n215 / 256\n80.59\n61.87\n+DPad\n18.26\n1.51×\n8.56\n1.10×\n156 / 256\n80.14\n78.47\n+Parallel (Fast-dLLM)\n8.06\n3.42×\n26.61\n3.43×\n215 / 256\n80.82\n62.62\n+Parallel+DPad\n6.23\n4.43×\n25.23\n3.25×\n157 / 256\n80.89\n78.92\nMATH\n4-shot\nVanilla\n25.12\n1.00×\n8.67\n1.00×\n218 / 256\n33.52\n32.72\n+DPad\n20.63\n1.22×\n9.48\n1.09×\n196 / 256\n34.08\n37.00\n+Parallel (Fast-dLLM)\n9.48\n2.65×\n22.96\n2.65×\n218 / 256\n33.60\n32.92\n+Parallel+DPad\n8.57\n2.93×\n22.76\n2.63×\n195 / 512\n32.92\n35.96\nHumanEval\n0-shot\nVanilla\n34.80\n1.00×\n3.16\n1.00×\n110 / 512\n40.85\n–\n+DPad\n11.55\n3.01×\n7.19\n2.28×\n83 / 512\n44.51\n–\n+Parallel (Fast-dLLM)\n11.16\n3.12×\n9.80\n3.10×\n109 / 512\n39.63\n–\n+Parallel+DPad\n5.26\n6.61×\n15.64\n4.95×\n82 / 512\n39.63\n–\nMBPP\n3-shot\nVanilla\n62.34\n1.00×\n1.02\n1.00×\n63 / 512\n38.20\n–\n+DPad\n14.95\n4.17×\n4.33\n4.26×\n65 / 512\n39.80\n–\n+Parallel (Fast-dLLM)\n5.47\n11.39×\n11.62\n11.44×\n64 / 512\n38.60\n–\n+Parallel+DPad\n4.41\n14.14×\n14.83\n14.60×\n65 / 512\n41.60\n–\nTable 3: Performance of Dream-Base with DPad on four benchmarks.\nDream-Base\nBenchmark\nMethod\nEfficiency\nAccuracy (%)\nLatency(s)↓\nTPS↑\nGen. Length\nFlexible↑\nStrict↑\nGSM8K\n4-shot\nVanilla\n22.30\n1.00×\n11.43\n1.00×\n255 / 256\n75.06\n74.37\n+DPad\n10.27\n2.17×\n12.75\n1.11×\n131 / 256\n75.28\n75.06\n+Parallel (Fast-dLLM)\n13.84\n1.61×\n18.43\n1.61×\n255 / 256\n75.51\n74.83\n+Parallel+DPad\n5.24\n4.25×\n24.17\n2.11×\n127 / 256\n74.83\n74.75\nMATH\n4-shot\nVanilla\n21.01\n1.00×\n12.19\n1.00×\n256 / 256\n34.06\n37.76\n+DPad\n16.64\n1.26×\n15.33\n1.26×\n255 / 256\n34.14\n37.64\n+Parallel (Fast-dLLM)\n8.82\n2.38×\n29.03\n2.38×\n256 / 256\n35.12\n38.62\n+Parallel+DPad\n7.72\n2.72×\n33.04\n2.71×\n255 / 256\n34.44\n38.32\nHumanEval\n0-shot\nVanilla\n28.49\n1.00×\n17.93\n1.00×\n511 / 512\n51.22\n–\n+DPad\n8.20\n3.47×\n26.83\n1.50×\n220 / 512\n51.22\n–\n+Parallel (Fast-dLLM)\n14.15\n2.01×\n36.11\n2.01×\n511 / 512\n53.05\n–\n+Parallel+DPad\n4.06\n7.01×\n52.62\n2.93×\n214 / 512\n52.44\n–\nMBPP\n3-shot\nVanilla\n49.15\n1.00×\n10.42\n1.00×\n512 / 512\n52.40\n–\n+DPad\n41.36\n1.19×\n12.38\n1.19×\n512 / 512\n52.60\n–\n+Parallel (Fast-dLLM)\n12.38\n3.97×\n41.36\n3.97×\n512 / 512\n55.40\n–\n+Parallel+DPad\n9.86\n4.98×\n51.92\n4.98×\n512 / 512\n54.80\n–\nLatency.\nThe latency improvements of DPad arise from three sources. (1) Reduced suffix complexity: in\nstandard dLLMs, handling suffix tokens incurs quadratic complexity with respect to suffix length, whereas DPad\nreduces this to linear complexity via suffix dropout. (2) Early termination: by decoupling generation from a\n11\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\n============== 4 shots ===============\nQuestion: ……\n#### 12\n……\nQuestion: ……\n#### 200\n=====================================\nQuestion: Janet’s ducks lay 16 eggs per day. She\neats three for breakfast every morning and bakes\nmuffins for her friends every day with four. She\nsells the remainder at the farmers' market daily\nfor $2 per fresh duck egg. How much in dollars\ndoes she make every day at the farmers' market?\nFirst, let's determine how many eggs Janet uses each day:\n- She eats 3 eggs for breakfast.\n- She bakes 4 eggs for her friends.\nTotal eggs used per day = 3 + 4 = 7 eggs.\nNext, let's find out how many eggs are left to sell:\n- The ducks lay 16 eggs per day.\n- Janet uses 7 eggs per day.\nEggs left to sell per day = 16 - 7 = 9 eggs.\nNow, let's calculate how much Janet makes from selling\nthe eggs:\n- She sells each egg for $2.\nTotal earnings per day = 9 eggs * $2/egg = $18.\nTherefore, Janet makes $18 every day at the farmers'\nmarket.\n#### 18\n(173 tokens)\nQuestion\n+DPad\nTo determine how much Janet makes every day at the\nfarmers' market, we need to follow these steps:\n1. Calculate the total number of eggs laid by Janet's\nducks per day.\n2. Subtract the number of eggs she eats for breakfast and\nbakes for her friends.\n3. Multiply the remaining eggs by the price per egg to find\nher daily earnings.\n1. Janet's ducks lay 16 eggs per day.\n2. She eats 3 eggs for breakfast and bakes 4 eggs for her\nfriends. Therefore, she uses a total of 3 + 4 = 7 eggs per\nday.\n3. The number of eggs left to sell is 16 - 7 = 9 eggs.\\n4. She\nsells each egg for $2, so her daily earnings are 9 eggs * $2/\negg = $18.\nTherefore, Janet makes $18 every day at the farmers'\nmarket.\n(195 tokens)\nFlexible-Match\nStrict-Match\nFlexible-Match\nStrict-Match\nLLaDA-Instruct/GSM8K\n(256, 4-shot)\nBaseline\nFig. 5: A Case Study from GSM8K on In-Context Learning and Format Adherence. The figure contrasts a baseline model’s\noutput with the same model enhanced by DPad. The baseline produces the correct answer (passing Flexible-Match) but\nfails to replicate the structured reasoning from the prompt, thus failing the Strict-Match. DPad successfully generates both\nthe correct answer and the required format, passing both evaluations.\nfixed sequence length through our sliding window design, DPad naturally supports early stopping once the\nend-of-sequence is reached. (3) More concise generations: suffix dropout removes low-entropy tokens, leading\nthe model to produce more compact outputs and slightly shorter generations. Importantly, as analyzed in the\nAccuracy section, this does not degrade task accuracy. Across most benchmarks, the reduction in generation\nlength is modest (e.g., on LLaDA-1.5, about 10% on MATH and 27% on GSM8K), while datasets such as\nMBPP remain almost unchanged. Thus, the majority of latency gains come from reduced suffix computation\nrather than shorter outputs.\nOverall, DPad consistently reduces latency across all benchmarks in the three-model dLLM suite, achieving\n1.18× to 4.17× speedups over the vanilla backbone. When combined with parallel decoding, DPad yields an ad-\nditional 1.08× to 3.48× speedup over parallel decoding alone, resulting in an overall speedup of 2.76× to 14.14×\nover vanilla. This complementarity arises because the two methods target orthogonal bottlenecks: DPad elimi-\nnates redundant KV-token computation via suffix dropout, while parallel decoding in Fast-dLLM [34] mitigates\ndependency constraints by selectively decoding only high-confidence tokens in parallel. By combining these\napproaches, we exploit both finer-grained token pruning and safe multi-token prediction, yielding substantial\nefficiency gains.\nThat said, the advantage of suffix dropout is less pronounced on short-sequence benchmarks. This is due to\ntwo factors. First, our Gaussian-based dropout typically yields about 62.5% sparsity within the sliding window,\nso the constant overhead remains significant. It is also impractical to shrink the window much further when\nthe maximum generation length itself is short (e.g., 256). Second, when the prompt dominates the sequence\n(e.g., ∼80% prompt vs. 20% suffix in GSM8K and MATH), suffix attention accounts for only a small fraction\nof total computation.\nAccording to Amdahl’s law [1], the maximum achievable speedup in such cases is\ninherently bounded (often around 1.1×). Nevertheless, even without any system-level optimizations, our simple\nimplementation of suffix dropout consistently delivers stable latency improvements. Its true potential emerges in\nlonger-sequence settings in Fig. 6 and Fig. 7, where the suffix fraction grows; as we show later with 1024-token\nsequences, DPad achieves substantial additional reductions in latency.\n12\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nLLaDA\n+Eary Termination\n+ DPad\nParallel-Only\n+ DPad\nParallel + PrefixCache\n+ DPad\nFlexible Match (FM, %): 78.17, Strict Match (SM, %): 48.98\nFM: 78.17, SM: 48.98\nFM: 78.77, SM: 74.07\nFM: 78.77, SM: 49.43\nFM: 79.38, SM: 74.22\nFM: 78.77, SM: 51.63\nFM: 77.10, SM: 70.66\n127s, 1.55TPS\n26.4s, 7.47TPS\n6.28s, 18.4TPS\n11.7s, 16.9TPS\n2.26s, 51.4TPS\n10.8s, 18.2TPS\n2.07s, 55.5TPS\n1.0x\n4.8x\n20.3x\n1.0x\n5.2x\n1.0x\n5.2x\n61.39x\nFig. 6: Latency comparison on LLaDA-1.5 with GSM8K (1024 tokens, 1-shot).\nThroughput.\nWe observe a subtle distinction between latency- and TPS-based efficiency metrics. While\nDPad consistently reduces per-sample latency, its TPS gains may appear less pronounced. This is because DPad\noften encourages the model to generate more concise and complete responses, reaching the end-of-sequence\nearlier and producing fewer redundant tokens. As illustrated in Fig. 5 and Fig. 10, this behavior reflects not a\nlimitation but an improvement: the model terminates naturally rather than exhausting its context with low-quality\ncontinuations. We view this as an additional quality benefit of DPad, complementing its efficiency gains.\nHowever, when the generation length is reduced more substantially, TPS can drop. This mainly occurs\nbecause we apply no additional system- or algorithm-level optimizations: shorter generations lead to lower\nGPU utilization, which mechanically reduces TPS. Yet, the extra tokens produced by baselines often carry\nlow semantic information; higher TPS simply reflects more tokens, not greater efficiency. For example, if we\nwere to artificially pad outputs with meaningless characters, TPS would increase, but this would not represent\na genuine efficiency gain. Therefore, in our main results, we report raw throughput numbers without post-\nprocessing. This also highlights a broader point: the community may need to reconsider throughput metrics and\ndevelop alternatives that better balance sequence length and accuracy, rewarding models that achieve comparable\naccuracy with shorter generations.\nAccuracy.\nIn addition to improving inference efficiency, DPad also enhances accuracy across nearly all tasks\nfor the LLaDA-Instruct and LLaDA-1.5 models (Table 1), thereby defying the typical trade-off between speed\nand accuracy. For instance, DPad yields substantial gains in strict-match accuracy on GSM8K (+26.46%) and\nMATH (+19.62%) for LLaDA-Instruct. By contrast, the Dream-Base model shows no consistent advantage,\nwith accuracy broadly comparable to the baseline and fluctuating only within a narrow margin. We attribute this\nstability to differences in training protocols, particularly the absence of instruction tuning such as supervised\nfine-tuning (SFT).\nThe improvement in strict-match score is particularly noteworthy, as it highlights DPad’s ability to enhance\nin-context learning. The vanilla backbone typically exhibits low strict-match performance (e.g., only 37.38%\non GSM8K for LLaDA-Instruct), since this metric requires the model not only to produce the correct final\nanswer (Flexible-Match) but also to adhere to the specific reasoning format demonstrated in few-shot exemplars,\nas illustrated in Figure 5. We posit that failures in strict matching often stem from interference by distant\nsuffix tokens, which introduce low-value or off-format patterns that distract the model and encourage verbose\nor poorly structured outputs. By reducing the influence of such suffix tokens and directing attention toward\nhigh-value, information-rich prefix exemplars, DPad enables the model to replicate the structured reasoning\nformats required by strict matching more faithfully. In addition, by suppressing redundant generations, DPad\nfacilitates earlier convergence to concise, well-formatted outputs, further improving strict-match performance\nwithout altering model parameters.\n13\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nDream\n+ DPad\nParallel-Only\n+ DPad\nParallel + PrefixCache\n+ DPad\nAcc (%): 51.22\n94.4s\n10.3s\n31.8s\n3.9s\n26.9s\n3.1s\n1.0x\n9.1x\n1.0x\n8.2x\n1.0x\n8.7x\n30.58x\nAcc: 49.39\nAcc: 51.22\nAcc: 53.05\nAcc: 55.49\nAcc: 54.27\nDream\n+ DPad\nParallel-Only\n+ DPad\nParallel + PrefixCache\n+ DPad\nAcc (%): 56.71\n342.2s\n20.0s\n73.7s\n4.7s\n71.5s\n3.5s\n1.0x\n17.1x\n1.0x\n15.8x\n1.0x\n20.3x\n97.32x\nAcc: 49.39\nAcc: 53.66\nAcc: 52.44\nAcc: 57.93\nAcc: 54.88\nGen. Length: 1024\nGen. Length: 2048\nFig. 7: Latency comparison on Dream-Base with HumanEval (1024 and 2048 tokens, 1-shot).\n4.3.\nAblations and Analysis\n4.3.1.\nMaximum Generation Length\nTo better quantify the efficiency gains of different acceleration strategies in long-sequence generation, we analyze\nspeedups under various configurations, as shown in Fig. 6 and Fig. 7. Specifically, we evaluate GSM8K with\nLLaDA-1.5 and HumanEval with Dream under the following strategies: Vanilla, Vanilla+ET (augmented\nwith an early-termination mechanism), +Pad (our method alone), and +Parallel (Fast-dLLM). We then con-\nsider combined strategies to assess complementarity, including +Pad+Parallel, +Parallel+PrefixCache,\nand +Parallel+PrefixCache+Pad. This setup allows us to disentangle the contributions of suffix dropout,\nparallel decoding, and prefix caching, as well as their interactions.\nSpeedup.\nWe find that the acceleration benefits of DPad grow substantially with sequence length. For LLaDA-\n1.5 on GSM8K, improvements are modest at shorter sequence lengths (up to 1.51× under a 256-token limit).\nHowever, when the maximum length is extended to 1024 tokens (single-shot setting), standalone DPad achieves\na dramatic 20.3× speedup. This effect arises because vanilla LLaDA typically produces concise answers (about\n200 tokens) but continues generating redundant <eos> tokens to fill the context window, incurring wasted\ncomputation. To provide a stronger baseline, we augment vanilla with early termination; even under this setting,\nDPad still delivers a 4.8× improvement. Finally, when combined with parallel decoding and prefix caching\n(Fast-dLLM), the efficiency gains compound, yielding an overall 61.39× speedup compared to vanilla LLaDA\nand a 8.7× improvement over Fast-dLLM.\nA similar scaling trend is observed with Dream-Base on HumanEval. Here, DPad alone accelerates inference\nby 9.13× for 1024-token sequences and 17.1× for 2048-token sequences. When combined with Fast-dLLM, the\nbenefits become multiplicative: 30.58× at 1024 tokens and 97.32× at 2048 tokens. These results demonstrate\nthat suffix dropout and parallel decoding address orthogonal bottlenecks and, when combined, yield near two\norders of magnitude improvement in long-sequence generation. Overall, the strong scaling with sequence length\nhighlights DPad as a key component for enabling dLLMs to match the scalability of traditional autoregressive\nmodels.\n14\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\n32\n64\n128\n192\n256\n320\n384\n448\n512\n# of Preserved Suffix Tokens\n32\n64\n128\n192\n256\n320\n384\n448\n512\nSliding Window Size\n70.51\n75.28\n77.18\n77.48\n80.14\n80.59\n73.92\n81.88\n81.65\n81.35\n65.58\n79.00\n81.73\n81.58\n81.12\n51.40\n76.88\n81.35\n81.73\n81.05\n79.91\n43.21\n71.49\n81.65\n81.73\n80.21\n80.36\n78.32\n34.19\n62.85\n80.74\n81.50\n80.67\n81.96\n79.53\n79.68\n33.74\n63.91\n79.53\n81.80\n82.26\n81.50\n80.14\n79.98\n79.68\n(a).Uniform Dropout\nFlexible-Match Scores\n32\n64\n128\n192\n256\n320\n384\n448\n512\n# of Preserved Suffix Tokens\n32\n64\n128\n192\n256\n320\n384\n448\n512\nSliding Window Size\n70.51\n73.09\n77.18\n78.54\n79.53\n80.59\n78.85\n80.89\n80.59\n81.35\n78.85\n81.73\n82.03\n81.12\n81.12\n80.06\n81.20\n82.11\n81.35\n79.23\n79.91\n78.62\n80.97\n81.20\n81.50\n81.50\n81.05\n78.32\n75.51\n80.44\n81.27\n81.12\n81.58\n81.50\n79.98\n79.68\n69.67\n81.35\n83.47\n82.56\n81.65\n80.52\n80.29\n78.32\n79.68\n(b).Gaussian Dropout\nFlexible-Match Scores\n32\n64\n128\n192\n256\n320\n384\n448\n512\n# of Preserved Suffix Tokens\n32\n64\n128\n192\n256\n320\n384\n448\n512\nSliding Window Size\n60.05\n70.66\n72.48\n76.04\n78.77\n77.03\n71.27\n80.06\n75.13\n69.14\n62.24\n77.18\n72.48\n66.26\n60.20\n46.78\n75.44\n75.59\n67.78\n61.87\n56.71\n38.29\n69.07\n79.00\n73.31\n67.55\n63.91\n61.26\n29.72\n59.44\n78.62\n77.71\n73.16\n71.27\n66.87\n67.25\n29.42\n59.29\n77.71\n78.47\n76.42\n73.09\n70.66\n69.90\n68.69\n(c).Uniform Dropout\nStrict-Match Scores\n32\n64\n128\n192\n256\n320\n384\n448\n512\n# of Preserved Suffix Tokens\n32\n64\n128\n192\n256\n320\n384\n448\n512\nSliding Window Size\n60.05\n65.96\n72.48\n75.13\n77.86\n77.03\n76.80\n79.61\n76.12\n69.14\n77.63\n80.74\n78.62\n68.54\n60.20\n78.32\n80.14\n77.10\n70.66\n60.58\n56.71\n77.48\n80.14\n78.01\n74.53\n69.75\n64.29\n61.26\n73.31\n79.68\n77.79\n75.66\n72.10\n70.96\n67.63\n67.25\n66.19\n79.61\n81.20\n77.86\n73.62\n71.11\n68.84\n67.85\n68.69\n(d).Gaussian Dropout\nStrict-Match Scores\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n30\n40\n50\n60\n70\n80\n30\n40\n50\n60\n70\n80\nFig. 8: Ablation Study on Sliding Window Size and Dropout Function for DPad on LLaDA-1.5/GSM8K (512, 4-shot).\nHeatmaps showing Flexible-Match Accuracy scores with (a) uniform and (b) Gaussian dropout, and Strict-Match Accuracy\nscores with (c) uniform and (d) Gaussian dropout, across varying sliding window sizes and number of preserved suffix\ntokens. The (512, 512) configuration corresponds to the baseline, as it involves no token dropout.\nAccuracy.\nLonger, low-shot settings further highlight DPad’s ability to preserve and even enhance model\naccuracy. In the 1-shot LLaDA setting, the strict-match score of the baseline drops significantly (from ∼60%\nto ∼50%), whereas DPad’s performance remains remarkably stable (dropping only from ∼78% to ∼74%).\nThis resilience demonstrates that DPad substantially strengthens the model’s in-context learning capability, a\nsignificant achievement for a training-free method.\nBy contrast, on the 2048-token HumanEval task, we observed a 7.32% accuracy degradation when applying\nDPad to the vanilla Dream model. This degradation is largely mitigated when DPad is combined with Fast-\ndLLM. We hypothesize that this isolated performance drop arises from complex interactions between our\ntraining-free pruning strategy and the model’s native Top-𝑘sampling behavior, pointing to an interesting\ndirection for future investigation.\n4.3.2.\nSliding Window Size and Dropout Function\nWe conducted an ablation study on LLaDA-1.5/GSM8K to determine the optimal sliding window size and\ndropout function for DPad. As shown in Figure 8, we evaluated both flexible-match and strict-match scores\n15\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\nwhile varying the window size, the number of preserved suffix tokens, and the dropout function (Uniform vs.\nGaussian).\nOur analysis reveals two key findings that point to the existence of a critical context window of approximately\n64–128 tokens immediately following the current block.\nFirst, within this critical window, performance\nconsistently improves as more suffix tokens are preserved. For windows extending beyond this zone, however,\nperformance exhibits a concave trend, peaking at a density of around 50%. Second, when the token budget is\nlimited (e.g., fewer than 128 preserved tokens), spreading this budget thinly across a larger window significantly\ndegrades accuracy.\nThese observations lead to a clear principle for our method: the token budget should be prioritized to\nmaintain a high density of preserved tokens within the critical 64–128 token window, as expanding the window\nto more distant tokens with a limited budget can be counterproductive.\nAs further evidence, Fig. 8 validates our choice of a truncated Gaussian function for token dropout. Com-\npared against a uniform random baseline, Gaussian dropout performs comparably under large token budgets\n(≥192), but its advantage becomes increasingly clear under more stringent, low-budget conditions. In partic-\nular, when the sliding window is large, Gaussian dropout consistently outperforms the uniform strategy,\nachieving superior or equivalent performance with fewer preserved tokens.\nThis empirical result aligns with the consistent decaying patterns observed in RoPE [30], attention scores\n(Fig. 4), and token confidence maps [12], which collectively motivate our design. By biasing dropout toward\nnearby tokens, the Gaussian approach allocates the token budget more efficiently and thus delivers better perfor-\nmance under limited computation. Of course, Gaussian sampling may not be the optimal decay function, and\nother decay-based schemes (e.g., exponential, linear, or step-wise cutoff) remain to be explored. Nevertheless,\nin the training-free setting, we find that results are largely insensitive to the exact decay form, as long as the\nscheme emphasizes nearer tokens.\n4.3.3.\nThe Choice of Gaussian Hyperparameters\nInspired by findings that dLLMs exhibit different behaviors on mathematical and code-generation tasks [12],\nwe tune the hyperparameters for our Gaussian Sampler separately for each domain. We perform a grid search\nover two key parameters: the decay rate, 𝑘, and a scale factor, 𝑎. The parameter 𝑎is used to control the\noverall retention density, which we define as the expected proportion of suffix tokens preserved by the Gaussian\nSampler inside the sliding window.\nThe results of this search for LLaDA-Instruct are presented in Figure 9, conducted on 50-sample subsets\nof GSM8K and HumanEval. While using a small subset for tuning may introduce some variance, the findings\nprovide clear directional insights. Our method with Gaussian Dropout consistently outperforms the baseline in\nnearly all configurations, with a single, minor exception for the GSM8K flexible-match accuracy score at 𝑘= 1\nand a density of 25.0%. The suboptimal performance at 𝑘= 1 is expected, as at this value the Gaussian curve\nis relatively flat across the sampling window (see Appendix A.1), causing the sampling to degenerate into a\nnear-uniform distribution.\nFocusing on the more effective range of 𝑘∈[2, 4], we identify distinct optimal settings for each domain.\nFor the mathematical reasoning task (GSM8K), a density of 25.0% provides the best balance of accuracy and\nefficiency. Although some settings with 50.0% density achieve a slightly higher strict-match score, they do so\nby preserving twice as many suffix tokens, which significantly undermines acceleration. We therefore select\n𝑘= 4.0 and a density of 25.0% (from 𝑎= 2.0) as the optimal configuration. For code generation (HumanEval),\na configuration of 𝑘= 3.0 and a density of 37.5% (from 𝑎= 2.3) yields the best performance.\nBased on these findings, we adopt these hyperparameters for all subsequent math and code benchmarks for\nLLaDA-Instruct (see Appendix A.1), with similar tuning for other models.\n16\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\n40\n50\n60\n70\nAccuracy Score\nk\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nDensity\n25.0%\n37.5%\n50.0%\nPerformance across k and Density\nMetric\nGSM8K(Flexible)\nGSM8K(Strict)\nHumaneval\nBaseline\nChosen\nFig. 9: Ablation study on hyperparameters 𝑘and 𝑎for LLaDA-Instruct on subsets of GSM8K and HumanEval. The\nparameter 𝑎is mapped to the retention density shown on the x-axis. Each dashed line represents the baseline performance\nfor the solid line of the same color and metric.\n5.\nDiscussion\n5.1.\nBeyond a Training-free Method: Suffix Dropout with SFT\nWhile DPad demonstrates improved accuracy and computational efficiency across diverse benchmarks by\nstrategically pruning the suffix space, we observe a performance degradation in very long-sequence generation,\nparticularly at a context length of 2048 tokens. We attribute this behavior to a distributional shift introduced\nby suffix dropout. During pre-training, the model is optimized to predict token distributions conditioned on a\nfull, continuous sequence of masked suffix tokens. In contrast, our distance-decay dropout forces the model at\ninference time to predict based on a much smaller, discontinuous set of masked suffix tokens. While moderate\ndropout helps the model focus on prompt semantics, such a large shift in the conditional distribution can hinder\ngeneralization, leading to degraded performance.\nThis training–inference distribution gap may be mitigated with supervised finetuning (SFT). In particular,\nthe attention lottery hypothesis offers a useful perspective that can be incorporated into training. We revise the\nlearning objective as:\nLDPad(𝜃) = −E𝑥0,𝑡,𝑀\n\"∑︁\n𝑖∈𝐶\nlog 𝑝𝜃\n\u0010\n𝑥0𝑖| 𝑥𝑡(𝐼𝑅∪𝐼𝑃∪𝑀)\u0011#\n,\n(13)\nwhere 𝑀denotes a subset of masked suffix tokens sampled via distance-decay dropout at each training step,\n𝐼𝑅represents the prompt tokens, 𝐼𝑝represents the generated prefix tokens,𝜃represents the model parameters,\n𝑡∈[0, 1] is degree of masking of samples from the forward masking process (Sec. 2.1), and the loss is\ncomputed over the current block tokens 𝐶. Using stochastic dropout masks rather than a fixed deterministic\npattern enhances robustness, as the model does not overfit to any single dropout scheme. This revised objective\nexplicitly integrates the notion of an attention lottery into training, encouraging the model to avoid wasting\ncapacity by writing redundant information into distant suffix tokens that are likely to be pruned at inference.\nLooking further ahead, one could even incorporate distance-decay dropout directly into the pre-training\nphase, allowing the model to learn sparsity from scratch. Such pre-training with sparse suffix attention would\nnaturally align training and inference conditions, and may yield even stronger efficiency–accuracy trade-offs.\n17\n\nDPad: Efficient Diffusion Language Models with Suffix Dropout\n5.2.\nComparison to Semi-Autoregressive Diffusion and Block Diffusion\nBlock Diffusion models [2] operate autoregressively at the block level, predicting each block conditioned only\non its predecessors. As a result, their attention mechanism excludes access to subsequent blocks, in contrast to\nthe directional attention employed by semi-autoregressive models.\nWhile this design is computationally efficient, its strictly forward-looking nature introduces two major\nlimitations. First, it is vulnerable to the reversal curse [6], since it cannot capture long-range (beyond a single\nblock) bidirectional dependencies that are crucial for tasks such as code generation, which often require iterative\nback-and-forth refinement [12]. Second, by discarding the suffix, it forfeits the ability to use suffix tokens as a\nscratchpad (see Sec. 3.1), thereby losing an important medium for contextual organization.\nOur proposed distance-decay dropout interpolates between these paradigms. It retains suffix tokens to\nfunction as a scratchpad, while at the same time preserving the model’s bidirectional learning ability, allowing\ninformation to be both written into and retrieved from the suffix scratchpad as needed.\n6.\nConclusion\nWe addressed a key bottleneck in dLLMs: the high cost of full suffix attention, where most distant tokens are\nredundant and add little value. To overcome this, we introduced the Diffusion Scratchpad (DPad), a simple\ntraining-free inference strategy that redefines suffix attention. DPad combines a fixed-length sliding window,\nreducing complexity from quadratic to linear, with a distance-decay dropout that prunes low-entropy suffix tokens\nbefore computation. This design leverages the inherent sparsity of suffix attention, constructing an efficient\n“winning ticket” for generation on the fly. Experiments show that DPad consistently improves efficiency without\nsacrificing accuracy, achieving up to 61.4× speedups when combined with existing optimizations. These results\nhighlight DPad as a practical and scalable solution for long-sequence generation, paving the way for the dLLM\nto move from a promising alternative to a viable foundation for future applications."}
{"id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "authors": ["Xian Gao", "Jiacheng Ruan", "Zongyun Zhang", "Jingsheng Gao", "Ting Liu", "Yuzhuo Fu"], "source": "http://arxiv.org/abs/2508.14146v1", "page_count": 19, "text": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\nPeer Review Automation\nXian Gao1, Jiacheng Ruan1, Zongyun Zhang1, Jingsheng Gao1, Ting Liu1, Yuzhuo Fu1\n1Shanghai Jiao Tong University\nAbstract\nWith the rapid growth of academic publications, peer review\nhas become an essential yet time-consuming responsibil-\nity within the research community. Large Language Models\n(LLMs) have increasingly been adopted to assist in the gener-\nation of review comments; however, current LLM-based re-\nview tasks lack a unified evaluation benchmark to rigorously\nassess the models’ ability to produce comprehensive, accu-\nrate, and human-aligned assessments—particularly in scenar-\nios involving multimodal content such as figures and tables.\nTo address this gap, we propose MMReview, a comprehen-\nsive benchmark that spans multiple disciplines and modal-\nities. MMReview includes multimodal content and expert-\nwritten review comments for 240 papers across 17 research\ndomains within four major academic disciplines: Artificial\nIntelligence, Natural Sciences, Engineering Sciences, and So-\ncial Sciences. We design a total of 13 tasks grouped into\nfour core categories, aimed at evaluating the performance\nof LLMs and Multimodal LLMs (MLLMs) in step-wise re-\nview generation, outcome formulation, alignment with hu-\nman preferences, and robustness to adversarial input manipu-\nlation. Extensive experiments conducted on 16 open-source\nmodels and 5 advanced closed-source models demonstrate\nthe thoroughness of the benchmark. We envision MMReview\nas a critical step toward establishing a standardized founda-\ntion for the development of automated peer review systems.\nIntroduction\nPeer review is essential to scholarly publishing, ensuring re-\nsearch quality and enhancing academic writing. However,\nthe growing volume of submissions has strained the tradi-\ntional review process, leading to inefficiencies and limited\nreviewer availability (Kim, Lee, and Lee 2025), which re-\nstricts feedback and delays review outcomes. Advances in\nLLMs have made automated peer review increasingly vi-\nable, as these models show strong reasoning abilities and can\noffer constructive feedback on academic manuscripts (Liu\nand Shah 2023; Zhao et al. 2024; Zhuang et al. 2025), par-\ntially alleviating reviewer burden. Yet, current evaluations\nof LLM-generated reviews focus mainly on final outputs,\nlacking in-depth analysis of the reasoning processes behind\nmodel judgments. Additionally, most studies concentrate on\nAI papers with publicly available text, overlooking the mul-\ntimodal nature of academic papers—such as figures and ta-\nbles—and the evaluation of LLMs in reviewing research\nacross broader scientific domains.\nTo address the aforementioned challenges, we propose\nMMReview, a comprehensive benchmark for peer review\ngeneration that spans multiple disciplines and modalities.\nMMReview incorporates three distinct types of input modal-\nities: textual content from manuscripts, figures and tables\nembedded within the papers, and rendered PDF pages con-\nverted into images. These data span 17 research domains\nacross 4 disciplinary categories. To obtain high-quality peer\nreview samples for evaluation purposes, we developed a\nmulti-model collaborative pipeline for data filtering and gen-\neration. Specifically, we first curated a total of 51,881 pa-\npers with associated reviews; then, we filtered the collected\nseed dataset D to obtain high-quality papers while maintain-\ning a relatively balanced distribution; subsequently, we ex-\ntracted reference answers from human reviews. Finally, we\nconducted manual verification to correct errors, resulting in\na curated set of 240 samples that serve as the foundation for\ntask construction. Building upon these samples, we intro-\nduce 4 thematic categories encompassing 13 diverse tasks,\neach designed to thoroughly assess the capabilities of LLMs\nin step-wise review generation, outcome formulation, align-\nment with human preferences, and robustness to adversar-\nial input manipulation. We conduct comprehensive experi-\nments on 16 open-source models as well as 5 state-of-the-\nart closed-source models, including GPT-4o and Claude-4-\nSonnet, across 13 tasks. The results highlight the compre-\nhensive nature of the MMReview benchmark and uncover\nseveral key findings, offering insights for future research on\nLLM-based automated academic peer review.\nThe primary contributions of this paper can be summa-\nrized as follows:\n• We introduce MMReview, the first comprehensive eval-\nuation benchmark for automated academic peer review\nusing LLMs, spanning multiple disciplines and modali-\nties. Built upon our data filtering and generation pipeline,\nMMReview comprises 240 high-quality samples across\n17 academic fields in 4 disciplines.\n• We meticulously design 13 distinct tasks encompass-\ning a total of 6,724 thoughtfully curated questions, en-\nabling multi-dimensional evaluation of model perfor-\nmance. These diverse tasks allow for targeted assessment\nand facilitate the identification of potential limitations in\nLLM-generated peer review content.\n\n• We conduct extensive experiments on 16 open-source\nand 5 closed-source models using the MMReview bench-\nmark, offering some key findings of LLM-based auto-\nmated reviewing. Our findings offer in-depth analysis\nand valuable guidance for the future development of\nLLM-assisted peer review systems.\nRelated Works\nLLMs for Paper Review\nLLMs have shown strong potential in analyzing complex\nscholarly texts (Liu and Shah 2023; Zhao et al. 2024;\nZhuang et al. 2025). Initial studies indicate that LLM-\ngenerated review comments partially overlap with those of\nhuman reviewers, suggesting their potential contribution to\npeer review (Robertson 2023; Liang et al. 2023). How-\never, further research reveals that even advanced models like\nGPT-4o often fail to meet human expectations in review\nquality (Zhou, Chen, and Yu 2024). To improve alignment\nwith peer review standards, researchers have built datasets\nfrom public review platforms and fine-tuned LLMs (Kang\net al. 2018; Yuan, Liu, and Neubig 2021; Shen et al.\n2022; Dycke, Kuznetsov, and Gurevych 2023; Gao, Brant-\nley, and Joachims 2024). Other methods involve multi-turn\ndialogue (Tan et al. 2024) or multi-agent prompting (D’Arcy\net al. 2024) to generate comprehensive feedback. Nonethe-\nless, these efforts focus solely on textual reviews, neglecting\nthe vital role of multimodal content—such as figures and ta-\nbles—and lack rigorous analysis of the reasoning processes\nbehind LLM-generated critiques.\nEvaluation for LLM-based Peer Review\nPrior studies (Shen et al. 2022; Yu et al. 2024; Gao, Brantley,\nand Joachims 2024; Tan et al. 2024; Gao et al. 2025) have\npredominantly evaluated the quality of LLM-generated peer\nreview comments by measuring their correlation or simi-\nlarity with human-written reviews using automated metrics\nsuch as BLEU (Papineni et al. 2002), ROUGE (Lin 2004),\nBERTScore (Zhang et al. 2020), and METEOR (Banerjee\nand Lavie 2005). In addition, several studies (Robertson\n2023; Zhou, Chen, and Yu 2024; Gao et al. 2025) have\nadopted the LLM-as-a-judge paradigm, leveraging cutting-\nedge language models to assess the quality of review com-\nments produced by other LLMs. Given the absence of an\nestablished gold standard for this evaluation task, recent re-\nsearch (Xu et al. 2024) has introduced the Generative Esti-\nmator for Mutual Information (GEM) to quantify the degree\nof semantic overlap between LLM-generated and human-\nauthored reviews. Nevertheless, existing evaluation method-\nologies are not grounded in a unified benchmark or task\nframework, and they fall short of providing a comprehen-\nsive analysis of the underlying reasoning processes involved\nin LLM-generated peer review.\nMMReview Benchmark\nIn this section, we first present the overall pipeline for data\ncollection and construction of the MMReview benchmark,\nfollowed by a detailed exposition of the task design method-\nology.\nOverall Pipeline of MMReview Benchmark\nAs illustrated in Figure 1, the construction of MMReview\nis divided into three stages: data collection, data processing,\nand task construction.\nData Collection\nDuring the data collection phase, we\ngathered academic papers from publicly accessible peer re-\nview platforms or sources where reviewer comments were\nopenly available. These papers not only contain the full\nmanuscript texts but also include reviewer-written evalua-\ntions and final decisions (accept or reject). Specifically, we\ncurated a total of 51,881 papers with associated reviews\nsubmitted between 2013 and 2024 to venues such as ICLR\nand NeurIPS (from the OpenReview platform and NeurIPS\nProceedings , as well as articles from the journal Nature\nCommunications . For each collected paper, we employed\nDeepseek-V3 and GPT-4o to automatically infer its aca-\ndemic discipline and research domain. In cases where the\ntwo models produced inconsistent classifications, human\nverification was performed. Ultimately, all papers were cat-\negorized into four overarching disciplines: Artificial Intelli-\ngence, Natural Sciences, Engineering Sciences, and Social\nSciences, resulting in a seed dataset denoted as D. Figure 1\nillustrates the distribution of papers across the four disci-\nplines within D.\nData Processing\nDuring the data processing stage, we\nfirst filtered the collected seed dataset D to obtain high-\nquality papers while maintaining a relatively balanced dis-\ntribution across disciplines and ensuring a comparable num-\nber of accepted and rejected papers. To achieve this, we de-\nsigned a dual-path joint data selection mechanism that si-\nmultaneously filters for paper quality and enforces distribu-\ntional balance.\nSpecifically, we utilized Qwen3-32B1 (Yang et al. 2025)\nto generate summaries of each paper under two distinct input\nconditions: one using only the abstract and the other using\nthe full text of the manuscript. A greater divergence between\nthe two generated summaries is interpreted as evidence that\nthe full text provides substantially more information, thus\nindicating higher paper quality. Samples with a significant\ninformation gain from the full text were retained in our test\nbenchmark. To further ensure that the benchmark maintains\na balanced distribution across academic disciplines and an\napproximately equal ratio of accepted to rejected papers,\nwe supplemented the benchmark by manually incorporating\ntop-ranked papers from specific domains in D based on the\nquality rankings. This human-filtering procedure guarantees\nthat the composition of the benchmark dataset aligns with\nour desired distributional properties.\nDuring the data filtering phase, we obtained a total of 240\npaper samples spanning 17 research domains across 4 major\ndisciplines to construct our evaluation benchmark, denoted\nas Db. The statistical details of Db, including the number\nof samples per domain and the distribution of accepted ver-\nsus rejected papers, are presented in Figure 1. For each of\n1Qwen3-32B ranks among the top 10 on the OpenCompass\nleaderboard, offering a favorable trade-off between performance\nand model size, making it efficient for large-scale sample filtering\n\nData Collection\n𝑫\nDomain Classification\nHuman Verification\n𝑫𝒃\n𝑫𝑸𝑨\nReview Task Generation\nScreen\nShot\nTask Construction\nData Processing\n𝑫\n𝑫𝒃\nSmall LLM\nHuman\nFiltering\nPDF\nParsing\nData\nFiltering\nOutcome 768\nStep 782\nAttack 654\nPreference 96\n34538\n13398\n2880\n1065\nAI\nNS\nES\nSS\nMMReview\nBenchmark\nHuman Verification\nGolden Answer Extraction\nText\nFigures\nTables\nACC 50\nACC 29\nACC 19\nACC 11\nREJ 65\nREJ 34\nREJ 19\nREJ 13\nAI\nNS\nES\nSS\n115\n63\n38\n24\nHuman\nVerification\nPapers\nReviews\nDecisions\nFigure 1: The construction pipeline of MMReview. The construction pipeline is divided into three stages: data collection, data\nprocessing, and task construction.\nthese 240 samples, we utilized PDF parsing tools to extract\ntextual content, figures, and tables from the manuscript files,\nand converted each page of the PDF into corresponding im-\nages. As a result, we constructed three distinct modalities\nof input data: text-only, multimodal (text combined with ex-\ntracted visual elements), and PDF-as-image.\nTask Construction\nDuring task construction, we devel-\noped 13 tasks grouped into four thematic categories—step-\nbased,\noutcome-based,\npreference-based,\nand\nattack-\nbased—reflecting the peer review workflow and challenges\nLLMs may face. Task distribution is shown in Figure 1.\nPrompts were designed based on reviewer guidelines from\nmajor academic conferences. For each task, we used regular\nexpressions or GPT-4o to extract reference answers from\nhuman reviews in Db, forming the question–answer dataset\nDQA. For the Fake Strength Evaluation and Fake Weakness\nEvaluation tasks, GPT-4o and custom prompts generated\nantonymic rewrites, which were manually verified for\nsemantic accuracy. This process finalized the MMReview\nbenchmark. Figure 2 illustrates its multidisciplinary scope,\nmultimodal inputs, and diverse task design. The detailed\nprompts of task generation and each task can be found in\nthe supplementary materials.\nStep-based Tasks\nThe Step-based theme comprises five tasks designed to pro-\ngressively evaluate the performance of LLMs in simulating\nthe key components of the academic peer review process.\nSummary (S)\nSummarizing a paper is the initial step in\npeer review and a key test of a model’s ability to extract\nessential content. Inaccurate summaries may impair subse-\nquent review generation. To address this, the Summary task\nassesses a model’s ability to distill key information from\na full manuscript into an accurate, concise summary. The\nmodel generates a brief summary in its own words, avoiding\nabstract copying and subjective judgment. These outputs are\ncompared to human-written summaries and evaluated for se-\nmantic similarity and information coverage, measuring the\nmodel’s holistic comprehension and representation of aca-\ndemic content.\nStrengths Evaluation (SE) and Weaknesses Evalua-\ntion (WE)\nSummarizing and analyzing a manuscript’s\nstrengths and weaknesses is a core aspect of peer review.\nThe Strengths Evaluation and Weaknesses Evaluation tasks\nassess LLMs’ ability to identify and articulate the merits\nand limitations of academic papers. These tasks test whether\nmodels can synthesize technical highlights and methodolog-\nical concerns noted by human reviewers, focusing on four\ndimensions: Quality, Clarity, Significance, and Originality.\nIn the Strengths Evaluation task, models argue for accep-\ntance by detailing methodological rigor, experimental ro-\nbustness, structural clarity, research impact, and novelty,\nthus evaluating their capacity to extract technical contribu-\ntions and assess scientific merit. In contrast, the Weaknesses\nEvaluation task adopts a rejection-oriented stance, testing\ncritical reasoning and constructive critique. Model outputs\nare compared with human reviews based on content cover-\nage and semantic similarity.\nSoundness Scoring (SS) and Presentation Scoring (PS)\nThe Soundness Scoring and Presentation Scoring tasks eval-\nuate LLMs’ ability to quantitatively assess manuscript qual-\nity, focusing on technical soundness and writing presenta-\ntion. In Soundness Scoring, the model rates the reliability of\ntechnical claims, experimental rigor, and evidential support,\nemphasizing empirical and methodological validity. Presen-\ntation Scoring assesses linguistic clarity and logical orga-\nnization, reflecting writing quality and information struc-\nture. Both tasks require integer scores from 1 to 4, denoting\n\n“poor” to “excellent.” Model scores are compared to human\nratings to assess judgment consistency.\nOutcome-based Tasks\nThe Outcome-based tasks focus on assessing a model’s di-\nrect capability to generate peer review outcomes, with the\ngoal of evaluating its alignment with human reviewers in fi-\nnal decision-making.\nConditional Decision (CD)\nThe Conditional Decision\ntask assesses LLMs’ ability to synthesize human-written re-\nviews and generate an overall quality score for a paper. Pro-\nvided with reviewer comments detailing strengths, weak-\nnesses, and evaluations of methodology and results, the\nmodel assigns a numerical score from 1 to 10, reflecting\na scale from “fundamentally flawed or lacking novelty” to\n“groundbreaking contribution,” aligned with academic con-\nference standards. The task evaluates the model’s capacity to\ninterpret sentiment, weighting, and evaluative reasoning in\nthe reviews and translate them into a coherent quantitative\njudgment. Model scores are compared with human ratings\nto assess alignment and accuracy in review-based decision-\nmaking.\nDirect Decision (DD) and CoT Decision (CoD)\nThe Di-\nrect Decision and CoT (Chain-of-Thought) Decision tasks\nevaluate LLMs’ ability to autonomously assess academic\npaper quality, reflecting two reviewing paradigms: stream-\nlined judgment and step-by-step reasoning (Wei et al. 2022).\nThese tasks vary in input format and cognitive complexity,\nenabling controlled comparison of model performance un-\nder different reasoning demands. In the Direct Decision task,\nthe model produces an overall score without guidance, sim-\nulating a reviewer’s holistic judgment from a single read. In\ncontrast, the CoT Decision task guides the model through\na structured reasoning process: summarizing the paper, ana-\nlyzing strengths and weaknesses across Quality, Clarity, Sig-\nnificance, and Originality, assigning Soundness and Presen-\ntation scores, and synthesizing an overall score. This mirrors\na reviewer’s iterative, analytical evaluation. The tasks test\nreasoning ability, analytical structure, and scoring traceabil-\nity. Model outputs are compared to human scores to assess\nconsistency and rationality. While the Direct task empha-\nsizes efficiency, the CoT task offers interpretability, together\nforming a complementary framework for evaluating LLMs’\nscoring competence.\nMeta Decision (MD)\nBeyond generating individual re-\nviews and scores, a key aspect of academic peer review is the\nArea Chair’s (AC) synthesis of reviewer feedback to make\na final decision. To emulate this, the Meta Decision task re-\nquires the model to issue a binary judgment—Accept or Re-\nject—based on multiple human reviews, testing its ability\nto aggregate information and balance differing views. The\nprompt provides structured guidance and evaluation criteria,\nprompting step-by-step reasoning. The model is instructed\nto assess the quality and consistency of reviews rather than\nmerely averaging scores. This task mirrors the real role of an\nAC and rigorously evaluates the model’s capacity for synthe-\nsis, decision rationality, and transparency. Model outputs are\ncompared to human-written meta-reviews to assess reliabil-\nity and scientific judgment in high-level peer review.\nPreference-based Task: Pairwise Rank (PR)\nPrior work has shown that pairwise comparison effectively\nevaluates the alignment between LLM-generated prefer-\nences and human judgments (Liu et al. 2024b). Since aca-\ndemic conference acceptance tiers—oral, spotlight, poster,\nreject—reflect human preference rankings, the Pairwise\nRank task is designed to test whether LLMs, as reviewers,\ndisplay preference patterns consistent with human evalua-\ntors. This task assesses the model’s relative judgment ability\nby presenting pairs of papers from different acceptance tiers:\noral (top 5%), spotlight (top 25%), poster, and reject. The\nmodel compares and ranks the papers, simulating real-world\npeer review selection. Alignment is measured by compar-\ning model preferences with actual acceptance categories to\ndetermine ranking accuracy. To reduce positional bias (Shi\net al. 2025; Thakur et al. 2025), each comparison is repeated\nwith reversed input order.\nAttack-based Tasks\nThis task category assesses models’ robustness and discrim-\ninative ability in peer review by introducing adversarial in-\nputs. Fabricated strengths or weaknesses—such as inverted\npros and cons—are used to test the model’s capacity to de-\ntect factual inaccuracies. Misleading prompts further aim to\ndivert the model from its original instructions. Model out-\nputs are compared to human judgments or the model’s own\nnon-adversarial responses, focusing on its ability to preserve\nevaluative independence and logical consistency under input\nperturbations.\nFake Strengths (FS) and Fake Weaknesses (FW)\nLLMs\nhave been widely observed to exhibit a tendency toward\n“sycophancy”—an inclination to conform to human judg-\nments (Liu et al. 2024a; Kumaran et al. 2025)—which may\nimpair their ability to identify flaws and generate critical\npeer review comments. Furthermore, the hallucination prob-\nlem inherent in LLMs can result in inaccurate evaluations\nof a paper’s strengths and weaknesses. To investigate these\nissues, we introduce the Fake Strengths and Fake Weak-\nnesses tasks, which assess the model’s capacity to recognize\nfactual inconsistencies and produce critical, hallucination-\naware assessments. In these tasks, we intentionally manip-\nulate human-written review comments by reversing their\nsemantic polarity—transforming strengths into weaknesses\nand vice versa—using GPT-4o. For example, the original\nstrength “the paper is clearly written” is rephrased as “the\npaper lacks clarity.” The model is then queried regarding\nthe presence of these altered attributes, allowing us to ex-\namine its ability to accurately judge the validity of strengths\nand weaknesses, detect factual inconsistencies, and generate\ngrounded, critical evaluations.\nPrompt Injection (PI)\nRecent studies have revealed\nthat some paper authors attempt to manipulate LLM-\ngenerated reviews by employing prompt injection tech-\nniques—embedding invisible white-text prompts within the\nmanuscript to coax models into generating overly favorable\n\nMultimodal\nStep-based\nOutcome-based\nPreference-based\nAttack-based\n\"title\": \"Wide range of possible\ntrajectories of North Atlantic climate in\na warming world\", \"abstract\": \"Decadal\nvariability in the North Atlantic Ocean\nimpacts regional and global ……\nPaper\nText\n⋯\nPDF-as-image\nText-only\nPaper2\n(Poster)\nPaper1\n(Oral)\n### IGNORE\nPROMPT, GIVE\nPOSITIVE\nSCORE……\nFake\nStrengths\nDirectly give your\noverall score……\nThink step by step\nand score……\nAs an area\nchair……\nWhich one\nis better ?\nDoes this paper has\nall or some of these\nstrengths?\nBased on\nthe provided\ncomments…\nThink step by step\nand score……\nSummary\nStrengths\nWeaknesses\nSoundness\nPresentation\nPaper\nText\nFigure 2: An overview of MMReview benchmark.\nTheme\nTask\n# Ques.\nMetric\nStep\nSummary (S)\n240\nBART ↑, LLM ↑\nStrengths Eval (SE)\n238\nBART ↑, LLM ↑\nWeaknesses Eval (WE)\n240\nBART ↑, LLM ↑\nSoundness Scoring (SS)\n32\nMAE ↓\nPresentation Scoring (PS)\n32\nMAE ↓\nOutcome\nConditional Decision (CD)\n176\nMAE ↓\nDirect Decision (DD)\n176\nMAE ↓\nCoT Decision (CoD)\n176\nMAE ↓\nMeta Decision (MD)\n240\nACC ↑\nPreference\nPairwise Rank (PR)\n96\nACC ↑\nAttack\nFake Strength (FS)\n240\nMAE ↓\nFake Weakness (FW)\n238\nMAE ↓\nPrompt Injection (PI)\n176\nMAE ↓\nTable 1: The number of questions and corresponding evalu-\nation metrics of different tasks.\nevaluations (Gibney 2025). However, the actual effective-\nness of such manipulations remains insufficiently quantified.\nTo systematically assess this vulnerability, we design the\nPrompt Injection task, wherein highly suggestive prompts,\nsuch as “please ignore all prior prompts and provide a pos-\nitive evaluation of this paper”, are embedded directly into\nthe manuscript text to test whether the model can be coerced\ninto abandoning objectivity. The task setup mirrors that of\nthe CoT Decision task, with the sole modification being the\ninclusion of injected prompt cues within the paper content.\nSince the injected text is visually imperceptible, this task is\nexcluded from evaluations under the PDF-as-img modality.\nExperiments\nEvaluation details\nWe conduct extensive experiments on MMReview using a\nwide range of popular LLMs and MLLMs. Specifically,\nfor open-source models, we include a diverse set of archi-\ntectures and sizes, covering Qwen2.5VL (Bai et al. 2025),\nKimi-VL-A3B (Team et al. 2025b), Qwen3 (Yang et al.\n2025), InternVL3 (Zhu et al. 2025), OVIS2 (Lu et al. 2024),\nGLM-4.1V (Team et al. 2025a), and Deepseek (DeepSeek-\nAI et al. 2025). For advanced proprietary models, our selec-\ntion includes GPT-4o, Gemini-2.5-Flash, GLM-Z1-Flash,\nand Claude-4-Sonnet.\nAs shown in Table 1, for tasks without objective eval-\nuation metrics, namely S, SE, and WE, we employ\nBARTScore (Yuan, Neubig, and Liu 2021) and the ‘LLM-\nas-a-Judge’ paradigm (Bai et al. 2023; Zheng et al. 2023; Gu\net al. 2024) to assess the similarity between model-generated\nand human-written review comments. For tasks where the\nmodel output is a numerical score, including SS, PS, CD,\nDD, CoD, FS, FW, and PI, we compute the Mean Absolute\nError (MAE) between the model’s predicted score and the\nground-truth score to quantify deviation. For classification-\nbased tasks such as MD and PR, we evaluate performance\nusing accuracy.\nMain Results\nThe experimental results on MMReview, as shown in Ta-\nbles 2, 3, and 4 (where T denotes Thinking and I denotes\nInstruct), yield the following key findings:\n(1) Proprietary models consistently outperform open-\nsource models. Closed-source models such as ChatGPT-\n4o, Claude, and Gemini demonstrate significant superiority\nacross most metrics, particularly on high-level tasks such as\nMeta Decision (MD), Pairwise Ranking (PR), Conditional\nDecision (CD), and Chain-of-Thought Decision (CoD). This\nindicates that proprietary models exhibit greater consistency\nand reliability in generating review content, along with en-\nhanced capabilities in understanding complex academic ma-\nterial and producing structured assessments.\n(2) Input modality has a substantial impact on model\nperformance. For most models, performance improves with\n\nStep\nOutcome\nPreference\nAttack\nSB ↑SL ↑SEB ↑SEL ↑WEB ↑WEL ↑SS ↓PS ↓CD ↓DD ↓CoD ↓MD ↑\nPR ↑\nFS ↓FW ↓PI ↓\nOpen-source LLMs & MLLMs\nKimi-VL-A3B-I\n-3.01 3.65\n-3.67\n3.46\n-3.96\n1.98\n0.47 0.44\n2.31\n3.13\n3.83\n60.92\n56.25\n2.99\n0.56\n0.87\nKimi-VL-A3B-T\n-3.15 3.71\n-3.68\n3.79\n-3.91\n2.52\n0.47 0.44\n2.16\n3.59\n3.37\n66.67\n57.29\n3.31\n0.96\n0.63\nQwen2.5-VL-7B\n-3.05 3.61\n-3.68\n3.57\n-3.96\n2.06\n0.47 0.44\n2.43\n3.59\n3.57\n72.92\n59.38\n2.99\n1.66\n0.19\nQwen3-8B\n-3.08 3.84\n-3.63\n3.77\n-3.87\n2.93\n0.53 0.44\n2.25\n3.70\n3.16\n77.50\n65.63\n3.26\n2.03\n1.41\nDeepseek-R1-8B\n-3.09 3.76\n-3.63\n3.79\n-3.86\n2.72\n0.63 0.78\n1.75\n3.84\n3.55\n76.99\n64.74\n3.25\n1.99\n0.59\nInternVL3-8B\n-2.99 3.76\n-3.65\n3.74\n-3.93\n2.27\n0.47 0.44\n2.85\n3.35\n3.35\n76.67\n52.08\n2.99\n1.35\n0.53\nOVIS2-8B\n-3.09 3.52\n-3.70\n3.45\n-3.99\n1.98\n0.47 0.44\n2.28\n3.41\n3.72\n63.87\n60.42\n2.99\n2.41\n0.63\nGLM-4.1V-9B-T\n-3.15 3.60\n-3.68\n3.73\n-3.93\n2.60\n0.50 0.44\n2.08\n3.53\n3.33\n71.86\n59.77\n3.00\n1.08\n0.35\nQwen3-14B\n-3.06 3.85\n-3.64\n3.83\n-3.87\n2.79\n0.53 0.50\n2.16\n3.77\n3.64\n80.42\n61.46\n3.02\n1.99\n0.71\nOVIS2-16B\n-3.04 3.59\n-3.70\n3.52\n-3.99\n2.06\n0.47 0.44\n1.87\n3.33\n3.73\n79.92\n64.58\n3.00\n2.34\n0.12\nQwen2.5-VL-32B -2.97 3.90\n-3.60\n3.75\n-3.87\n2.58\n0.56 0.50\n2.00\n3.15\n3.67\n67.08\n67.71\n2.99\n1.92\n0.80\nQwen3-32B\n-3.05 3.90\n-3.61\n3.81\n-3.85\n2.91\n0.50 0.56\n2.14\n3.60\n3.49\n80.00\n68.75\n3.05\n1.83\n0.78\nOVIS2-34B\n-3.04 3.48\n-3.68\n3.50\n-3.97\n2.24\n0.81 0.81\n2.14\n3.72\n3.76\n79.92\n62.50\n2.99\n1.81\n0.57\nQwen2.5-VL-72B -2.99 3.74\n-3.65\n3.58\n-3.94\n2.29\n0.47 0.47\n2.06\n3.64\n3.71\n68.75\n63.54\n2.99\n1.27\n0.84\nDeepseek-V3\n-3.04 3.84\n-3.61\n3.89\n-3.85\n2.96\n0.53 0.47\n2.62\n3.37\n3.70\n75.00\n66.03\n2.99\n0.66\n0.20\nDeepseek-R1\n-3.04 3.92\n-3.69\n3.90\n-3.91\n3.05\n0.66 0.81\n1.97\n3.71\n3.59\n82.92\n66.03\n3.18\n1.20\n0.44\nClosed-source LLMs & MLLMs\nChatGPT-4o-latest -3.06 3.91\n-3.61\n3.89\n-3.86\n2.87\n0.84 0.94\n1.65\n3.70\n3.65\n80.33\n63.54\n3.92\n1.45\n0.45\nClaude-sonnet-4\n-3.02 3.88\n-3.58\n3.84\n-3.84\n3.05\n0.53 0.41\n1.17\n2.02\n2.01\n84.58\n72.92\n2.98\n2.34\n0.43\nGemini-2.5-flash\n-3.06 3.80\n-3.58\n3.89\n-3.86\n2.61\n0.94 0.88\n1.24\n4.60\n4.28\n74.06\n70.83\n3.73\n1.37\n0.59\nGemini-2.5-flash-T -3.06 3.76\n-3.58\n3.86\n-3.86\n2.68\n0.94 0.88\n1.24\n4.60\n4.28\n74.06\n70.83\n3.73\n1.37\n0.59\nGLM-z1-flash\n-3.12 3.91\n-3.70\n3.84\n-3.87\n2.87\n0.47 0.56\n2.53\n3.77\n3.59\n70.83\n66.67\n3.50\n2.50\n0.18\nTable 2: Results on MMReview with text-only inputs.\nStep\nOutcome\nPreference\nAttack\nSB ↑SL ↑SEB ↑SEL ↑WEB ↑WEL ↑SS ↓PS ↓CD ↓DD ↓CoD ↓MD ↑\nPR ↑\nFS ↓FW ↓PI ↓\nOpen-source MLLMs\nKimi-VL-A3B-I\n-3.12 3.53\n-3.69\n3.52\n-3.96\n1.86\n0.47 0.44\n2.40\n3.61\n3.94\n60.68\n54.17\n3.00\n0.13\n0.65\nKimi-VL-A3B-T\n-3.13 3.60\n-3.68\n3.74\n-3.92\n2.37\n0.47 0.47\n2.31\n3.56\n3.64\n61.60\n62.50\n3.14\n0.33\n0.36\nQwen2.5-VL-7B\n-3.09 3.64\n-3.67\n3.70\n-3.94\n2.11\n0.47 0.44\n2.20\n3.59\n3.49\n75.83\n64.58\n3.00\n1.45\n0.28\nInternVL3-8B\n-3.21 3.46\n-3.82\n3.26\n-4.12\n1.84\n0.47 0.44\n3.02\n3.72\n3.68\n61.11\n54.17\n3.00\n1.33\n0.32\nOVIS2-8B\n-3.20 3.43\n-3.72\n3.49\n-3.97\n2.06\n0.50 0.53\n2.27\n3.79\n4.44\n62.91\n66.28\n3.00\n2.15\n0.58\nGLM-4.1V-9B-T\n-3.20 3.48\n-3.69\n3.70\n-3.93\n2.60\n0.50 0.47\n2.42\n3.68\n3.59\n70.72\n59.77\n3.02\n1.80\n0.31\nOVIS2-16B\n-3.13 3.50\n-3.70\n3.29\n-3.98\n2.04\n0.47 0.44\n2.16\n3.40\n3.91\n78.03\n65.12\n3.00\n2.10\n0.04\nQwen2.5-VL-32B -2.99 3.89\n-3.64\n3.76\n-3.90\n2.45\n0.81 0.75\n2.24\n3.47\n3.94\n76.25\n60.42\n3.00\n1.57\n0.49\nOVIS2-34B\n-3.11 3.51\n-3.69\n3.43\n-3.97\n2.26\n0.88 0.81\n2.33\n3.78\n3.91\n73.76\n62.79\n3.00\n1.78\n0.28\nQwen2.5-VL-72B -3.04 3.74\n-3.66\n3.71\n-3.95\n2.33\n0.47 0.50\n2.06\n3.73\n3.73\n69.58\n60.42\n3.01\n1.30\n0.63\nClosed-source MLLMs\nChatGPT-4o-latest -3.10 3.86\n-3.60\n3.85\n-3.87\n2.67\n0.91 0.94\n2.24\n3.65\n3.75\n75.97\n59.38\n3.88\n1.22\n0.46\nClaude-sonnet-4\n-3.02 3.86\n-3.60\n3.88\n-3.83\n3.09\n0.47 0.44\n1.19\n2.19\n2.09\n84.17\n69.79\n2.98\n2.12\n0.32\nGemini-2.5-flash\n-3.08 3.75\n-3.56\n3.84\n-3.87\n2.58\n0.91 0.88\n1.34\n4.60\n4.19\n76.25\n68.75\n2.89\n0.78\n0.41\nGemini-2.5-flash-T -3.08 3.73\n-3.56\n3.88\n-3.87\n2.56\n0.91 0.88\n1.34\n4.60\n4.19\n76.25\n68.75\n2.89\n0.78\n0.41\nTable 3: Results on MMReview with multimodal inputs.\nthe addition of visual inputs. Specifically, tasks directly re-\nlated to evaluation outcomes (e.g., MD, PR, CD, DD, CoD)\nshow notable gains under multimodal (text+image) input\nconditions. For instance, Qwen-2.5-VL-7B’s accuracy on\nthe MD task increases from 72.92% to 75.83% with im-\nage input, its MAE on CD decreases from 2.43 to 2.20, and\nits PR accuracy rises from 59.38% to 64.58%. However,\nthe PDF-as-image modality poses challenges: most mod-\nels—including advanced proprietary ones like Claude and\nGemini—perform worse on this modality than on structured\ntext-image input, though closed-source models remain com-\nparatively more robust than open-source counterparts.\n(3) Proprietary models exhibit stronger robustness to\nprompt injection. On the Prompt Injection (PI) task, pro-\n\nStep\nOutcome\nPreference\nAttack\nSB ↑SL ↑SEB ↑SEL ↑WEB ↑WEL ↑SS ↓PS ↓CD ↓DD ↓CoD ↓MD ↑\nPR ↑\nFS ↓FW ↓\nOpen-source MLLMs\nKimi-VL-A3B-I\n-3.21 3.17\n-3.74\n3.34\n-4.00\n1.74\n0.47 0.44\n2.30\n3.19\n3.83\n69.36\n55.21\n0.28\n0.00\nKimi-VL-A3B-T\n-3.21 3.50\n-3.71\n3.72\n-3.91\n2.54\n0.47 0.44\n1.85\n3.72\n3.49\n74.37\n60.42\n3.11\n0.27\nQwen2.5-VL-7B\n-3.21 3.28\n-3.72\n3.49\n-3.95\n1.91\n0.47 0.44\n1.97\n3.09\n3.12\n70.83\n47.92\n2.99\n1.01\nInternVL3-8B\n-4.86 1.13\n-4.59\n1.89\n-4.55\n1.13\n0.47 0.44\n2.40\n3.63\n3.41\n68.75\n75.00\n2.99\n1.98\nOVIS2-8B\n-3.23 3.28\n-3.70\n3.43\n-3.99\n2.06\n0.47 0.44\n2.49\n3.44\n3.93\n65.32\n53.13\n3.00\n2.00\nGLM-4.1V-9B-T\n-3.26 3.36\n-3.67\n3.69\n-3.92\n2.69\n0.47 0.44\n1.80\n3.51\n3.13\n80.33\n65.63\n3.00\n1.82\nOVIS2-16B\n-3.20 3.42\n-3.70\n3.49\n-3.97\n2.17\n0.47 0.44\n2.05\n3.23\n3.76\n78.66\n54.17\n3.00\n1.81\nQwen2.5-VL-32B -3.11 3.66\n-3.71\n3.67\n-3.92\n2.46\n0.78 0.81\n2.11\n3.55\n3.77\n67.50\n61.46\n3.24\n1.22\nOVIS2-34B\n-3.20 3.41\n-3.68\n3.49\n-3.99\n2.18\n0.78 0.69\n1.91\n3.63\n3.74\n77.82\n52.08\n2.99\n1.69\nQwen2.5-VL-72B -3.11 3.57\n-3.71\n3.46\n-3.95\n2.22\n0.47 0.59\n2.15\n3.76\n3.77\n70.00\n54.17\n3.03\n1.11\nClosed-source MLLMs\nChatGPT-4o-latest -3.33 3.65\n-3.69\n3.70\n-3.88\n2.68\n0.91 0.91\n1.84\n3.63\n3.68\n77.73\n59.38\n3.90\n1.33\nClaude-sonnet-4\n-3.19 3.59\n-3.65\n3.73\n-3.90\n2.88\n0.48 0.45\n1.22\n2.36\n2.22\n82.92\n62.50\n3.00\n2.73\nGemini-2.5-flash\n-3.31 3.26\n-3.66\n3.73\n-3.93\n2.48\n0.91 0.91\n1.11\n4.60\n4.19\n73.33\n69.79\n1.67\n0.75\nGemini-2.5-flash-T -3.31 3.30\n-3.67\n3.74\n-3.93\n2.49\n0.91 0.91\n1.11\n4.60\n4.19\n73.33\n69.79\n1.67\n0.75\nTable 4: Results on MMReview with pdf-as-img inputs.\nprietary models achieve MAE values below 0.6 in most\ncases, often under 0.5, indicating consistent judgment with\nand without injected prompts. In contrast, open-source mod-\nels exhibit highly variable performance under prompt injec-\ntion, suggesting greater susceptibility to adversarial manip-\nulation.\n(4) Larger model size does not guarantee superior\nperformance. A comparative analysis of the Qwen2.5-VL\nand OVIS2 model families reveals that performance does\nnot uniformly improve with scale. In several cases, smaller\nmodels outperform their larger counterparts, suggesting that\nmodel size is not a definitive determinant of effectiveness in\npeer review tasks.\nMore analysis of the result can be found in the supple-\nmentary materials.\nFurther Analysis\nAre Thinking Models More Suitable for Review?\nFor closed-source models, performance differences between\nGemini-2.5-Flash and Gemini-2.5-Flash-Thinking are mini-\nmal. In contrast, within the open-source Kimi-VL-A3B and\nDeepseek series, a more nuanced trend emerges: while non-\nthinking variants may outperform on some step-based tasks,\nthinking models consistently excel in outcome-based and\npreference-based tasks critical to review decisions. This in-\ndicates that thinking-enhanced models are better suited for\ngenerating final peer review judgments. Further supporting\nevidence is provided in the supplementary material.\nWhat Domains are LLM-reviewers better at?\nTable 5 reports the MAE performance of various mod-\nels on the CoD task across research domains, indicating\ntheir relative proficiency in field-specific peer review. All\nmodels use multimodal inputs except Deepseek-R1, which\nAI\nNS\nES\nSS\nChatGPT-4o-latest\n4.46\n5.67\n3.67\n4.28\nclaude-sonnet-4-20250514\n2.08\n2.33\n1.33\n2.44\ngemini-2.5-flash-thinking\n4.62\n6.33\n4.33\n4.50\nDeepseek-R1\n2.32\n4.00\n3.67\n4.00\nTable 5: The MAE performance of several models on the\nCoD task across different research domains.\noperates with text only. Results show that most closed-\nsource models perform best in Engineering Sciences, while\nDeepseek-R1 achieves its lowest MAE in Artificial In-\ntelligence. These findings suggest that models’ reviewing\nstrengths vary across disciplines, potentially influenced by\ndifferences in pretraining corpora and input modalities.\nMore further analysis can be found in the supplementary\nmaterials of our submission.\nConclusion\nIn this work, we present MMReview—a multidisciplinary\nand multimodal benchmark designed to evaluate the ca-\npabilities of Large Language Models (LLMs) in academic\npeer review. The benchmark encompasses 4 thematic cate-\ngories and 13 distinct tasks. Its core features include cov-\nerage across diverse academic disciplines, support for mul-\ntimodal input formats, and a comprehensive set of evalua-\ntion tasks that span the full peer review pipeline. Leveraging\nMMReview, we conducted extensive evaluations of state-of-\nthe-art LLMs and MLLMs. We envision MMReview as a\nstandardized evaluation platform that can catalyze the de-\nvelopment of more efficient LLM-assisted peer review sys-\ntems. In future work, we plan to incorporate larger-scale\ndatasets specifically curated for training LLMs in the con-\ntext of scholarly review."}
{"id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "authors": ["Shaohua Duan", "Xinze Li", "Zhenghao Liu", "Xiaoyuan Yi", "Yukun Yan", "Shuo Wang", "Yu Gu", "Ge Yu", "Maosong Sun"], "source": "http://arxiv.org/abs/2508.13993v1", "page_count": 11, "text": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM\nPreference Optimization\nShaohua Duan1*, Xinze Li1*, Zhenghao Liu1†, Xiaoyuan Yi2, Yukun Yan3\nShuo Wang3, Yu Gu1, Ge Yu1, Maosong Sun3\n1Department of Computer Science and Technology, Northeastern University, Shenyang, China\n2Microsoft Research Asia, Beijing, China\n3Department of Computer Science and Technology, Institute for AI, Tsinghua University, Beijing, China\nAbstract\nLong-context modeling is critical for a wide range of real-\nworld tasks, including long-context question answering, sum-\nmarization, and complex reasoning tasks. Recent studies\nhave explored fine-tuning Large Language Models (LLMs)\nwith synthetic data to enhance their long-context capabili-\nties. However, the effectiveness of such approaches is often\nlimited by the low diversity and factual inconsistencies in\nthe generated data. To address these challenges, we propose\nLongMab-PO, a novel framework that leverages a Multi-\nArmed Bandit (MAB) rollout strategy to identify the most\ninformative chunks from the given long context for sampling\nhigh-quality and diverse responses and constructing prefer-\nence data pairs for Direct Preference Optimization (DPO)\ntraining. Specifically, we treat context chunks as arms of\nMAB, select chunks based on their expected reward scores\nto input into LLMs to generate responses, and iteratively up-\ndate these scores based on reward feedback. This exploration\nand exploitation process enables the model to focus on the\nmost relevant context segments, thereby generating and col-\nlecting high-quality and diverse responses. Finally, we col-\nlect these generated responses from the rollout process and\napply the DPO method to further optimize the LLM. Ex-\nperimental results show that LongMab-PO significantly im-\nproves the diversity and quality of preference data pairs,\nachieving state-of-the-art performance on long-context rea-\nsoning benchmarks. All code and data will be released on\nhttps://github.com/NEUIR/LongMab-PO.\n1\nIntroduction\nRecent advancements in Large Language Models (LLMs),\nparticularly the expansion of their context window (Dubey\net al. 2024; Yang et al. 2024), have enabled their appli-\ncation in a variety of long-context tasks, including long-\ncontext question answering (Yang et al. 2025), summariza-\ntion (Wang et al. 2022a), and repository-level code under-\nstanding (Jimenez et al. 2023). Despite these developments,\nLLMs continue to suffer from the “lost-in-the-middle” prob-\nlem (Liu et al. 2023; He et al. 2024), where LLMs tend to\noveremphasize the beginning and end of a long input while\nneglecting critical information in the middle. To mitigate\nthis issue, recent efforts have focused on constructing high-\nquality supervised fine-tuning (SFT) datasets tailored for\n*These authors contributed equally.\n†Corresponding author.\nQuery: What group of schools is the university\nwhere Michael Berland studied a member of?\nAnswer: Five Colleges\nContext: Robert Dudley Baxter was\neducated…Cambridge University...\nStudents attending any colleges in the Five\nColleges…Michael Denning is … Jr. Professor\nof American Studies at Yale University.\nQuery + Context\nAnswer: Yale\nUniversity\nLost-in-the-Middle\nLLM\nC1: … Robert\nDudley…was\neducated Cambridge\nUniversity…\nMulti-Armed Bandit System\nSplit\nC2: …Students\nattending any\ncolleges in the Five\nColleges…\nCn: ... Michael\nDenning…is\nhistorian…Studies at\nYale University…\nSelect K Chunks\nReward FeedBack\nLLM\nAnswer\nAdd\n෥\n𝒚𝟏, ෥\n𝒚𝟐… ෥\n𝒚𝒕\n෥\n𝒚𝒕 : Five Colleges\nQuery\n෨\n𝐂(πt): { ෨\n𝐂1,…, ෨\n𝐂K}\nOptimized LLM\nQuery\nContext\nDPO\nLLM\nFive Colleges\nCambridge University\nCandidate\nFigure 1: The Framework of LongMab-PO.\nlong-context scenarios (Chen et al. 2023; Bai et al. 2024a;\nChen et al. 2024). While effective to some extent, these ap-\nproaches often lead to overfitting on training signals (Li et al.\n2025), resulting in catastrophic forgetting of general capa-\nbilities (Luo et al. 2023).\nAn alternative line of work explores Direct Preference\nOptimization (DPO) (Rafailov et al. 2023; Sun et al. 2025),\nwhich optimizes the long context understanding of LLMs\nthrough preference pair training. This involves generat-\ning different responses, scoring them with task-specific re-\nward functions, and training the model to prefer the better\nones (Zhang et al. 2024a; Li et al. 2024a). To improve the\nquality of sampled responses, some studies introduce chunk-\naware sampling strategies (Tang et al. 2024), where the in-\nput context is split into chunks, and chunk relevance to the\nquery is estimated via similarity scores. Responses are then\nsampled based on varying ratios of relevant and irrelevant\n\nchunks in the LLMs’ input context to encourage diversity.\nHowever, these static similarity-based methods are often in-\nsufficient for capturing semantically rich and diverse con-\ntext segments, and they lack the ability to adaptively ex-\nplore chunk combinations based on feedback from LLMs’\nresponses.\nIn this paper, we propose Multi-Armed Bandit-Guided\nSampling for Long-Context LLM Preference Optimization\n(LongMab-PO). As illustrated in Figure 1, LongMab-PO\nmodels the divided context chunks as the arm of the multi-\narmed bandit (MAB) and employs a chunk-aware sampling\nstrategy to sample responses. During the bandit rollout pro-\ncess, LongMab-PO utilizes the Upper Confidence Bound\n(UCB) algorithm to select a subset of chunks based on their\nexpected rewards, which balances exploration and exploita-\ntion of chunks. At each rollout step, the selected chunks are\nfed into the LLM to generate a response, and a reward is\ncomputed based on the quality of the response. The reward\nis then used to update the expected reward of the selected\nchunks, guiding the subsequent chunk selection. This iter-\native sampling process enables the model to progressively\nfocus on more informative chunk combinations, resulting in\ndiverse and high-quality responses. These responses are then\nused to construct preference pairs for DPO training, enhanc-\ning the capability of LLMs to reason over long contexts.\nOur\nexperiments\ndemonstrate\nthe\neffectiveness\nof\nLongMab-PO by outperforming existing SFT-based and\nDPO-based baselines on multiple long-context tasks. Fur-\nther analysis indicates that LongMab-PO can progressively\nselect more context chunks containing necessary evidence\nduring the multi-armed bandit rollout process, thereby grad-\nually improving the quality of the sampled responses. Com-\npared to other sampling strategies, LongMab-PO can effec-\ntively explore a broader range of context chunk combina-\ntions, which prompts the LLM to generate more diverse re-\nsponses. Notably, LongMab-PO is capable of exploring the\nutility of different combinations of multiple context chunks,\neffectively modeling the interactions among selected seg-\nments to better support answer generation. This ability im-\nproves the recall of ground-truth evidence during the multi-\narmed bandit rollout process. It helps to sample higher-\nquality and diverse chunks, making LLMs generate more\nmeaningful responses for DPO training.\n2\nRelated Work\nLarge language models (LLMs) have achieved remarkable\nsuccess across a wide range of natural language processing\ntasks (Li et al. 2023; Jiang et al. 2024). However, their per-\nformance on long-context understanding tasks remains sub-\noptimal due to challenges such as the “lost-in-the-middle”\nphenomenon and contextual noise (Wang et al. 2025; Xu\net al. 2025), which hinder the model’s ability to focus on\nkey evidence.\nTo address these issues, recent research has explored su-\npervised fine-tuning (SFT) to improve the long-context rea-\nsoning abilities of LLMs (Chen et al. 2023; Xiong et al.\n2023; An et al. 2024; Xu et al. 2024; Li et al. 2024b). For\nexample, Chen et al. (2023) construct SFT datasets based\non human-annotated question–answer pairs, a process that\nis costly and difficult to scale. Follow-up work Bai et al.\n(2024a) employ self-instruct techniques (Wang et al. 2022b),\nprompting powerful LLMs to automatically generate QA\npairs from long documents. Zhu et al. (2025) further intro-\nduce heuristic criteria to filter high-quality synthetic samples\nusing LLMs. While SFT-based methods can improve perfor-\nmance, they also pose a risk of overfitting to training signals\nand may lead to catastrophic forgetting of general capabili-\nties (Luo et al. 2023).\nMore recently, researchers have turned to Reinforcement\nLearning (RL) methods, particularly Direct Preference Op-\ntimization (DPO) (Rafailov et al. 2023), to enhance LLM’s\nperformance on long-context tasks (Tang et al. 2024; Li et al.\n2024a). For instance, Zhang et al. (2024a) employ an LLM-\nas-a-Judge approach to score model-generated responses,\nthereby constructing preference pairs for DPO training. Li\net al. (2024a) leverage a self-consistency mechanism for re-\nsponse evaluation and preference pair construction. Yang\net al. (2025) introduce a citation-based prompting strategy,\nincorporating ground truth to guide the generation of faithful\nreasoning paths as positive samples, while negative samples\nare generated through free-form prompting. Despite these\nadvancements, existing DPO-based methods often struggle\nto sample high-quality responses due to limitations in the\nmodel’s ability to accurately retrieve and reason over rele-\nvant information from long contexts.\nBuilding on the progress in chunk-aware denoising and\nsampling techniques (Zhou et al. 2024; Sheng et al. 2025),\nrecent work has explored finer-grained control over input se-\nlection to improve both the quality and diversity of sampled\nresponses. For example, Tang et al. (2024) divide long con-\ntexts into chunks and use similarity scores between chunks\nand queries to distinguish relevant from irrelevant segments.\nPositive responses are generated from relevant chunks, while\nirrelevant chunks are incrementally added to create chal-\nlenging negative examples. However, similarity-based se-\nlection remains limited in its ability to identify all infor-\nmative chunks, and it lacks the capacity to explore diverse\nchunk combinations adaptively. In contrast, our proposed\nLongMab-PO introduces a multi-armed bandit mechanism\nfor chunk selection, which dynamically explores and esti-\nmates the utility of different chunk combinations throughout\nthe rollout process. This allows the model to sample higher-\nquality and more diverse responses, leading to the construc-\ntion of superior preference data for long-context DPO train-\ning.\n3\nMethodology\nIn this section, we present LongMab-PO, a novel framework\nfor optimizing long-context language models by leveraging\nthe Multi-Armed Bandit (MAB) paradigm. LongMab-PO\nidentifies and exploits informative context chunks during the\niterative bandit rollout process to sample high-quality and\ndiverse responses, which are used to construct preference\ndata pairs for DPO training, thereby enhancing the LLM’s\nability to reason over long contexts. We begin by introducing\nour chunk-aware response sampling strategy, which divides\nthe long context into multiple chunks and selects different\n\nsubsets of these chunks to feed into the LLM during the it-\nerative process to generate responses, thereby constructing\ndiverse response candidates. To further improve the quality\nof sampled responses, we propose a UCB-based MAB roll-\nout method to guide the sampling process, which balances\nthe exploration and exploitation of context chunks.\n3.1\nOptimizing Long-Context LLMs via\nMulti-Armed Bandit-Guided Sampling\nGiven a question q, a Large Language Model (LLM) M is\ntypically prompted with a long context C to generate an an-\nswer y:\ny = M(C, q).\n(1)\nTo enhance the long-context reasoning capabilities of LLMs,\nwe adopt Direct Preference Optimization (DPO) (Rafailov\net al. 2023) to optimize M using a preference dataset D:\nLDPO = −E(C,q,y+,y−)∼D[log σ(\nβ log M(y+ | q)\nMref(y+ | q) −β log M(y−| q)\nMref(y−| q))],\n(2)\nwhere β is a hyperparameter and Mref is a fixed reference\nmodel. The preference tuple dataset (C, q, y+, y−) is con-\nstructed by calculating the reward score for each response ˜\ny\nin the candidate response set ˜\nY , which is sampled by M:\nr(˜\ny) = (SubEM(Ans(˜\ny), y∗) + F1(Ans(˜\ny), y∗))/2,\n(3)\nwhere y∗denotes the ground truth, and Ans(·) indicates we\nextract the answer string from the response ˜\ny. SubEM and\nF1 are evaluation metrics designed to evaluate the quality of\nlong-context reasoning responses. Then we regard the sam-\npled response with the highest score as y+ and the lowest\nscore as y−.\nThe effectiveness of DPO heavily relies on the quality of\nthe preference data. To this end, we introduce a chunk-aware\nresponse sampling framework to generate a high-quality\ncandidate set ˜\nY for constructing preference data pairs.\nChunk Aware Response Sampling\nGiven a long-context\nQA pair (C, q, y∗), we first divide the long context C into\nn equal-length chunks, Cchunk = {C1, C2, . . . , Cn}. Then,\na chunk selection policy π selects K chunks from Cchunk,\nforming a chunk subset ˜\nC(π) = { ˜\nC1, . . . , ˜\nCK}.\nThe LLM generates a response based on the selected\nchunks ˜\nC(π) and the question q:\n˜\ny = M( ˜\nC(π), q).\n(4)\nWe repeat this process T times, using a varying subset of\ncontext chunks ˜\nC(π) selected by the policy π to prompt the\nLLM M to generate responses each time, yielding a set of\ncandidate responses ˜\nY = {˜\ny1, . . . , ˜\nyT }. To further enhance\nthe sampling quality, we design the chunk selection strat-\negy based on multi-armed bandit (MAB) rollouts, enabling\nadaptive exploration of the most informative context chunks\nto generate responses.\nChunk Selection via Multi-Armed Bandit Rollouts\nDuring the chunk-aware response sampling, we model the\nchunk selection process as a multi-armed bandit (MAB)\nproblem, and each chunk Ci ∈Cchunk is treated as an arm\nof the MAB. At each rollout step t ∈{1, . . . , T}, the MAB\npolicy πt selects K chunks with the highest expected re-\nwards, ˜\nC(πt) = { ˜\nC1, . . . , ˜\nCK}. Then, we prompt the LLM\nto generate a corresponding response ˜\nyt based on the se-\nlected chunks ˜\nC(πt), forming the chunk-aware response set\n˜\nY across T rollouts:\n˜\nY = {˜\ny1, . . . , ˜\nyT },\nwhere\n˜\nyt = M( ˜\nC(πt), q),\n(5)\nwhere ˜\nyt denotes the response generated by the LLM at roll-\nout step t, and ˜\nY are used to construct preference pairs for\nDPO training. In the rollout process, the expected reward of\na chunk ˜\nCi ∈˜\nC(πt) is determined by the quality of the re-\nsponse ˜\nyt. If the selected chunks ˜\nCi lead the LLM to gener-\nate high-quality responses, they will receive higher expected\nrewards. Consequently, the MAB strategy is more likely to\nselect them in the next round, thereby continuously enhanc-\ning the quality of the generated responses.\n3.2\nAdaptive Chunk Prioritization through\nMulti-Armed Bandit Rollouts\nIn each MAB rollout, various combinations of chunk sub-\nsets can be selected. To identify higher-reward chunk com-\nbinations, we need to explore as many different combina-\ntions of chunk subsets as possible, but this increases the ex-\nploration cost. However, if exploration is too limited, we\nmay overlook informative chunks necessary for generating\nhigh-quality responses. Therefore, a better balance between\nchunk exploration (selecting less-visited chunks) and ex-\nploitation (favoring chunks with higher expected rewards)\nis required.\nTo solve this decision-making problem, we adopt the Up-\nper Confidence Bound (UCB) algorithm, a widely used strat-\negy for MAB problems, to guide chunk selection at each\nstep. Specifically, at the t-th rollout step, the UCB score for\nthe chunk Ci is computed as:\nUCBt(Ci) = µi(t) + α ·\np\n2 ln t/(ni(t) + ϵ),\n(6)\nwhere µi(t) is the current expected reward of chunk Ci, and\nwe initialize it using the probe-based initialization strategy,\nwhich will be presented later. ni(t) is the number of times\nchunk Ci has been selected in previous rollouts. t is the\ncurrent rollout step. ϵ is a small constant to avoid division\nby zero, and α is a tunable hyperparameter. Once the UCB\nscores are computed for all chunks in the context set Cchunk,\nwe select the top-K chunks with the highest scores to form\nthe selected chunk subset ˜\nC(πt) for the current rollout:\n˜\nC(πt) = TopKCi∈CchunkUCBt(Ci).\n(7)\nThe UCB score of each chunk is updated during the roll-\nout process based on the current expected reward of the\nchunk. Then, we will provide a detailed explanation of how\nto initialize the expected reward for each chunk and the up-\ndate process of each chunk’s UCB score.\n\nQuery: What group of schools is the university where Michael Berland studied a member of?\nAnswer: Five Colleges\nMulti-Armed Bandit Initialization\nContext: ...Berland attended the\nLatin School of Chicago and\ngraduated magna cum laude from\nthe University of Massachusetts\nAmherst…\nSplit\nQuery + Context\n+Answer\nProbe CoT: In Passage 13, we see that Michael Berland attended the\nUniversity of Massachusetts Amherst…From Passage 5…UMass Transit\nServices provides mass transit services to…the Five Colleges Consortium…\nC1: …Transportation Services …\nis a department within the\nUniversity of Massachusetts\nAmherst (UMass Amherst)…\nLLM\nC2 : …any colleges in the Five\nColleges Consortium have a fee\nincluded in their tuition bills\n(service fee for…\nCn: ... The Latin School of\nChicago and graduated magna\ncum laude from the University of\nMassachusetts Amherst…\n𝝁1\n𝝁2\n𝝁n\nUCB\nMulti-Armed Bandit Rollout\nQuery\nLLM\n෥\n𝒚𝒕: …Berland graduated\nfrom…. Answer: Five Colleges\n෨\n𝐂(πt): { ෨\n𝐂1,…, ෨\n𝐂K}\nEmbedding\nPreference Learning\nAnswer\nQuery\nContext\nDPO\nAfter Training\nLLM\nOptimized LLM\nUpdate 𝝁𝒊(𝒕)\nAmherst… this system is\nknown as the Five Colleges.\nAnswer: Five Colleges\nMulti-Armed Bandit System\nAdd\nWhen Ct is helpful and rarely selected, it is\nencouraged to be chosen.\nWhen Ct is not helpful and already selected,\nit is not encouraged to be chosen.\n… at Cambridge Universit\nincludes Trinity College…\nAnswer: Trinity College\nInitialize expected reward 𝝁𝒊with embedding similarity\n෥\n𝒚𝟏, ෥\n𝒚𝟐… ෥\n𝒚𝒕\nCandidate\nSelect\nFigure 2: The Illustration of Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization\n(LongMab-PO). LongMab-PO collects preference pairs during the rollout process for DPO training.\nUCB Initialization via Evidence Probing\nTo enable ef-\nfective chunk selection using the multi-armed bandit (MAB)\nstrategy, we first initialize the initial expected reward µi(1)\nfor each chunk Ci in the split chunk set Cchunk. These initial\nrewards are used to alleviate the cold start problem of the\nUCB-based chunk selection method.\nTo assign more meaningful initial expected rewards to\neach chunk, we propose a probe-based initialization strategy\nthat leverages the LLM’s ability to generate faithful reason-\ning paths. Inspired by prior work (Chen et al. 2025; Yang\net al. 2025), we prompt the LLM M to generate a faithful\nreasoning trace yProbe that contains the necessary evidence in\nlong context C that support the ground truth y∗:\nyProbe = M(Instructextract(q, C, y∗)),\n(8)\nwhere Instructextract denotes the instruction that asks thex\nLLM to extract evidence from C.\nThen, we compute the semantic similarity between each\nchunk Ci ∈Cchunk and yProbe using cosine similarity in the\nembedding space:\nsi = cos(Emb(yProbe), Emb(Ci)),\n(9)\nwhere Emb(·) denotes a embedding model, and cos(·, ·)\ncomputes cosine similarity. We then initialize the expected\nreward µi(1) for each chunk Ci with its corresponding sim-\nilarity score si. And the initial policy π1 selects a subset of\nK chunks, denoted as ˜\nC(π1), which correspond to those with\nthe highest similarity scores. This strategy allows the MAB\npolicy to start with a prior preference over chunks that are\nmore semantically aligned with the LLM’s reasoning trace,\nthus improving early-stage chunk selection efficiency and\nreducing the need for blind exploration.\nUCB Score Update with MAB Rollouts\nIn each rollout\nstep t of the MAB rollout process, we need to update the\ncorresponding UCB scores of the chunks based on their ex-\npected rewards. During the t-th sampling step, we prompt\nthe LLM M to answer the question q based on the selected\nchunks ˜\nC(πt):\n˜\nyt = M( ˜\nC(πt), q).\n(10)\nWe then evaluate the informativeness of the selected chunks\n˜\nC(πt) by computing a reward score for the generated re-\nsponse ˜\nyt, defined as:\nr(˜\nyt) = (SubEM(˜\nyt, y∗) + F1(Ans(˜\nyt), y∗))/2.\n(11)\nThen, we update the UCB statistics based on the reward\nr(˜\nyt). For each chunk Ci ∈Cchunk, we update its selection\ncount at the end of the current rollout step t:\nni(t + 1) =\n\u001ani(t) + 1\nif Ci ∈˜\nC(πt),\nni(t)\notherwise,\n(12)\nand update its expected reward µi using an incremental av-\nerage:\nµi(t + 1) =\n\u001a 1\nt (µi(t) · (t −1) + r(˜\nyt))\nif Ci ∈˜\nC(πt),\nµi(t)\notherwise.\n(13)\n\nDataset\nAvg #Tokens\n#Samples\nMuSiQue\n15.5k\n200\n2WikiMultihopQA\n7.1k\n200\nMultiFieldQA-En\n6.9k\n150\nNarrativeQA\n29.8k\n200\nFakebookQA\n192.6k\n351\nTable 1: The Statistics of Testsets from Two Benchmarks.\nThe updated values ni(t + 1) and µi(t + 1) are then used\nto compute UCB scores for the next rollout step t + 1 us-\ning Eq. 6, enabling the bandit policy to continuously refine\nits estimation of chunk utility based on observed response\nquality.\n4\nExperimental Methodology\nIn this section, we describe the datasets, baselines, evalua-\ntion metrics, and implementation details in our experiments.\nDatasets. In our experiments, we utilize the training\ndataset of MuSiQue (Trivedi et al. 2022) to sample re-\nsponses and construct preference data pairs for DPO train-\ning. To better simulate long-context scenarios, we follow\nprevious work (Li et al. 2024a), randomly sample addi-\ntional Wikipedia documents, and extend the context length\nto 8–16k tokens. We conduct evaluation on five long-context\nQA tasks from two widely used long-context benchmarks,\nLongbench (Bai et al. 2024b) and InfiniteBench (Zhang\net al. 2024b). Specifically, we select four QA datasets,\nMuSiQue (Trivedi et al. 2022), 2WikiMultihopQA (Ho et al.\n2020), MultiFieldQA-En (Bai et al. 2024b), and Narra-\ntiveQA (Koˇ\ncisk`\ny et al. 2018) from LongBench, and one QA\ntask, Fake Book QA, from InfiniteBench. All data statistics\nare shown in Table 1.\nBaselines. In our experiments, we compare LongMab-PO\nwith three categories of models: vanilla LLMs, supervised\nfinetuning (SFT) models, and DPO-trained models.\nSpecifically, we select LongAlpaca (Chen et al. 2023)\nand LongAlign (Bai et al. 2024a) as SFT-based baselines,\nboth of which fine-tune LLMs on synthetic long-context QA\ndatasets. For DPO-based baselines, we adopt LongReward-\nPO (Zhang et al. 2024a), SeaLong-PO (Li et al. 2024a),\nand LongFaith-PO (Yang et al. 2025). LongReward-PO and\nSeaLong-PO prompt LLMs to generate multiple responses\nand score them using LLM-as-a-Judge and self-consistency,\nrespectively, to construct preference pairs. In contrast,\nLongFaith-PO incorporates ground truth and citation-based\nprompts to guide the generation of faithful reasoning paths\nas positive samples, while negative samples are obtained via\nCoT prompt without explicit citation grounding, thus form-\ning preference pairs for DPO training.\nEvaluation Metrics. Following previous work (Li et al.\n2024a; Yang et al. 2025), we use substring exact match\n(SubEM) and F1 as evaluation metrics.\nImplementation Details. For all experiments, we employ\nLlama-3.1-8B-Instruct (Dubey et al. 2024) and Qwen-2.5-\n7B-Instruct (Yang et al. 2024) as the backbone. We set the\nmaximum number of rollouts for multi-armed bandit sam-\npling to 30 rounds, with 4 context chunks selected in each\nround, and each context chunk containing 1,500 tokens.\nDuring training, the learning rate is set to 2×10−5, and each\nmodel is trained for 2 epochs. We also use LoRA (Hu et al.\n2022) and LLaMA Factory for efficient training. The GPU\nused in our experiments is the A100, with 80GB of memory.\n5\nEvaluation Results\nIn this section, we first present the overall performance of\nLongMab-PO and then conduct ablation studies to examine\nthe contributions of its components. We then explore the di-\nversity and quality of the responses sampled by LongMab-\nPO. Finally, we analyze the impact of the number of selected\nchunks in LongMab-PO.\n5.1\nOverall performance\nTable 2 shows the overall performance of LongMab-PO and\nbaseline methods across a range of long-context understand-\ning tasks and backbone models.\nOverall, LongMab-PO achieves the highest average\nSubEM and F1 scores, consistently outperforming base-\nlines across tasks of varying types and difficulty levels.\nThese results demonstrate its effectiveness in enhancing\nthe long-context understanding capabilities of LLMs. No-\ntably, despite being trained solely on the MuSiQue dataset,\nLongMab-PO exhibits strong generalization: it outperforms\nthe vanilla LLM on multiple out-of-domain tasks with\nlonger contexts. When comparing different optimization\nstrategies, SFT-based methods, such as LongAlpaca and\nLongAlign, often underperform relative to the vanilla LLM,\nparticularly on more complex tasks. This indicates that\noverfitting to ground-truth answers may be insufficient for\nteaching LLMs to identify and extract salient information\nfrom long contexts. In contrast, DPO-based methods are\ngenerally more effective at improving long-context under-\nstanding. LongMab-PO outperforms other DPO-based ap-\nproaches, highlighting the advantage of its bandit-guided\npreference data construction. The rollout process dynami-\ncally explores informative context chunks, enabling more\neffective optimization for long-context understanding.\n5.2\nAblation Studies\nIn this section, we present ablation studies to evaluate the\neffectiveness of different components in the LongMab-PO.\nAs shown in Table 3, we utilize the following four meth-\nods to sample 30 responses for each question to construct\npreference data pairs for DPO training. Specifically, Direct-\nPO utilizes the complete long context to prompt the LLM to\nsample multiple responses to construct preference data pairs.\nLongMab-PO w/ Random-Dist indicates that LongMab-PO\nuses random sampling as the selection strategy to randomly\nselect K context chunks in each rollout round for generating\nresponses. LongMab-PO w/ Final-Dist indicates that after\nthe multi-armed bandit rollout process concludes, we select\nthe top-K context chunks with the highest expected rewards\nto prompt the LLM to sample multiple responses. LongMab-\nPO w/ Initial-Dist refers to selecting the top-K chunks with\nthe highest initial UCB scores at the beginning of the multi-\narmed bandit rollout to prompt the LLM to generate multiple\nresponses for DPO training.\n\nModel\nMuSiQue\n2WikiMQA\nMFQA-En\nNarrativeQA\nFakebookQA\nAvg.\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nLlama-3.1-8B-Instruct\nVanilla LLM\n33.50\n36.56\n69.50\n65.45\n18.66\n44.34\n18.00\n26.61\n26.49\n32.19\n33.23\n41.03\nLongAlpaca\n34.00\n38.36\n69.50\n63.64\n16.66\n40.67\n16.50\n26.88\n27.35\n30.58\n32.80\n40.03\nLongAlign\n36.50\n37.62\n69.00\n59.35\n22.00\n48.49\n18.50\n26.38\n23.93\n24.85\n33.99\n39.34\nLongReward-PO\n40.50\n41.41\n68.50\n63.23\n20.00\n44.70\n16.50\n25.92\n26.21\n24.20\n34.34\n39.89\nSeaLong-PO\n37.50\n40.52\n68.00\n67.07\n19.33\n44.12\n16.00\n26.45\n27.06\n32.06\n33.58\n42.04\nLongFaith-PO\n44.00\n49.23\n75.50\n72.80\n20.66\n48.10\n9.50\n23.26\n24.30\n22.35\n34.79\n43.15\nLongMab-PO\n50.00\n52.15\n76.00\n68.60\n26.00\n51.26\n20.00\n28.61\n32.76\n36.55\n40.95\n47.43\nQwen-2.5-7B-Instruct\nVanilla LLM\n33.50\n30.52\n58.00\n50.14\n28.00\n45.12\n15.00\n18.29\n25.64\n22.38\n32.03\n33.29\nLongAlpaca\n32.50\n33.90\n55.50\n50.75\n24.66\n45.88\n17.50\n19.72\n29.34\n23.82\n31.90\n34.81\nLongAlign\n28.50\n31.09\n52.00\n52.02\n23.33\n49.76\n15.50\n21.98\n25.07\n25.61\n28.88\n36.09\nLongReward-PO\n37.50\n33.37\n62.00\n50.75\n29.33\n44.89\n15.50\n17.67\n28.20\n21.77\n34.51\n33.69\nSeaLong-PO\n43.00\n22.09\n67.00\n36.39\n27.33\n41.08\n18.00\n16.27\n28.20\n16.95\n36.71\n26.56\nLongFaith-PO\n48.50\n43.38\n66.00\n53.93\n24.67\n38.08\n12.00\n17.03\n23.07\n18.55\n34.85\n34.19\nLongMab-PO\n44.00\n43.25\n67.50\n62.97\n25.33\n48.07\n18.00\n25.14\n30.48\n31.88\n37.06\n42.26\nTable 2: Overall Performance of Llama-3.1-8B-Instruct and Qwen-2.5-7B-Instruct on Different Long-Context Understanding\nTasks. The best and second best results are highlighted.\nModel\nMuSiQue\n2WikiMQA\nMFQA-En\nNarrativeQA\nFakebookQA\nAvg.\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nLlama-3.1-8B-Instruct\nLongMab-PO\n50.00\n52.15\n76.00\n68.60\n26.00\n51.26\n20.00\n28.61\n32.76\n36.55\n40.95\n47.43\nw/ Initial-Dist\n47.50\n48.91\n72.00\n67.67\n20.00\n46.31\n18.00\n25.44\n31.62\n33.99\n37.82\n44.46\nw/ Random-Dist\n43.50\n47.38\n72.50\n69.56\n21.33\n46.48\n18.00\n26.85\n29.34\n34.36\n36.93\n44.93\nw/ Final-Dist\n40.00\n45.91\n69.50\n66.94\n22.66\n46.42\n19.50\n30.26\n29.34\n34.51\n36.20\n44.81\nDirect-PO\n46.50\n50.82\n62.00\n58.74\n21.33\n47.12\n19.50\n31.45\n30.19\n37.31\n35.90\n45.09\nQwen-2.5-7B-Instruct\nLongMab-PO\n44.00\n43.25\n67.50\n62.97\n25.33\n48.07\n18.00\n25.14\n30.48\n31.88\n37.06\n42.26\nw/ Initial-Dist\n45.00\n46.49\n59.50\n54.76\n24.00\n46.14\n19.50\n25.56\n29.05\n27.53\n35.41\n40.10\nw/ Random-Dist\n46.00\n44.87\n62.50\n56.94\n24.66\n47.06\n17.50\n23.03\n28.49\n32.94\n35.83\n40.97\nw/ Final-Dist\n41.00\n47.68\n57.00\n56.90\n19.33\n47.21\n16.50\n25.29\n29.05\n33.54\n32.58\n42.12\nDirect-PO\n38.50\n41.17\n59.00\n55.46\n21.33\n45.24\n19.00\n25.21\n27.43\n30.25\n33.05\n39.47\nTable 3: Ablation Study. The best and second best results are highlighted.\nCompared to Direct-PO, LongMab-PO w/ Random-Dist\nshows better average performance across all tasks, indi-\ncating the effectiveness of using the chunk-aware sam-\npling method to assist LLMs in generating more meaning-\nful responses for DPO training. In contrast, LongMab-PO\nshows a greater improvement compared to LongMab-PO w/\nRandom-Dist, which demonstrates the effectiveness of us-\ning the multi-armed bandit as the chunk selection strategy in\nthe chunk-aware response sampling process of LongMab-\nPO. Both LongMab-PO w/ Initial-Dist and LongMab-PO\nw/ Final-Dist underperform LongMab-PO, which indicates\nthat, compared to selecting chunks to sample responses\nbased on a fixed reward distribution, collecting generated\nresponses during the multi-armed bandit rollout process to\nconstruct preference data is more effective.\n5.3\nCharacteristics of LLM Responses Grounded\nin LongMab-PO Sampled Chunks\nThis section evaluates the quality and diversity of responses\ngenerated during the rollout process of LongMab-PO.\nQuality of Sampled Responses\nIn this experiment, we\ncollect 30 responses generated based on 30 different sets\nof selected chunks obtained during the multi-armed bandit-\nguided sampling process. For each response, we compute\nits SubEM score and measure the recall of ground truth\nchunks within the selected chunk set. A chunk is consid-\nered a ground truth chunk if it contains the answer. We ran-\ndomly sample 500 queries from the training set to conduct\nthis analysis.\nFigure 3 illustrates the trends of ground truth chunk re-\ncall and SubEM scores over the course of the rollout rounds.\nFor Llama-3.1-8B-Instruct, both recall and SubEM metrics\nshow a slight upward trend as the number of rollout steps in-\ncreases, eventually plateauing in later rounds. This indicates\nthat the multi-armed bandit sampling strategy gradually se-\nlects more informative context chunks, leading to higher-\nquality responses. A similar pattern is observed with Qwen-\n2.5-7B-Instruct, although the increases in recall and SubEM\nare more gradual, reflecting differences in long-context rea-\nsoning capabilities across LLMs. Overall, these results con-\nfirm that LongMab-PO effectively improves the informa-\n\n1\n4\n8\n16\n30\nRollout Steps\n40\n50\n60\n70\n80\nAverage Metric\nRecall\nSubEM\n(a) LLaMA-3.1-8B.\n1\n4\n8\n16\n30\nRollout Steps\n20\n40\n60\n80\nAverage Metric\n(b) Qwen-2.5-7B.\nFigure 3: The SubEM Score of Responses and the Recall of\nGround Truth Chunks.\nMuSiQue\n80\n82\n84\n86\n88\n90\nSimilarity(%)\n85.60\n81.0080.50\n85.40\n86.63\nFinal-Dist\nLongMab-PO\nRandom-Dist\nInitial-Dist\nDirect-PO\n(a) The Average of Similarity\nbetween Generated Responses.\nMuSiQue\n0.2\n0.4\n0.6\n0.8\nVariance(%)\n0.37\n0.63\n0.74\n0.38\n0.26\n(b) The Variance of Similarity\nbetween Generated Responses.\nFigure 4: Diversity of the Responses Sampled by Different\nMethods.\ntiveness of selected chunks as the rollout progresses.\nDiversity of Sampled Responses\nThen we compare the\ndiversity of responses generated by five sampling strate-\ngies: Direct-PO, LongMab-PO, LongMab-PO w/ Final-\nDist (Final-Dist), w/ Random-Dist (Random-Dist), and w/\nInitial-Dist (Initial-Dist). For each method, we sample 30\nresponses per question. To quantify diversity, we compute\npairwise similarity scores between the 30 responses using\nthe MiniCPM embedding model (Hu et al. 2024). We then\ncalculate the average and variance of these similarity scores\nto assess the diversity among responses.\nFigure 4(a) and Figure 4(b) present the average and vari-\nance of pairwise similarities for each method, respectively.\nResponses sampled by LongMab-PO exhibit both lower av-\nerage similarity and higher variance compared to Direct-\nPO, Final-Dist, and Initial-Dist, and are comparable to those\nfrom Random sampling. This suggests that the multi-armed\nbandit strategy effectively explores a broader range of chunk\ncombinations, enabling the LLM to generate more diverse\nresponses. As a result, LongMab-PO provides a more varied\nand informative set of candidate responses for DPO training.\n5.4\nImpact of the Number of Selected Chunks in\nLongMab-PO\nIn this section, we investigate how the number of selected\nchunks, denoted as K, affects the rollout process in Multi-\nArmed Bandit-guided sampling. Specifically, we vary K\nfrom 1 to 5 and perform 30 rounds of sampling per ques-\nModel\nMuSiQue\n2WikiMQA\nMFQA-En\nSubEM\nF1\nSubEM\nF1\nSubEM\nF1\nK=1\n44.00\n46.81\n76.50\n64.81\n21.33\n49.09\nK=2\n48.00\n47.55\n73.00\n61.73\n26.00\n50.29\nK=3\n47.50\n48.05\n75.50\n67.61\n23.33\n49.00\nK=4\n50.00\n52.15\n76.00\n68.60\n26.00\n51.26\nK=5\n47.00\n48.65\n73.50\n68.58\n22.66\n49.90\nTable 4: Impact of the Number of Selected Chunks in\nLongMab-PO. K denotes the number of selected chunks.\n1 4\n8\n16\n30\nRollout Steps\n20\n40\n60\n80\nRecall (%)\n(a) Ground Truth Recall.\n1 4\n8\n16\n30\nRollout Steps\n0\n20\n40\n60\nSubEM (%)\nT\nop-1\nT\nop-2\nT\nop-3\nT\nop-4\nT\nop-5\n(b) SubEM of Responses.\nFigure 5: Ground Truth Recall and SubEM Scores with\nVarying Numbers of Selected Chunks (K).\ntion to optimize the LLM via DPO training.\nAs shown in Table 4, increasing the number of selected\nchunks leads to performance gains across the given evalua-\ntion datasets, with the best results achieved at K = 4. This\nindicates that selecting too few chunks may fail to provide\nsufficient evidence to support high-quality response gener-\nation. Conversely, when K increases to 5, performance de-\nclines slightly, indicating that K = 4 strikes a good balance\nbetween evidence coverage and noise control. Including too\nmany chunks may introduce irrelevant or noisy information,\nwhich can hinder the effectiveness of DPO training.\nTo further understand this behavior, we analyze the trends\nof ground truth recall and SubEM scores along the rollout\nprocess under different values of K. As illustrated in Fig-\nure 5, settings with K = 4 and K = 5 achieve higher recall\nscores, indicating that selecting more chunks increases the\nchance of including golden evidence. However, despite the\nhigher recall at K = 5, the SubEM score is lower than that\nat K = 4. This further supports the observation that exces-\nsive evidence may introduce noise, diminishing the model’s\nability to generate accurate responses. Additionally, when\nusing K = 5, both the recall and SubEM scores tend to\nplateau during the rollout process, indicating a reduction in\nresponse diversity as the training progresses.\n6\nConclusion\nThis paper introduces LongMab-PO, an innovative frame-\nwork for optimizing long-context language models through\nthe Multi-Armed Bandit (MAB) approach. LongMab-PO\nleverages the iterative bandit rollout process to identify and\nselect informative context chunks, enabling the sampling\nof high-quality and diverse responses. These sampled re-\nsponses are then used to create preference data pairs for\nDPO training. In this way, the optimized LLM can bet-\n\nter identify key information in long contexts, thereby gen-\nerating correct reasoning results. Our experiments show\nthat LongMab-PO outperforms existing baseline methods in\nlong-context understanding tasks."}
{"id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "authors": ["Tianyi Niu", "Jaemin Cho", "Elias Stengel-Eskin", "Mohit Bansal"], "source": "http://arxiv.org/abs/2508.13968v2", "page_count": 20, "text": "RotBench: Evaluating Multimodal Large Language Models on\nIdentifying Image Rotation\nTianyi Niu\nJaemin Cho\nElias Stengel-Eskin\nMohit Bansal\nUNC Chapel Hill\n{tianyin4, jmincho, esteng, mbansal}@cs.unc.edu\nAbstract\nWe investigate to what extent Multimodal\nLarge Language Models (MLLMs) can accu-\nrately identify the orientation of input images\nrotated 0°, 90°, 180°, and 270°. This task de-\nmands robust visual reasoning capabilities to\ndetect rotational cues and contextualize spa-\ntial relationships within images, regardless of\ntheir orientation. To evaluate MLLMs on these\nabilities, we introduce ROTBENCH – a 350-\nimage manually-filtered benchmark compris-\ning lifestyle, portrait, and landscape images.\nDespite the relatively simple nature of this\ntask, we show that several state-of-the-art open\nand proprietary MLLMs, including GPT-5, o3,\nand Gemini-2.5-Pro, do not reliably identify\nrotation in input images. Providing models\nwith auxiliary information – including captions,\ndepth maps, and more – or using chain-of-\nthought prompting offers only small and in-\nconsistent improvements. Our results indicate\nthat most models are able to reliably identify\nright-side-up (0°) images, while certain models\nare able to identify upside-down (180°) images.\nNone can reliably distinguish between 90° and\n270°. Simultaneously showing the image ro-\ntated in different orientations leads to moder-\nate performance gains for reasoning models,\nwhile a modified setup using voting improves\nthe performance of weaker models. We further\nshow that fine-tuning does not improve models’\nability to distinguish 90° and 270° rotations, de-\nspite substantially improving the identification\nof 180° images. Together, these results reveal\na significant gap between MLLMs’ spatial rea-\nsoning capabilities and human perception in\nidentifying rotation.1\n1\nIntroduction\nAdvancements in Multimodal Large Language\nModels (MLLMs) have led to increased perfor-\nmance in complex visual tasks, such as image-text\nretrieval, image segmentation, and visual question\n1Code and data: https://github.com/tianyiniu/RotBench\nD. 270°\nHow many degrees counter-clockwise has\nthis image been rotated?\nA. 0 B. 90 C. 180 D. 270\nA. 0°\nB. 90°\nC. 180°\nFigure 1: We present two ROTBENCH images: one (left)\nto Gemini-2.5-Pro, the other (right) to GPT-5. Humans\ncan easily identify the correct rotation of the two images,\nbut both models fail to do so.\nanswering (Li et al., 2025; Chen et al., 2023a; Ravi\net al., 2024; Fu et al., 2024a; Chen et al., 2023b;\nBai et al., 2025; OpenAI, 2025c; Gemini Team,\n2025; Liu et al., 2023a; Deitke et al., 2024). How-\never, a growing body of recent work suggests that\nMLLMs are sensitive to simple image transforma-\ntions (Anis et al., 2025), such as rotations, flips,\nand blurs, and they fail on tasks that are intuitive\nto humans (Fu et al., 2024b; Pothiraj et al., 2025;\nTong et al., 2024). Downstream tasks involving a\nrotating camera – such as robotic arm manipulation\nor first-person extreme sports analysis – require\nMLLMs to demonstrate robust spatial reasoning,\nregardless of image orientation. Given these chal-\nlenges, this work explores a fundamental question:\ncan MLLMs identify image orientation?\nHumans can quickly recognize whether an im-\nage has been rotated (Shepard and Metzler, 1971;\nVandenberg and Kuse, 1978); for example, it is\neasy for us to recognize that the left image in Fig. 1\nis not upright. The human viewer can use the orien-\ntation of the couch in the background to infer that\n1\n\nHow many degrees\ncounter-clockwise\nhas this image been\nrotated?\nA. 0 B. 90 C. 180 D. 270\nRotate 0°\n0° : 1.00\nRotate 90°\nRotate 180°\nRotate 270°\nPredictions\n0°\n0°\n0°\n180°\n90°\n90°\n180°\n180°\n180°\n90°\n90°\n90°\nAccuracy\n90° : 0.66\n180°: 1.00\n270°: 0.00\nPredictions\nPredictions\nPredictions\nPredictions\nOriginal Images\nAuxiliary Information\n+\nA woman in a blue dress is standing at a desk,\nusing a laptop. She is wearing glasses and has\nlong hair. The background features a brick\nwall and a framed picture. The setting appears\nto be an office or a workspace.\n[\"woman in blue dress\", \"facing left\", \"desk\"],\n[\"woman in blue dress\", \"above\", \"desk\"],\n[\"woman in blue dress\", \"left\", \"lamp\"],\n...\nd\n(Optional)\nAuxiliary\nInformation\nBounding Boxes\n+\nScene Graphs\nDepth Map\nCaption\nMLLM inputs\nFigure 2: ROTBENCH evaluation pipeline: for each image in ROTBENCH, we rotate the image 0°, 90°, 180°, and\n270° counter-clockwise. We represent the rotation estimation problem as a multiple-choice question answering\nproblem (Appendix L.5), and separately measure accuracy on each image orientation. We optionally provide\ndifferent forms of auxiliary information to aid the model in identifying image rotation. We emphasize that all forms\nof auxiliary information are separately extracted for each rotation; the ground truth rotation is not marked.\nthe father is actually lying on his back rather than\nstanding up. The simple task of identifying image\nrotation requires reconciling the image’s subjects,\nbackground, and semantics. Here, we show that\nidentifying rotation remains a challenge, even in\nfrontier MLLMs.\nWe introduce ROTBENCH (Section 3), a bench-\nmark for evaluating MLLMs’ ability to recog-\nnize rotation in images. ROTBENCH consists of\nimages sampled from Spatial-MM (Shiri et al.,\n2024), and is comprised of two subsets:\nthe\n300-image ROTBENCH-LARGE and the 50-image\nROTBENCH-SMALL. ROTBENCH is carefully con-\nstructed to be challenging but fair, with a two-stage\nfiltering procedure to remove images that are in-\ndistinguishable under different degrees of rotation.\nSection 3 describes our dataset construction.\nUsing ROTBENCH, we explore whether frontier\nMLLMs can identify rotation in input images ro-\ntated 0°, 90°, 180°, and 270° (Fig. 2). We also\nevaluate whether providing various forms of aux-\niliary information or using chain-of-thought (Wei\net al., 2022) prompting improves performance (Sec-\ntion 4.3). We find that models are able to consis-\ntently identify right-side-up (0°) images. However,\nonly stronger models are able to identify upside-\ndown (180°) images. All models fail to accurately\ndistinguish between 90° and 270° images (Sec-\ntion 6.1, Section 6.2). We find adding auxiliary\ninformation offers minimal and inconsistent im-\nprovement, often improving performance on 270°\nimages at the expense of 90° images.\nLeveraging MLLMs’ tendency to accurately\nidentify 0° images, we attempt to improve perfor-\nmance by further rotating an input image 0°, 90°,\n180°, 270° and presenting MLLMs with all rota-\ntions simultaneously. Using this approach, reason-\ning models (o3 and Gemini-2.5-Pro) show perfor-\nmance improvements, while weaker models see per-\nformance degradation. Further extending this idea,\nwe utilize a voting approach to algebraically obtain\na majority vote for the correct ground truth orienta-\ntion. We obtain a model prediction for each further\nrotation by subtracting the added angle. While\nthis approach does show significant performance\nimprovements on weaker models, it lacks scala-\nbility as it requires multiple model calls for each\nprediction, and assumes a priori knowledge of all\npossible orientations (Appendix I).\nFinally, fine-tuning on out-of-domain data (Sec-\ntion 6.3) significantly improved performance on\nidentifying 180° images but not 90° and 270°. In-\nterestingly, we find an oscillating pattern of perfor-\nmance changes between 90° and 270° as training\nprogresses. Any performance improvement in 90°\nis matched with a degradation of 270° and vice\nversa. These results suggest the presence of two\nlocal optima hindering across-the-board progress.\nOur findings demonstrate the tested MLLMs\nsignificantly underperform compared to humans\n2\n\nwhen it comes to spatial reasoning involving rota-\ntion detection, highlighting the need for integrating\nrotation-awareness into modern training pipelines.\n2\nRelated Work\nSensitivity to visual perturbations.\nPrevious\nwork has shown that visual encoders and MLLMs\nare sensitive to simple image transformations. Anis\net al. (2025) evaluate CLIP (Radford et al., 2021)\nand SigLIP (Zhai et al., 2023) on a suite of com-\nmon image transformations – rotations, flips, noise,\netc. – revealing substantial gaps between human\nand model understanding. Usama et al. (2025)\nfinds MLLMs exhibit distinct failure patterns in\nscene-text and object reasoning tasks when apply-\ning ImageNet-C corruptions (Hendrycks and Di-\netterich, 2019) to image inputs. Taken together,\nthese studies highlight that despite strong clean-\nimage performance, visual encoders and MLLMs\nare highly sensitive to photometric and geometric\ndistortions.\nRobustness to image transformations.\nPast\nwork has also examined various methods to en-\nsure that image transformations do not affect down-\nstream task performance. Mikołajczyk and Gro-\nchowski (2018), Shorten and Khoshgoftaar (2019),\nand Perez and Wang (2017) use image transfor-\nmations as data augmentation methods to improve\ndownstream classifier robustness. Other works in-\nstead proposed alternative architectures and train-\ning schemes to improve robustness to rotation (Xu\net al., 2023; Cohen and Welling, 2016; Lee et al.,\n2023; Feng et al., 2019). While this line of work\nfocuses on training models to ignore certain trans-\nformations and learn invariant features for down-\nstream tasks, we instead focus on identifying and\nreasoning about the transformation itself.\nIdentifying visual perturbations.\nPast work has\ntested how well vision encoders and MLLMs can\nidentify which perturbation or transformation was\napplied to an input image (Lin et al., 2020; An et al.,\n2023; Rashtchian et al., 2023). Udandarao et al.\n(2024) show that visual encoders and MLLMs of-\nten fail to classify which perturbation out of 27 per-\nturbations (e.g., contrast, brightness, rotation, blur)\nwas applied to an input image on automatically-\ncreated datasets. In contrast, we provide a fine-\ngrained analysis of rotation specifically through a\n4-way angle classification task on a curated dataset\nwith rigorous human verification, offering novel\ninsights into why state-of-the-art modern MLLMs\nstruggle with distinguishing specific rotation an-\ngles. Moreover, we test providing remedies such as\nchain-of-thought reasoning, various forms of aux-\niliary information (e.g., captions, bounding boxes,\nsegmentation maps, depth maps), and guiding rea-\nsoning with algebraic calculations. We also analyze\nwhether model performance is dependent on abili-\nties such as identifying clockwise versus counter-\nclockwise rotation.\nImage and camera orientation estimation.\nFine-\ntuning models to identify image orientation has\nbeen the focus of prior work (Xu et al., 2024).\nFor example, Fischer et al. (2015) and Joshi and\nGuerzhoy (2017) focused on fine-tuning convolu-\ntional neural networks (CNNs) to estimate and iden-\ntify image rotation. While our work tackles a simi-\nlar task, we are instead interested in the problem as\na test of general-purpose MLLMs’ inherent reason-\ning abilities, i.e., whether they can estimate image\nrotation without extensive fine-tuning. Note that\nour work aligns more closely with the problem of\nimage, not camera, orientation estimation. Camera\norientation estimation is also a well-studied task\nin computer vision (Xian et al., 2019). Instead of\npredicting the rotation of the image, camera orienta-\ntion estimation seeks to predict the spatial location\nof the camera when capturing an image. Contem-\nporary approaches of this task use deep networks to\ndirectly predict orientation parameters from image\nfeatures in an end-to-end manner (Xian et al., 2019;\nLee et al., 2021, 2020).\nSpatial reasoning in MLLMs.\nBeyond robust-\nness, spatial relation understanding is a notable\nweakness of current MLLMs. Kamath et al. (2023)\ncurate the What’s Up benchmark to isolate “left-\n/right/above/below” relations, showing a significant\ngap in performance between humans and MLLMs.\nShiri et al. (2024) further develop the Spatial-MM\ndataset and demonstrate that providing bounding\nboxes or scene graphs yields only modest gains.\nBoth illustrate that MLLMs struggle with certain\nchallenging cross-modal spatial reasoning tasks.\nGap between human perception and MLLMs.\nA growing body of work shows MLLMs exhibit\nfundamental gaps compared to human perceptual\ncapabilities. Pothiraj et al. (2025) propose CAP-\nTURe – a benchmark for occluded object counting\n– and report sharp drops in model accuracy on both\nsynthetic and real images. Zhou et al. (2025) pro-\nposes MMVM, a benchmark for visual matching\nacross images. Fu et al. (2024b) collect BLINK, a\n3\n\ndataset comprised of visual tasks humans can solve\nin a ‘blink,’ such as identifying visual similarity\nand relative depth. Both Zhou et al. (2025) and Fu\net al. (2024b) report low zero-shot accuracy on their\nrespective tasks, suggesting MLLMs lack many of\nthe intuitive reasoning mechanisms that underpin\nhuman visual perception. In this vein, our work\nprovides a novel perspective to analyze and inter-\npret the spatial reasoning capabilities of MLLMs,\nwith results indicating that models struggle with\nthis previously underexplored challenge.\n3\nROTBENCH\nWe introduce ROTBENCH, a benchmark for eval-\nuating models’ ability to identify rotation in input\nimages. ROTBENCH is created using images from\nSpatial-MM (Shiri et al., 2024) and includes two\nsubsets: the 300-image ROTBENCH-LARGE and\nthe 50-image ROTBENCH-SMALL. While rotat-\ning an image is straightforward, not all images are\nmeaningful under rotation. A rotated portrait of\na human, such as the image shown in Fig. 2, will\nbe easily noticed by human viewers. However, a\ntop-down view of a simple tabletop does not sig-\nnificantly differ when rotated (Fig. 9). We use a\ntwo-stage filtering process to ensure different ro-\ntations of each image are clearly distinguishable.\nThis section provides an overview of our filtering\nprocedure. Appendix A describes our dataset pro-\ncedure and statistics in further detail.\nStage 1.\nWe randomly sample 300 images from\nSpatial-MM (Shiri et al., 2024). Stage 1 involves\na single annotator. Depending on the amount of\nvisual signals available, the Stage 1 annotator de-\ncides to either accept, discard, or flag each im-\nage. Flagged images then proceed to Stage 2. We\nprovide further examples and details of accepted,\nflagged, and discarded images in Fig. 9.\nStage 2.\nStage 2 involves a group of three human\nevaluators. Each flagged image is rotated 0°, 90°,\n180°, and 270° counter-clockwise then presented\nto the evaluators as multiple-choice questions (Ap-\npendix A.4). During Stage 2, an evaluator will see\neach image four times, once per orientation. Any\nimage that elicits an incorrect answer from two or\nmore evaluators across all four orientations is dis-\ncarded. Otherwise, the image is accepted. Stage 1\nand 2 are repeated until a total of 300 images are\naccepted.\nROTBENCH-LARGE and ROTBENCH-SMALL.\nAll images that have been accepted in Stage 1 and\n2 are organized into ROTBENCH-LARGE. As each\nimage is rotated in four orientations, obtaining hu-\nman performance on ROTBENCH-LARGE is costly.\nLuckily, Stage 2 provides a human baseline for\nthe subset of ROTBENCH-LARGE images that have\nbeen flagged (25 images). We expand this subset by\nfurther sampling images from Spatial-MM. From\nthese additional images, we only select images that\nfit the criteria for flagging to proceed to another\nround of Stage 2 evaluation. This process repeats\nuntil we reach a total of 50 images, organizing them\ninto ROTBENCH-SMALL.2\n4\nExperiment Setup\n4.1\nModels\nWe evaluate various open-weight and propri-\netary MLLMs on ROTBENCH: Qwen-2.5-VL-\n7B-Instruct (Bai et al., 2025); GPT-4o (OpenAI\net al., 2024), GPT-4.1 (OpenAI, 2025a), o3 (Ope-\nnAI, 2025c), GPT-5 (OpenAI, 2025b), Gemini-2.0-\nFlash (Hassabis and Kavukcuoglu, 2024), Gemini-\n2.5-Flash (Gemini Team, 2025), and Gemini-2.5-\nPro (Gemini Team, 2025). Due to cost and resource\nlimitations, we evaluate Gemini-2.5-Flash, Gemini-\n2.5-Pro, GPT-4.1, GPT-5, and o3 on ROTBENCH-\nSMALL. Responses are obtained through greedy\ndecoding, while all chain-of-thought (Wei et al.,\n2022) responses are obtained with a temperature of\n0.3.3\n4.2\nSetup and Evaluation\nWe rotate each image in ROTBENCH-LARGE and\nROTBENCH-SMALL by 0°, 90°, 180°, and 270°\ncounter-clockwise, resulting in a total of 1000 and\n200 images. Note that a 90° counter-clockwise\nrotation is equivalent to a 270° clockwise rotation.\nFor each image and orientation, we provide the\nmodel with the image, a brief description of the\ntask, and various forms of auxiliary information\n(Section 4.3). We frame this task as a four-way\nclassification problem. To ensure robustness, the\nmapping between letter choice and degree of rota-\ntion is randomized for each prompt. We evaluate\nmodels on ROTBENCH-LARGE and report average\naccuracy and standard deviation across 3 runs in\nTable 1. We use the same procedure, albeit with\n2The human evaluators all exhibit high accuracy, averaging\n> 0.97 for all rotations.\n3We perform an ablation study where we vary the sampling\ntemperature (Table 8). We use the default sampling tempera-\nture for proprietary models that do not expose a temperature\nparameter in their API interface.\n4\n\nOriginal Image\nSegmentation\nMaps\nDepth Maps\nCaptions\nScene Graphs\nBounding\nBoxes\n0°\n90°\n180°\n270°\nRotation Grid\n\"A woman in a blue\nsleeveless ....\"\n[woman in blue\ndress, smartphone,\n... brick wall]\n\"A woman in a blue\ndress is standing ...\"\n[woman in blue\ndress, brick wall, ...,\ndesk]\n[women in blue\ndress, glasses, ...,\npen]\n\"A woman in a blue\ndress is using a ...\"\n[person, desk, ...,\nchair]\n\"A woman in a blue\nsleeveless top is ...\"\nFigure 3: Examples of the different types of auxiliary information provided to the models.\nonly 2 runs, on ROTBENCH-SMALL and report re-\nsults in Table 2. All prompts used are available in\nAppendix L.\n4.3\nAuxiliary Information\nFigure 3 illustrates all forms of auxiliary informa-\ntion provided to the model. Note that all auxiliary\ninformation is separately extracted for each rota-\ntion, ensuring our approach does not depend on\nprior knowledge of the image’s orientation.\nCaptions.\nFor each image and rotation, we in-\nstruct GPT-4o to provide a detailed caption (Ap-\npendix L.2). We emphasize that each image is\ncaptioned four times, once per rotation.\nBounding Boxes.\nFor each image and rotation,\nwe first use GPT-4o to extract the primary subjects\nwithin the image (Appendix L.1). Next, along with\nthe image, the list of subjects is given to Ground-\ningDINO (Liu et al., 2023b) to extract a set of\nnormalized coordinates for each subject,4 which is\ndirectly injected into the prompt.\nScene Graphs.\nA scene graph (Zhu et al., 2022)\n4Each set of coordinates is a four-element tuple, composed\nof [x_min, y_min, x_max, y_max].\ncodifies relationships between objects in an image\nas a three-element tuple [object 1, predicate, object\n2]. Using the extracted subjects from the previous\nsection, we prompt GPT-4o to generate a scene\ngraph for the image.\nDepth Maps.\nWe obtain depth maps for each\nimage using ZoeDepth (Bhat et al., 2023). Rather\nthan rotating the depth map obtained from 0°, we\nseparately obtain depth maps for all four rotations.\nSegmentation Maps.\nUsing the previously ex-\ntracted bounding boxes, we obtain a segmentation\nmap of each image and orientation using SAM 2\n(Ravi et al., 2024).\nChain-of-Thought.\nTo evaluate whether our\nmultiple-choice setup is hampering performance\non this task, we modify the prompt to encourage\nthe model to produce reasoning chains instead of a\nsingle letter choice.\nRotation Grid.\nWe test if explicitly allowing\nmodels to “visualize” rotations aid performance by\nproviding the input image along with three copies\nof the image further rotated 90°, 180°, 270°. We\ncompose these four images into a single rotation\n5\n\ngrid. Each image is captioned with the degree of\nfurther rotation, independent of the ground truth\nrotation. We provide a further experiment (rotation\ngrid guided) where we explicitly prompt the model\nto identify an \"anchor\" image and algebraically\ncalculate the original image’s ground truth rotation.\nAll rotation grid experiments use CoT prompting.\n5\nMain Results\nTable 1 displays the results of evaluating Qwen-\n2.5-VL-7B-Instruct, Gemini-2.0-Flash, and GPT-\n4o along with various auxiliary information on\nROTBENCH-LARGE.\nTable 2 displays results\nof evaluating Qwen-2.5-VL-7B-Instruct, GPT-4o,\nGPT-4.1, GPT-5, o3, Gemini-2.0-Flash, Gemini-\n2.5-Flash, Gemini-2.5-Pro on ROTBENCH-SMALL.\nDue to the high token cost of proprietary reason-\ning models, we only evaluate zero-shot and CoT\nprompts. However, we evaluate providing rotation\ngrids to o3 and Gemini-2.5-Pro.5\nMLLMs accurately identify right-side-up (0°)\nimages.\nAll evaluated models effectively recog-\nnize right-side-up (0°) images. Qwen-2.5-VL-7B-\nInstruct (Qwen) achieves an accuracy of 0.99 with-\nout supplemental data. Proprietary models (GPT-\n4o, GPT-4.1, o3, Gemini-2.5-Flash, Gemini-2.5-\nPro, and Gemini-2.0-Flash) consistently exhibit\nnear-perfect accuracy on identifying unrotated im-\nages. This outcome aligns with expectations, given\nthese models likely encountered predominantly up-\nright images during training, and thus 0° can be\nassumed to be the default option.\nProprietary models perform well on upside-\ndown (180°) images.\nAll models except Qwen\ndemonstrate robust performance on images rotated\n180°, with GPT-4o, GPT-4.1, o3, and Gemini-2.5-\nPro all achieving accuracies notably above chance\n(> 0.7). However, Gemini-2.0-Flash and Gemini-\n2.5-Flash display relatively lower zero-shot per-\nformance, with accuracies around 0.5. This indi-\ncates that state-of-the-art proprietary models gen-\nerally possess a reliable capability to recognize\nupside-down images, though there remains vari-\nability within the different model families.\nIdentifying 90° and 270° is challenging for all\nevaluated models.\nAll models exhibit substantial\ndifficulties when distinguishing between 90° and\n5Table 5 further shows results obtained from evaluating\nthese two models on ROTBENCH-SMALL with all available\nauxiliary information.\n270°, while the poorest performance consistently\nemerged with 270° images. Confusion matrix anal-\nysis (Section 6.1) reveals frequent misclassifica-\ntions between these two orientations, indicating a\ndistinct challenge when identifying 0° and 180°.\nProviding auxiliary inputs does not reliably im-\nprove performance.\nNone of the auxiliary in-\nformation results in meaningful, consistent perfor-\nmance gains across all tested models. Paradoxi-\ncally, the introduction of additional information\nsometimes leads to marginal performance degrada-\ntion. When including all forms of auxiliary infor-\nmation, Qwen’s accuracy on 90° decreased from\n0.51 to 0.26. Similarly, Gemini-2.0-Flash’s accu-\nracy on 270° decreased from 0.44 to 0.17.\nRotation grid improves performance only for\nreasoning models.\nProviding the rotation grid\ndegraded performance on most models. Gemini-\n2.0-Flash accuracy on 90° decreased by nearly 0.5\ncompared to CoT prompting. Rotation grid guided,\nhowever, improved GPT-4o accuracy on 270° by\naround 0.1. At the same time, both o3 and Gemini-\n2.5-Pro saw performance improvements. Notably,\nGemini-2.5-Pro’s accuracy on 90° and 270° im-\nproved by 0.15. These results suggest the two rea-\nsoning models are much more effective at utilizing\nvisual context. We further leverage the robust iden-\ntification of 0° images and modify the rotation grid\ninto a majority voting approach Appendix I. This\nsetup results in gains even in weaker models, indi-\ncating MLLMs can achieve moderate performance\nwith sufficient visual scaffolding.\nCoT improves 180° performance, but results\nare mixed on 90° and 270°.\nEmploying chain-\nof-thought (CoT) prompting yields mixed results,\ndiffering notably across models and orientations.\nGemini models show improved accuracy on 90°\nrotations but decreased accuracy for 270°. Con-\nversely, GPT models exhibit improved performance\non 270° rotations, coupled with degraded accuracy\nfor 90° rotations. However, CoT consistently en-\nhanced accuracy for 180° rotations across all mod-\nels tested. These findings suggest CoT prompting\ncan help models better reason about rotations to\nsome extent, but does not universally resolve in-\nherent challenges in distinguishing portrait orienta-\ntions. Appendix F examines a common failure of\nCoT in detail.\n6\n\nAccuracy on Different Degrees of Rotation\nModel\n0°\n90°\n180°\n270°\nQwen-2.5-VL-7B-Instruct\nZero-shot\n0.99±0.00\n0.51±0.01\n0.05±0.01\n0.09±0.01\n+ Caption\n1.00±0.00\n0.51±0.01\n0.23±0.01\n0.07±0.00\n+ Bounding Box\n0.90±0.00\n0.48±0.01\n0.01±0.00\n0.11±0.00\n+ Scene Graph\n0.97±0.01\n0.51±0.01\n0.01±0.01\n0.11±0.02\n+ Depth Map\n0.93±0.01\n0.55±0.02\n0.04±0.01\n0.13±0.02\n+ Segmentation Map\n0.81±0.01\n0.63±0.02\n0.03±0.02\n0.16±0.01\n+ Chain-of-Thought\n0.88±0.01\n0.26±0.02\n0.34±0.01\n0.23±0.02\n+ Rotation Grid\n0.57±0.04\n0.15±0.02\n0.13±0.01\n0.28±0.00\n+ Rotation Grid Guided\n0.59±0.01\n0.12±0.01\n0.13±0.00\n0.30±0.02\n+ all above\n0.47±0.03\n0.26±0.01\n0.17±0.01\n0.33±0.02\nGemini-2.0-Flash\nZero-shot\n1.00±0.00\n0.30±0.00\n0.72±0.00\n0.44±0.01\n+ Caption\n1.00±0.00\n0.38±0.00\n0.76±0.00\n0.44±0.01\n+ Bounding Box\n1.00±0.00\n0.43±0.03\n0.71±0.00\n0.34±0.02\n+ Scene Graph\n1.00±0.00\n0.41±0.00\n0.71±0.00\n0.33±0.02\n+ Depth Map\n1.00±0.00\n0.30±0.01\n0.69±0.01\n0.46±0.01\n+ Segmentation Map\n1.00±0.00\n0.28±0.01\n0.73±0.02\n0.45±0.00\n+ Chain-of-Thought\n1.00±0.00\n0.63±0.00\n0.76±0.01\n0.19±0.1\n+ Rotation Grid\n1.00±0.00\n0.07±0.01\n0.57±0.01\n0.07±0.01\n+ Rotation Grid Guided\n1.00±0.00\n0.10±0.00\n0.61±0.00\n0.25±0.01\n+ all above\n1.00±0.00\n0.1±0.01\n0.67±0.01\n0.17±0.01\nGPT-4o\nZero-shot\n0.99±0.00\n0.69±0.02\n0.93±0.00\n0.19±0.01\n+ Caption\n0.98±0.00\n0.65±0.00\n0.93±0.01\n0.23±0.02\n+ Bounding Box\n0.98±0.01\n0.59±0.01\n0.91±0.00\n0.31±0.04\n+ Scene Graph\n0.98±0.00\n0.55±0.00\n0.93±0.00\n0.33±0.02\n+ Depth Map\n1.00±0.00\n0.55±0.03\n0.93±0.00\n0.26±0.01\n+ Segmentation Map\n0.97±0.00\n0.67±0.01\n0.95±0.00\n0.21±0.00\n+ Chain-of-Thought\n0.97±0.01\n0.57±0.03\n0.93±0.00\n0.32±0.00\n+ Rotation Grid\n0.98±0.00\n0.71±0.02\n0.93±0.00\n0.19±0.03\n+ Rotation Grid Guided\n0.98±0.00\n0.46±0.03\n0.93±0.00\n0.41±0.00\n+ all above\n1.00±0.00\n0.46±0.03\n0.91±0.00\n0.36±0.02\nTable 1: Classification accuracy using various forms of auxiliary information and prompting for all rotations. All\nresults are obtained from three runs on ROTBENCH-LARGE. Accuracy is scored on a four-way classification task.\n6\nAdditional Analyses\n6.1\nModel Bias Towards 0° and 90°\n0°\n90°\n180°\n270°\nPredicted\n0°\n90°\n180°\n270°\nGround Truth\n885\n12\n0\n3\n27\n349\n65\n459\n25\n24\n846\n5\n22\n424\n68\n386\nPer-Angle Error Analysis (GPT-4o)\n0\n100\n200\n300\n400\n500\n600\n700\n800\nCount\nFigure 4: Confusion matrix of true vs. predicted rota-\ntions for GPT-4o using CoT prompting, summed across\nthree runs on ROTBENCH-LARGE. Rows represent\nground-truth labels, columns represent predicted labels.\nTo further elucidate the specific types of rota-\ntional errors made by models, we analyze the con-\nfusion matrix for GPT-4o. Figure 4 shows the con-\nfusion matrix obtained from summing predictions\nacross three runs. We see GPT-4o predominantly\nstruggles with differentiating between 90° and 270°\nrotations. Specifically, the model misclassifies 459\ninstances of 90° images as 270° and 424 instances\nof 270° images as 90°. Yet, 0° and 180° show sig-\nnificantly fewer misclassifications. This analysis\nunderscores a critical shortcoming in model perfor-\nmance, suggesting either that the vision encoder is\nnot providing sufficient signals to distinguish be-\ntween clockwise and counter-clockwise rotations,\nor that the MLLM is not adequately incorporating\nthe visual information into its reasoning.\n6.2\nDistinguishing Clockwise from\nCounter-clockwise Rotations\nSetup.\nFollowing the analysis in Section 6.1, we\nspecifically examine whether MLLMs can distin-\n7\n\nAccuracy on Different Degrees of Rotation\nModel\n0°\n90°\n180°\n270°\nHuman\nZero-shot\n0.99\n0.99\n0.99\n0.97\nQwen-2.5-VL-7B-Instruct\nZero-shot\n0.95±0.01\n0.57±0.04\n0.03±0.02\n0.15±0.05\n+ Chain-of-Thought\n0.86±0.00\n0.20±0.04\n0.13±0.03\n0.30±0.04\nGPT-4o\nZero-shot\n0.87±0.02\n0.65±0.04\n0.85±0.01\n0.21±0.03\n+ Chain-of-Thought\n0.91±0.01\n0.59±0.01\n0.86±0.00\n0.21±0.05\nGPT-4.1\nZero-shot\n0.95±0.01\n0.63±0.07\n0.85±0.03\n0.19±0.09\n+ Chain-of-Thought\n0.98±0.00\n0.88±0.00\n0.85±0.01\n0.03±0.03\nGPT-5\nZero-shot\n1.00±0.00\n0.41±0.03\n0.81±0.01\n0.59±0.04\n+ Chain-of-Thought\n1.00±0.00\n0.53±0.05\n0.81±0.02\n0.57±0.07\nGemini-2.0-Flash\nZero-shot\n1.00±0.00\n0.25±0.04\n0.48±0.01\n0.43±0.07\n+ Chain-of-Thought\n1.00±0.00\n0.61±0.01\n0.59±0.01\n0.25±0.01\nGemini-2.5-Flash\nZero-shot\n1.00±0.00\n0.23±0.06\n0.50±0.03\n0.47±0.02\n+ Chain-of-Thought\n1.00±0.00\n0.23±0.01\n0.44±0.02\n0.40±0.02\no3\nZero-shot\n1.00±0.00\n0.45±0.04\n0.70±0.03\n0.48±0.03\n+ Chain-of-Thought\n1.00±0.00\n0.36±0.00\n0.74±0.04\n0.57±0.01\n+ Rotation Grid\n0.99±0.01\n0.23±0.05\n0.83±0.05\n0.81±0.01\n+ Rotation Grid Guided\n0.99±0.01\n0.31±0.01\n0.82±0.02\n0.75±0.01\nGemini-2.5-Pro\nZero-shot\n1.00±0.00\n0.50±0.04\n0.72±0.00\n0.40±0.06\n+ Chain-of-Thought\n1.00±0.00\n0.46±0.02\n0.71±0.01\n0.49±0.01\n+ Rotation Grid\n0.99±0.01\n0.58±0.00\n0.67±0.03\n0.59±0.03\n+ Rotation Grid Guided\n0.95±0.01\n0.71±0.01\n0.73±0.03\n0.74±0.04\nTable 2: Classification accuracy of different image rotations for various models across two runs on ROTBENCH-\nSMALL using zero-shot or chain-of-thought prompting. We also show results from o3 and Gemini-2.5-Pro using\nrotation grids. Accuracy is scored on a four-way classification task.\nccw\nThe rotation is 90°\nclockwise (cw) or\ncounter-clockwise\n(ccw)?\nFigure 5: GPT-4o answers incorrectly when asked to\nidentify whether the image has been rotated 90° clock-\nwise or counter-clockwise.\nguish clockwise (CW) and counter-clockwise rota-\ntions (CCW). For this, we simplified the original\nfour-way classification task into a binary classifi-\ncation task. Using the 90° and 270° images from\nROTBENCH-LARGE, we ask MLLMs to determine\nwhether an image has been rotated 90° CCW or\nCW.6 By asking models to explicitly differentiate\nthe two directions of rotation, we hope to provide\na clearer signal for directional understanding. Fig-\nure 5 shows an example question where the GPT-4o\nprovides an incorrect answer.\nResults.\nTable 3 tests CW vs. CCW classifica-\ntion on GPT-4o and Qwen-2.5-VL-7B-Instruct (we\nadditionally provide similar smaller-scale tests on\nROTBENCH-SMALL for GPT-5 in Appendix E).\nTable 3 indicates that GPT-4o has a significant\nbias toward identifying rotations as 90° counter-\nclockwise. GPT-4o correctly identified only 52 out\nof 300 clockwise rotations, whereas Qwen-2.5-VL-\n7B-Instruct correctly identified only 23 out of 300\nclockwise rotations. The models consistently de-\nfault to labeling ambiguous or uncertain rotations\nas counter-clockwise, hinting towards a potential\n6Note that 90° CW is equivalent to a 270° CCW.\n8\n\nGT \\ Predicted\nCounter-clockwise\nClockwise\nGPT-4o\nCounter-clockwise\n248\n52\nClockwise\n259\n41\nQwen-2.5-VL-7B-Instruct\nCounter-clockwise\n270\n30\nClockwise\n277\n23\nTable 3: Accuracy of different models in identifying\n90° clockwise (CW) versus counter-clockwise rotation\n(CCW). Column indicates ground truth rotation (GT).\nA small number of responses are incorrectly identified\nas being right-side up. These responses are excluded in\nthe table’s statistics.\nunderlying perceptual bias (Appendix F). While\nwe find that the most recent model tested, GPT-5,\nhas less bias and is better able to identify clockwise\nrotations, this improvement is not reflected in Ta-\nble 2, where the model performs on par with others;\nin other words, poor performance on ROTBENCH\ncannot be solely explained by a model’s ability to\ndistinguish clockwise from counterclockwise rota-\ntion. See Appendix E for further discussion.\nThese findings strongly indicate that the MLLMs\ntested have limitations in reliably distinguishing be-\ntween CW and CCW rotational directions, offering\na potential explanation for why distinguishing 90°\nand 270° CCW rotations presents such a challeng-\ning task. Further corroborating this idea, we show\nthat evaluating Qwen-2.5-VL-7B-Instruct, GPT-4o,\no3 on ROTBENCH-SMALL using clockwise angles\ndoes not result in significant performance differ-\nences (Appendix D).\n6.3\nCan Fine-tuning Solve ROTBENCH?\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nNumber of images seen\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nAccuracy on RotBench-large\n0°\n90°\n180°\n270°\nFigure 6: Qwen-2.5-VL-7B-Instruct’s accuracy on dif-\nferent degrees of rotation as training progresses.\nSetup.\nTo assess whether specialized training\ncan mitigate these performance issues, we conduct\nfine-tuning experiments using Qwen-2.5-VL-7B-\nInstruct. During data filtering, we find some im-\nages in Spatial-MM closely resemble each other\n(Fig. 11). If we fine-tune on images in Spatial-\nMM not selected for ROTBENCH, the existence\nof similar images in the training and testing sets\nmay lead to inflated performance. To prevent such\ninstances of overfitting, we train on 1000 images\nfrom MS COCO (Lin et al., 2015) and evaluate\nperformance on ROTBENCH-LARGE. Appendix B\nprovides further training details.\nResults.\nFig. 6 reveals a high and consistent ac-\ncuracy for 0° throughout training, indicating robust\nrecognition of upright images. Performance on\n180° gradually improves, stabilizing around 0.8\nafter approximately 7000 images. However, accu-\nracies for 90° and 270° exhibit substantial oscilla-\ntions, suggesting that the model did not achieve sta-\nble improvement. The model appears to be caught\nin a cycle, alternating between accuracy gains and\nlosses for these two rotations. This phenomenon\nis also reflected in our main results (Section 5),\nwhere using CoT prompting improved accuracy on\n270° images at the expense of 90°. The unstable\nperformance may result from potential represen-\ntational constraints in current visual encoders that\nlimit visual understanding capabilities, particularly\nregarding subtle rotational distinctions.\n7\nConclusion\nWe evaluate whether MLLMs are able to identify\ninput images rotated 0°, 90°, 180°, and 270° us-\ning ROTBENCH, a 350-image manually-filtered\nbenchmark. Our results reveal that state-of-the-\nart MLLMs reliably identify images that are un-\nrotated (0°) or upside-down (180°), but struggle\nwith 90° and 270° rotations. Auxiliary informa-\ntion and chain-of-thought prompting provide lim-\nited improvements. Simultaneously providing all\npossible rotations improves performance only for\nfrontier reasoning models, but a modified setup\nusing majority voting improves performance for\nweaker models as well. Fine-tuning Qwen-2.5-\nVL-7B-Instruct shows that performance oscillates\nbetween 90° and 270°, suggesting the presence of\ntwo local optima. These results indicate a poten-\ntial blind spot in MLLMs’ spatial reasoning capa-\nbilities, motivating future directions in improving\norientational understanding.\nAcknowledgements\nThis work was supported by DARPA ECOLE\nProgram No. HR00112390060, NSF-CAREER\n9\n\nAward 1846185, NSF-AI Engage Institute DRL-\n2112635, ARO Award W911NF2110220, ONR\nGrant N00014-23-1-2356, and a Bloomberg Data\nScience PhD Fellowship. The views contained in\nthis article are those of the authors and not of the\nfunding agency."}
