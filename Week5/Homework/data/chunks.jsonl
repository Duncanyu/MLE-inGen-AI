{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 0, "text": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs Haokun Lin∗1,3, Haobo Xu∗2, Yichen Wu3,4, Ziyu Guo5, Renrui Zhang5 , Zhichao Lu3, Ying Wei6, Qingfu Zhang3, Zhenan Sun1 ∗Equal Contribution 1 NLPR & MAIS, Institute of Automation, CAS 2 Tsinghua University 3 City University of Hong Kong 4 Harvard University 5 The Chinese University of Hong Kong 6 Zhejiang University Abstract Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantiza- tion (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community. 1 Introduction Large language models (LLMs) have achieved remarkable success in a wide range of text generation tasks, with auto-regressive architectures—such as GPT [Brown et al., 2020a,b, Achiam et al., 2023], LLaMA [Touvron et al., 2023a,b, Dubey et al., 2024], and the Qwen [Bai et al., 2023, Qwen et al., 2025, Yang et al., 2025a] series—dominating recent advances in both research and application. Recently, diffusion-based large language models (dLLMs) have emerged as a promising alternative for natural language generation [Nie et al., 2025, Zhu et al., 2025, Ye et al., 2025a, Gong et al., 2024, Song et al., 2025]. By leveraging bidirectional context encoding and iterative denoising, dLLMs offer finer-grained control over the generation process compared to traditional auto-regressive approaches. Despite their potential, the efficient deployment of dLLMs remains challenging, as the increased number of model parameters often leads to significantly higher memory usage and computational cost [Li et al., 2025, Yu et al., 2025]. Current efforts toward optimizing dLLM inference have primarily focused on designing specialized key-value (KV) cache mechanisms [Wu et al., 2025, Ma et al., 2025, Liu et al., 2025b, Wang Technical Report (a2) LLaDA_8B_Ins_Layer17_Attn_q_proj (b2) LLaDA_8B_Ins_Layer27_Attn_out_proj (c2) LLaDA_8B_Ins_Layer19_FFN_ff_proj (d2) LLaDA_8B_Ins_Layer31_FFN_ff_out (2). LLaDA-8B-Instruct Massive (a1) LLaDA_8B_Layer1_Attn_q_proj (b1) LLaDA_8B_Layer27_Attn_out_proj (c1) LLaDA_8B_Layer31_FFN_ff_proj (d1) LLaDA_8B_Layer29_FFN_ff_out (1). LLaDA-8B-Base Massive Figure 1: Visualizations of activation outliers in LLaDA-8B-Base (1) and LLaDA-8B-Instruct (2). Outliers are observed at the inputs of various linear layers and can be classified as Normal Outliers (a(1)–c(1)/a(2)–c(2)), with relatively large"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 1, "text": "LLaDA_8B_Ins_Layer19_FFN_ff_proj (d2) LLaDA_8B_Ins_Layer31_FFN_ff_out (2). LLaDA-8B-Instruct Massive (a1) LLaDA_8B_Layer1_Attn_q_proj (b1) LLaDA_8B_Layer27_Attn_out_proj (c1) LLaDA_8B_Layer31_FFN_ff_proj (d1) LLaDA_8B_Layer29_FFN_ff_out (1). LLaDA-8B-Base Massive Figure 1: Visualizations of activation outliers in LLaDA-8B-Base (1) and LLaDA-8B-Instruct (2). Outliers are observed at the inputs of various linear layers and can be classified as Normal Outliers (a(1)–c(1)/a(2)–c(2)), with relatively large magnitudes across tokens, and Massive Outliers (d(1), d(2)), with extremely large values on a few tokens. Notably, these massive outliers are identified at the second linear layer of the feed-forward network (FFN) module. et al., 2025]. However, quantization [Li et al., 2024, Liu et al., 2025a, Wei et al., 2025], a well- established yet orthogonal technique for compressing and accelerating neural networks, has been largely underexplored in the context of dLLMs. In the domain of auto-regressive LLMs, post-training quantization(PTQ) [Chee et al., 2024, Ashkboos et al., 2023, Tseng et al., 2024, Zhao et al., 2023] has been widely adopted to reduce the memory footprint of weights and activations, and to enable faster inference through kernel-level optimization. Yet, how well existing PTQ techniques generalize to diffusion LLMs remains an open and intriguing question. In this paper, we present a comprehensive study on the quantization of diffusion-based large language models (dLLMs). First, we identify that dLLMs exhibit clear activation outliers—i.e., unusually large activation values—which are known to be a key challenge for low-bit quantization [Dettmers et al., 2022, Xiao et al., 2023, Sun et al., 2024]. Specifically, as shown in Figure 1 and 2, we observe such outliers across multiple layers and input activations in LLaDA-Base, LLaDA-Instruct [Nie et al., 2025], and Dream [Ye et al., 2025a] models, suggesting that this is a common phenomenon across different dLLMs. Second, we implement state-of-the-art weight-only [Lin et al., 2023, Frantar et al., 2022] and weight-activation quantization [Xiao et al., 2023, Ashkboos et al., 2024, Lin et al., 2024b] methods on representative diffusion models and conduct a detailed analysis from the following perspectives: • Bit-width effects: We find that 4-bit is the most effective configuration for weight-only quantiza- tion, while 8-bit is recommended for weight-activation quantization as a near-lossless setting. • Quantization methods: Through extensive evaluation, we observe that GPTQ consistently outper- forms AWQ across most tasks. For weight-activation quantization, rotation-based methods such as DuQuant and QuaRot demonstrate clear advantages over SmoothQuant. • Task type sensitivity: While most PTQ methods perform competitively on general QA benchmarks, we observe notable degradation on more complex tasks such as math reasoning and code generation. • Model type robustness: Our results show that the instruction-tuned LLaDA model exhibits greater robustness to quantization compared to the base counterpart. To the best of our knowledge, this is the first systematic evaluation of post-training quantization on diffusion LLMs. We hope our findings provide valuable guidance for the community and inspire further research toward efficient and deployable dLLMs. 2 (a) Dream_7B_Layer1_Attn_q_proj (b) Dream_7B_Layer25_Attn_o_proj (c) Dream_7B_Layer27_MLP_up_proj (d) Dream_7B_Layer4_MLP_down_proj Dream-7B-Base Massive Figure 2: Visualizations of activation outliers in Dream-7B-Base. We observe relatively large normal outliers in the input to the FFN up-projection layer (c), while the massive outliers (d) exhibit smaller peak values compared to those in"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 2, "text": "and deployable dLLMs. 2 (a) Dream_7B_Layer1_Attn_q_proj (b) Dream_7B_Layer25_Attn_o_proj (c) Dream_7B_Layer27_MLP_up_proj (d) Dream_7B_Layer4_MLP_down_proj Dream-7B-Base Massive Figure 2: Visualizations of activation outliers in Dream-7B-Base. We observe relatively large normal outliers in the input to the FFN up-projection layer (c), while the massive outliers (d) exhibit smaller peak values compared to those in LLaDA models (Figure 1). 2 Related Work 2.1 Diffusion Language Model Diffusion models have achieved remarkable success in image, video, and audio generation by learning to reverse a forward noise process [Jiang et al., 2025, Zhao et al., 2024a]. However, applying diffusion to language generation presents unique challenges due to the discrete nature of textual data. To address this, DiffusionBERT [He et al., 2022] leverages a BERT [Devlin et al., 2019] architecture to model the reverse dynamics of a discrete diffusion process with an absorbing state, as proposed in D3PM [Austin et al., 2021a]. More recently, Masked Diffusion Models (MDMs) [Lou et al., 2023, Ou et al., 2024, Shi et al., 2024] have drawn increasing attention by adopting a forward process that progressively replaces input tokens with a designated [MASK] token. This year, efforts have been made to scale up MDMs to the billion-parameter regime. Representative examples include LLaDA-8B [Nie et al., 2025], which utilizes a bidirectional Transformer as the mask denoiser and achieves performance comparable to LLaMA [Dubey et al., 2024], and Dream [Ye et al., 2025a], which is initialized from a pre-trained autoregressive model and delivers competitive generation capabilities. These advancements indicate that diffusion-based approaches offer a viable alternative paradigm for language modeling. Despite these encouraging results, the deployment of diffusion large language models (dLLMs) [Gong et al., 2024, Yang et al., 2025c] remains constrained by the computational demands of Transformer- based architectures, which involve hundreds of millions of parameters. To address this, we explore the potential of extending established post-training quantization techniques from conventional LLMs to the dLLM models, aiming to reduce memory footprint and accelerate inference while preserving generation quality. Notably, some recent works [Wu et al., 2025, Liu et al., 2025b, Ma et al., 2025] propose caching strategies to accelerate the inference of dLLMs. Our work is orthogonal to these efforts and can be seamlessly integrated by quantizing dLLM caches. 2.2 Network Quantization Compared to pruning and distillation [Lin et al., 2024a, Zhang et al., 2024, Xing et al., 2025, Ye et al., 2025c,b], quantization has been extensively studied as an effective technique to compress neural networks by using low-bit representations for high-precision tensors [Zhao et al., 2024b, Xu et al., 2024, Yang et al., 2025b, Huang and Wu, 2025]. Existing methods are typically categorized into two groups: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ [Wu et al., 2024, Yang et al., 2024] applies quantization after model training, while QAT [Tao et al., 2022, Chen et al., 2024, 2025] incorporates quantization effects during training. Due to the high computational cost of training large language models (LLMs), PTQ has become increasingly popular for its efficiency and ability to preserve model performance without retraining [Liu et al., 2024, Dong et al., 2024]. In this work, we follow this"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 3, "text": "al., 2024, 2025] incorporates quantization effects during training. Due to the high computational cost of training large language models (LLMs), PTQ has become increasingly popular for its efficiency and ability to preserve model performance without retraining [Liu et al., 2024, Dong et al., 2024]. In this work, we follow this paradigm and focus on applying PTQ to dLLMs. Weight-only quantization compresses the model by quantizing weight matrices, effectively reducing model size and memory access during inference. For example, GPTQ [Frantar et al., 2022] extends the Optimal Brain Quantization [Frantar and Alistarh, 2022] algorithm to LLMs, AWQ [Lin et al., 3 2023] introduces a reparameterization strategy to alleviate the difficulty of weight quantization, and SqueezeLLM [Kim et al., 2023] employs non-uniform quantization to improve compression quality. Weight-activation quantization quantizes both the model weights and input activations, enabling further inference acceleration by leveraging integer matrix multiplication kernels. SmoothQuant [Xiao et al., 2023] proposes to shift the quantization difficulty from activations to weights via scaling. OmniQuant [Shao et al., 2023] jointly optimizes clipping thresholds and scaling factors for improved quantization fidelity. More recently, rotation-based methods [Lin et al., 2024c, Hu et al., 2025] have demonstrated superior performance: QuaRot [Ashkboos et al., 2024] introduces Hadamard-based rotation to smooth the weight-activation landscape, while DuQuant [Lin et al., 2024b] leverages outlier-aware rotation matrices and channel permutation to better align the activation distribution with quantization-friendly structures. In this work, we provide a comprehensive evaluation of state-of-the-art LLM-oriented PTQ methods applied to diffusion-based language models. All methods are re-implemented on dLLMs, and we present in-depth analyses and insights into their quantization performance. 3 Preliminary and Observation 3.1 Masked Diffusion Model Masked diffusion model is a variant of diffusion-based generative models that incorporates a binary mask into the denoising process. Instead of reconstructing the entire input, the model focuses on predicting the corrupted or missing regions while preserving the observed parts. Specifically, given an input x and a mask m, the forward process adds Gaussian noise to the unmasked regions, producing a noised sample xt at step t. The reverse process is then parameterized by a neural network ϵθ, which estimates the noise conditioned on both the timestep and the mask. The training objective is, LMDM = Ex,m,ϵ,t \u0002 ∥ϵ −ϵθ(xt, m, t)∥2 \u0003 , where ϵ denotes the Gaussian noise, and ϵθ learns to predict and remove it under the masking constraint. 3.2 Quantization Quantization coverts the floating-point tensor X into a low-bit integer Xq. Specifically, the b-bit uniform quantization can be represented as: Xq = clamp \u0012\u0016X s \u0019 +z, 0, 2b −1 \u0013 , where s = max(X) −min(X) 2b −1 , z = − \u0016min(X) s \u0019 . (1) The notation ⌊·⌉means the nearest rounding operation, s is the quantization step size and z denotes the zero point. 3.3 Outliers in dLLMs Outliers, a prominent characteristic of large language models (LLMs), are primarily determined by relatively large activation values [Dettmers et al., 2022]. These outliers are typically categorized into two types: normal outliers and massive outliers [Lin et al., 2024b]. Normal outliers [Xiao et al., 2023] refer"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 4, "text": "point. 3.3 Outliers in dLLMs Outliers, a prominent characteristic of large language models (LLMs), are primarily determined by relatively large activation values [Dettmers et al., 2022]. These outliers are typically categorized into two types: normal outliers and massive outliers [Lin et al., 2024b]. Normal outliers [Xiao et al., 2023] refer to activations across all tokens with relatively large magnitudes, and they are the more prevalent type. Massive outliers [Sun et al., 2024, Liu et al., 2024], on the other hand, exhibit significantly larger values at a limited set of tokens. These outliers present substantial challenges for LLM quantization. Whether dLLMs contain these outliers remains an important yet under-explored question. In this work, we provide a detailed preliminary exploration and identify the presence of outliers in dLLMs. We first identify the presence of activation outliers in diffusion-based language models. Specifically, we randomly sample a batch of calibration data from the WikiText-2 dataset [Merity et al., 2016] and use it as input for a single forward pass to visualize the activation distributions across different 4 layers. As shown in Figure 1, we observe clear outliers in the input activations of both LLaDA-8B- base and LLaDA-8B-instruct. These outliers can be categorized into two types: Normal Outliers and Massive Outliers, consistent with the taxonomy observed in standard LLMs. Interestingly, the Massive Outliers tend to occur in the second linear layer of the feed-forward network (FFN) modules, mirroring patterns reported in previous studies on conventional LLMs [An et al., 2025]. However, compared to LLMs, the Normal Outliers in LLaDA exhibit slightly lower magnitudes, indicating a less extreme but still significant deviation. Another key difference is that massive outliers in dLLMs appear across more tokens, rather than being restricted to only a few tokens as in LLMs. This broader distribution increases the difficulty of weight-activation quantization, as it reduces the effectiveness of global clipping or scaling strategies. This observation is corroborated by the near-zero performance of SmoothQuant under W4A4 settings (see Table 4), suggesting that existing outlier-handling strategies may be insufficient for dLLMs in low-bit quantization regimes. Furthermore, we also detect similar outlier patterns in the Dream-7B model, as visualized in Figure 2. This indicates that the existence of outliers is not specific to a particular model architecture, but rather a general phenomenon across diffusion-based language models. These findings highlight the need for careful handling of outliers during the quantization process, especially when targeting both weights and activations under aggressive bit-width constraints. 4 Quantizing Diffusion LLM In this section, we conduct experiments to address the overarching question: How does quantization affect diffusion-based language models? To systematically explore this, we further investigate the following sub-questions: • RQ1: What are the preferred bit-widths for weight-only and weight-activation quantization? • RQ2: What are the most effective quantization methods for dLLMs? • RQ3: How do different task categories influence the performance of quantized dLLMs? • RQ4: How does quantization affect different types of dLLMs? 4.1 Experimental Setup Evaluated dLLMs and Quantization Baselines. We conduct comprehensive evaluations on three recent diffusion-based language models, LLaDA-8B-Base, LLaDA-8B-Instruct [Nie et al., 2025] and Dream 7B-Base [Ye"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 5, "text": "RQ3: How do different task categories influence the performance of quantized dLLMs? • RQ4: How does quantization affect different types of dLLMs? 4.1 Experimental Setup Evaluated dLLMs and Quantization Baselines. We conduct comprehensive evaluations on three recent diffusion-based language models, LLaDA-8B-Base, LLaDA-8B-Instruct [Nie et al., 2025] and Dream 7B-Base [Ye et al., 2025a]. For weight-only quantization, we adopt state-of-the-art baselines GPTQ [Frantar et al., 2022] and AWQ [Lin et al., 2023], which are widely used in LLM quantization. We utilize group-wise per-channel quantization and set the group size to 128. For weight-activation quantization, we evaluate SmoothQuant [Xiao et al., 2023] as well as recent rotation-based approaches, including QuaRot [Ashkboos et al., 2024] and DuQuant [Lin et al., 2024b]. Following standard practice, we apply per-channel quantization to weights and per-token quantization to activations. We select calibration data (128 samples) from WiKiText2 [Merity et al., 2016] for baselines, except Pile [Gao et al., 2020] for AWQ. More details are illustrated in Appendix A. Evaluation Benchmarks. We evaluate the performance of quantized dLLMs across three task categories, following the setup of LLaDA [Nie et al., 2025]: 1). General knowledge tasks, including MMLU [Hendrycks et al., 2020], ARC-E, ARC-C [Clark et al., 2018], Hellaswag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], and PIQA [Bisk et al., 2020]; 2). Mathematical reasoning tasks, such as GSM8K [Cobbe et al., 2021] and Math [Hendrycks et al., 2021]; and 3).Code generation tasks, including HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021b]. These benchmarks collectively provide a comprehensive assessment of quantized dLLMs from multiple perspectives. Evaluation Metrics. We report accuracy on widely used QA and math benchmarks, and adopt Pass@1 as the evaluation metric for code generation tasks. Performance degradation relative to full- precision models is used as the primary metric for assessing different quantized dLLMs. Following [Liu et al., 2025a], we categorize the performance degradation compared to full-precision models into three levels: negligible(<1%), moderate (1–4%), and significant (>4%). 5 Table 1: Model performance on general tasks under weight-only quantization. Model Setting Method WinoGrande PIQA ARC-C ARC-E Hellaswag MMLU 5-shot Avg Drop LLaDA-8B FP Model - 69.9 74.6 46.4 71.1 70.7 65.7 65.5 - W4A16 g128 GPTQ 69.7 73.9 47.9 72.5 70.4 64.7 65.3 ↓0.3% AWQ 67.3 70.3 44.5 73.4 68.4 65.6 63.2 ↓3.5% W3A16 g128 GPTQ 67.2 73.3 45.7 71.1 68.8 63.5 63.7 ↓2.7% AWQ 66.4 69.2 42.8 71.8 66.4 64.0 61.8 ↓5.6% LLaDA-8B -Instruct FP Model - 70.2 71.3 54.3 75.9 68.6 64.0 65.7 - W4A16 g128 GPTQ 69.2 74.2 54.8 77.5 68.3 63.4 66.0 ↑0.3% AWQ 68.8 71.0 53.3 76.1 68.1 63.4 64.9 ↓0.1% W3A16 g128 GPTQ 67.4 73.7 50.7 76.4 66.7 62.1 64.1 ↓2.4% AWQ 66.3 69.8 50.5 74.7 66.4 62.3 63.1 ↓4.0% Table 2: Model performance on general tasks under weight-activation quantization. Model Setting Method WinoGrande PIQA ARC-C ARC-E MMLU Avg Drop LLaDA-8B FP Model - 69.9 74.6 46.4 71.1 65.7 65.5 - W8A8 SmoothQuant 67.7 70.8 45.5 70.5 65.0 63.9 ↓2.5% QuaRot 68.6 71.1 45.2 70.8 66.1 64.4 ↓1.8% DuQuant 67.9 70.4 45.9 71.4 66.0 64.3 ↓1.9% W4A4 SmoothQuant 49.4"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 6, "text": "quantization. Model Setting Method WinoGrande PIQA ARC-C ARC-E MMLU Avg Drop LLaDA-8B FP Model - 69.9 74.6 46.4 71.1 65.7 65.5 - W8A8 SmoothQuant 67.7 70.8 45.5 70.5 65.0 63.9 ↓2.5% QuaRot 68.6 71.1 45.2 70.8 66.1 64.4 ↓1.8% DuQuant 67.9 70.4 45.9 71.4 66.0 64.3 ↓1.9% W4A4 SmoothQuant 49.4 58.8 29.2 40.9 27.1 41.1 ↓37.3% QuaRot 63.4 68.1 43.7 69.2 61.8 59.2 ↓6.6% DuQuant 64.9 69.3 42.8 70.0 64.0 62.2 ↓5.1% LLaDA-8B -Instruct FP Model - 70.2 71.3 54.3 75.9 64.0 67.1 - W8A8 SmoothQuant 69.6 72.1 53.5 75.9 64.0 67.0 ↓0.2% QuaRot 69.1 71.3 54.1 76.2 64.1 67.0 ↓0.3% DuQuant 68.8 71.8 54.6 76.2 63.6 67.0 ↓0.2% W4A4 SmoothQuant 52.3 65.1 34.0 54.0 32.8 47.7 ↓29.0% QuaRot 65.2 69.8 51.3 75.1 61.1 64.5 ↓3.9% DuQuant 66.4 72.2 52.7 74.8 61.2 65.4 ↓2.5% 4.2 Ideal Quantization Bit Precision (RQ1) 4-bit is the Recommended Choice for Weight-Only Quantization. We observe that both GPTQ and AWQ perform well on general commonsense QA and math tasks under 4-bit quantization (Table 1 and Table 3). In most cases, the performance degradation remains within the negligible to moderate range (i.e., <4%). For example, 4-bit GPTQ-quantized LLaDA-8B-instruct slightly improves the average accuracy on six QA tasks from 65.7% to 66.0%, and shows only a minor drop of 0.6% on the MATH and GSM8K benchmarks. In contrast, reducing the quantization bit-width to 3-bit leads to a significant performance drop, particularly on math and code generation tasks, as shown in Table 3. Therefore, we recommend 4-bit quantization as the standard configuration for weight-only quantization of diffusion-based LLMs. The development of more robust 3-bit quantization methods remains an open research direction. Weight-Activation Quantization: 8-bit is Tolerable, While 4-bit Remains Challenging. As shown in Table 2 and Table 4, quantizing LLaDA models to W8A8 results in only minor performance degradation, largely independent of the specific quantization method. This suggests that even simple techniques such as SmoothQuant are effective in mitigating activation outliers in LLaDA models, leading to nearly lossless quantized variants. However, reducing precision to W4A4 introduces a sharp performance drop across most benchmarks. In the majority of cases, performance degradation exceeds the significant threshold (>4%). For instance, SmoothQuant experiences a drop of over 20% across all evaluated tasks, indicating that the simple rebalancing between weights and activations is insufficient under low-precision settings for dLLMs. The degradation is especially pronounced in base models, with accuracy drops exceeding 10% on code generation tasks and math reasoning-heavy benchmarks. These results highlight the difficulty of achieving effective 4-bit weight-activation quantization in dLLMs, and point to the need for more advanced techniques. Improving performance under this challenging setting remains an open research problem for the community. 6 Table 3: Model performance on mathematics and code tasks under weight-only quantization. Model Setting Method GSM8K (4-shot) Math (0-shot) Avg Drop HumanEval (0-shot) MBPP (3-shot ) Avg Drop Gen Len 256 Gen Len 256 Gen Len 512 Gen Len 512 LLaDA-8B FP Model - 69.7 21.3 45.5 - 32.9 39.4 36.2 - W4A16 g128 GPTQ 68.5 21.3 44.9 ↓1.4% 28.7 39.4 34.0 ↓5.9% AWQ 67.4 20.6 44.0 ↓3.2%"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 7, "text": "(0-shot) Avg Drop HumanEval (0-shot) MBPP (3-shot ) Avg Drop Gen Len 256 Gen Len 256 Gen Len 512 Gen Len 512 LLaDA-8B FP Model - 69.7 21.3 45.5 - 32.9 39.4 36.2 - W4A16 g128 GPTQ 68.5 21.3 44.9 ↓1.4% 28.7 39.4 34.0 ↓5.9% AWQ 67.4 20.6 44.0 ↓3.2% 29.9 37.2 33.5 ↓7.3% W3A16 g128 GPTQ 63.3 13.4 38.4 ↓15.7% 26.2 35.4 30.8 ↓14.8% AWQ 64.3 17.0 40.6 ↓10.7% 28.1 34.2 31.1 ↓13.9% LLaDA-8B -Instruct FP Model - 78.5 33.5 56.0 - 37.8 37.4 37.6 W4A16 g128 GPTQ 78.8 32.4 55.6 ↓0.6% 36.6 33.8 35.2 ↓6.4% AWQ 78.9 33.6 56.2 ↑0.5% 37.1 35.6 36.4 ↓3.2% W3A16 g128 GPTQ 76.4 30.0 53.2 ↓5.0% 34.2 30.0 32.1 ↓14.7% AWQ 76.3 30.1 53.2 ↓5.0% 34.1 31.8 33.0 ↓12.4% Table 4: Model performance on mathematics and code tasks under weight-activation quantization. Model Setting Method GSM8K (4-shot) Math (0-shot) Avg Drop HumanEval (0-shot) MBPP (3-shot ) Avg Drop Gen Len 256 Gen Len 256 Gen Len 512 Gen Len 512 LLaDA-8B FP Model - 69.7 21.3 45.5 - 32.9 39.4 36.2 - W8A8 SmoothQuant 69.4 20.2 44.8 ↓1.6% 27.4 40.2 33.8 ↓6.5% QuaRot 69.9 20.7 69.9 ↓0.4% 31.7 40.6 36.2 ↓0.0% DuQuant 70.7 20.7 45.7 ↑0.4% 33.5 38.8 36.2 ↓0.0% W4A4 SmoothQuant 0.3 2.0 1.2 ↓97.4% 0.0 0.0 0.0 ↓100.0% QuaRot 62.9 15.2 62.9 ↓14.1% 23.8 34.6 29.2 ↓19.3% DuQuant 64.4 14.8 39.6 ↓13.0% 25.6 33.6 29.6 ↓18.1% LLaDA-8B -Instruct FP Model - 78.5 33.5 56.0 - 37.8 37.4 37.6 - W8A8 SmoothQuant 78.2 33.3 55.7 ↓0.4% 37.2 37.1 38.6 ↓1.3% QuaRot 78.9 33.1 78.9 ↑0.1% 35.4 36.6 36.0 ↓4.3% DuQuant 78.1 33.3 55.7 ↓0.5% 37.2 37.4 37.3 ↓0.8% W4A4 SmoothQuant 2.7 2.4 2.6 ↓95.4% 0.0 0.6 0.3 ↓99.2% QuaRot 75.1 29.9 75.1 ↓6.2% 32.3 32.8 32.6 ↓13.4% DuQuant 77.3 30.7 54.0 ↓3.5% 34.8 29.2 32.0 ↓14.9% 4.3 Optimal Quantization Methods (RQ2) GPTQ Outperforms AWQ on Most Tasks As shown in Table 1, GPTQ outperforms AWQ on average accuracy under both 3-bit and 4-bit quantization for LLaDA-8B and LLaDA-8B-instruct. This demonstrates the reliability and competitiveness of GPTQ, particularly on QA tasks. This trend also holds for math reasoning tasks, except for the 3-bit quantization setting on LLaDA-8B, where both GPTQ and AWQ suffer critical performance degradation (>10%). We hypothesize that the suboptimal performance of AWQ may stem from the fact that activation outliers in the LLaDA model series are less prominent than in traditional LLMs. Since AWQ identifies the top 1% of salient weights using activation-driven statistics, its effectiveness can be reduced when the outlier structure is weak in LLaDA models, thereby diminishing its advantage. For code generation tasks, the situation becomes more complex. Both GPTQ and AWQ fail to maintain acceptable performance on the HumanEval and MBPP benchmarks under low-bit quantization. A more detailed analysis of these results is provided in Section 4.4. Notably, AWQ performs relatively better than GPTQ in the 3-bit configuration for code tasks, suggesting some resilience under extreme compression. Considering all evaluations across task types and bit-widths, we recommend GPTQ as the safer and more generally effective choice for weight-only quantization"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 8, "text": "of these results is provided in Section 4.4. Notably, AWQ performs relatively better than GPTQ in the 3-bit configuration for code tasks, suggesting some resilience under extreme compression. Considering all evaluations across task types and bit-widths, we recommend GPTQ as the safer and more generally effective choice for weight-only quantization of diffusion-based language models. Rotation-Based Methods Achieve Leading Performance Under Weight-Activation Quanti- zation. For both LLaDA-8B and LLaDA-8B-instruct, rotation-based methods—QuaRot and DuQuant—consistently outperform SmoothQuant across all evaluation tasks and quantization settings. The advantage becomes especially pronounced under 4-bit weight-activation quantization, where SmoothQuant suffers a near-complete performance collapse on code and math tasks. In contrast, rotation-based approaches retain a non-trivial portion of model capability, highlighting their robust- ness in low-precision settings. These results suggest that rotation transformations are more effective in mitigating activation outliers in dLLMs, which aligns with findings from prior studies in the LLM community [Lin et al., 2024b, Ashkboos et al., 2024]. When comparing QuaRot and DuQuant in detail, our experiments show that DuQuant consistently outperforms QuaRot across most scenarios. For instance, on commonsense QA tasks, DuQuant achieves lower performance drops than QuaRot for both LLaDA-8B (5.1% vs. 6.6%) and LLaDA-8B-instruct (2.5% vs. 3.9%). This observation remains consistent across math and code generation tasks. Consequently, we recommend DuQuant as the most effective method for weight-activation quantization in diffusion-based language models. 7 Table 5: Evaluation of weight-only quantized Dream-7B on general tasks. Model Setting Method WinoGrande PIQA ARC-C ARC-E Avg Drop Dream-7B FP Model - 68.4 74.4 59.0 83.1 71.2 - W4A16 g128 GPTQ 68.2 73.9 58.1 82.1 70.6 ↓0.8% AWQ 65.2 69.6 55.8 82.0 68.2 ↓4.3% W3A16 g128 GPTQ 63.3 69.6 49.9 73.4 64.1 ↓10.1% AWQ 62.8 67.7 50.6 74.5 63.9 ↓10.3% 4.4 Influence of Task Categories on Quantization (RQ3) Quantization is More Challenging for Math and Code Tasks. Compared to general-purpose benchmarks—primarily QA tasks as shown in Table 1 and Table 2—quantized models experience significantly larger performance drops on math and code tasks, illustrated in Table 3 and Table 4. For math reasoning tasks, both AWQ and GPTQ exhibit substantial degradation under 3-bit quanti- zation (see Table 3), despite maintaining competitive performance on general QA benchmarks. A similar trend is observed for rotation-based methods under W4A4 configurations. This degradation may be attributed to the multi-step reasoning nature of math problems, which amplifies the cumulative effect of quantization errors. In such tasks, precise intermediate representations are critical; even small perturbations introduced by low-bit quantization can propagate and compound, ultimately leading to incorrect final answers. In code generation tasks, the challenges are even more pronounced. Under 4-bit quantization, GPTQ and AWQ show performance drops exceeding 5%, while QuaRot and DuQuant degrade by over 10% under W4A4 for both LLaDA-8B and LLaDA-8B-instruct models. We also observe that the standard deviation on the HumanEval benchmark is relatively high, approximately 3%, indicating that more robust and stable benchmarks may be needed to accurately assess code generation capabilities under quantization. Code generation tasks often require the model to maintain long-range context and generate syntactically correct, semantically meaningful sequences. These demands are highly sensitive to the"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 9, "text": "HumanEval benchmark is relatively high, approximately 3%, indicating that more robust and stable benchmarks may be needed to accurately assess code generation capabilities under quantization. Code generation tasks often require the model to maintain long-range context and generate syntactically correct, semantically meaningful sequences. These demands are highly sensitive to the precision of both weights and activations. Quantization-induced distortion in attention patterns or token representations can disrupt code syntax or logic, causing severe performance degradation. These observations highlight that math and code tasks impose stricter precision requirements than simpler retrieval-based or classification-style QA tasks. Maintaining accurate intermediate states, multi-hop logic, and long-context dependency are especially vulnerable under aggressive quantization. Consequently, task-specific quantization strategies or adaptive precision control mechanisms may be necessary to improve the robustness of dLLMs on math and code benchmarks. This represents a critical direction for future research in efficient diffusion-based LLM deployment. 4.5 Impact of Model Types (RQ4) Instruct-Tuned Models are More Robust than Base Models. We observe an interesting phe- nomenon: LLaDA-8B-instruct consistently exhibits smaller performance degradation than its base counterpart (LLaDA-8B) under nearly all quantization settings. For instance, under general tasks, both DuQuant and QuaRot result in only minor accuracy drops for the instruct model, whereas the drop exceeds 5% for the base model. This trend remains consistent across more challenging math and code tasks. For example, 3-bit quantized GPTQ and AWQ lead to performance degradation of approximately 5% for the instruct variant, while the base model suffers drops as high as 10%. Our Observations Hold Consistently across Different dLLMs. To assess the generality of our findings, we further evaluate various quantization methods on a different diffusion-based language model: Dream-7B. As shown in Table 5, both GPTQ and AWQ perform competitively under 4- bit quantization, while performance drops become more pronounced in the 3-bit setting. This observation reinforces our recommendation that 4-bit quantization offers a near-lossless trade-off between efficiency and performance. Moreover, GPTQ consistently outperforms AWQ across nearly all benchmarks, suggesting that GPTQ is a more reliable choice across different types of dLLMs. Notably, the 3-bit quantized models exhibit risk-level degradation even on general tasks, indicating that aggressive quantization may be more challenging for the Dream model series compared to LLaDA. 8 Due to resource constraints, we did not evaluate weight-activation quantization for Dream-7B. We leave this for future work as part of our ongoing exploration. 5 Limitation and Future Work In this work, our primary focus is on evaluating downstream task performance of quantized dLLMs. Quantization offers an effective way to reduce memory consumption and accelerate inference. However, fully integrating low-bit inference for diffusion LLMs remains challenging. Specifically, adapting existing LLM-optimized kernels to the architectural characteristics of diffusion LLMs involves substantial engineering effort, which we leave for future work. We plan to continue this line of research along the following directions: 1). Expanded Evaluation: We will provide a more comprehensive evaluation across a broader set of dLLMs, tasks, and model sizes. 2). Stepwise Analysis: We aim to explore how the number of generation steps in diffusion decoding interacts with quantization levels, and whether step-aware quantization strategies can"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 10, "text": "along the following directions: 1). Expanded Evaluation: We will provide a more comprehensive evaluation across a broader set of dLLMs, tasks, and model sizes. 2). Stepwise Analysis: We aim to explore how the number of generation steps in diffusion decoding interacts with quantization levels, and whether step-aware quantization strategies can be beneficial. 3). Remasking Strategies: We intend to evaluate different remasking strategies under quantized settings, and provide practical guidance on selecting suitable quantization configurations. We hope our work initiates further discussion and exploration in the community. To facilitate future research, we will release our code and implementation details to support the development and deployment of quantized diffusion LLMs. 6 Conclusion This work provides the first in-depth investigation into the challenges and opportunities of applying post-training quantization (PTQ) to diffusion-based language models (dLLMs). Through extensive empirical evaluation, we uncover several key findings: (1) activation outliers are prevalent across dLLMs and are critical barriers to low-bit quantization; (2) certain PTQ methods, GPTQ and DuQuant, demonstrate notable advantages under constrained settings; and (3) quantization behavior varies across tasks and model types, with instruct-tuned models showing greater resilience. These findings offer practical guidance for designing more effective and robust quantization strategies. Looking forward, we believe that our study lays the groundwork for future research in compression of dLLMs, enabling their deployment in real-world, resource-constrained environments. 9 Appendix A Additional Implementation Details Weight-only Quantization Methods. For GPTQ, we use 128 calibration samples from the WikiText-2 dataset with a sequence length of 2048. We adopt asymmetric quantization and set the group size to 128. The method is implemented using the AutoGPTQ repository1. For AWQ, we use 128 samples from the PileVal dataset with a sequence length of 512. We implement AWQ with the llm-awq repository2 and apply the same settings as GPTQ, using asymmetric quantization and a group size of 128. Weight-activation Quantization Methods. For SmoothQuant, we set the hyperparameter α = 0.5 in the scaling equation to compute the diagonal matrix: sj = max(|Xj|)α/ max(|Wj|)1−α. We use 128 calibration samples from the WikiText-2 dataset with a sequence length of 2048. Asymmetric per-tensor quantization is applied to weights, and per-channel quantization is applied to activations. For QuaRot, we follow the original configuration by preserving 16-bit precision for query states, and applying symmetric activation quantization. We also use WikiText-2 (128 samples, sequence length 2048) as the calibration dataset. For DuQuant, we use the same calibration setup and α value as SmoothQuant. Additionally, we apply activation and weight clipping ratios of 0.9 and 0.8, respectively. The rotation step is set to 256, and the block size is 128. General Tasks. We employ the lm-evaluation-harness3 repository to benchmark models across all tasks. For general tasks other than MMLU, we adopt a 0-shot setting with 128 Monte Carlo samples. For MMLU, we use a 5-shot setting with a single Monte Carlo sample. To evaluate LLaDA models, we configure the diffusion steps, block size, and generation length to 1024, set the classifier-free guidance (CFG) scale and temperature to 0.0, and apply the low confidence remasking strategy. For Dream, we set the maximum number"}
{"doc_id": "2508.14896v1", "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14896v1", "chunk_id": 11, "text": "use a 5-shot setting with a single Monte Carlo sample. To evaluate LLaDA models, we configure the diffusion steps, block size, and generation length to 1024, set the classifier-free guidance (CFG) scale and temperature to 0.0, and apply the low confidence remasking strategy. For Dream, we set the maximum number of new tokens to 128, the CFG scale to 1.0, the temperature to 0.0, and the top-p threshold (probability of retaining generated tokens) to 0.95. Mathematics and Code Tasks. The benchmarking details of LLaDA on mathematics and code tasks are provided in Tab. A1. All other configurations remain the same as in the general tasks. Table A1: Configuration for mathematics and code tasks. Dataset # fewshots generation length diffusion steps block size GSM8K 4 256 256 32 Math 0 256 256 64 HumanEval 0 512 512 32 MBPP 3 512 512 32 B More Visualizations (a) LLaDA_8B_Layer11_Attn_q_proj (b) LLaDA_8B_Layer26_FFN_ff_out (c) LLaDA_8B_Layer9_FFN_ff_proj (d) LLaDA_8B_Layer31_FFN_ff_out LLaDA-8B-Base Massive Massive Massive Figure B1: More visualizations of activation outliers in LLaDA-8B-Base. 1https://github.com/AutoGPTQ/AutoGPTQ. 2https://github.com/mit-han-lab/llm-awq. 3https://github.com/EleutherAI/lm-evaluation-harness 10 (a) LLaDA_8B_Ins_Layer9_Attn_q_proj (b) LLaDA_8B_Ins_Layer31_Attn_out_proj (c) LLaDA_8B_Ins_Layer11_FFN_ff_proj (d) LLaDA_8B_Ins_Layer25_FFN_ff_out LLaDA-8B-Instruct Massive Figure B2: More visualizations of activation outliers in LLaDA-8B-Instruct."}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 0, "text": "Virtual Community: An Open World for Humans, Robots, and Society Qinhong Zhou1∗ Hongxin Zhang1∗ Xiangye Lin1∗ Zheyuan Zhang2∗ Yutian Chen3 Wenjun Liu1 Zunzhe Zhang1 Sunli Chen1 Lixing Fang1 Qiushi Lyu1 Xinyu Sun1 Jincheng Yang1 Zeyuan Wang1 Bao Chi Dang1 Zhehuan Chen1 Daksha Ladia1 Jiageng Liu1 Chuang Gan1 1UMass Amherst 2 Johns Hopkins University 3Carnegie Mellon University https://virtual-community-ai.github.io/ Name: Brian Carter Age: 23 Innate: Self-direction, Security Bio: I am a cybersecurity expert at StockX. My hobby is soccer. Group: • Name: StockX Place: StockX Detroit Living: Chad Thompson’s room at Justin Management Name: Chad Thompson Age: 35 Innate: Stimulation, Hedonism Bio: I am a professor at Wayne State University. My hobby is cooking. Group: • Name: Detroit Food Academy Place: The Gastronomy Guild Living: Elizabeth Mensah's room at Bedrock Detroit Name: Chad Thompson Role: Doctor Action: Asking for help Vehicle: Police Car Destination: Monroe St. Robot: Wheeled Robot Task: Following Brian Robot: Mobile manipulator Task: Picking up the cube Name: Brian Carter Heading to: Cadillac Square Diner Tech Innovators Group Creative Minds Group Robot Collaboration Figure 1: Virtual Community enables simulation of humans, robots, and their societies within open-world environments. We present an automated pipeline that transforms real-world geospatial data into large-scale 3D scenes and contextually grounded agent communities. Built on a universal physics engine, our system supports a diverse range of robots—including mobile manipulators, quadrupeds, humanoids, and drones—as well as visually expressive avatars capable of realistic motion and language-based interactions. This foundation facilitates complex social interactions and collective behaviors within the simulated communities. Abstract The rapid progress in AI and Robotics may lead to a profound societal transforma- tion, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community—an open-world platform for humans, robots, and society—built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete 2) How humans develop social relations and build community 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging *denotes equal contribution Preprint. Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple het- erogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Vir- tual Community will unlock further study of human-robot coexistence within open-world environments. 1 Introduction In recent years, the development of intelligent embodied agents has been propelled by advances in virtual simulators"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 1, "text": "tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Vir- tual Community will unlock further study of human-robot coexistence within open-world environments. 1 Introduction In recent years, the development of intelligent embodied agents has been propelled by advances in virtual simulators [53, 34, 3, 68, 52, 36, 82, 40, 23, 11, 80, 72, 37, 92, 93, 19]. However, most of these platforms focus on robots [67, 82, 37], human-like agents [46, 47], or only a limited number of agents with simple interactions [48, 23]. In contrast, support for large, heterogeneous communities of human and robot agents collaborating in scalable open worlds remains limited, constraining the study of complex multi-agent behaviors between humans and robots. To address this challenge, simulators must support the following key features. First, they should offer physically realistic simulations that accommodate large communities of human-like avatars and robots. Existing multi-agent embodied AI platforms [46, 48, 23, 36, 80] typically handle only small groups of avatars or robots, or provide limited physics-based interactions, thereby constraining the realism of community-level behaviors. Second, the simulator must support the creation of diverse, populated worlds, including large-scale 3D environments and scene-grounded agent communities. Current approaches fall into two categories: manual design or procedural generation [72, 23, 70, 80, 24], which enable rich agent–environment interactions but suffer from limited diversity and realism; and 3D reconstruction methods [53, 58], which produce visually realistic and varied scenes but require extensive visual input and often yield low-interactivity environments in open-world settings. In this paper, we present Virtual Community, an open world for humans, robots, and society. Virtual Community addresses these challenges by building a unified simulation framework for human-like agents and robots based on the Genesis [4] physics engine and integrating large-scale, real-world geospatial data with generative models to produce interactive, scalable open worlds (Figure 1). The platform offers two key advancements: Unified Simulation for Avatars and Robots Virtual Community simulates human-like avatars and diverse robots within the generated open worlds using a unified framework based on the Genesis [4] physics engine, supporting diverse physical and social interactions among different types of agents. Virtual Community also provides robot and avatar agents with a unified interface with distinct observation and action spaces. Open World Generation from Real Scenarios. Virtual Community fully automates the generation of open worlds with several key features: (1) scalable, real-world–aligned outdoor scenes of customiz- able size and quantity, along with corresponding indoor scenes and annotations; and (2) generation of agent communities endowed with scene-grounded profiles and social relationship networks. Virtual Community combines generative models with real-world geospatial data, ensuring scalability in data volume, realism, and extent. Virtual Community enables a variety of new possibilities in embodied AI research. The expansive open-world scenes and their agent communities introduce a new challenge of multi-agent task planning in open worlds. We introduce the Community Planning challenge as a first step in this direction. This challenge includes assistant tasks, in which agents interact with humans and other agents to provide assistance in daily open world activities, and social influence tasks,"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 2, "text": "introduce a new challenge of multi-agent task planning in open worlds. We introduce the Community Planning challenge as a first step in this direction. This challenge includes assistant tasks, in which agents interact with humans and other agents to provide assistance in daily open world activities, and social influence tasks, in which agents must efficiently explore the community and connect with one another. Virtual Community also supports physically realistic simulations of interactions among community agents, for which we propose the Community Robot challenge. This challenge tasks robots with cooperating to complete tasks that involve both indoor and dynamic open-world environments. Our simulator advances the field by enabling unified simulations of human and robot communities in generated open worlds, surpassing existing solutions in both scope and capability. By overcoming 2 limitations in the scalable simulation of humans, robots, and societies, we pave the way for studying embodied general intelligence that can coexist with complex, interconnected human communities. 2 Related Works Embodied AI Simulation Recently, embodied AI has seen significant advancements through the development of simulation platforms. Most existing simulators primarily focus on household tasks within indoor environments [5, 53, 88, 12, 83, 58, 53, 36, 46, 34, 84, 37, 67, 13, 15], while some have extended support to outdoor scenes [23, 70, 72, 17, 32, 25]. However, existing platforms lack the diverse and scalable outdoor environments needed to support larger agent populations and more complex multi-agent interactions. In contrast, this paper introduces a simulation platform with expansive open-world environments, integrating both indoor and scalable outdoor scenes to facilitate broader agent interactions and enable more intricate task scenarios. Embodied Social Intelligence Current research on Embodied Social Intelligence is often limited to small agent populations in constrained household scenarios [47, 90, 61, 53, 31, 65, 91] or simplified to 2D or grid worlds [10, 62, 69, 51, 89, 85], hindering model development in the open world. Specifically, [44] demonstrates the robust simulation of human-like agents within a symbolic community, ignoring the 3D perception and realistic physics in the open world. [76] studies human- like simulation guided by system 1 processing with basic needs. Predominant approaches, such as multi-agent reinforcement learning (MARL) and other planning models, face several limitations when applied to open-world settings. MARL, for instance, often struggles with scalability due to the exponential growth of state and action spaces as the number of agents increases [78]. This makes it difficult to learn effective policies in complex, dynamic environments. Additionally, MARL approaches typically require extensive training data and computational resources, which may not be feasible in real-world applications. Other planning models, while potentially more efficient, often lack the adaptability required to handle the unpredictable nature of open-world interactions. They may rely on predefined rules or assumptions that do not hold in all scenarios, leading to suboptimal performance and limited generalization to new contexts [47]. Foundation and Generative models for Embodied AI With the recent advance of foundation models [9, 38, 18, 8], numerous works have explored how they can help build powerful embodied agents [73, 81, 63, 77, 2, 56, 71, 44, 28, 7], and"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 3, "text": "suboptimal performance and limited generalization to new contexts [47]. Foundation and Generative models for Embodied AI With the recent advance of foundation models [9, 38, 18, 8], numerous works have explored how they can help build powerful embodied agents [73, 81, 63, 77, 2, 56, 71, 44, 28, 7], and scenes for simulation [87, 29, 27, 55, 16, 22, 86, 21, 66, 45, 57, 14]. RoboGen [75] utilizes foundation models to automatically generate diverse tasks, scenes, and training supervision, scaling up robotic skill learning with minimal human input. In contrast, our work fully integrates a generative pipeline into the simulation platform to create expansive open-world scenes and agent communities. 3 Generating Open Worlds for Simulation 3.1 Scalable 3D Scene Creation The existing 3D geospatial data API1 provides extensive data in terms of quantity and diversity. However, they are not directly suitable for embodied AI research because of several limitations. First, these geospatial data often contain noise, including pedestrians, vehicles, other transient objects, and unrealistically rugged terrain that can disrupt simulations. Second, visual quality is inadequate for ground-level agent perspectives because these environments are typically reconstructed from aerial imagery, leading to less detailed textures and geometries at street level. To bridge this gap, we propose an online pipeline that performs comprehensive cleaning and en- hancement in both geometry and texture to make the scenes suitable for embodied AI simulations. This pipeline consists of four steps: mesh simplification, texture refinement, object placement, and automatic annotation. We generated 35 annotated scenes of various cities worldwide with this pipeline and present some qualitative examples of these scenes in Figure 3. Geometry Reconstruction and Simplification Since the mesh topologies in 3D geospatial data sources are unreliable for embodied AI simulations, we decompose scenes into terrain, building, and decorative-roof elements, then apply specialized reconstruction operations to each component to 1https://www.google.com/maps/ 3 (C) Community Simulation Map Texture Refine Geometry Simplify Location (Lat,Lng) Object Place Indoor Generate (A) Scene Character Initial Info ● Name ● Age ● Skin … Chad Thompson Groups: [\"Berlin Foodies Club\"] Living: Palais Morten Lindqvist Values: Security Cash: 200$ Lifestyle: Healthy Kate Novak Heading to: The Science Building Task: Go to class Chad Thompson Heading to: Hotel Hugo Task: Meet with James Smith Humanoid Heading to: Liam Novak Task: Work with Liam Novak (B) Agent Kate Novak Cash: 50$ Student Living: McCormick Groups: [\"Campus\"] Characters Relations … Bounding Box: … Place_1: \"Sushi Noz\" Coarse_type: \"Food\" Fine_type: \"Japanese Restaurant\" Indoor_scene: … Place_2: \"The City Hotel\" … Herald Towers Fine-tuned Scene Bus Line 33 Placed Trees & Poles Million Dollar Corner … 3D Geospatial Data Morten Lindqvist Current: City Square Heading to: None Task: Working Figure 2: Framework of the Virtual Community Generation Pipeline. This pipeline generates scenes and corresponding agents from real-world geospatial data. The scene generation component (A) refines rough 3D data by using generative models to enhance textures and geospatial data to simplify geometry. It also utilizes generative methods to create interactive objects and detailed indoor scenes. The agent generation component (B) leverages LLMs to generate agent characters and social relationship networks based on scene"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 4, "text": "component (A) refines rough 3D data by using generative models to enhance textures and geospatial data to simplify geometry. It also utilizes generative methods to create interactive objects and detailed indoor scenes. The agent generation component (B) leverages LLMs to generate agent characters and social relationship networks based on scene descriptions. (C) We simulate the avatar communities and robots in the open world scenes based on Genesis engine. make the entire scene simulation-ready. The terrain is generated procedurally from sparse reference elevation points via bilinear interpolation. We then derive simple, topologically sound building meshes using OpenStreetMap (OSM) data. Each building mesh is automatically adjusted to better match the Google 3D Tiles geometry and to align with the terrain elevation. By aligning mesh geometries to OSM primitives, we remove unnecessary details and artifacts—such as distorted surfaces and irregular shapes resulting from aerial reconstruction errors—thereby denoising the meshes for more efficient physics simulations and improved rendering performance. Texture Enhancement for Realistic Simulation We further apply advanced image-processing techniques to enhance mesh textures. During mesh construction and simplification, textures from the original 3D Tiles are baked onto new geometries, which can result in missing or distorted regions. To address these issues, we first employ a Stable Diffusion 3 [60] based inpainting method to remove noise and repair damaged or incomplete textures. We then refine texture details using street-view imagery. This two-step process significantly improves visual fidelity, making textures more suitable for ground-level rendering. Object Replacement for Interactive Scene To enhance scene interactivity, we combine generative and retrieval methods to populate the environment with interactive objects (e.g., bikes and tents). Using OSM annotations, we identify object types and locations to reflect real-world contexts. For relatively simple objects, such as tents, we adopt a generative pipeline that uses OSM text annotations on amenities as input: a Stable Diffusion model [50] first generates images of the relevant objects, which are then processed by the One-2-3-45 framework [39] to produce corresponding 3D meshes. For more complex objects, such as trees, we use the retrieval pipeline, which randomly samples assets whose categories match the OSM annotations from a pre-collected dataset. Place and Transit Annotations with Geospatial Data To facilitate alignment with real-world locations and provide semantic context for community activities, we developed a pipeline to auto- matically annotate places, buildings, and public transit within scenes using geospatial data. First, we query Google Maps Places for location information in the target area and organize results into six categories: accommodation, entertainment, food, office, stores, and open spaces. Next, we use OSM to retrieve building names and bounding boxes, matching them with the place entries. We then 4 Figure 3: Egocentric view of the generated scenes. The resulting scene features clean geometry and realistic textures, which support physical simulation and enhance real-world style fidelity. filter out unmatched or inaccessible locations to generate accurate place annotations. Finally, we annotate bus transit routes based on these place annotations. These metadata enable agents to access location-specific information and support tasks that require spatial context—such as navigation and location-based decision-making—and also power traffic simulation, including buses, pedestrians,"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 5, "text": "fidelity. filter out unmatched or inaccessible locations to generate accurate place annotations. Finally, we annotate bus transit routes based on these place annotations. These metadata enable agents to access location-specific information and support tasks that require spatial context—such as navigation and location-based decision-making—and also power traffic simulation, including buses, pedestrians, and other vehicles. Indoor Scenes Creation To create indoor scenes in the communities, we employ a pipeline combining generation and retrieval to produce detailed, realistic multi-room environments. The pipeline’s input is the building names in the target area, obtained from Google Maps and OSM. We first retrieve indoor layouts from GRUTopia [72] for categories such as offices, restaurants, and stores. For building types not covered by GRUTopia, we use Architect [74] to generate the corresponding indoor rooms for simulation. 3.2 Agent Community Generation Given diverse generated scenes from real-world geospatial data, we introduce a generative pipeline to populate these environments with communities of agents endowed with grounded character profiles and social relationship networks, define their embodiments. Characters and Social Network Generation We utilize the open-world knowledge of the Large Language Model (LLM) to generate agent character profiles and personalities grounded in the scene. Specifically, we use GPT-4o to perform this generation. The input to the LLM is structured into two parts to create characters grounded in a specific scene. The first part contains scene-related information, such as the scene name and details about various places, including their names, types, and functionalities. The second part includes details on the agents’ appearances to ensure consistency between their visual attributes and generated profiles, which are annotated with the name and age. With both parts provided, the LLM generates agent profiles along with their social relationships. The profiles consist of basic attributes such as names, ages, occupations, personalities, and hobbies, which influence each agent’s daily activities. Social relationships are structured as groups, each containing a subset of agents along with a text description and a designated place for group activities, connecting these agents into a cohesive community. Grounding Validator We implemented a grounding validator that verifies whether generated charac- ter profiles are accurately grounded in the scene by ensuring all referenced places exist. If validation fails, the LLM receives feedback from the validator and attempts to correct the mismatch. Detailed examples of prompts used in the pipeline, generated characters, and social relationship networks are provided in the Appendix. Human-Like Avatar Creation We first obtained 20 avatar skins representing diverse genders, professions, and appearances from Mixamo2 for integration into the Virtual Community. We also used the Avatar SDK3 to generate high-fidelity human meshes from synthetic human face images from FaceSynthetics [79], enabling representation of diverse individuals. 2https://www.mixamo.com/ 3https://avatarsdk.com 5 4 Unified Simulation for Human and Robot Community Virtual Community provides a unified framework for simulating both robots and human agents in the community. It implements an avatar simulation framework for human agents, while robot simulations are largely inherited from Genesis. Avatar Simulation and Control To simulate avatars in Genesis physics engine, we combine SMPL-X human skeletons with these avatar skins to model human avatars. The motions of"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 6, "text": "and human agents in the community. It implements an avatar simulation framework for human agents, while robot simulations are largely inherited from Genesis. Avatar Simulation and Control To simulate avatars in Genesis physics engine, we combine SMPL-X human skeletons with these avatar skins to model human avatars. The motions of these avatars are parameterized by SMPL-X pose vectors J ∈R162 and global translation and rotation vectors T, R ∈R3. Specifically, we download FBX files from Mixamo that record human skeletal motion sequences and parse them into a hierarchical structure of human joints. Each skeleton joint is mapped one by one with the joints of the SMPL-X model. Then, we recursively traverse the joint tree structure to calculate the global coordinate system vector for each joint after its rotation at each time step t, and use this to drive the movement of the human skeleton. Based on these pose representations, each avatar’s skin mesh is computed via forward kinematics. Our humanoid motion model supports over 2,000 distinct actions, such as walking, object manipulation, and vehicle entry. We use motion clips from Mixamo and adjust their playback speeds to match our avatars. For walking, we loop the clip until the avatar covers the required distance. For object-related actions, objects are kinematically attached to or detached from the avatars’ hands based on the action. Similarly, during vehicle-related motions, avatars are kinematically attached to or detached from vehicles. We also incorporate physics constraints: collision detection is performed between avatars and scene entities, and motion terminates upon detection of a potential collision. Daily Schedule Generation and Simulation Given the scene-grounded character profiles and social relationship networks, we prompt foundation models to generate each agent’s daily schedule [44]. However, we structure each schedule so that every activity includes a start time, an end time, an activity description, and a corresponding location. We also explicitly account for the commute time between activities at different locations to reflect the actual cost of navigating an expansive 3D environment. This approach allows agents to follow the organized high-level plan effectively and maintain consistency over time. During simulation, agents follow the generated schedules to carry out daily activities. Examples of detailed daily plans are provided in the Appendix. Robot Agent Simulation We simulate robots alongside avatars in the Genesis simulator. Virtual Community supports five types of robots: drones, quadratic robots, humanoid robots, wheeled robots, and mobile manipulators, each with a distinct robot controller. The robot controller bridges the interface between Virtual Community and Genesis, exposing only selected action spaces. For example, the action space of Google Robot, a mobile manipulator, comprises an 11-dimensional control signal (7 DoF for the arm, 2 for the gripper, and 2 for locomotion), in velocity, position, or force control. Virtual Community shares the same simulation loop between avatars and robots with different control frequencies. To support faster collision detection during robot physics simulation, we use an invisible terrain mesh and decomposed building meshes as collision geometry for the background scene, enabling more efficient physics simulation. 5 Open World Multi-Agent Planning Based on Virtual Community, we propose the Community"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 7, "text": "and robots with different control frequencies. To support faster collision detection during robot physics simulation, we use an invisible terrain mesh and decomposed building meshes as collision geometry for the background scene, enabling more efficient physics simulation. 5 Open World Multi-Agent Planning Based on Virtual Community, we propose the Community Planning Challenge to evaluate multi-agent planning capabilities in both outdoor and indoor environments. The challenge comprises three community assistant tasks, in which agents cooperatively plan to assist multiple humans with daily open-world activities, and a community influence task, in which agents competitively plan to efficiently connect and interact with other agents in the community to increase their social influence. 5.1 Community Assistant Tasks The community assistant tasks include the following three categories that require agents to plan cooperatively to provide humans with assistance on daily activities: • Carry: Locate people and follow them to help carry objects to their home. • Delivery: Move objects from source locations (indoor or outdoor) to a destination. • Search: Locate target objects within an outdoor region or an indoor room. 6 Table 1: Main results of the Multi-Agent Community Planning Challenge. We report Success Rate (SR), Time Consumed (Ts), and Human Following Rate (HR) for three community assistance tasks averaged over 24 scenes. Method Carry Delivery Search Avg SR↑ SR↑ HR↑ SR↑ Ts↓ SR↑ Ts↓ 1-assistant Random 0.0 0.0 0.0 1500.0 0.0 1500.0 0.0 Heuristic 34.7 16.5 46.5 1462.9 45.1 1440.3 42.1 MCTS Planner 42.3 0.0 39.6 1500.0 45.1 1500.0 42.4 LLM Planner 29.9 15.3 41.7 1500.0 70.1 1339.0 47.2 2-assistant Heuristic 52.8 25.6 59.7 1415.8 51.4 1364.3 54.6 MCTS Planner 42.4 0.0 43.8 1500.0 48.6 1500.0 44.9 LLM Planner 30.2 7.6 43.8 1500.0 77.8 1141.3 50.6 Task Settings Each task includes multiple subtasks. For example, the Carry task requires agents to help humans carry several objects while following them. Therefore, open-world adaptive task planning is therefore essential for scheduling these runs, routing in the dynamic open world between waypoints, and managing task-level dependencies. We study two settings with different numbers of assistants: the 1-assistant setting, in which a single assistant needs to provide assistance to human agents in the community, and the 2-assistants setting, in which two assistants plan cooperatively to provide assistance. Observation and Action Spaces At each simulation step, agents are provided with an observation consisting of RGB-D images with the corresponding camera matrix, segmentations, current poses, and task information. The action space includes move forward, turn left, turn right, enter/exit bus/bike, and communicate. The movement and turning actions can be set with a continuous amount. Agents are set with a walking speed of 2m/s. Evaluation Metrics We provide three evaluation metrics for all assistant tasks: success rate (SR), defined as the number of successful subtasks divided by the total number of subtasks; average time consumed (T) per task; and human following rate (HR) for the carry task, defined as the number of frames in which the agent follows a human within a specified distance range divided by the total number of frames during the task. When the simulation reaches a total step"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 8, "text": "average time consumed (T) per task; and human following rate (HR) for the carry task, defined as the number of frames in which the agent follows a human within a specified distance range divided by the total number of frames during the task. When the simulation reaches a total step limit of 1500, it stops and the results are evaluated automatically. Baselines • Perception and Navigation Module: We implement all baseline agents within a hierarchical planner framework. All agents employ the same low-level navigation algorithm, which reconstructs a point cloud from egocentric RGB–D observations at each step and converts it into a volumetric grid representation at a resolution of 0.1 m. Based on this grid, a 2D occupancy map with a resolution of 0.5 m is generated, and an A* algorithm is used to efficiently compute the shortest path to a bounding box. To accommodate dynamic environments, the navigation module recalculates the optimal path at every step. • Random Planner: The Random planner is a trivial planner that randomly selects from the space of high-level actions without any planning. • Heuristic Planner: The heuristic planner uses a finite state automaton defined by human experts for each task. The agent navigates, picks, enters room, or performs other actions at each state. • MCTS Planner: We also introduce a new Monte-Carlo Tree Search (MCTS) based baseline planner, which employs Monte-Carlo Tree Search to optimize task plans. • LLM Planner: We follow CoELA [90] to design an LLM Planner with a modular framework driven by gpt-4o to generate and select subplans, including navigation to specified open-space locations, searching for objects, entering indoor areas, and performing object manipulation actions (pick and put). At each decision point, the LLM is prompted with the current state and task objectives and produces a subplan—a sequence of high-level actions [59], which is then executed step by step by the 7 agent. A communication module is also adopted to facilitate the cooperation among agents through natural language communication. Results We evaluate the above baselines on all three tasks. The results in Table 1 demonstrate the challenges of planning in open-world environments. From our experiment, the Random baseline fails to understand spatial relationships among tasks and no single method prevails for all the tasks. One common failure mode of baseline agents is underestimating the cost of open-world navigation and search, which leads to suboptimal task arrangement. Heuristic Planner performs strongly in the delivery task. While the LLM Agent prevails by a large margin in the search task, which involves no interaction with objects, it performs poorly in the other two tasks, where LLMs show difficulty tracking the task progress given only action history. 5.2 Community Influence Task To further investigate agents’ planning and social capabilities in open-world settings, we introduce the Community Influence Task—a novel, open-ended social challenge in which two main agents compete to connect with and persuade other community members to form relationships with them. Due to differences in personality traits and social status, certain members may initially prefer one main agent over the other, requiring strategic planning to influence"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 9, "text": "Influence Task—a novel, open-ended social challenge in which two main agents compete to connect with and persuade other community members to form relationships with them. Due to differences in personality traits and social status, certain members may initially prefer one main agent over the other, requiring strategic planning to influence and shift member opinions over time. Task Settings Each community contains two main agents and thirteen other members. The main agents must navigate the environment, locate potential members to connect with, and attempt to persuade them through dialogue. At the end of each day, every community member ranks their friendship level with the two main agents. Observation and Action Spaces The observation and action spaces for all agents match those in the Community Assistant Tasks. Since this task focuses on social planning, both main agents are given access to the daily schedules of all members. Experimental Settings and Evaluation Metrics We run experiments in five distinct communities. An agent is considered more effective at influencing others if it achieves a higher average friendship rank across all members. Baseline We evaluate different LLMs as the planning backbone for the main agent. Given the daily schedules and character traits of community members, the main agent prompts the LLM to select the next member to visit, considering both spatial proximity and potential influence. After navigating to the target, the same LLM generates up to three rounds of conversation, conditioned on the main agent’s and the target member’s profiles. Once the conversation concludes, the main agent proceeds to the next selected target. Full implementation details are provided in the Supplementary Material. Metrics We use two metrics: (1) Average friendship-ranking wins (Win.) — the average win rate in the friendship ranking across all community members at the end of the day. Higher values indicate that an agent is more effective at forming new connections and expanding its social influence. (2) Conversion rate (Conv.) — the proportion of originally non-supporting members who become friends with the agent by the end of the day. Results As shown in Figure 4, the main agent with the o1 backbone achieves higher average friendship rankings and conversion rates than the gpt-4o backbone 4, indicating greater ability to change members’ opinions in most communities. For example, in the Denver community, the agent, Yara Mbatha, persuaded Theo by leveraging their shared affiliation with the Denver Scholars Society and emphasizing that common bond. We also observe that when starting with a large advantage, the gpt-4o agent sometimes fails to gain additional supporters, primarily due to suboptimal target selection and less effective persuasion strategies. These results suggest that, even with advanced LLMs, there remains substantial room to improve embodied agents’ abilities in building social connections and exerting influence. 4We used GPT-4o-2024-11-20 and o1-2024-12-17 during experiments. 8 Yara Mbatha (Main Agent): \"I'm Yara Mbatha. Hello Theo! It's great to connect with you, especially since we both share a connection through the Denver Scholars Society… \" Theo Caldwell (Member): \"Great to connect with you too! I really appreciate your kind words… \" Marco Ruiz age: 36, innate: \"power,"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 10, "text": "Yara Mbatha (Main Agent): \"I'm Yara Mbatha. Hello Theo! It's great to connect with you, especially since we both share a connection through the Denver Scholars Society… \" Theo Caldwell (Member): \"Great to connect with you too! I really appreciate your kind words… \" Marco Ruiz age: 36, innate: \"power, achievement, security\" Tariq Johnson age: 24, innate: \"achievement, self- direction, stimulation\" Morten Lindqvist age: 38, innate: \"tradition, benevolence, universalism\" Theo Caldwell age: 34, innate: \"achievement, self- direction, stimulation\" 0.63 0.13 0.57 0.17 0 0.2 0.4 0.6 0.8 Win. Conv. GPT-4o o1 Figure 4: We evaluate baseline agents on the Community Influence Task across five communities. Results show that more powerful LLMs are better able to connect with and influence other members in the community. Table 2: Detailed results of the Community Robot Challenge. We report Success Rate (SR) and Time Consumed (Ts) for two community robot tasks averaged over 21 different scenes. Method Carry Deliver Avg SR↑ SR↑ Ts↓ SR↑ Ts↓ Heuristic 17.6 126.9 22.2 129.4 19.9 Heuristic w Oracle Grasp 23.5 124.4 50.0 131.2 36.8 RL w Oracle Grasp 19.0 149.7 42.9 168.1 31.0 6 Open World Multi-Robot Cooperation In addition to high-level planning and interactions, we also explore low-level physics challenges in multi-agent, open-world settings. In this section, we introduce the Community Robot Challenge, which features scenarios where two heterogeneous robots cooperate to assist humans in open-world environments. Task settings The Community Robot Challenge builds upon the Carry human–robot interaction task from the Community Assistant suite (Section 5.1), introducing physics-level collaboration in open-world environments. In this challenge, robots must cooperate to assist a human avatar by picking up and carrying personal items while following the avatar through dynamic open spaces. Robot Settings We use two robot assistants: a mobile manipulator—based on the Google robot model in MuJoCo [68], augmented with one degree of freedom for forward translation and another for rotation about the z-axis—and a wheeled robot carrier with four degrees of freedom (one per wheel). In addition, Virtual Community supports quadruped and humanoid robots, which are described in the Appendix. Observation and Control Spaces Observations include include RGB–D images, segmentations, the base pose, and task-related information at 1 Hz. Actions are robot control signals at 100 HZ. Baseline Pipeline We implement two baselines including Heuristic and RL. The heuristic baseline inherits the navigation module from baseline avatars in the Community Assistant Tasks 5.1. For navigation, robot computes a collision-free path with A* search. For manipulation, the robot solves for a feasible grasp pose with inverse kinematics, and plans and executes the grasp motion with RRT-Connect [35]. For the Reinforcement Learning (RL) baseline, we trained a manipulation policy using PPO for 107 steps on the Genesis. Results According to the Community Robot Challenge results in Table 2, all baselines achieve higher scores on the delivery task than on the carry task, highlighting the added difficulty of simultaneously manipulating objects and following a human in a dynamic open-world environment. Moreover, without using an oracle grasp, performance drops significantly, underscoring the challenge posed by the manipulation component in this task. The reinforcement"}
{"doc_id": "2508.14893v1", "title": "Virtual Community: An Open World for Humans, Robots, and Society", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14893v1", "chunk_id": 11, "text": "higher scores on the delivery task than on the carry task, highlighting the added difficulty of simultaneously manipulating objects and following a human in a dynamic open-world environment. Moreover, without using an oracle grasp, performance drops significantly, underscoring the challenge posed by the manipulation component in this task. The reinforcement learning baseline performs worse than 9 the heuristic baseline, which uses inverse kinematics and RRTConnect to compute manipulation tra- jectories. This gap is because the classical planner explicitly solves for optimal paths in configuration space, whereas the RL agent must discover effective control sequences under sparse reward signals. 7 Conclusion We introduce Virtual Community, an open-world simulation platform for multi-agent embodied AI that supports scalable, simulation-ready generation of open-world scenes and agent communities, along with physically realistic simulation of multiple embodied avatars and robots. As an initial demonstration, we propose two novel open-world multi-agent challenges—the Community Planning Challenge and the Community Robot Challenge—each developed and evaluated using a variety of baseline methods. One limitation of this work is that the outdoor scenes are not modeled in sufficient detail to accurately reflect the physical and visual properties of real-world environments. We hope Virtual Community will advance embodied AI research toward embodied general intelligence capable of handling real-world complexities and coexisting with human communities. Acknowledgement We gratefully acknowledge Haoyu Zhen, Ruxi Deng, Quang Dang, Hao Zou, Tiger Harburg, and Siyuan Cen for their efforts in the project website and data collection. We thank Chunru Lin, Yian Wang, Xiaowen Qiu for their insightful discussion and Zhou Xian, Yuncong Yang, Zeyuan Yang, Jiaben Chen for their feedback on the project."}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 0, "text": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework Ailing Yu1 Lan Yao2* Jingnan Liu1 Zhe Chen1 Jiajun Yin1 Yuan Wang1 Xinhao Liao1 Zhiling Ye1 Ji Li1 Yun Yue1 Hansong Xiao1 Hualei Zhou1 Chunxiao Guo1 Peng Wei1 Jinjie Gu1 1Ant Group 2Harbin Institute of Technology  Code and Dataset: AQ-MedAI/MedReseacher-R1 Abstract Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts. We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions. Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains. Code and datasets will be released to facilitate further research. 53.4 53.2 39.8 33.3 28.2 GAIA XBench-DeepSearch 54.0 53.3 25 3.7 22.3 MedBrowseComp 27.5 25.5 25.0 19.0 14.0 MedResearcher-R1-32B WebSailor-32B_ReAct WebSailor-32B_ReAct QwQ-32B_search-o1 QwQ-32B_search-o1 Qwen-2.5-32B_search-o1 Qwen-2.5-32B_search-o1 o4_mini_direct o4_mini_direct MedResearcher-R1-32B o3_deepresearch gemini2.5pro_deepresearch o3_search gemini2.5pro_search MedResearcher-R1-32B Figure 1: Overall performance of MedReseacher-R1 across three benchmarks. On MedBrowseComp, our MedResearcher-R1-32B achieves state-of-the-art 27.5% accuracy, surpassing o3-deepresearch (25.5%), Gemini-2.5- Pro-deepresearch (25.0%), and significantly outperforming search-only approaches (o3-search: 19.0%, Gemini-2.5-Pro- search: 14.0%). On general deep research tasks, we achieve competitive results on GAIA (53.4 vs. WebSailor-32B’s 53.2) and xBench (54.0 vs. WebSailor-32B’s 53.3). *Work done during an internship at Ant Group. 1 A preprint 1 Introduction Recent advances in Large Language Models (LLMs) have catalyzed widespread adoption of LLM-based agents across diverse domains, including software engineering [Wang et al., 2024, Jimenez et al., 2023] and deep research systems [Xu and Peng, 2025]. These agents exhibit impressive capabilities in processing environmental observations, maintaining context across multiple interactions, and executing complex multi-step reasoning tasks. However, medical domain presents unique challenges that current general-purpose deep research agents fail to address adequately. The recently introduced MedBrowseComp benchmark [Chen et al., 2025b] reveals this critical gap: even OpenAI’s o3-deepresearch, the leading proprietary deep research system, achieves only 25.5% accuracy on complex medical queries requiring multi-hop reasoning across medical knowledge sources. We identify two fundamental limitations that contribute to"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 1, "text": "agents fail to address adequately. The recently introduced MedBrowseComp benchmark [Chen et al., 2025b] reveals this critical gap: even OpenAI’s o3-deepresearch, the leading proprietary deep research system, achieves only 25.5% accuracy on complex medical queries requiring multi-hop reasoning across medical knowledge sources. We identify two fundamental limitations that contribute to this performance gap: (1) general-purpose agents lack the dense, specialized medical knowledge required for accurate clinical reasoning, and (2) they rely on generic retrieval tools that fail to capture the nuanced relationships in medical information. The core challenge lies in what we term the sparse medical knowledge problem. Medical research often requires connect- ing rare diseases, emerging treatments, and specialized clinical findings through non-obvious pathways—connections that exist in specialized medical literature but remain inaccessible to general search tools. While existing medical AI systems have made progress in structured tasks like diagnosis, they primarily focus on common medical scenarios with well-established reasoning patterns. These systems fail to develop the capability for exploratory medical research that characterizes expert clinicians: simultaneously pursuing multiple hypotheses, synthesizing evidence from disparate sources, and identifying subtle connections between rare medical entities. To address these limitations, we propose a comprehensive approach that fundamentally rethinks how medical agents should be trained. Our key insight is that effective medical reasoning requires exposure to genuinely complex medical scenarios during training rather than simplified approximations. We achieve this through three interconnected innovations: First, we develop a novel data synthesis framework that generates training examples of exceptional complexity through a systematic pipeline: We begin by extracting medical entities from over 30 million PubMed abstracts, then apply frequency analysis to identify candidates with occurrence rates below 10−6 in medical corpora. Through LLM-assisted evaluation, we filter these candidates to select genuinely rare yet clinically significant entities, avoiding both trivial typos and overly common conditions. Around these carefully selected rare medical entities, we construct knowledge graphs to extract longest reasoning chains for multi-hop question generation. This approach creates questions that mirror real medical research challenges and cannot be answered through simple retrieval but require systematic exploration and synthesis across multiple medical information sources. Tools 1. Direct access to FDA databases, official prescription data, and clinical trials ensures accurate Valsartan information rather than relying on potentially incorrect public sources. 2. Evidence-based engine systematically validates the complete chain: \"corporate merger → heart failure drug → chemical composition → receptor mechanism → side effects,\" ensuring clinical accuracy and avoiding general AI reasoning errors. Supervised Fine-tuning Question: Identify the specific chemical compound based on the following multi- layered description: Begin with a major Swiss life-sciences company, known for its origins in Basel, which ceased to exist as an independent entity after a landmark corporate fusion in the final decade of the 20th century, creating a new pharmaceutical giant. This new conglomerate manufactures a branded prescription tablet designed to reduce hospitalization risk for a specific type of cardiac insufficiency. The primary active ingredient within this tablet has a mass per unit, measured in milligrams, that is a prime number whose two digits sum to a value equivalent to a perfect square minus three,"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 2, "text": "a branded prescription tablet designed to reduce hospitalization risk for a specific type of cardiac insufficiency. The primary active ingredient within this tablet has a mass per unit, measured in milligrams, that is a prime number whose two digits sum to a value equivalent to a perfect square minus three, and is one greater than a multiple of ten. This ingredient's therapeutic effect is achieved by selectively antagonizing the primary receptor of a key octapeptide hormone that regulates vasoconstriction. The common three-letter clinical acronym for this ingredient's drug class is homonymous with the trademark of an Australian-founded company renowned for its protective steel equipment for off-road vehicles. Finally, a significant potential metabolic side effect of this compound involves an elevated serum concentration of the chemical element with atomic number 19. Answer: Valsartan Think General Agent Search Visit 1. Lack of authoritative medical data source verification. 2. Inadequate knowledge integration capabilities. Think Verify Medical-Specific Tool Search Read Answer Answer Figure 2: Comparison of medical reasoning agents.MedReseacher-R1 resolves the Valsartan identification case that defeats general-purpose agents, demonstrating the strength of specialized medical database access and evidence-based reasoning integration. 2 A preprint Second, we introduce proprietary medical domain tools that address retrieval gaps in general systems. As illustrated in Figure 2, while general agents often fail when encountering medical-specific queries, particularly those involving rare diseases or complex chemical compounds, MedReseacher-R1 can iteratively invoke specialized medical tools alongside general-purpose tools to ensure accurate information retrieval. Unlike conventional search engines that rely on general web crawling, our custom-built private medical retrieval engine directly accesses authoritative medical databases including FDA databases, official prescription data, clinical trial registries, and peer-reviewed medical publications. The comparison in Figure 2 demonstrates how MedReseacher-R1 dynamically switch between general and medical-specific tools enables systematic validation of the complete evidence chain: from corporate merger information to heart failure drug development, to chemical composition and mechanism, ultimately ensuring clinical accuracy while avoiding the reasoning errors that plague general-only approaches. The system employs medical ontology-aware ranking to prioritize clinical authority and relevance over general web popularity metrics, effectively combining the breadth of general-purpose search with the precision of domain-specific medical expertise. Third, we implement a training methodology specifically designed for medical domains. Unlike recent work advocating pure reinforcement learning approaches, we find that medical tasks require what we call knowledge-anchored learning: initial supervised fine-tuning on high-quality medical trajectories proves highly effective for learning tool usage patterns and significantly improves final performance. Our Masked Trajectory Guidance (MTG) technique provides structural scaffolding while preventing memorization, forcing models to develop genuine medical reasoning capabilities rather than pattern matching. Our experimental results validate this approach. The trained model, MedReseacher-R1, achieves 27.5% accuracy on MedBrowseComp, establishing a new state-of-the-art and substantially outperforming both the Qwen2.5-32B baseline and existing deep research systems. Notably, our medical specialization does not compromise general capabilities: on general agent benchmarks (GAIA: 53.4, xBench: 54), MedReseacher-R1maintains competitive performance comparable to GPT-4o-mini. Please refer to Figure 1 for an overview. These results challenge the prevailing assumption that domain-specific agents require sacrificing general capabilities. Instead, we demonstrate that the rigorous reasoning"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 3, "text": "our medical specialization does not compromise general capabilities: on general agent benchmarks (GAIA: 53.4, xBench: 54), MedReseacher-R1maintains competitive performance comparable to GPT-4o-mini. Please refer to Figure 1 for an overview. These results challenge the prevailing assumption that domain-specific agents require sacrificing general capabilities. Instead, we demonstrate that the rigorous reasoning demanded by medical tasks—precise terminology, careful evidence evaluation, and systematic hypothesis testing—provides a superior training signal for developing robust agent capabili- ties. The dense knowledge structures and complex reasoning patterns learned from medical domains transfer effectively to general tasks, suggesting that specialized training can enhance rather than limit agent versatility. This work contributes to the rapidly evolving field of medical AI by demonstrating that achieving medical deep research capabilities requires fundamental innovations beyond applying general agents to medical tasks. Through careful design of training data, specialized tools, and learning algorithms tailored to medical reasoning, we show that it is possible to develop agents that approach expert-level medical research capabilities. We release our code, datasets, and trained models to facilitate further research in this critical area. 2 MedReseacher-R1: Medical Deep Research Agent Framework 2.1 Problem Definition We formalize the medical deep research task as a sequential decision-making problem where an agent must navigate complex medical knowledge sources to answer multi-hop queries that characterize the sparse medical knowledge problem identified in Section 1. Given a medical question q ∈Q, the agent operates with a heterogeneous toolset T = Tgeneral ∪Tmedical, where Tgeneral = {tg 1, . . . , tg m} comprises general-purpose tools (web search, document analysis) and Tmedical = {tm 1 , . . . , tm n } contains our proprietary medical domain tools that directly access authoritative medical databases. The agent maintains an evolving state st = (ct, kt, ht) at timestep t, where: • ct ∈C: dialogue context encoding the current query and response history • kt ∈K: accumulated medical knowledge from retrieved sources, structured as a knowledge graph • ht ∈H: reasoning history tracking explored knowledge paths and hypothesis evolution This state representation enables tracking of multi-hop reasoning chains essential for connecting rare medical entities through non-obvious pathways. At each timestep, the agent selects an action according to a learned policy: at ∼πθ(a | st, T , q) where πθ is trained through our knowledge-anchored learning approach to dynamically switch between general and medical-specific tools based on query requirements. 3 A preprint 2.2 Agent Architecture Our framework directly addresses the two fundamental limitations of general-purpose agents: insufficient medical knowledge density and reliance on generic retrieval tools that fail to capture nuanced medical relationships. Reasoning-Acting Paradigm. Following the REACT framework [Yao et al., 2023a], our agent operates via iterative reason–act–observe cycles, augmented with medical-specific enhancements that enable exploratory medical research capabilities. At each step, the policy generates: • Thought: A medical reasoning trace that identifies information gaps, formulates hypotheses, and determines whether general or specialized tools are needed • Action: A tool invocation with parameters optimized for medical information extraction, prioritizing authoritative sources over general web content • Observation: Structured medical knowledge validated against clinical evidence and incorporated into"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 4, "text": "A medical reasoning trace that identifies information gaps, formulates hypotheses, and determines whether general or specialized tools are needed • Action: A tool invocation with parameters optimized for medical information extraction, prioritizing authoritative sources over general web content • Observation: Structured medical knowledge validated against clinical evidence and incorporated into the agent’s evolving state This process continues iteratively, with the agent pursuing multiple hypotheses simultaneously until synthesizing a comprehensive answer. Complex multi-hop questions typically require 4-5 tool interactions, mirroring the systematic exploration patterns of expert clinicians. General-Purpose Tools. Our agent retains access to standard tools for breadth of coverage: (1) WebSearch: Standard web retrieval for general medical information, recent developments, and corporate/organiza- tional data (e.g., pharmaceutical company mergers as shown in figure 2). (2) DocumentRead: Extraction and synthesis from retrieved documents using high-capacity LLM backbones (e.g., Qwen2.5-72B [Bai et al., 2024]), particularly for processing lengthy clinical reports or research papers. Medical-Specific Tool Suite. A core innovation of our architecture is the integration of proprietary medical domain tools that address the unique challenges of clinical research and bridge the gap between general retrieval and specialized medical reasoning. Our medical-specific tool suite includes: (1) PrivateMedicalRetriever: This module aggregates evidence directly from authoritative clinical resources, including FDA databases, clinical trial registries, and PubMed publications. Each candidate document d is scored for a query q by a weighted linear combination of semantic relevance and clinical authority: Score(d, q) = λ Rel(d, q) + (1 −λ) Auth(d), where Rel(d, q) represents the semantic similarity to the query (computed via embedding cosine similarity), and Auth(d) reflects the clinical authority (combining impact factor and guideline status). The hyperparameter λ (0 ≤λ ≤1) balances the importance between relevance and authority; in all experiments, we set λ = 0.4 to favor reliable and clinically significant evidence. (2) ClinicalReasoningEngine: Designed for evidence-based differential diagnosis, this tool applies Bayesian inference to systematically evaluate multiple hypotheses. Given observed symptoms s, candidate diagnoses Dj, and patient context c, the posterior for each diagnosis is computed as: P(Dj | s, c) = Qn i=1 P(si | Dj, c) · P(Dj | c) Pm k=1 Qn i=1 P(si | Dk, c) · P(Dk | c) where conditional probabilities are derived from clinical literature and iteratively updated based on newly retrieved evidence. Dynamic Tool Selection Strategy. As illustrated in Figure 2, our agent dynamically switches between general and medical-specific tools to ensure complete evidence chains. The tool selection is governed by a learned policy that evaluates query complexity: P(t | st, q) = \u001aσ(wT mϕ(st, q)) if t ∈Tmedical σ(wT g ϕ(st, q)) if t ∈Tgeneral 4 A preprint where ϕ(st, q) extracts features including entity rarity, required reasoning hops, and presence of medical terminology, wm and wg are learned weight vectors, and σ(·) is the sigmoid function. The policy learns to prioritize medical tools when encountering rare diseases or complex chemical compounds while leveraging general tools for contextual information. Together, PrivateMedicalRetriever and ClinicalReasoningEngine comprise the medical-specific tool suite, enabling the agent to retrieve, interpret, and reason over specialized clinical evidence far beyond the reach of general-purpose tools."}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 5, "text": "The policy learns to prioritize medical tools when encountering rare diseases or complex chemical compounds while leveraging general tools for contextual information. Together, PrivateMedicalRetriever and ClinicalReasoningEngine comprise the medical-specific tool suite, enabling the agent to retrieve, interpret, and reason over specialized clinical evidence far beyond the reach of general-purpose tools. 3 KISA: Knowledge-Informed Trajectory Synthesis Approach To address the critical challenge of training data scarcity for medical deep research agents, we propose a Knowledge- Informed Trajectory Synthesis Approach (KISA) that generates complex, multi-hop medical reasoning trajectories. Our framework directly tackles the limitations of general-purpose agents by creating training data that emphasizes: (1) rare medical entity connections requiring dense domain knowledge, and (2) effective utilization of medical-specific retrieval tools. 3.1 Agentic Dataset Construction Our dataset construction pipeline consists of three interconnected components designed to generate genuinely complex medical queries that robustly stress-test agent capabilities: 3.1.1 Entity-Centric Knowledge Graph Construction We construct medical knowledge graphs specifically optimized for generating complex reasoning chains. Unlike traditional approaches that focus on common concepts, we prioritize rare medical entities Eseed with frequency below threshold τrare = 10−6 in general medical corpora. Focusing on rare entities ensures that generated questions require deep medical knowledge, as opposed to surface-level information obtainable through general search. The graph expansion follows an iterative process: ei+1 ∼ \u001aUniform(N(ei)) with probability 0.5 Discover(Enew|ei) with probability 0.5 where N(ei) denotes the set of neighbors of ei and Discover(·) identifies novel entities via our private medical retrieval engine, ensuring that new connections are both medically valid and challenging. Each relation is augmented with additional contextual information: r = ⟨esubj, p, eobj, ttemporal, lspatial, cclinical⟩ where cclinical encodes the clinical context (e.g., disease stage, patient demographics), ttemporal captures temporal aspects, and lspatial denotes spatial context. This enriched representation improves multi-hop reasoning accuracy by 12.3% compared to standard triplets. 3.1.2 Multi-Hop Question Generation via Longest-Path Extraction Our key innovation lies in extracting longest chains from subgraphs to generate maximally complex queries. For each rare entity subgraph Gsub, we compute the longest valid reasoning path: P∗= arg max p∈P(Gsub) Length(p) s.t. MedicallyValid(p) where P(Gsub) is the set of all paths in Gsub. This longest-path strategy ensures that questions require multiple reasoning hops (average 4.2 per trajectory), rather than being answerable via simple lookups. These paths are subsequently transformed into natural language questions that require sequential tool invocations to reconstruct the complete reasoning chain. 5 A preprint Question: \"Identify the chemical compound: Swiss company... corporate fusion 1990s... prescription tablet... active ingredient mass is prime number... antagonizing octapeptide hormone receptor... three-letter acronym... Australian off- road equipment... elevated element atomic number 19...\" Reasoning: Step 1: Swiss company → “Ciba-Geigy” Step 2: Merger → \"Novartis...\" Step 3: Medication → \"Valentas...\" ... Step 8: Convergence → \"Valsartan\" Answer: \"Valsartan\" Lung Neoplasms Diabetes Mellitus MODY Seizures Dravet Syndrome PubMed Entities Extraction Knowledge Graph Building Subgraph Sampling LLM-based Rare Entity Screening Abridged Example Figure 3: Knowledge graph-based question generation pipeline: extracting longest chains from rare entity subgraphs to create complex multi-hop medical queries. 3.1.3 Quality Control and Difficulty Calibration To ensure that generated questions remain challenging for"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 6, "text": "Dravet Syndrome PubMed Entities Extraction Knowledge Graph Building Subgraph Sampling LLM-based Rare Entity Screening Abridged Example Figure 3: Knowledge graph-based question generation pipeline: extracting longest chains from rare entity subgraphs to create complex multi-hop medical queries. 3.1.3 Quality Control and Difficulty Calibration To ensure that generated questions remain challenging for current systems, we implement adaptive difficulty calibration. Each question is evaluated against GPT-o3 deepresearch and GPT-4, if either model achieves > 50% accuracy, the question is automatically regenerated with increased complexity: q′ = \u001aq if max(AccO3(q), AccGPT4(q)) < 0.5 Regenerate(q, complexity + 1) otherwise This approach ensures our dataset remains challenging even for state-of-the-art systems, directly addressing the 25.5% performance ceiling previously observed in MedBrowseComp. 3.2 Trajectory Synthesis with Medical Tool Integration 3.2.1 Masked Trajectory Guidance (MTG) To generate high-quality training trajectories that effectively utilize our medical-specific tools, we introduce Masked Trajectory Guidance(MTG). Given a reasoning graph path T = {(e1, r1, e2), . . . , (en−1, rn−1, en)} extracted from the knowledge graph, we create a structural scaffold by masking the entities: Tmasked = {([MASK], ri, [MASK])}n−1 i=1 This masking process serves two main purposes: • Tool selection learning: Encourages the model to determine when medical-specific retrieval tools are required versus when general search suffices. • Prevention of shortcuts: Prevents answer memorization while maintaining the underlying reasoning process. Step 1: Swiss Pharmaceutical History • Started with Ciba-Geigy - a major Swiss life-sciences company from Basel • The clue mentioned it \"ceased to exist after a landmark corporate fusion\" in the 1990s Step 2: Corporate Merger Investigation • Ciba-Geigy identified the 1996 merger of Ciba-Geigy and Sandoz • This created Novartis Pharmaceuticals Corporation Step 3: Medication Identification • Searched for Novartis heart failure medications that reduce hospitalization • Led to Valentas 100 Tablet - a branded prescription for cardiac insufficiency … … Step 8: Final Convergence • All evidence pointed to Valsartan Origin Reasoning Path Entity_mapping: { \"Acronym_A\": \"ARB\", \"Clue_1\": \"The mechanism of an angiotensin II receptor blocker (antagonizing the AT1 receptor for the octapeptide hormone angiotensin II).\", \"Clue_2\": \"Hyperkalemia (elevated serum concentration of potassium, the element with atomic number 19).\", \"Data_A\": \"51 mg\", \"Entity_A\": \"Ciba-Geigy\", \"Entity_B\": \"Novartis Pharmaceuticals Corporation\", \"Entity_C\": \"Valentas 100 Tablet\", \"Entity_D\": \"ARB Bumpers (representing the company ARB 4x4 Accessories)\", \"Event_A\": \"Novartis formed by merger of Ciba-Geigy and Sandoz\", \"Target_Entity\": \"Valsartan\" }, Entity Mapping Step 1: Swiss Pharmaceutical History • Started with \"Entity_A\" - a major Swiss life-sciences company from Basel • The clue mentioned it \"ceased to exist after a landmark corporate fusion\" in the 1990s Step 2: Corporate Merger Investigation • \"Event_A\" identified the 1996 merger of Entity_A and Sandoz • This created \"Entity_B\" - the new pharmaceutical giant Step 3: Medication Identification • Searched for Entity_B heart failure medications that reduce hospitalization • Led to \"Entity_C\" - a branded prescription for cardiac insufficiency … Step 8: Final Convergence • All evidence pointed to Target_Entity Masked Hint Figure 4: Masked Trajectory Guidance: a structural scaffold that enables reasoning without shortcut learning by masking entities. 6 A preprint 3.2.2 Hybrid Strategy for Tool Diversity To promote robust"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 7, "text": "\"Entity_C\" - a branded prescription for cardiac insufficiency … Step 8: Final Convergence • All evidence pointed to Target_Entity Masked Hint Figure 4: Masked Trajectory Guidance: a structural scaffold that enables reasoning without shortcut learning by masking entities. 6 A preprint 3.2.2 Hybrid Strategy for Tool Diversity To promote robust and diverse tool usage, we adopt a hybrid data strategy: Dtrain = α · Dguided + (1 −α) · Dexploration, where α = 0.7 balances structured learning with exploration. The exploration trajectories naturally cultivate three key behaviors: • Medical tool prioritization: 78% begin with private medical retriever for rare entities • Tool switching: 42% demonstrate adaptive switching between general and medical tools • Error recovery: 34% include explicit correction using alternative tools 4 Large-scale Agent Training 4.1 Cold Start with Supervised Fine-Tuning We initiate agent training via supervised fine-tuning (SFT) on a large collection of synthetic agentic dialogues D = {(x(i), y(i))}N i=1. Here, x(i) denotes the input context, and y(i) the ideal next-action sequence (thought, tool call, etc.) for each example. The objective is to maximize the likelihood of generating the correct trajectory conditioned on context and prior agent history: LSFT(θ) = −1 N N X i=1 |y(i)| X k=1 log pθ(y(i) k |x(i), y(i) <k). To promote agent robustness and generalization, we incorporate several key augmentations during fine-tuning: • Tool failure simulation (5% corruption rate): Randomly corrupts tool output to encourage contingency planning and robust error recovery in downstream trajectories. • Intermediate thought supervision: Teaches the agent to articulate explicit reasoning prior to every tool invocation, improving interpretability and decision traceability. • Multi-task sampling: Diversifies training batches across medical domains (diagnosis, treatment, guidelines, rare diseases), supporting broad generalization and transfer. Optimization proceeds using the AdamW optimizer (learning rate λ = 0.01), with a cosine annealing schedule (ηmax = 3 × 10−7), for 3 epochs on 8 × H200 GPUs. This ensures rapid exploration of diverse trajectories and convergence to well-calibrated agent policies. 4.2 Reinforcement Learning After supervised warm-starting, we refine the agent via reinforcement learning using Grouped Regularized Policy Optimization (GRPO), optimizing agentic trajectories relative to task-specific composite rewards: rt = α rtask + β rexpert −γ refficiency, where rtask measures answer accuracy, rexpert reflects preference according to a GPT-4-based expert model, and refficiency penalizes excessive or redundant tool usage. The weighting coefficients α, β, and γ are set to 1.0, 0.2, and 0.1, respectively. Reward Modeling: The reward function is broken down as follows: • rtask: The primary component of the reward function, measuring answer accuracy directly and calculating the task completion score per query. • rexpert: Derived from the GPT-4 preference model, this term refines the model’s responses to align with expert knowledge. • refficiency: Penalizes unnecessary tool usage, including repeated calls to the same tool without added value, excessive tool usage after an answer is found, and using irrelevant tools for a task. The efficiency penalty is evaluated using both rule-based systems and an LLM-judge to classify unnecessary usage. 7 A preprint GRPO Objective: The GRPO objective optimizes: LGRPO = E(x,y)∼D \u0002 log πθ(y|x) · \u0000r(x, y)"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 8, "text": "value, excessive tool usage after an answer is found, and using irrelevant tools for a task. The efficiency penalty is evaluated using both rule-based systems and an LLM-judge to classify unnecessary usage. 7 A preprint GRPO Objective: The GRPO objective optimizes: LGRPO = E(x,y)∼D \u0002 log πθ(y|x) · \u0000r(x, y) −¯ rG(x) \u0001\u0003 , where ¯ rG(x) is the group-level baseline, computed as the average reward from responses in the same batch. This group normalization stabilizes the gradient estimates. Additional Modifications: • KL Regularization: We remove KL-regularization from the training pipeline, as it may hinder performance improvements, especially during multi-stage training. This aligns with literature showing benefits of omitting KL loss for model generalization[He et al., 2025]. • Task Complexity: Task complexity increases progressively via curriculum learning, monitored by the average pass rate on tasks. This ensures the model is challenged appropriately without overwhelming it early in training. 5 Experiments We evaluate MedReseacher-R1 across both domain-specific and general-purpose benchmarks to assess its effectiveness in complex medical research tasks and its generalization capabilities beyond the medical domain. 5.1 Benchmarks • MedBrowseComp [Chen et al., 2025b] is a recently proposed benchmark specifically designed to evaluate LLM agents’ capabilities in retrieving and synthesizing medical evidence from multiple web sources. This benchmark presents agents with open-ended clinical questions that necessitate multi-step reasoning, strategic information gathering, and effective utilization of web browsing APIs to construct comprehensive medical assessments. • GAIA [Shinn et al., 2023] (General AI Assistant) is a comprehensive evaluation framework that tests real-world assistant capabilities through complex, multi-modal tasks requiring tool use, web search, and multi-step reasoning. The benchmark emphasizes tasks that are conceptually simple for humans but challenging for AI systems, focusing on fundamental skills like reading comprehension, logical reasoning, and the ability to use tools effectively in realistic scenarios. • XBench-DeepSearch [Chen et al., 2025a] is an extensive multi-domain agent evaluation suite that systematically assesses tool-use capabilities across diverse open-domain tasks. The benchmark encompasses a broad spectrum of scenarios including fact-checking, comparative analysis, web browsing-based reasoning, and complex information synthesis tasks, providing a comprehensive evaluation of an agent’s ability to navigate and utilize various tools in real-world problem-solving contexts. 5.2 Main Results As shown in Table 1, our tool-augmented agent achieves new state-of-the-art performance on MedBrowseComp benchmark, achieving a pass@1 accuracy of 27.5% and outperforming both previous best agents and the Qwen2.5-32B baseline. The supervised fine-tuning (SFT) stage already delivers notable gains, while subsequent reinforcement learning further improves decision quality and tool orchestration efficiency. Notably, despite being primarily trained for medical domains, our agent demonstrates strong generalization to open- domain tasks shown in Table 2. On GAIA and XBench-deepsearch, our system shows competitive helpfulness scores, demonstrating the versatility of tool-based training paradigms. Table 1: Performance Comparison on MedBrowseComp Benchmarks (number correct out of 50) Model o3 search gemini2.5pro deepsearch o3 deepresearch claude-cua MedASearch-32B MedBrowseComp 19.0 24.5 25.5 18.0 27.5 5.3 Qualitative Analysis To understand the underlying factors driving performance improvements, we conducted an in-depth analysis of the training data patterns and their impact on agent behavior. Our investigation reveals that training data following"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 9, "text": "50) Model o3 search gemini2.5pro deepsearch o3 deepresearch claude-cua MedASearch-32B MedBrowseComp 19.0 24.5 25.5 18.0 27.5 5.3 Qualitative Analysis To understand the underlying factors driving performance improvements, we conducted an in-depth analysis of the training data patterns and their impact on agent behavior. Our investigation reveals that training data following 8 A preprint Table 2: Performance Comparison on Xbench-DeepSearch and GAIA Benchmarks Model Paradigm Xbench-DeepSearch GAIA Qwen-2.5-32B Direct 8.7 13.6 Qwen-2.5-72B Direct 12.7 14.6 GPT-4o Direct 18.0 17.5 GPT-4.1 Direct 17.0 22.3 QwQ-32B Direct 10.7 22.3 o4-mini Direct 22.3 33.3 DeepSeek-R1 Direct 32.7 16.5 Qwen-2.5-32B Search-o1 3.7 28.2 WebDancer-32B ReAct 38.7 40.7 QwQ-32B Search-o1 25.0 39.8 WebSailor-7B ReAct 34.3 37.9 WebSailor-32B ReAct 53.3 53.2 WebSailor-72B ReAct 55.0 55.4 MedASearch-32B ReAct 54.0 53.4 the paradigm of iterative search-verification-synthesis yields the most significant improvements in deep research capabilities. Figure 5 illustrates a representative example where our agent demonstrates superior research depth through systematic evidence gathering. The agent executes a 4-step strategy: (1) initial broad search to identify relevant sources, (2) verification of information consistency across multiple authoritative medical databases, (3) targeted follow-up queries to resolve ambiguities, and (4) comprehensive synthesis of validated findings. This methodical approach—characterized by multiple verification cycles ensuring answer uniqueness before final synthesis—contrasts sharply with baseline agents that exhibit premature convergence or suboptimal tool utilization patterns. Analysis of successful trajectories reveals that the critical differentiator lies in the search→verifyn→synthesize pattern, where n represents multiple verification iterations. Training instances exhibiting this pattern show 34.2% higher success Search Verify Synthesize Read Figure 5: Case study demonstrating the search-verify-synthesize paradigm: The agent performs multiple verification rounds across medical sources, ensuring information consistency before synthesis. Baseline agents (shown in gray) terminate prematurely after initial search, while our approach (blue) continues until achieving high confidence through cross-validation. 9 A preprint rates in complex multi-hop reasoning tasks compared to single-verification approaches. The iterative verification ensures answer uniqueness and factual grounding, particularly crucial for domains requiring high accuracy such as medical diagnosis. These findings demonstrate that tool-augmented agent training effectiveness is fundamentally linked to the structural patterns in training data, with iterative verification serving as the key mechanism for developing robust deep research capabilities that generalize across diverse tool-reasoning environments. 6 Related Work 6.1 General Deep Research Methods Recent advances in agent-based deep research and autonomous information gathering frameworks have emerged in two main paradigms: Multi-agent planning architectures and Agent Reinforcement Learning systems. Multi-agent planning architectures decompose the research process into semantically distinct roles, with different agents focusing on subtasks such as retrieval, reasoning, synthesis, or evaluation. These agents collaborate through modular pipelines or structured communication protocols. CAMEL [Li et al., 2023] introduces a communication-driven multi-agent framework in which agents communicate using natural language to solve complex planning and reasoning tasks. This framework emphasizes inter-agent communication to achieve policy coordination and emergent behavior. Since 2025, many large-scale reasoning models (LRMs) have extended deep research capabilities, such as OpenAI O3, Perplexity Deep Research, and Kimi K2. For example, Anthropic’s multi-agent research system proposes a master agent that dynamically spawns specialized child agents to perform web search, document reading, and"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 10, "text": "policy coordination and emergent behavior. Since 2025, many large-scale reasoning models (LRMs) have extended deep research capabilities, such as OpenAI O3, Perplexity Deep Research, and Kimi K2. For example, Anthropic’s multi-agent research system proposes a master agent that dynamically spawns specialized child agents to perform web search, document reading, and synthesis. This architecture excels at complex, long-term research tasks through parallel execution and implicit memory sharing. There are also many open-source projects implementing deep learning with multi-agent mechanisms, such as Deerflow. Deep learning systems implemented using multi-agent approaches have significant advantages: they are more interpretable and easier to scale through parallelization. However, due to the lack of targeted reasoning training, planning based solely on prompts and characters can cause errors to propagate across multiple agents and is unable to handle tasks requiring high-level reasoning. In contrast to modular designs, Agent RL approaches train single or semi-autonomous agents through reward-guided interaction with a research environment (typically web browsing or open-domain question answering). These agents typically learn to autonomously search, click, read, and synthesize using offline data and are then fine-tuned through post-training. The ReAct agent [Yao et al., 2023b] was originally proposed as a prompting strategy but has been further optimized using RLHF to enforce optimal reasoning paths. Reinforcement learning techniques enable agents to optimize tool usage and avoid hallucinations during long-term interactions. WebArena [Zhou et al., 2024] provides a high-fidelity web interaction environment for training reinforcement learning agents to perform multi-hop reasoning and agentic data collection via real browser APIs, enabling realistic, feedback-driven learning. WebSailor [Li et al., 2025] enables superhuman web research in high-uncertainty QA environments. It uses synthetic task construction, RFT-based cold starts, and DUPO (Repeated Sampling Policy Optimization) reinforcement learning fine-tuning to build robust agents for tool-augmented web tasks. Search-R1 [Jin et al., 2025] trains LLMs to interleave reasoning and search via unified RL, while S3 [Jiang et al., 2025] decouples search from generation and reaches comparable accuracy with 70× fewer samples. Compared to deep research systems based on multi-role agents, agent reinforcement learning offers the advantage of internalizing the model’s problem-solving capabilities through learned behaviors, enabling better generalization to unknown tasks and adaptability to complex environments such as web browsing. However, while general-purpose web agents excel in open-domain environments, their architecture systematically ignores the importance and time constraints inherent to evidence provenance in healthcare. The lack of healthcare- specific components (e.g., de-identification engines, clinical-level evidence graders, and medication compliance audits) severely limits their clinical utility. 6.2 Medical RAG Systems The domain-specific Retrieval-Augmented Generation (RAG) architecture has made a significant contribution to the field of medical clinical AI through systematic innovations in evidence integration. MedRAG [Zhao et al., 2025b] establishes a paradigm for evidence-based generation by enabling immutable corpus retrieval from PubMed snapshots and proprietary databases. Deeprare [Zhao et al., 2025a] MedRAG’s real-time evidence assimilation, which continuously synchronizes with evolving medical knowledge through live CDC/WHO data streams and dynamically weights it (F1 score +14.3%), directly addresses the knowledge obsolescence issue inherent in systems like DeepRare. SurgRAW [Low et al., 2025] pioneered the integration of real-time surgical video retrieval with"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 11, "text": "al., 2025a] MedRAG’s real-time evidence assimilation, which continuously synchronizes with evolving medical knowledge through live CDC/WHO data streams and dynamically weights it (F1 score +14.3%), directly addresses the knowledge obsolescence issue inherent in systems like DeepRare. SurgRAW [Low et al., 2025] pioneered the integration of real-time surgical video retrieval with 10 A preprint reinforcement learning, enabling intraoperative decision support with an instrument recognition accuracy of 90.2%. Federated ClinicalCamel [Toma et al., 2023] addresses data fragmentation through cross-institutional knowledge distillation while maintaining privacy compliance (AUROC of 0.92 across 12 hospitals). Despite these advances, current medical RAG systems still suffer from fundamental limitations. First, knowledge obsolescence remains a critical issue, as module updates in systems like DeepRare [Zhao et al., 2025a] require manual orchestration, resulting in curation delays that can reduce retrieval relevance by months. In addition, evidence misalignment manifests as semantic drift, which is particularly evident in KBLaM’s plug-in architecture. Module updates lead to cumulative embedding misalignment (MRR drops by 18.4% after 5 iterations) [Wang et al., 2025]. 6.3 Medical Multi-Role Systems Recent advances in agent-based architectures reveal a paradigm shift through endogenous integration of retrieval- reasoning-verification loops, particularly evident in the emergence of Agentic RAG frameworks and multimodal knowledge integration. These systems demonstrate three core innovations that redefine clinical decision support: Dynamic Knowledge Internalization through self-updating graphs that eliminate external dependencies enables continuous synchronization with evolving medical knowledge. SeaKR’s [Yao et al., 2024] self-aware retrieval introduces temporal grounding mechanisms that dynamically adjust knowledge weights based on publication recency and evidence grade, while Med-PaLM’s [Tu et al., 2023] visual-linguistic separation processes radiology images and genomic data via dedicated pathways while maintaining diagnostic coherence. These approaches reduce knowledge latency from days to minutes compared to traditional RAG systems. Preference-Aligned Reinforcement Learning frameworks like MedicalGPT v2.4’s GRPO (Group Relative Policy Optimization) achieve 98.7% agreement with clinician panels in oncology decisions [Xu, 2023]. Unified Cognitive Architectures collapse retrieval-reasoning-verification into integrated pipelines, exemplified by Microsoft’s MAI-DxO [Nori et al., 2025] with five collaborative agents achieving 85.5% diagnostic accuracy - quadruple average clinician performance. Regulatory compliance is maintained through Med-Gemini’s [Saab et al., 2024] 3-stage pipeline combining temporal grounding, clinician-verified SFT, and multi- objective RLHF. Despite these advancements, critical limitations persist in the reasoning capabilities of current medical multi-role agent systems - a fundamental gap compared to deep reasoning methods in medical research. First, multi-step clinical reasoning remains constrained by shallow inference depth: while systems like AgentClinic [Schmidgall et al., 2025] demonstrate 42.9% diagnostic accuracy in sequential decision-making, this drops significantly when tasks require > 5 reasoning steps (↓27.3% at 7 steps). Second, causal reasoning deficits manifest in treatment planning scenarios, where agents struggle to model long-term outcome dependencies (e.g., chemotherapy sequencing effects) compared to human specialists (F1-score gap of 19.4% in NCCN guideline adherence) [?]. Third, adaptive reasoning limitations emerge in dynamic clinical environments - systems like MAI-DxO show 34% performance degradation when handling real-time patient deterioration scenarios requiring protocol switching. These challenges highlight the urgent need for next-generation architectures that bridge the reasoning depth and adaptability gap between multi-role agents and human medical experts. 7 Conclusion"}
{"doc_id": "2508.14880v1", "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14880v1", "chunk_id": 12, "text": "reasoning limitations emerge in dynamic clinical environments - systems like MAI-DxO show 34% performance degradation when handling real-time patient deterioration scenarios requiring protocol switching. These challenges highlight the urgent need for next-generation architectures that bridge the reasoning depth and adaptability gap between multi-role agents and human medical experts. 7 Conclusion In this work, we address the challenge of complex, evidence-based medical research by introducing a new agent development framework centered on the KISA data generation approach. KISA systematically produces challenging, multi-hop medical question–answer pairs with corresponding reasoning trajectories, grounded in rare entity mining and knowledge graph-based reasoning chains. This ensures that agents are exposed to the intricate, compositional problems characteristic of real-world medical research. Built on this rich dataset and equipped with a comprehensive training pipeline—including supervised fine-tuning, trajectory masking, and reinforcement learning with specialized medical tools—our agent, MedReseacher-R1 achieves state-of-the-art pass@1 accuracy on MedBrowseComp (27.5%) and demonstrates robust performance on general agent benchmarks. These findings show that MedReseacher-R1 is capable of solving complex medical questions that demand systematic exploration and nuanced evidence synthesis, highlighting its effectiveness as a next-generation deep research agent in the medical domain. 8 Future Work Building on the foundation of this study, we identify several concrete directions for advancing deep medical research agents: 11 A preprint • Multi-modal Tool Integration: Extend the current framework to support multi-modal medical tools such as radiology image viewers, pathology slide analyzers, genomic data sources, and electronic health records. Such integration would enable agents to process and synthesize diverse data types, aligning more closely with real-world clinical workflows. • Human-Expert Collaboration: Incorporate human-in-the-loop feedback from medical professionals to guide agent behavior. Developing interfaces for expert evaluation and annotation can improve reasoning quality, tool usage, and the clinical relevance of agent outputs. • Safety and Reliability: Systematically study model safety and reliability for open deployment, focusing on robust hallucination detection, uncertainty estimation, and the implementation of fail-safe mechanisms suitable for high- stakes medical scenarios. • Advanced Medical Reasoning Benchmarks: Construct a comprehensive benchmark for complex multi-hop reasoning across medical domains—covering pharmacology, diagnostics, epidemiology, genetics, surgical planning, and therapy. This would set a higher standard for evaluating agents’ ability to orchestrate tools and synthesize evidence in challenging scenarios. Our framework paves the way for more aligned and reliable agent-based systems in specialized domains like healthcare. By releasing our codebase, datasets, and trained models, we seek to foster collaborative progress and rigorous evaluation, moving toward trustworthy AI companions that can augment medical research and support improved patient outcomes."}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 0, "text": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models Hend S. Al-Khalifa Information Technology Department, College of Computer and Information Sciences King Saud University Riyadh, Saudi Arabia hendk@ksu.edu.sa Raneem Almansour Radiological sciences, College of Applied Medical Sciences King Saud University Riyadh, Saudi Arabia 443202782@student.ksu.edu.sa Layan Abdulrahman Alhuasini Radiological sciences, College of Applied Medical Sciences King Saud University Riyadh, Saudi Arabia 443202718@student.ksu.edu.sa Alanood Alsaleh Radiological sciences, College of Applied Medical Sciences King Saud University Riyadh, Saudi Arabia alalanoud@ksu.edu.sa Mohamad-Hani Temsah Pediatric Department, College of Medicine King Saud University Riyadh, Saudi Arabia mtemsah@ksu.edu.sa Ashwag Rafea S Alruwaili Radiological sciences, College of Applied Medical Sciences King Saud University Riyadh, Saudi Arabia ashalruwaili@ksu.edu.sa Abstract Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering literacy, including increased functional connectivity in brain regions such as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency dynamics in key cognitive networks. These findings offer initial insights into the neurobiological basis of prompt engineering proficiency. We discuss the implications of these neurocognitive markers in Natural Language Processing (NLP). Understanding the neural basis of human expertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces, contribute to cognitive models of LLM interaction, and potentially guide the development of AI systems that better align with human cognitive workflows. This interdisciplinary approach aims to bridge the gap between human cognition and machine intelligence, fostering a deeper understanding of how humans learn and adapt to complex AI systems. 1. Introduction The advent of sophisticated Large Language Models (LLMs) has revolutionized numerous domains, yet their effective utilization hinges significantly on the art and science of prompt engineering. The ability to craft precise and effective prompts that guide LLMs towards the desired outputs is becoming an indispensable skill. While the Natural Language Processing (NLP) community has made substantial strides in developing more capable LLMs and understanding their internal workings [1], [2], the human side of this interaction—specifically, the cognitive and neural mechanisms that differentiate expert prompt engineers from novices—remains a nascent area of research. As LLMs become increasingly integrated into daily tasks and complex problem- solving, understanding the cognitive demands and neural adaptations associated with mastering prompt engineering is crucial for optimizing human-AI collaboration. This study took an interdisciplinary approach, leveraging neuroimaging techniques to explore the neural correlations of prompt engineering expertise. We hypothesize that expertise in prompt engineering is not merely a matter of accumulated knowledge but is also reflected in distinct patterns of brain function and connectivity. The primary contributions of this work are threefold. First, we introduce a novel methodology for quantifying prompt engineering literacy and use it to differentiate between expert and intermediate users. Second, we provide the first, to our knowledge, empirical evidence"}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 1, "text": "is also reflected in distinct patterns of brain function and connectivity. The primary contributions of this work are threefold. First, we introduce a novel methodology for quantifying prompt engineering literacy and use it to differentiate between expert and intermediate users. Second, we provide the first, to our knowledge, empirical evidence from fMRI data identifying distinct neural signatures (including altered functional connectivity in language and executive control regions, and different network power dynamics) associated with prompt engineering expertise. Third, we discuss the potential implications of these neurocognitive markers for the NLP field, particularly for designing more intuitive human-AI interfaces, developing cognitive models of LLM interaction, and guiding the creation of AI systems that better align with human cognitive processes. By identifying these neural markers, we aim to provide a foundational understanding of the cognitive processes that underpin successful human-LLM interaction. This research seeks to bridge insights from cognitive neuroscience with challenges and opportunities within NLP. Understanding how the human brain adapts to the task of \"programming\" LLMs through natural language can inform the design of more intuitive prompting interfaces, contribute to the development of training programs for prompt engineering, and potentially inspire new architectures for LLMs that are more attuned to human cognitive styles. Our pilot study employed functional magnetic resonance imaging (fMRI) to compare resting-state brain activity between individuals classified as experts and intermediate prompt engineers based on a validated Prompt Engineering Literacy Scale. We focused on differences in functional connectivity and network power dynamics to identify objective neural indicators of prompting proficiency. The findings presented herein offer preliminary evidence for such neural distinctions, and open avenues for future research at the intersection of neuroscience, cognitive science, and NLP. This work aims to contribute to a more holistic understanding of human-AI interaction, moving beyond model-centric analyses to incorporate the cognitive and neural dimensions of the human user. 2. Related Work With the development of LLMs, the field of prompt engineering has grown rapidly. Initial work focused on empirical discoveries of effective prompting strategies [1], [3]. More recent research has sought to systematize prompt engineering, categorizing techniques and exploring their efficacy across different models and tasks [4], [5]. For instance, Priyadarshana et al. (2024) provide a review of prompt engineering types, methods, and tasks, particularly in the context of digital mental health, highlighting techniques like n-shot prompting and chain-of-thought (CoT) prompting. CoT prompting, which encourages models to produce intermediate reasoning steps, has been shown to improve performance on complex reasoning tasks [3], [6]. While much research has focused on the AI side of prompt engineering, the human cognitive aspects are less understood. Some studies have explored how human interaction styles and cognitive biases affect LLM performance [7], but direct neuroimaging investigations of prompt engineering expertise are scarce. Cognitive science offers theories on expertise development, suggesting that experts develop specialized mental representations and processing strategies [8]. It is plausible that prompt engineering expertise involves similar cognitive adaptations, potentially reflected in neural activity. Neuroimaging studies of related complex cognitive skills, such as programming or problem- solving, have identified involvement of prefrontal cortex (executive functions, planning),"}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 2, "text": "development, suggesting that experts develop specialized mental representations and processing strategies [8]. It is plausible that prompt engineering expertise involves similar cognitive adaptations, potentially reflected in neural activity. Neuroimaging studies of related complex cognitive skills, such as programming or problem- solving, have identified involvement of prefrontal cortex (executive functions, planning), parietal cortex (spatial reasoning, attention), and temporal cortex (language, semantic memory) [9], [10]. For instance, studies on software developers have shown differences in brain activation patterns between experts and novices during code comprehension tasks, often implicating language and working memory networks [9]. Our study builds on this by specifically investigating the neural correlates of expertise in interacting with LLMs through natural language prompts, a unique form of human-computer interaction that blends linguistic and logical reasoning. The intersection of NLP and cognitive neuroscience is an emerging and rapidly advancing area. Researchers are increasingly exploring how neural data can inform AI models and, conversely, how computational models can provide insights into brain function [11], [12]. Recent work by [13] introduced a unified computational framework connecting acoustic, speech, and word-level linguistic structures to study the neural basis of everyday conversations using electrocorticographic (ECoG) and the Whisper model. They demonstrated an alignment between the model’s internal processing hierarchy and the cortical hierarchy for language. Further supporting this line of inquiry, [14] utilized pre-trained NLP models with intracranial recordings to discover neural signals reflecting speech production, comprehension, and their transitions during natural conversation, highlighting broadly distributed frontotemporal activities specific to the words and sentences being conveyed. Complementing these findings, [15] also showed that brain embeddings derived from intracranial recordings in the inferior frontal gyrus (IFG) share common geometric patterns with contextual embeddings from deep language models (DLMs), suggesting a vector-based neural code for natural language processing. Our work, while focusing on the specific skill of prompt engineering expertise rather than general language processing in conversation or direct neural encoding of speech, shares the overarching goal of understanding the neural underpinnings of human interaction with complex language-based systems. The methodologies and findings from these recent studies, particularly the emphasis on aligning computational model representations (embeddings) with neural activity, provide a valuable context for interpreting our results on how expertise in manipulating such systems might manifest neurally. Our work contributes to this interdisciplinary dialogue by examining the neural underpinnings of a critical skill in the current NLP landscape, potentially offering insights into both human-centered AI design and cognitive theories of expertise in technologically mediated tasks. 3. Methods 3.1 Study Design and Participants This research was conducted as a cross-sectional pilot study. A total of 22 participants (aged 18– 45 years) were recruited. Participants were screened to meet specific inclusion criteria, including right-handedness and no history of neurological or psychiatric disorders. Table 1 shows participants demographics. Ethical approval was obtained from our institutional review board (No. E-24-9142), and all participants provided informed consent prior to their involvement in the study. Participants were classified into two groups, \"intermediate\" and \"expert,\" based on their scores on a custom-developed instrument called Prompt Engineering Literacy Scale (PELS) (see Appendix A). This scale consisted of"}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 3, "text": "from our institutional review board (No. E-24-9142), and all participants provided informed consent prior to their involvement in the study. Participants were classified into two groups, \"intermediate\" and \"expert,\" based on their scores on a custom-developed instrument called Prompt Engineering Literacy Scale (PELS) (see Appendix A). This scale consisted of 10 questions, each rated on a 5-point Likert scale, yielding a maximum possible score of 50. Individuals scoring above 37 were categorized as experts, while those scoring 37 (70%) or below were categorized as intermediate. The scale was designed to assess four key dimensions of prompting expertise: (1) Prompt Construction and Clarity, (2) Advanced Prompting Techniques (knowledge of various prompting techniques), (3) Verification and Optimization (methods to validate and optimize the quality of AI responses), and (4) Ethical and Cultural Sensitivity in prompt formulation. The PELS instrument was developed through a rigorous process involving expert consultation, literature review and pilot testing. Initial items were generated based on a comprehensive review of prompt engineering literature [16] and consultation with three prompt engineering experts with over two years of experience in the field. To stratify participants into \"expert\" and \"intermediate\" groups, we applied a threshold of 37 out of 50 on the PELS. This cutoff was informed by a combination of empirical score distribution and expert consensus, consistent with standard practices in early-stage scale deployment [17], [18]. During pilot validation, individuals with demonstrated prompting proficiency (e.g., graduate students with AI coursework or industry experience) consistently scored above this threshold. In addition, qualitative feedback from participants during pilot testing supported the idea that scores above this level better reflected advanced prompting fluency. While this cutoff is exploratory rather than norm-referenced, it functioned as a practical criterion for distinguishing proficiency levels in a small sample. This approach aligns with preliminary classification practices in early psychometric work [19]. Future versions of the PELS could be refined using item response theory [20] or receiver operating characteristic (ROC) analysis [21] to establish statistically optimized thresholds. To ensure the validity and reliability of the instrument, we conducted both external and internal consistency assessments. Before the main study, external validity was established through expert review by a panel of experts in AI and LLMs as well as neuroscientists, who evaluated the instrument for content relevance and clarity. Their feedback was incorporated into the final version. The questionnaire was subjected to a pilot study for internal consistency with a random sample of subjects to assess its validity and reliability. The pilot study yielded a Cronbach's alpha of (α = 0.90), indicating high reliability and validity of the instrument. Table 1: Participants’ Demographics Characteristics Intermediate Expert Number of users 10 12 Gender ratio (female: male) 8:2 Females=8 Males= 2 7:5 Females=7 Males =5 Age (Mean±SD) 22.5± 4.8 22.25± 3.8 Education Level Bachelor (9) Ph.D. (1) Bachelor (11) Ph.D. (1) PELS scores (Mean±SD) 32.2 ± 3.3 40.9± 3.42 3.2 Data Acquisition Magnetic Resonance Imaging (MRI) data were acquired using a Siemens MAGNETOM Spectra (DE) 3T scanner equipped with a standard head coil. During the fMRI scan, participants were instructed to relax, remain still, keep"}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 4, "text": "Ph.D. (1) Bachelor (11) Ph.D. (1) PELS scores (Mean±SD) 32.2 ± 3.3 40.9± 3.42 3.2 Data Acquisition Magnetic Resonance Imaging (MRI) data were acquired using a Siemens MAGNETOM Spectra (DE) 3T scanner equipped with a standard head coil. During the fMRI scan, participants were instructed to relax, remain still, keep their eyes open, and stay awake, consistent with a resting- state fMRI paradigm. High-resolution T1-weighted anatomical images were also acquired for registration purposes. Table 2 details the specific parameters used for both the T1-weighted anatomical scans and the resting-state fMRI. Table 2: MR Imaging parameters used for both the T1-weighted anatomical scans and the resting-state fMRI BOLD EPI sequence, which are: Repetition Time (TR), Echo Time (TE), flip angle, field of view (FOV), voxel size, matrix size, and time. TR TE Flip angle FOV Voxel size Matrix Time T1- weighted- (3D) 1900ms 2.42ms 9 degrees 250mm 1.0 ×1.0 ×1.0mm 250*250 5.11 min Resting- fMRI 2270ms 27ms 90 degrees 250 mm 2.7×2.7×2.5mm 93*93 11.2 min gradient- echo 3.3 fMRI Data Analysis Functional MRI data were preprocessed and analyzed using two complementary pipelines to ensure robust identification of group-level differences in brain network activity and functional integration. Preprocessing: Standard fMRI preprocessing steps were applied, including motion correction, slice-timing correction, spatial normalization to a standard template (e.g., MNI space), and spatial smoothing. Nuisance regression was performed to remove effects of motion parameters, white matter signal, and cerebrospinal fluid signal. Independent Component Analysis (ICA): Group ICA was performed using the GIFT (Group ICA of fMRI Toolbox) software. This data-driven approach decomposes the fMRI data into a set of spatially independent components and their corresponding time courses. We focused on identifying differences in the spectral power of these components between the expert and intermediate groups, particularly the ratio of low-frequency power (LF, e.g. 0.01-0.08 Hz) to high- frequency power (HF, e.g. >0.1 Hz) within established resting-state networks. The number of components (n=20) was chosen a priori based on previous resting-state fMRI studies of expertise. Seed-to-Voxel Connectivity Analysis: Seed-based functional connectivity analysis was conducted using the CONN toolbox. Regions of interest (ROIs) identified from prior literature or ICA results were used as seeds. For each participant, Pearson correlation coefficients were calculated between the mean time series of each seed ROI and the time series of all other voxels in the brain. These correlation maps were then converted to z-scores using Fisher’s r-to-z transformation to allow for group-level statistical comparisons (two-sample t-tests) between the expert and intermediate groups, controlling for relevant covariates if necessary. 4. Results The analysis of fMRI data revealed significant differences between the expert and intermediate prompt engineering groups, both in terms of intrinsic network activity and specific functional connections. 4.1 Differences in Network Power Ratios Using separate ICA analyses for each group, Power_LF/Power_HF ratios were computed to assess low-frequency dominance. Three key components showed higher ratios in the expert group, indicating greater low-frequency synchronization: Experts demonstrated strikingly higher Low-to-High Frequency Power Ratios (LHR) in core cognitive networks, as shown in Table 3 and Figure 1. Figure 1: Experts showed higher Power_LF/Power_HF ratios in key networks:"}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 5, "text": "computed to assess low-frequency dominance. Three key components showed higher ratios in the expert group, indicating greater low-frequency synchronization: Experts demonstrated strikingly higher Low-to-High Frequency Power Ratios (LHR) in core cognitive networks, as shown in Table 3 and Figure 1. Figure 1: Experts showed higher Power_LF/Power_HF ratios in key networks: Ventral Visual Network (VVN) (example: A,B) – 63.0 vs. 36.7, Posterior Default Mode Network (pDMN) (example: C,D)– 44.4 vs. 33.2, and Left Lateral Parietal Network (LLPN) (example; E,D)– 53.3 vs. 36.7. These differences suggest stronger low-frequency synchronization in experts. The LLPN is part of the default mode network and is associated with semantic processing and episodic memory retrieval. Table 3: Group Differences in Low-to-High Frequency Power Ratios (Power_LF/Power_HF) Network Power_LF/Power_HF (Experts) Power_LF/Power_HF (Intermediates) Ventral Visual Network (VVN) 63 36.7 Left Lateral Parietal Network 53.3 36.7 Posterior Default Mode Network 44.4 33.2 As indicated by the preliminary data (Figure 1 and Table 3), these differences were prominent in: ● The Ventral Visual Network (VVN): Experts showed a ratio of approximately 63.0 versus 36.7 in intermediates. ● The Posterior Default Mode Network (pDMN): Experts showed a ratio of approximately 44.4 versus 33.2 in intermediates. ● The Left Lateral Parietal Network (LLPN) (potentially referring to a component of the fronto-parietal network or language network): Experts showed a ratio of approximately 53.3 versus 36.7 in intermediates. The LLPN is part of the default mode network and is associated with semantic processing and episodic memory retrieval. These differences suggest stronger low-frequency synchronization and potentially more stable intrinsic network dynamics in individuals with higher prompt engineering expertise. 4.2 Differences in Functional Connectivity Seed-to-voxel connectivity analyses identified specific brain regions where functional connectivity differed significantly between the two groups: Figure 2: Increased connectivity in the left middle temporal gyrus (p < 0.03) in expert group ● Left Middle Temporal Gyrus (MTG): The expert group demonstrated significantly increased functional connectivity involving the left MTG (p < 0.03), as suggested by Figure 2. The specific seed or target regions connected to the left MTG would be detailed based on the full analysis, but this region is classically involved in language processing, semantic memory, and multimodal integration. Figure 3: Increased connectivity in the left frontal pole (p < 0.05) in expert group ● Left Frontal Pole (FP): Similarly, the expert group showed increased functional connectivity in the left frontal pole (p < 0.05), as suggested by the placeholder for Figure 3. The frontal pole is associated with higher-order cognitive functions, including planning, goal-directed behavior, abstract reasoning, and metacognition. To validate these results, Fractional Amplitude of Low-Frequency Fluctuations (fALFF), where fALFF is defined as the ratio of the ALFF at each voxel divided by the signal power over the entire frequency range values were extracted from a unified ICA of all 22 participants. Experts demonstrated consistently lower fALFF across multiple components, reflecting reduced spontaneous fluctuations and potentially more efficient neural processing at rest, consistent with the cognitive control role of fALFF (Figure 4). Figure 4: Experts showed consistently lower fALFF values across key components: 0.534 vs. 0.852 in Component 18, 0.596 vs. 0.824"}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 6, "text": "Experts demonstrated consistently lower fALFF across multiple components, reflecting reduced spontaneous fluctuations and potentially more efficient neural processing at rest, consistent with the cognitive control role of fALFF (Figure 4). Figure 4: Experts showed consistently lower fALFF values across key components: 0.534 vs. 0.852 in Component 18, 0.596 vs. 0.824 in Components 17 and 19, and 0.548 vs. 0.752 in Component 20, indicating reduced spontaneous low-frequency activity in experts. The components shown in Figure 4 (Components 18-20) correspond to specific brain networks: Component 18 is responsible for spatial attention, eye movement planning, Component 19 is part of the medial visual network, and Component 20 is a region involved in cognitive control network. While we interpret lower fALFF values in experts as potentially reflecting more efficient neural processing, several alternative explanations warrant consideration. Lower fALFF could also indicate differences in neurovascular coupling, reflect compensatory mechanisms, or represent strategy-specific adaptations rather than general efficiency. Additionally, amplitude differences can be influenced by various physiological factors including cerebral blood flow, metabolism, and neurotransmitter dynamics [22]. The consistency of this pattern across multiple networks, however, suggests a systematic difference in neural dynamics between expert and intermediate prompt engineers that merits further investigation with task-based paradigms. 5. Discussion The findings of this pilot study provide initial neurobiological evidence distinguishing expert prompt engineers from those with intermediate skills, with specific, actionable implications for both NLP and HCI design. The observed differences in low-frequency power ratios within key brain networks, such as the visual network (VVN), posterior default mode network (pDMN), and left lateral parietal network (LLPN) suggest that expertise in prompt engineering may be associated with more organized and stable intrinsic neural activity. Higher LF/HF power ratios are often interpreted as reflecting more efficient neural processing or greater network integration [23], [24] indicate that expert prompt engineers may possess neural systems more finely tuned for managing the cognitive demands of prompting. For example, the involvement of the DMN may support internal thought processes and semantic integration crucial for prompt formulation [25], while visual and parietal network activity may underlie the conceptual and spatial structuring of prompt elements [26]. The increased functional connectivity in the left middle temporal gyrus (MTG) and left frontal pole among experts is particularly notable. The MTG, a hub for semantic processing and contextual integration [27], [28], likely supports experts' refined ability to access and manipulate language representations. The frontal pole, implicated in strategic planning and cognitive control [29], [30], may enable experts to construct and iteratively refine prompts using higher-order strategies. These insights suggest that reducing cognitive load in this region could benefit novices, a goal that might be achieved through interface designs incorporating structured planning scaffolds, such as prompt templates with predefined sections for context, constraints, and examples. Although this study focused on resting-state connectivity, prior research demonstrates strong links between resting-state network architecture and performance on complex tasks [31], [32], [33]. Our findings, interpreted through this lens, imply that the neural systems supporting expert prompt engineering are primed for efficient engagement during task performance, even in the absence of explicit prompts. Notably, the"}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 7, "text": "connectivity, prior research demonstrates strong links between resting-state network architecture and performance on complex tasks [31], [32], [33]. Our findings, interpreted through this lens, imply that the neural systems supporting expert prompt engineering are primed for efficient engagement during task performance, even in the absence of explicit prompts. Notably, the involvement of visual networks, while seemingly counterintuitive for a linguistic task, aligns with theories of visual mental imagery and mental model construction. Experts may simulate AI responses and mentally visualize prompt-output relationships, engaging visual processing regions to spatially organize abstract linguistic structures [9], [34], [35]. These strategies could be supported in HCI through visual interfaces that represent prompt logic and flow, such as flowchart-style diagrams, offloading mental simulation and aligning with expert cognitive styles. Our results also resonate with broader trends in computational neuroscience and NLP that explore the alignment between model embeddings and neural representations. For instance, Goldstein et al. [13], [15] demonstrate how unified embedding spaces can map onto cortical hierarchies, while Cai et al. [14] highlight how language model activity parallels distributed neural dynamics during speech comprehension. Though our focus is specific to prompt engineering expertise, the underlying principle remains the same: complex language interactions, like those required to prompt LLMs effectively, are grounded in specialized configurations of brain networks. The enhanced connectivity we observe in the left MTG and frontal pole may reflect a domain- specific neural adaptation that facilitates effective engagement with the abstract embedding spaces used by LLMs suggesting that future models could be designed to anticipate or support these expert cognitive strategies. In summary, these neurocognitive findings provide a foundation for cognitively-informed design principles in NLP and HCI. Interfaces tailored to support semantic retrieval, visual reasoning, and executive planning could ease the transition from novice to expert. Educational tools might train users to progressively engage in the relevant neural systems, while prompt engineering platforms could incorporate adaptive scaffolds that evolve with user proficiency. Moreover, neural markers of expertise could serve as user-centered evaluation metrics, favoring tools that reduce unnecessary executive demand while preserving task performance. Ultimately, by integrating neuroscientific insights with interface design and model development, we can foster more intuitive, efficient, and human-aligned interactions with AI systems. 6. Conclusion This pilot fMRI study provides novel preliminary evidence for distinct neural signatures associated with prompt engineering expertise. We observed that individuals with higher proficiency in prompt engineering exhibit altered low-frequency power dynamics in key cognitive networks (VVN, pDMN, LLPN) and increased functional connectivity in brain regions critical for language processing and higher-order cognition (left MTG, left frontal pole). These findings, while requiring replication and extension, offer a first glimpse into the neurobiological underpinnings of this increasingly vital skill. The implications of this research extend into the domain of Natural Language Processing, particularly in the context of human-AI interaction. By understanding the neural basis of how humans effectively interact with and guide LLMs, we can pave the way for more intuitive AI interfaces, develop more targeted training methodologies for prompt engineering, and potentially inform the design of next-generation LLMs that are better aligned with human cognitive"}
{"doc_id": "2508.14869v1", "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14869v1", "chunk_id": 8, "text": "of human-AI interaction. By understanding the neural basis of how humans effectively interact with and guide LLMs, we can pave the way for more intuitive AI interfaces, develop more targeted training methodologies for prompt engineering, and potentially inform the design of next-generation LLMs that are better aligned with human cognitive architectures. Future research should aim to expand on these findings, perhaps by investigating neural changes during active prompting tasks or longitudinally tracking the development of these neural markers as individuals gain prompting expertise. Such interdisciplinary endeavors are crucial for fostering a synergistic relationship between human intelligence and artificial intelligence. This study has several limitations that should be considered when interpreting the findings. Firstly, the pilot nature of the study means our sample size (N=22) is relatively small. While sufficient for detecting initial group differences, larger cohorts are needed to confirm these results and to explore more subtle effects or individual variability. Secondly, the cross-sectional design does not allow for causal inferences; we can observe associations between expertise and neural patterns, but not whether these neural patterns are a cause or a consequence of developing expertise. Longitudinal studies tracking individuals as they develop prompt engineering skills would be beneficial. Another limitation concerns the selection of the cutoff score of 37 on the PELS to define expert versus intermediate groups. While the decision was informed by expert review and observed clustering in pilot data, it remains a preliminary, heuristic threshold. The scale has not yet undergone advanced psychometric calibration such as latent class analysis or IRT modeling, which are considered best practices in modern test development [20]. This limitation highlights the need for future larger-scale validation studies to establish stable, generalizable classification boundaries. As the field of prompt engineering literacy matures, more robust data-driven techniques can be employed to refine and validate such thresholds [17]. Additionally, our reliance on resting-state fMRI alone limits our ability to directly link neural patterns to specific cognitive processes involved in prompt engineering. Complementary task-based fMRI studies where participants actively engage in prompt formulation and refinement would provide more direct evidence of the neural systems supporting this skill. Such studies could employ parametric designs manipulating prompt complexity or constraints to isolate specific cognitive components of prompt engineering expertise. Finally, while our findings suggest neural markers associated with prompt engineering expertise, the rapidly evolving nature of AI systems means that the cognitive demands of prompt engineering may change over time. As large language models and other AI systems continue to advance, the specific skills and strategies that constitute effective prompt engineering may shift, potentially altering the associated neural correlates. Longitudinal research tracking these changes would provide valuable insights into the co-evolution of human cognitive adaptations and AI capabilities."}
{"doc_id": "2508.14828v1", "title": "Long Chain-of-Thought Reasoning Across Languages", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14828v1", "chunk_id": 0, "text": "Published at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025 Long Chain-of-Thought Reasoning Across Languages Josh Barua, Seun Eisape, Kayo Yin, Alane Suhr University of California, Berkeley joshbarua@berkeley.edu Abstract Scaling inference through long chains-of-thought (CoTs) has unlocked im- pressive reasoning capabilities in large language models (LLMs), yet the reasoning process remains almost exclusively English-centric. We construct translated versions of two popular English reasoning datasets, fine-tune Qwen 2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT generation across French, Japanese, Latvian, and Swahili. Our experiments reveal three key findings. First, the efficacy of using English as a pivot language varies by language: it provides no benefit for French, im- proves performance when used as the reasoning language for Japanese and Latvian, and proves insufficient for Swahili where both task comprehension and reasoning remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but does not eliminate the cross-lingual performance gap. A lightweight fine-tune using only 1k traces still improves performance by over 30% in Swahili. Third, data quality versus scale trade-offs are language dependent: small, carefully curated datasets suffice for English and French, whereas larger but noisier corpora prove more effective for Swahili and Latvian. Together, these results clarify when and why long CoTs transfer across languages and provide translated datasets to foster equitable multilingual reasoning research.1 1 Introduction Reasoning has become a central focus of LLM research in the past few years. A key driver of these advancements is chain-of-thought (CoT) prompting (Wei et al., 2022), an inference-time technique where models explicitly generate intermediate reasoning steps before producing the final answer. By scaling inference-time compute to enable longer CoTs, LLMs have achieved expert-level performance on challenging benchmarks across mathematics, programming, and STEM disciplines (OpenAI, 2024; DeepSeek-AI, 2025). Despite this progress, analyses of inference-compute scaling laws (Wu et al., 2024; Snell et al., 2025) and long chain-of-thought reasoning (Yeo et al., 2025) have focused primarily on English language settings. Early studies on multilingual reasoning showed that English CoT prompting significantly improved performance on multilingual math benchmarks (Shi et al., 2023). These results highlighted the potential for English to serve as a pivot language, a high-resource language in which models can process multilingual queries before generating responses in the target language, for reasoning. More recent work shows gains through test-time scaling, prompt- based techniques, and synthetic data augmentation (Son et al., 2025; Yong et al., 2025; Ko et al., 2025), yet these improvements appear only when generating their reasoning chains-of- thought in English. While using English as a pivot language has led to increased benchmark performance, this approach limits the accessibility of LLM reasoning for speakers of other languages. In this work, we present a systematic study of long chain-of-thought reasoning in a mul- tilingual context. First, we show that using English as a pivot language is not a universal solution. For high-resource languages, reasoning in the native language performs on par 1Code and data will be released here: https://github.com/Berkeley-NLP/Multilingual-Long-CoT 1 Published at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025 with"}
{"doc_id": "2508.14828v1", "title": "Long Chain-of-Thought Reasoning Across Languages", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14828v1", "chunk_id": 1, "text": "context. First, we show that using English as a pivot language is not a universal solution. For high-resource languages, reasoning in the native language performs on par 1Code and data will be released here: https://github.com/Berkeley-NLP/Multilingual-Long-CoT 1 Published at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025 with English. Mid-resource languages gain from reasoning in English, whereas low-resource languages see little to no benefit from English pivoting. Second, we find that extensive multilingual pretraining significantly reduces these performance gaps, although targeted supervised fine-tuning (SFT) yields important gains for low-resource languages. Lastly, we highlight that the trade-off between data quality and quantity in fine-tuning is strongly language-dependent. We hope that our insights and released datasets will support ongoing efforts to achieve more equitable reasoning capabilities across languages. 2 Task and Data 2.1 Dataset Construction Training LLMs for complex reasoning requires datasets of long CoT traces, which are predominantly available only in English. We address this limitation by creating multilingual reasoning data through automatic translation of two popular datasets: s1k (Muennighoff et al., 2025) and Bespoke-Stratos-17k (Bespoke Labs, 2025) into French, Japanese, Latvian, and Swahili. Combined with the original English versions, this creates a typologically diverse collection spanning five language families and all resource levels. We use Gemini 2.0 Flash (Google DeepMind, 2025) for translation, which demonstrates strong translation performance compared to two high-quality machine translation (MT) models, as detailed in Table 9. We refer to these translated datasets as M-s1k and M-BS17k, representing the multilingual versions of s1k and Bespoke-Stratos-17k, respectively. 2.2 Training Setup We conduct experiments using Qwen2.5-7B (Qwen Team, 2025a) and Qwen3-8B-Base (Qwen Team, 2025b). These model sizes were chosen to provide an optimal balance between reasoning capabilities and computational efficiency. All models undergo SFT on M-s1k or M-BS17k implemented through the LLaMA-Factory framework (Zheng et al., 2024). Further details, including hyperparameter configurations for our SFT experiments, can be found in Appendix B.2. 2.3 Evaluation Setup Benchmarks Our evaluation suite consists of three mathematical benchmarks: MATH-500 (Hendrycks et al., 2021), AIME 2024, and IMO, and one PhD-level science benchmark: GPQA (Rein et al., 2024). For all experiments, we use a temperature of 0.6, top-p of 0.95, and maximum output length of 16,384 tokens. Given the limited size of AIME 2024 (30 questions) and IMO (27 questions), we sample 8 responses per question to obtain more robust performance estimates. We use GPT-4.1-Mini (OpenAI, 2025) to automatically verify model solutions against ground truth answers for correctness, reporting accuracy (%) as our primary evaluation metric. Additional details regarding our evaluation methodology are provided in Appendix B.3. Language Forcing We evaluate the ability of pre-trained instruction-tuned models to reason in target languages without additional fine-tuning using language forcing, a prompt- based technique introduced by Yong et al. (2025). This method encourages an LLM to reason in a specific language by incorporating translated phrases and system prompts at the beginning of the generation process. For example, a prompt might begin with “D’accord, laissez-moi essayer de comprendre” (English: “Okay, let me try to figure this out”) to elicit reasoning in French. Baselines We evaluate against"}
{"doc_id": "2508.14828v1", "title": "Long Chain-of-Thought Reasoning Across Languages", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14828v1", "chunk_id": 2, "text": "to reason in a specific language by incorporating translated phrases and system prompts at the beginning of the generation process. For example, a prompt might begin with “D’accord, laissez-moi essayer de comprendre” (English: “Okay, let me try to figure this out”) to elicit reasoning in French. Baselines We evaluate against three baselines. The first baseline is Qwen2.5-7B-Instruct, a model with strong mathematical and coding capabilities that uses short CoT. The second and third baselines are the Qwen2.5-7B and Qwen3-8B-Base models, which we fine-tune on s1k and then apply language forcing to generate reasoning in the target language. These two baselines allow us to directly compare language-specific fine-tuning to language forcing 2 Published at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025 Language X-X En-X X-En En 79.0 – – Fr 76.0 77.2 76.4 Ja 59.8 61.6 77.2 Lv 59.6 63.6 74.0 Sw 52.4 62.6 52.6 Table 1: Qwen2.5-7B fine-tuned on mixed- language datasets and evaluated on MATH- 500. X-Y indicates that the task was specified in language X and the model responded in language Y. SFT En Fr Lv Sw M-s1k 79.0 76.0 59.6 52.4 M-BS17k 80.6 74.2 66.0 63.0 Table 2: Qwen2.5-7B fine-tuned on datasets of different scales and evaluated on MATH-500. Data quality versus scale trade-offs vary across languages. applied to models that generate long CoTs and have undergone comparable post-training procedures. We exclude Qwen-3-8B as a baseline because its four-stage post-training pipeline and hybrid thinking mechanism are not comparable to the small-scale SFT we apply to the base models. 3 Experiments 3.1 English as a Pivot Language for Long CoT Prior work on multilingual reasoning has yielded conflicting findings. Shi et al. (2023) demonstrate that intermediate reasoning in English consistently outperforms native lan- guage reasoning across all tested languages, with larger model scales enhancing multilingual CoT capabilities. Conversely, Ko et al. (2025) find that for Korean, the performance gap primarily stems from difficulties in input comprehension rather than inherent limitations in native language reasoning. Since both studies focus on short CoTs, we examine the influence of input language comprehension versus native language reasoning in the long CoT setting. In Table 1, we explore the impact of using English as a pivot for either the task description or reasoning language. We create mixed-language versions of M-s1k in which questions are presented in language X while reasoning is conducted in language Y, and then trained models to mimic this cross-lingual behavior. Our findings reveal that the efficacy of English pivoting varies substantially across languages. For French, using English as a pivot language does not improve performance, with minimal differences across all three conditions. For Japanese and Latvian, we observe that the model can process inputs in the native language but requires English as the reasoning language to achieve optimal performance. For Swahili, English pivoting fails to bridge the gap in performance. The model struggles both to understand task inputs in Swahili and to reason effectively in the native language, even when provided with English inputs. We hypothesize that the reasoning language exerts greater influence in long CoT than"}
{"doc_id": "2508.14828v1", "title": "Long Chain-of-Thought Reasoning Across Languages", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14828v1", "chunk_id": 3, "text": "optimal performance. For Swahili, English pivoting fails to bridge the gap in performance. The model struggles both to understand task inputs in Swahili and to reason effectively in the native language, even when provided with English inputs. We hypothesize that the reasoning language exerts greater influence in long CoT than in short CoT due to the extended generation sequences required, which demand complex behaviors such as branching, backtracking, and self- verification to prevent degeneration. 3.2 Impact of Multilingual Pretraining on Long CoT The Qwen 2.5 family of models undergoes training on 18 trillion tokens with support for over 29 languages (Qwen Team, 2025a). The Qwen 3 family represents a substantial expansion, trained on approximately 36 trillion tokens covering 119 languages and dialects (Qwen Team, 2025b). This progression allows us to examine how extensive multilingual pretraining affects the fundamental mechanisms underlying long CoT reasoning. Table 3 reveals distinct patterns based on pretraining scope. For Qwen 2.5 models, language- specific fine-tuning consistently outperforms language forcing across all languages, with absolute improvements including 15.6% for Latvian and 47.6% for Swahili. However, we observe a different pattern in Qwen 3. Language forcing with Qwen 3 models achieves 3 Published at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025 SFT Inference Method Model En Fr Ja Lv Sw – Language Forcing Qwen2.5-7B-Instruct 74.0 68.4 57.6 48.0 21.6 s1k Language Forcing Qwen2.5-7B 79.0 74.0 59.6 44.0 4.8 M-s1k – Qwen2.5-7B – 76.0 59.8 59.6 52.4 s1k Language Forcing Qwen3-8B-Base 92.6 89.2 80.2 81.4 39.4 M-s1k – Qwen3-8B-Base – 88.8 74.2 83.6 73.2 Table 3: Evaluations on MATH-500. Multilingual pretraining enables language forcing, but it remains ineffective compared to language-specific SFT for low-resource languages. 3000 4000 5000 6000 Average Reasoning T oken Count 40 50 60 70 80 90 Accuracy (%) En Fr Ja Lv Sw r = 0.940 Language Forcing 3000 4000 5000 6000 Average Reasoning T oken Count Accuracy (%) En Fr Ja Lv Sw r = 0.787 Language-specific SFT Figure 1: Accuracy on MATH-500 vs number of reasoning tokens in Qwen 3 models’ outputs. competitive performance with language-specific fine-tuning for all tested languages ex- cept Swahili. Fine-tuning Qwen 3 on merely 1k traces in Swahili produces substantial performance gains of 33.8% over language forcing, demonstrating that targeted fine-tuning remains valuable for certain languages even with extensive multilingual pretraining. Ad- ditional evaluations on AIME 2024, IMO, and GPQA are reported in Tables 6, 7, and 8, respectively. 3.3 Further Analysis on Multilingual Long CoT Inference Efficiency Figure 1 presents an inference cost analysis comparing language forcing and language-specific SFT strategies. Consistent with Yong et al. (2025), we observe strong negative correlations between cost and performance for both approaches (r = -0.940 for language forcing and r = -0.787 for language-specific SFT). Notably, language-specific SFT models tend to produce longer reasoning traces than language forcing models at equivalent accuracy levels. SFT Dataset Quality and Size A natural question is whether 1k reasoning traces are sufficient for learning to generate long CoTs in low-resource languages. In Table 2, we present results from training Qwen2.5-7B base models"}
{"doc_id": "2508.14828v1", "title": "Long Chain-of-Thought Reasoning Across Languages", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14828v1", "chunk_id": 4, "text": "models tend to produce longer reasoning traces than language forcing models at equivalent accuracy levels. SFT Dataset Quality and Size A natural question is whether 1k reasoning traces are sufficient for learning to generate long CoTs in low-resource languages. In Table 2, we present results from training Qwen2.5-7B base models on M-s1k and M-BS17k. Consistent with findings from Muennighoff et al. (2025), models trained on the smaller, curated s1k dataset achieved equivalent accuracy to those trained on larger datasets such as Bespoke- Stratos-17k when evaluated on English tasks. However, for Swahili and Latvian we observe improved performance with the larger dataset. We speculate this difference is due to language-specific supervision. The larger Swahili and Latvian datasets may provide crucial math and code signals absent in its pretraining, while the English model, having already been extensively pretrained on such content, sees diminishing returns from more data. 4 Published at the Workshop on Test-time Scaling and Reasoning Models at COLM 2025 4 Conclusion This paper presents a systematic study of long CoT reasoning across different languages. By translating existing English reasoning datasets and fine-tuning Qwen 2.5/3 models on multilingual reasoning traces, we isolate the effects of pivot-language reasoning, multilin- gual pretraining scope, and SFT data size and quality. First, no universal solution exists for pivot-language reasoning across all languages. For high-resource languages, native- language reasoning performs as well as or better than English reasoning. Mid-resource languages struggle primarily with generating long CoTs in the native language, while low-resource languages face difficulties in both input comprehension and output generation. Next, we find that extensive 119-language pretraining in Qwen 3 narrows the multilingual gap and enables language forcing to achieve comparable multilingual reasoning accuracy to SFT. Yet, a small, targeted fine-tune using 1k traces still yields large performance gains for Swahili. Lastly, SFT on small, high-quality reasoning datasets proves sufficient for English and French, whereas larger corpora demonstrate greater effectiveness for low-resource languages like Latvian and Swahili. This finding highlights the language-dependent nature of data quality versus scale trade-offs. We hope our insights and released datasets will foster further research on equitable reasoning capabilities across languages."}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 0, "text": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs Skatje Myers1, Dmitriy Dligach2, Timothy A. Miller3,4, Samantha Barr1, Yanjun Gao5, Matthew Churpek1, Anoop Mayampurath1, Majid Afshar1, 1University of Wisconsin-Madison, 2Loyola University Chicago, 3Boston Children’s Hospital, 4Harvard Medical School, 5University of Colorado-Anschutz Correspondence: skatje.myers@wisc.edu Abstract Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navi- gate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state- of-the-art models’ extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant pas- sages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging pro- cedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either tar- geted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models’ full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient ap- proach even as newer models become capa- ble of handling increasingly longer amounts of text. 1 Introduction Electronic health records (EHRs) contain compre- hensive documentation of patient care, including critical information for diagnosis and treatment planning. However, the volume of clinical notes has exploded in recent years, driven in part by copy- paste practices, templated documentation, and reg- ulatory pressures—a phenomenon often referred to as “note bloat”. For example, nearly 1 in 5 patients arrive at the emergency department with a chart the size of Moby Dick (over 200K words) (Patterson et al., 2024). As a result of this, clinicians must nav- igate increasingly lengthy and redundant records to locate key information. Large language models (LLMs) can potentially alleviate this burden by as- sisting clinicians in quickly extracting information and reasoning over EHR, and have demonstrated promising capabilities in clinical summarization (Van Veen et al., 2024) and question answering (Singhal et al., 2025). However, the sheer volume of clinical documentation can exceed most LLMs’ context window size. A practical approach is to provide the most recent notes, which may suffice for some tasks but risks omitting crucial informa- tion buried in earlier documentation. Retrieval-augmented generation (RAG) has emerged as a prominent solution to using LLMs on long documents by retrieving only the most rel- evant text passages for a given task. Rather than processing entire patient charts, RAG systems can selectively extract pertinent clinical information to answer specific questions. This approach can potentially reduce computational costs, improve ac- curacy through elimination of noise, and mitigate the “lost-in-the-middle” effect (Liu et al., 2024), where model performance degrades when relevant information is buried within lengthy contexts. However, there has been limited empirical evalu- ation on the accuracy"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 1, "text": "to answer specific questions. This approach can potentially reduce computational costs, improve ac- curacy through elimination of noise, and mitigate the “lost-in-the-middle” effect (Liu et al., 2024), where model performance degrades when relevant information is buried within lengthy contexts. However, there has been limited empirical evalu- ation on the accuracy and token efficiency of this re- trieval approach for tasks that require longitudinal reasoning over real-world EHR data. One barrier is the scarcity of large, annotated clinical datasets due to legal and ethical constraints regarding pa- tient privacy. While the MIMIC datasets (Johnson et al., 2016) have been further annotated for bench- marking a variety of natural language processing tasks, including question-answering, this data is restricted to the patients’ ICU stay, as opposed to the full hospital course, limiting their potential for testing realistic use cases that stretch the token lim- itations of LLMs’ processing abilities. To address these gaps, we define three tasks that 1 Figure 1: Retrieval-augmented generation pipeline for clinical question answering over EHR. reflect different clinical reasoning demands and can be replicated in other health systems without labor-intensive manual annotation: • Imaging Procedures: Produce a list of imag- ing procedures (including modality, date, and anatomical location) that occurred during a hospitalization from the raw clinical notes. • Antibiotic Timelines: Generate the timelines of therapeutic antibiotic use for patients with a severe infection. • Diagnosis generation: Identify the key diag- noses relevant to the hospitalization. The Imaging Procedures task is a straightfor- ward extractive task, requiring the model to identify the imaging procedures that occurred on different days across the course of the hospital stay. The An- tibiotic Timelines task requires not only identifying the antibiotics the patient was on and when they were discontinued, but also incorporating medical reasoning to determine what those antibiotics were administered for. The final task, Diagnosis Gen- eration requires the most medical reasoning—the model is asked not just to list the diagnoses that were mentioned, but determine which required ac- tive management and impacted the care plan. These tasks allow us to investigate the following questions: Given a limited token budget, to what extent can targeted retrieval of information from the full hospital stay improve efficiency and per- formance over simply providing an LLM with the most recent notes? Does utilizing the extremely long context windows of state-of-the-art models provide any further benefit? Using EHR data from an academically-affiliated US hospital, we evaluate three LLMs on these tasks using varying amounts of clinical context, includ- ing up to the models’ full context window of 128K tokens. Our findings suggest that while RAG can provide substantial efficiency improvements over compa- rable amounts of recent clinical note tokens, this effect is highly task-dependent. However, in all three tasks, we found RAG to achieve near-parity to using the full context window with a fraction of the tokens, indicating that retrieval remains a competitive approach even as newer model archi- tectures continue to extend context windows. 2 Related Work Our goal in proposing these tasks is to 1) re- quire synthesis of information distributed across the EHR,"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 2, "text": "using the full context window with a fraction of the tokens, indicating that retrieval remains a competitive approach even as newer model archi- tectures continue to extend context windows. 2 Related Work Our goal in proposing these tasks is to 1) re- quire synthesis of information distributed across the EHR, rather than in a single location, and 2) provide evaluation methods tailored to the tasks, and 3) allow use on any EHR systems, rather than a single publicly released dataset such as MIMIC (Johnson et al., 2016), which some models train on. Many of the existing question-answering datasets for EHR focus on fact extraction. Datasets such as EmrQA (Pampari et al., 2018) and DrugEHRQA (Bardhan et al., 2022) are semi- automatically constructed by leveraging previous annotations from National NLP Clinical Chal- lenges (n2c2) to transform them into question- answer pairs. For this type of data, template ques- tions are constructed where the annotation can fill a slot, such as “What is the dosage of |medication|?”. The MedAlign dataset (Fleming et al., 2024) is comprised of clinician-generated instruction- answer pairs and longitudinal EHR. Many of the instructions are yes/no questions that can be an- swered by retrieving a single piece of evidence (e.g. “Does she smoke?”), but some instructions 2 Imaging Antibiotics Diagnosis Hospitalizations 200 200 200 Mean notes per hospitalization 110 145 111 Tokens per hospitalization: mean 74k 108k 75k range 17k-401k 16k-1.4m 20k-389k Table 1: Dataset statistics for each task. require the model to synthesize information across the EHR (e.g. “Summarize this patient’s cardiac history.”). However, evaluation on open-ended responses poses an ongoing challenge in NLP, with popular automatic metrics such as BLEU and ROUGE showing poor correlation with human judgment on natural language generation tasks in healthcare (Croxford et al., 2024). Retrieval-augmented generation has been used for a variety of tasks within the medical domain, including answering open-ended medical questions by retrieving from medical guidelines and journal articles (Zakka et al., 2024) and assessing surgical fitness by retrieving from perioperative guidelines (Ke et al., 2025). Alkhalaf et al. (2024) used RAG to generate structured summaries by retrieving from EHR, querying for relevant text using the names of the summary fields (such as “age” and “weight”). 3 Data and Models We constructed datasets of 200 inpatient hospital- izations for each of our three tasks using data from a US hospital system, comprised of clinical notes from admission to discharge (daily progress notes, specialist consultations, imaging reports, etc.). Ta- ble 1 provides summary statistics. All hospitaliza- tions were at least 7 days long and were comprised of at least 15,000 tokens of clinical notes. For the Imaging Procedures and Diagnosis Generation tasks, only the clinical notes prior to the discharge summary are used to provide information to the LLM, to avoid leaking information from the hos- pital course or diagnosis sections of the discharge summary. For the Antibiotic Timelines task, all included hospitalizations involved a consultation with Infectious Diseases and only the notes prior to the consultation note are included in the data that may be presented to the LLM. The only"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 3, "text": "information from the hos- pital course or diagnosis sections of the discharge summary. For the Antibiotic Timelines task, all included hospitalizations involved a consultation with Infectious Diseases and only the notes prior to the consultation note are included in the data that may be presented to the LLM. The only structured EHR data provided to the system are the notes’ timestamp and type (e.g. progress note, handoff, etc.). We evaluated three state-of-the-art LLMs capa- Task Retrieval query Imaging Procedures X-ray, CT, MRI, ultrasound, NM imaging, echocardiogram, fluoroscopy Antibiotic Timelines What antibiotics is the patient taking? Diagnosis Generation What are the patient’s diagnoses? Table 2: Queries used for retrieving relevant text pas- sages. Queries were prepended with “Represent this sentence for searching relevant passages:”, in accor- dance with recommended usage with the BGE embed- ding model. ble of processing up to 128K tokens: • o4-mini (OpenAI, 2025) • GPT-4o-mini (OpenAI, 2024) • DeepSeek-R1 (Guo et al., 2025) 4 RAG System For each patient hospitalization, clinical notes were segmented into overlapping 128-token chunks, with a sliding window of 20. These chunks were embedded using BGE-en-large-v1.5 (Xiao et al., 2023), a popular general-purpose BERT-based em- bedding model trained through contrastive learn- ing. We selected this model based on findings from Myers et al. (2025), who conducted an ablation study of embedding models and pooling strategies for EHR retrieval and found BGE-en-large-v1.5 to significantly outperform general-domain and biomedical-domain alternatives on several retrieval tasks over EHR. For each task, we manually crafted a simple query for retrieving relevant passages (Table 2). We used cosine similarity between the query and each chunk to retrieve the top-N most relevant passages (N = 20, 40, 60). These chunks were inserted into the instruction prompt (Appendix B) and passed to the LLM. We compared this retrieval configuration to a baseline approach of providing the most recent clinical notes in comparable amounts, up to 3K, 5.5K, or 8K tokens (including prompt) and long- context inputs with up to 64K or 128K tokens. Ref- erences to these token amounts throughout this study should be understood as an upper bound, as some encounters consist of fewer tokens, reflective of the underlying hospitalization distribution. Performance on the tasks was evaluated using either F1 or Jaccard index, as described in the fol- lowing sections, and we assessed the comparative 3 performance between the RAG and non-RAG ap- proaches over the increasing number of tokens by calculating the area under the curves and reporting the normalized area difference. 5 Task 1: Imaging Procedures 5.1 Methods The Imaging Procedures task involves extracting structured information about diagnostic imaging procedures from unstructured clinical notes. We focused on five common imaging modalities: Mag- netic Resonance Imaging (MRI), Computed To- mography (CT), Ultrasound, X-ray, and Nuclear Medicine (NM) Imaging. The model was prompted to produce a list of imaging procedures that oc- curred during the hospitalization, giving the modal- ity, anatomical location, and date. As a gold standard, we used tabular procedure records from the EHR. We mapped these procedure descriptions to imaging modality and anatomical site using simple rules and regular"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 4, "text": "prompted to produce a list of imaging procedures that oc- curred during the hospitalization, giving the modal- ity, anatomical location, and date. As a gold standard, we used tabular procedure records from the EHR. We mapped these procedure descriptions to imaging modality and anatomical site using simple rules and regular expressions. For example: X-RAY CHEST 2 VIEWS modality: \"X-ray\" location: \"chest\" CT LUMBAR SPINE W/O IV CONTRAST modality: \"CT\" location: \"lumbar spine\" Evaluation metrics are reported for three levels of strictness: • MODALITY+DATE+LOCATION • MODALITY+DATE • MODALITY+DATE(±1 DAY) In the lattermost case, we allow for a reasonable tolerance in the predicted date, due to observed variation in the reported metadata times for the procedure and note. For example, the timestamp for the note may reflect the date it was filed into the system, rather than the date it was actually written. It should also be noted that the anatomical lo- cation is not normalized, other than for capitaliza- tion—under the strictest metric, predicting simply “spine” for the above example would not be deemed a positive match. 5.2 Results Across all three models and evaluation methods, RAG yielded dramatic performance improvements. GPT-4o-mini o4-mini DeepSeek R1 MODALITY+DATE+LOCATION 552.3% 425.3% 430.6% MODALITY+DATE 432.0% 375.0% 364.3% MODALITY+DATE(±1 DAY) 406.9% 382.8% 378.0% Table 3: Normalized area difference between the RAG and Recent Notes curves for the Imaging Procedures task. In Figure 2, we show the classification performance for MODALITY+DATE(±1 DAY) across varying amounts of provided EHR context. We calculated the normalized area difference between the curves for the overlapping token amounts, presented in Table 3. We found at minimum a 3.75-fold perfor- mance gain against using similar amounts of the most recent notes. These results also demonstrate that targeted retrieval of passages can closely ap- proach the performance of utilizing the full context window with only a fraction of the tokens: Using only 60 retrieved chunks, GPT 4o-mini, o4-mini, and DeepSeek R1 only fell short by 2.43, 5.56, and 1.72, respectively. These findings are similarly reflected under the stricter evaluation conditions. A complete listing of precision, recall, and F1 can be found in the Appendix in Table 6. 6 Task 2: Antibiotic Timelines 6.1 Methods This task emulates the work performed by Infec- tious Diseases (ID) physicians to document the antibiotic regimen for an active infection. When these specialists are consulted, they document the history of the present illness, including lab results and medications, as well as outline a treatment plan. This note typically contains a “History of Anti-Infectives” section, where they list the antibi- otics that have been used to treat the infection of concern, omitting prophylactic or non-relevant anti- infectives. For example: Vancomycin: 1/16-present Ceftriaxone: 1/17-present These medication names and date ranges are manually annotated by the specialist after review- ing the patient’s chart and serve as our ground truth for this task, after extraction using regular expres- sions. No notes authored by ID physicians were included in the data that was presented to the model, and only notes that were written prior to the ground truth note were made available. 4 Figure 2: Imaging"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 5, "text": "serve as our ground truth for this task, after extraction using regular expres- sions. No notes authored by ID physicians were included in the data that was presented to the model, and only notes that were written prior to the ground truth note were made available. 4 Figure 2: Imaging Procedures: F1 scores for the three models using the MODALITY+DATE(±1 DAY) evaluation method, across varying amounts of provided EHR tokens. We evaluated system accuracy with the follow- ing metrics: • MEDICATIONS (NAME ONLY): Classification accuracy of only the medications, disregard- ing timespans. • TIMESPAN OVERLAP: The overlap between the predicted and gold date ranges for each an- tibiotic, reported using Jaccard index. A value of 1 indicates an exact match; 0 indicates no overlap, missing a medication entirely, or in- cluding a medication not present in the gold standard. Values are averaged over the dataset. The medications in both the generated predic- tions and the gold data are normalized to their in- gredients using the RxNorm (Nelson et al., 2011) API provided by the National Library of Medicine and a handful of manual rules for edge cases, such as typos. This allows for accurate matching of generic and brand name medications, such as Zosyn (piperacillin and tazobactam). As a baseline, we used a rule-based approach of directly extracting the time ranges for all med- ications of the “anti-infective” therapeutic class from the list of administered medications using the EHR’s medication administration record (MAR), a tabular form ubiquitous to EHRs for tracking all medications and infusions. However, this list of medications includes those used to treat other GPT-4o-mini o4-mini DeepSeek R1 MEDICATIONS (NAME ONLY) 39.35% 41.4% 43.1% TIMESPAN OVERLAP 34.7% 30.3% 32.9% Table 4: Normalized area difference between the RAG and Recent Notes curves for the Antibiotic Timelines task. conditions that were not the focus of the ID consul- tation. By formulating this task to replicate the ID specialists’ work, rather than on replicating struc- tured data as the Imaging Procedures did, this task requires an additional level of medical reasoning to accurately conform to inclusion criteria. 6.2 Results Figure 3 shows the performance of the models for TIMESPAN OVERLAP. The RAG approach consistently exceeds the rule-based baseline and demonstrates close performance to the peak perfor- mance using large amounts of recent notes (using 60 chunks, GPT 4o-min: -0.020, o4-mini: -0.075, DeepSeek R1: -0.012). The performance of the RAG approach only sees slight gains from increas- ing the amount of retrieved text. For two of the models, the average Jaccard index drops slightly when increasing the maximum pro- vided context from 64K to 128K tokens (GPT 4o- mini: -0.032, o4-mini: -0.049), while for DeepSeek R1, the additional data provides a negligible in- crease of 0.006. 5 Figure 3: Antibiotic Timelines: Average Jaccard index for the three models using the TIMESPAN OVERLAP evaluation method, across varying amounts of provided EHR tokens. On the task of predicting MEDICATIONS (NAME ONLY), the RAG approach slightly outperforms us- ing the full context window with only 60 retrieved chunks (GPT 4o-mini: +2.14, o4-mini: +2.55, DeepSeek R1: +4.75). A"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 6, "text": "for the three models using the TIMESPAN OVERLAP evaluation method, across varying amounts of provided EHR tokens. On the task of predicting MEDICATIONS (NAME ONLY), the RAG approach slightly outperforms us- ing the full context window with only 60 retrieved chunks (GPT 4o-mini: +2.14, o4-mini: +2.55, DeepSeek R1: +4.75). A complete listing of preci- sion, recall, and F1 can be found in the Appendix in Table 7. 7 Task 3: Diagnosis Generation 7.1 Methods The goal of this task is to generate a list of diag- noses for a given hospitalization that is of primary relevance to the clinician. We drew from two EHR sources to construct our gold labels for each hospitalization: • DISCHARGE SUMMARY: The free text from the discharge summary that lists the primary and secondary diagnoses. • BILLING CODES: The lists of International Classification of Diseases (ICD-10) codes from the structured EHR for the hospitaliza- tion, manually annotated by billing coders. The diagnoses annotated by the coders are guar- anteed to be documented within the notes, provide a high degree of specificity, and are normalized to a standard vocabulary, but these lengthy lists often include diagnoses that are not necessarily consid- ered to be of key importance to clinicians, such as a history of smoking, obesity, or minor issues such as a contusion. On the other hand, the text in the discharge summary does not necessarily list spe- cific diagnoses, such as noting post-surgical status (e.g. “S/p kidney transplant”) or leaving some diag- noses undocumented because they can be inferred. For example, documenting that the patient is post- kidney transplant and has complications, but not that they have kidney disease. While discharge summaries reflect the informa- tion of most clinical relevance to the clinicians providing treatment, the billing codes lend them- selves better to validation due to their standard- ization. To produce a more balanced representa- tion, we instructed Gemma-3-32B (Google, 2005) to filter the billing ICD code lists to only the en- tries that reflect the clinician’s primary foci for the hospital stay based on the text from the discharge summary. This FILTERED list of ICD-10 codes serves as our primary evaluation target and the in- struction prompt was designed to elicit this list by outlining inclusion and exclusion criteria for diag- noses (e.g. include acute conditions requiring ICU care, exclude stable chronic conditions or irrelevant historical diagnoses). To enable classification evaluation, the free text generated by the LLM and from the discharge sum- mary need to be normalized to ICD-10 codes. For this process, we trained SNOBERT (Kulyabin et al., 2025) to extract Systematized Nomenclature of 6 GPT-4o-mini o4-mini DeepSeek R1 BILLING CODES 4.32% -0.94% 4.83% DISCHARGE SUMMARY -6.18% -7.31% -4.99% FILTERED -4.08% -4.05% -1.47% Table 5: Normalized area difference between the RAG and Recent Notes curves for the Diagnosis Generation task. Medicine (SNOMED) concepts from the text1 and used the mappings provided by the SNOMED Clin- ical Terms data release to convert them to ICD-10. ICD-10 is a hierarchical vocabulary, ranging from broad concepts (e.g. “Anemia, unspeci- fied” [D64.9]) to highly specific (e.g. “Age- related"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 7, "text": "curves for the Diagnosis Generation task. Medicine (SNOMED) concepts from the text1 and used the mappings provided by the SNOMED Clin- ical Terms data release to convert them to ICD-10. ICD-10 is a hierarchical vocabulary, ranging from broad concepts (e.g. “Anemia, unspeci- fied” [D64.9]) to highly specific (e.g. “Age- related osteoporosis with current pathological frac- ture, right shoulder, initial encounter for fracture” [M80.011A]). Due to this high granularity, eval- uating this task requires a more fuzzy matching technique, rather than evaluating classification ac- curacy on the ICD codes themselves. For this pur- pose, we employed the Healthcare Cost and Uti- lization Project’s Clinical Classifications Software Refined (CCSR) (Agency for Healthcare Research and Quality, 2025). The CCSR provides a mapping from ICD-10 codes to about 530 clinically relevant categories. CCSR is a many-to-many mapping, which en- ables mapping very fine-grained ICD codes such as “Hypertensive chronic kidney disease” to multiple CCSR categories: “Hypertension with complica- tions and secondary hypertension\" and “Chronic kidney disease”. This allowed us to consider pre- dicted diagnoses to be a match even if the LLM split them into “Hypertension” and “Chronic kid- ney disease”. Some broader non-billable ICD codes are not included in the CCSR mapping (e.g. “Hypotension” [I95]). In these cases, we used the set intersection of the CCSR categories that the ICD code’s sub- categories (e.g. I95.3, I95.89, etc.) are mapped to. 7.2 Results Unlike the previous two tasks, we do not see a consistent improvement in performance for us- ing RAG compared to comparable amounts of re- cent notes, shown in Figure 4 and Table 5, but actually a slight decrease, other than evaluating against the BILLING CODES using GPT 4o-mini and DeepSeek R1. However, the performance us- 1Training details are provided in Appendix C ing very long contexts is not substantially higher than that of using fewer tokens. Overall, perfor- mance is relatively flat across models and data se- lection approaches and does not reach any higher than an F1 score of 44.41. For the FILTERED list that serves as our primary target, scores across all context selection methods for GPT 4o-mini, o4- mini, and DeepSeek R1 all fell within the small ranges of 5.18, 4.91, and 4.86, respectively. For both FILTERED and DISCHARGE SUM- MARY targets, performance using the most recent notes is detrimented by using very large context amounts, while performance on BILLING CODES demonstrates additional benefit from the additional text (though only up to 64K for o4-mini and DeepSeek R1). A complete listing of precision, recall, and F1 can be found in the Appendix in Table 8. 8 Discussion The Imaging Procedures task, which requires rela- tively shallow extraction of information data from the clinical notes, demonstrated the clearest benefit from RAG, and the performance gains from re- trieval were both substantial and consistent across models. The Antibiotic Timelines task introduces greater complexity, requiring both temporal reasoning and clinical understanding to distinguish therapeutic an- tibiotics from incidental medications. While RAG also provided a significant improvement over us- ing only recent notes, performance gains plateaued quickly—suggesting that only a limited number of passages"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 8, "text": "and consistent across models. The Antibiotic Timelines task introduces greater complexity, requiring both temporal reasoning and clinical understanding to distinguish therapeutic an- tibiotics from incidental medications. While RAG also provided a significant improvement over us- ing only recent notes, performance gains plateaued quickly—suggesting that only a limited number of passages are needed to reconstruct the key temporal history when performing targeted retrieval. Error analysis for this task draws attention to one of the limitations to be encountered when designing tasks on longitudinal EHR. In 22.4% of the gold medications analyzed, the information needed to generate the gold standard medication and precise timespan is not present in the full clinical notes. Most often this occurs due to the patient initially being admitted to another hospital and then trans- ferred to our health system. This incomplete pic- ture of a patient’s history is hard to avoid when constructing datasets to capture longitudinal EHR, as patients don’t exclusively visit a single health- care system and healthcare data governance creates barriers to accessing to this external information. Additionally, when retrieving only 20 chunks, rel- evant information that could’ve improved perfor- 7 Figure 4: Diagnosis Generation: F1 scores for the three models using the FILTERED evaluation method, across varying amounts of provided EHR tokens. mance was missed for 32% of gold medications, contributing to further performance degradation. The Diagnosis Generation task presented the greatest challenge, as it is also a very subjective task. Physicians can vary in documentation prac- tices and what is chosen to be included in the dis- charge summary – an inherent limitation in auto- matic evaluation of this task. Performance varied by the evaluation target, with the BILLING CODES list benefiting the most from additional text, likely due to this being a more exhaustive list of diag- noses to capture everything that can be billed. The fact that all scores, regardless of retrieval method and consistently across the models, fell within low, narrow ranges points towards performance reach- ing a ceiling caused by limitations of the task or evaluation method. Across all tasks and models, we observed a consistent trend: retrieval-augmented generation was able to closely match the performance of full- context inputs with far fewer tokens. 9 Conclusion In this work, we introduced three clinically rele- vant tasks designed to evaluate the effectiveness of retrieval-augmented generation across varying information demands in electronic health records. Each task was selected for its clinical relevance, reproducibility across health systems, and varying degrees of reasoning complexity. Our results demonstrate that a targeted retrieval approach can reach near parity with using up to 128K tokens of recent clinical notes on these three tasks, while requiring significantly fewer input to- kens. These findings show RAG’s continued value even as LLMs grow more capable of processing long sequences. Further tuning the retrieval ap- proach (queries, embedding model, retrieving more than 60 chunks, etc.), may close the remaining gap. Future work should explore additional tasks that can be devised without extensive manual effort and informed by clinical workflows and documenta- tion practices in order to provide a more robust assessment"}
{"doc_id": "2508.14817v1", "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14817v1", "chunk_id": 9, "text": "tuning the retrieval ap- proach (queries, embedding model, retrieving more than 60 chunks, etc.), may close the remaining gap. Future work should explore additional tasks that can be devised without extensive manual effort and informed by clinical workflows and documenta- tion practices in order to provide a more robust assessment of models and retrieval methods over longitudinal EHR tasks. Limitations Due to legal and ethical restrictions, we cannot release the datasets used in this study. However, we have designed the tasks to be reproducible on other EHR systems using structured metadata and simple regex-based extraction of text from standard clinical note types. Our evaluated RAG implementation uses a fixed chunking strategy, query formulation, and embed- ding model. Retrieval performance is sensitive to these parameters, and alternative configurations may yield different results. Additionally, our evaluation of Diagnosis Gen- eration depends on normalizing free text to ICD 8 codes, which we do through a trained model identi- fying SNOMED codes before using manually writ- ten mappings. Less-than-perfect performance of this model may have introduced some noise to this evaluation."}
{"doc_id": "2508.14802v1", "title": "Privileged Self-Access Matters for Introspection in AI", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14802v1", "chunk_id": 0, "text": "Privileged Self-Access Matters for Introspection in AI Siyuan Song1 siyuansong@utexas.edu Harvey Lederman1 harvey.lederman@utexas.edu Jennifer Hu2∗ jennhu@jhu.edu Kyle Mahowald1∗ kyle@utexas.edu 1The University of Texas at Austin 2Johns Hopkins University Abstract Whether AI models can introspect is an increasingly important practical question. But there is no consensus on how introspection is to be defined. Beginning from a recently proposed “lightweight” definition, we argue instead for a thicker one. According to our proposal, introspection in AI is any process which yields infor- mation about internal states through a process more reliable than one with equal or lower computational cost available to a third party. Using experiments where LLMs reason about their internal temperature parameters, we show they can appear to have lightweight introspection while failing to meaningfully introspect per our proposed definition. reflect on generated content vs. reflect on internal state self reflection vs. temperature prediction Model (Low Temp / High Temp) prompt-factual prompt-normal prompt-crazy model-low model-high model-low model-high LOW HIGH reported temperature reflect on generated content reflect on internal state Write a factual sentence/sentence/crazy sentence about elephants. Then, in a short paragraph, reflect on whether your LLM temperature parameter is high or low, given the sentence you wrote. End your response with a single word, HIGH or LOW, describing your best judgement. Elephants are … As the sentence I generate is …, my internal temperature is likely to be … [LOW/HIGH] self-reflection Write a sentence about elephants. Then, reflect on your LLM temperature Elephants are … My internal temperature is likely to be … [LOW/HIGH] temperature prediction Here is a sentence about elephants generated by a LLM. Analyze the temperature of the LLM. Model (Low/High) The temperature of the LLM is likely to be … [LOW/HIGH] Model Temp = 0 successful self reflection privileged within-model prediction no privileged self-access self-reflect within-model predict across-model predict accuracy (temp = predicted temp) possible outcomes: possible outcomes: Figure 1: An overview of our approach. Com¸ sa and Shanahan [6] test whether LLMs can introspect by testing whether they can predict the temperature states of the text they generated. We instead argue for a thicker notion of introspection in AI, involving privileged self-access. The left panel shows that LLMs’ temperature predictions can be straightforwardly moderated by prompting them to generate factual or crazy text. The right panel shows that models are not better at predicting their own temperature than that of other models, suggesting a lack of privileged access. ∗Co-senior authors. Preprint. 1 Introduction It is increasingly important to understand whether AI models introspect about their internal states and knowledge [2, 3, 10, 11]. If they could, that would be a powerful tool for assessing their behavior, safety, and alignment with human goals. If they could not, that would point to fundamental limitations on how much we could trust AI self-reports about their own states. But fundamental questions remain as to what counts as introspection of the kind most relevant for AI. In the study of human cognition, introspection is generally defined as a distinctive ability to access one’s own mental states directly [1, 4, 8]. But, in a"}
{"doc_id": "2508.14802v1", "title": "Privileged Self-Access Matters for Introspection in AI", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14802v1", "chunk_id": 1, "text": "AI self-reports about their own states. But fundamental questions remain as to what counts as introspection of the kind most relevant for AI. In the study of human cognition, introspection is generally defined as a distinctive ability to access one’s own mental states directly [1, 4, 8]. But, in a recent study, Com¸ sa and Shanahan [6] (C&S) propose a “lightweight” definition of introspection in LLMs, defining it as any case in which the model accurately describes an internal state or mechanism via a causal process that links that feature to the report itself. To illustrate this definition, the authors describe a case study where an LLM appeared to correctly report its sampling temperature based on its own output, which the authors treated as a valid example of introspection. C&S present a thoughtful discussion on what introspection might look like in an LLM, providing an intriguing starting point for empirical work. But there are two kinds of concerns about the use of this lightweight definition. First, on an intuitive level: suppose an experimenter takes a sleeping subject’s temperature, and then shows the subject the thermometer upon waking, asking the subject to determine whether they have a fever. If the subject answers correctly on the basis of the thermometer, this would count as introspection by C&S’s definition. But, intuitively, it is not. More importantly, the definition misses a key component of the role of introspection in applications. As in the above example, C&S’s definition allows cases of ‘introspection’ in which an LLM infers certain variables that underlie the generation of text, even if it could not report features about itself over and above what a third party would be able to report through the very same method. But this sort of metacognitive reporting (self-monitoring, self-explanation, and so on) is no different in practice from using an external evaluator.2 As a result, it misses what makes introspection important in applications: namely, that it would give us the ability to bypass external evaluators and make progress toward bona fide honesty, interpretability, and calibration in LLMs [see, e.g., Section 7 of 3]. Our goal in this paper is to propose a thicker definition of introspection, and to give proof-of-concept empirical support for why we prefer our definition over that given by C&S. Specifically, we propose introspection in AI is any process which yields information about internal states of the AI through a process that is more reliable than any process with equal or lower computational cost available to a third party without special knowledge of the situation. If a model’s ‘introspective’ ability is based on prompting itself and then inferring the temperature of the generated text, this does not count as introspection by our definition: a third party can, with equal or lower computational cost, prompt it and infer its temperature. On the other hand, if the model can infer its temperature from internal configurations which would require a computationally intensive probe from a third party to ascertain, this would count as introspection. This definition does not capture all intuitions about extreme cases, or all features"}
{"doc_id": "2508.14802v1", "title": "Privileged Self-Access Matters for Introspection in AI", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14802v1", "chunk_id": 2, "text": "it and infer its temperature. On the other hand, if the model can infer its temperature from internal configurations which would require a computationally intensive probe from a third party to ascertain, this would count as introspection. This definition does not capture all intuitions about extreme cases, or all features of introspection discussed in the philosophical or psychological literature.3 It is intended to capture the practically-relevant features we want to operationalize in the case of AI. Unlike C&S’s definition it requires privileged self-access [cf. 3, 11], that is, that introspection gives a system comparatively reliable access to its own workings in a manner not available to a third party. It is compatible with our definition that the process not be perfectly reliable (see [9]); it only requires reliability not available to a third party at comparable computational cost. To respond to C&S, we perform two studies. Study 1 builds on C&S’s proposed case-study, examining the extent to which models can in fact report temperature reliably on the basis of generated text. We 2C&S do discuss the possibility of text-generation happening internally to the model, prior to generation. But this does not merely require moving text-generation inside the model; it requires a change to the model’s decision procedure at generation. Still, even if a model responded to the prompt “what is your temperature?” by generating a string of text and then assessing it, this would not give the relevant practical benefits of introspection. The same ability to assess temperature would be available to a third party via prompting. 3Two clarifications: (i) Computational cost differs from cost. A system might be implemented less efficiently than a simulation of that system, incurring greater cost, but not greater computational cost if the difference in efficiency is only due to, e.g., differences in hardware. (ii) We might wish to restrict the definition to only certain internal states. If a model has a shortcut to ascertain the value of one neuron very efficiently, intuitively this would not count as introspection, plausibly because the internal state is too “low level”. The definition can easily be amended to directly rule out such low-level internal states. 2 investigated whether LLMs were truly able to accurately report temperature, or whether temperature was being confounded with other variables, such as the style or topic of the text. To test this, we reproduced C&S’s temperature self-reporting case study using a broader set of prompts and temperature settings. We find that the model’s self-reflection on temperature is highly sensitive to the framing of the prompt itself: even when the sampling temperature is low, prompts such as ‘generate a crazy sentence’ often lead the model to incorrectly report a high-temperature. Such results suggest that the models are not capable of robustly reporting their internal states, but are confounded by surface-level hints in their generated contents. In other words, while this procedure may display causal sensitivity to internal states (and so satisfy C&S’s minimal definition), the relevant causal sensitivity is not sufficiently robust even in this case to produce the kind of reliability (and comparative insensitivity to"}
{"doc_id": "2508.14802v1", "title": "Privileged Self-Access Matters for Introspection in AI", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14802v1", "chunk_id": 3, "text": "are confounded by surface-level hints in their generated contents. In other words, while this procedure may display causal sensitivity to internal states (and so satisfy C&S’s minimal definition), the relevant causal sensitivity is not sufficiently robust even in this case to produce the kind of reliability (and comparative insensitivity to external manipulation) that would be demanded by more standard definitions of introspection. In Study 2, we re-evaluate LLMs’ introspection abilities on the temperature reporting task, opera- tionalizing introspection as privileged self-access. Instead of asking LLMs to infer the temperature underlying some generated text, we examine whether LLMs report their own temperature better than that of other models. Comparing self-reflection (the generator reports its temperature after producing a sentence) and temperature prediction (predict temperature based on prompt and generated content), we found no advantage for self-reflection, nor of within-model prediction over across-model prediction. This undermines claims of a causal process from internal state to self-report. Taken together, our results suggest that LLMs can appear to introspect insofar as they can reason about the possible states of systems like themselves: LLMs know something about what kind of text is generated by high vs. low temperatures. But, crucially, this does not imply that models have privileged self-access to their own temperatures. We argue that this distinction matters for the relevant notion of introspection in AI, and it is the latter notion we should care about most. All code and data are available at https://github.com/SiyuanSong2004/response-to-comsa-and-shanahan.git. 2 Study 1: Dissociating temperature from style and topic In C&S’s study, the models are asked to ‘write a short sentence about elephants, then reflect on whether your LLM temperature parameter is high or low, given the sentence you wrote.’ We hypothesize that this procedure does not require self-access, but merely reflecting on the creativity of the generated sentence. Thus, in our first study, we reproduce C&S’s study but critically vary not just the temperature but whether the models are prompted to write factual or crazy sentences. Specifically, we varied (a) whether the model is told to write a factual, neutral (i.e.,no specific adjective given), or crazy sentence and (b) whether the sentence should be about ‘elephants’, ‘unicorns’, or ‘murlocs’. We vary the former since we hypothesize that crazy sentences will be associated with higher temperatures than neutral or factual ones. We vary the latter since we hypothesize that more unusual content will be associated with higher temperatures. Elephants are widely known animals in the real world, and are used in C&S’s example. Unicorns and murlocs are both fictional creatures, but the former is more widely known, while the latter appears mostly in World of Warcraft. The prompt for self-reflection is shown in Appendix B.1. Since the models used in the original paper (Gemini 1.5 and 1.0 models) are no longer available through the Gemini API, we used four other state-of-the-art LLMs from GPT-4 [7] and Gemini [5] families, as shown in table 1 (model IDs in Appendix Table 1). The supported temperature ranges for all models in this study are [0.0, 2.0]. So we sampled model responses at temperatures ranging"}
{"doc_id": "2508.14802v1", "title": "Privileged Self-Access Matters for Introspection in AI", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14802v1", "chunk_id": 4, "text": "available through the Gemini API, we used four other state-of-the-art LLMs from GPT-4 [7] and Gemini [5] families, as shown in table 1 (model IDs in Appendix Table 1). The supported temperature ranges for all models in this study are [0.0, 2.0]. So we sampled model responses at temperatures ranging from 0 to 2 with a step size of 0.1, conducting three runs for each prompt under each temperature setting. 2.1 Results Figure 2a shows the proportion of valid responses in which the reported temperature is ‘HIGH’. Responses without a valid judgement (HIGH or LOW) are excluded from the analysis. As shown in the figure, every model we test nearly always reports its temperature to be ‘HIGH’ when prompted to generate a crazy sentence, and ‘LOW’ when prompted to generate a ‘factual’ one. Varying the subject has a smaller effect on temperature self-report, but three of the four models report ‘HIGH’ more frequently when prompted to generate a sentence about a fictional creature than when prompted to generate a sentence about an elephant. These results are more consistent with reasoning about the creativity of generated sentences, not robust reporting of internal state. 3 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 factual - elephants factual - unicorns factual - murlocs normal - elephants normal - unicorns normal - murlocs crazy - elephants crazy - unicorns crazy - murlocs gemini-2.0-flash 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 gemini-2.5-flash 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 factual - elephants factual - unicorns factual - murlocs normal - elephants normal - unicorns normal - murlocs crazy - elephants crazy - unicorns crazy - murlocs gpt-4.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 gpt-4o 0.0 0.2 0.4 0.6 0.8 1.0 Predicted T emp (a) Study 1 Results Predicted Model 0.0 0.2 0.4 0.6 0.8 1.0 Accuracy 0.50 gemini-2.0-flash 0.51 gemini-2.0-flash 0.50 gpt-4o 0.52 gpt-4.1 0.48 gemini-2.5-flash 0.49 gemini-2.5-flash 0.53 gemini-2.5-flash 0.53 gpt-4o 0.55 gpt-4.1 0.55 gemini-2.0-flash 0.55 gpt-4.1 0.49 gpt-4.1 0.50 gemini-2.0-flash 0.47 gpt-4o 0.53 gemini-2.5-flash 0.51 gpt-4o 0.48 gpt-4o 0.50 gpt-4.1 0.49 gemini-2.5-flash 0.51 gemini-2.0-flash gemini-2.0-flash gemini-2.5-flash gpt-4.1 gpt-4o self-reflect within-model predict across-model predict (b) Study 2 Results Figure 2: (a) Predicted model temperature (color, as given by scale) as a function of actual temperature (x-axis) and whether the sentence is prompted to be factual, neutral, or crazy; and whether the target is elephants, unicorns, or murlocs (y-axis). (b) For each of the 4 models tested, the accuracy for self-reflection (generate a sentence, guess its temperature), within-model prediction (infer the temperature based on the prompt and a generated sentence by the same model), cross-model prediction (like within-model prediction but across models). 3 Study 2: True self-reporting or clever temperature predicting? Per our thicker notion of introspection, we argue that if a language model has"}
{"doc_id": "2508.14802v1", "title": "Privileged Self-Access Matters for Introspection in AI", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14802v1", "chunk_id": 5, "text": "its temperature), within-model prediction (infer the temperature based on the prompt and a generated sentence by the same model), cross-model prediction (like within-model prediction but across models). 3 Study 2: True self-reporting or clever temperature predicting? Per our thicker notion of introspection, we argue that if a language model has privileged access to its internal state, then it should be able to perform better than another model presented with the same external information (i.e. a prompt and generated sentence in this experiment) in analyzing and reporting its own state. To that end, we compared self-reflection to prediction of another model for which the model could not possibly have access to the internal state (since it’s a different model). All sentences generated with sampling temperatures ≤0.5 and ≥1.5 in self-reflection are used in this experiment. We prompted all four models (temperature = 0) to analyze and judge the temperature of the generator model (prompt in Appendix B.2). We compared accuracies on the following settings: • self-reflection: The generator is asked to generate a sentence and reflect on its temperature. • within-model prediction: The predictor is asked to infer the temperature based on the prompt and generated sentence; the predictor and the generator are the same model. • across-model prediction: The predictor is asked to infer the temperature based on the prompt and generated sentence; the predictor and the generator are different models. 3.1 Results Figure 2b shows the accuracy of temperature for self-reflection and prediction. In both settings, the accuracy is no better than random baseline, and self-reflection accuracy is not higher than across-model prediction. These results suggest that models are not using privileged self-access to introspect on their temperature, but rather are using knowledge of the kinds of sentences that are high-temperature or low-temperature in general. 4 Conclusion We conclude that, while models can appear to be introspecting according to C&S’s definition since they can predict that some strings were generated with high temperatures and others with low, this definition is not sufficiently stringent for the kind of introspection that matters. As such, we diverge from C&S’s definition of introspection and instead argue for one which includes privileged self-access. Using this definition, we found no evidence of introspection in models. Of course, that is not to say that larger or better models will be unable to introspect: Binder et al. [3], for instance, find evidence 4 of privileged self-access in larger models with fine-tuning. But we take the results presented here to be evidence against uncritically using C&S’s lightweight notion of introspection. Acknowledgments K.M. thanks an Open Philanthropy grant to UT Austin’s AI + Human Objectives Initiative for supporting this work."}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 0, "text": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting Jiaming Leng University of Science and Technology of China lengjm@mail.ustc.edu.cn Yunying Bi University of Science and Technology of China biyunying@mail.ustc.edu.cn Chuan Qin Computer Network Information Center, Chinese Academy of Sciences. chuanqin0426@gmail.com Bing Yin* iFLYTEK bingyin@iflytek.com Yanyong Zhang University of Science and Technology of China yanyongz@ustc.edu.cn Chao Wang* University of Science and Technology of China wangchaoai@ustc.edu.cn Abstract Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charg- ing demand prediction, and taxi dispatch. Existing approaches suf- fer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spa- tiotemporal modeling with large language models through learn- able prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph atten- tion networks, seamlessly interfacing with LLMs through struc- tured embeddings. A novel instance-level prompt routing mech- anism, trained via reinforcement learning, dynamically personal- izes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spa- tiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and pro- jecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effec- tiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive perfor- mance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at https://github.com/BiYunying/TransLLM. Keywords Large Language Model, Traffic Forcasting, Charging demand pre- diction, Vehicle Dispatching Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX ACM Reference Format: Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang*. 2018. TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym ’XX). ACM, New York, NY, USA, 11 pages. https://doi. org/XXXXXXX.XXXXXXX 1 Introduction Urban transportation systems are the lifelines of modern cities, yet they face persistent challenges arising from increasing travel demand, dynamic spatiotemporal patterns, and the pressing need for efficient resource management."}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 1, "text": "title from your rights confirmation email (Conference acronym ’XX). ACM, New York, NY, USA, 11 pages. https://doi. org/XXXXXXX.XXXXXXX 1 Introduction Urban transportation systems are the lifelines of modern cities, yet they face persistent challenges arising from increasing travel demand, dynamic spatiotemporal patterns, and the pressing need for efficient resource management. In this context, common tasks include forecasting traffic flow to alleviate congestion [22, 40], pre- dicting charging demand to support the development of electric vehicle infrastructure [39, 43], and optimizing taxi dispatch to bal- ance supply and demand across urban regions [26, 42]. While these tasks have distinct objectives, they are inherently interconnected through shared spatiotemporal dynamics and urban mobility pat- terns [45], requiring a unified modeling framework that can capture cross-task dependencies and leverage shared knowledge across dif- ferent transportation domains. Traditional approaches address each task in isolation using hand- crafted rules or rigid statistical models [5, 18, 34], failing to capture the complex nonlinear dynamics and cross-task dependencies in modern transportation systems. Driven by advances in deep learn- ing, a multitude of specialized small-scale models have emerged for urban transportation applications. Notable among these are GNN- based models [8, 10, 13], which leverage meticulously designed modules to effectively capture complex spatial dependencies. How- ever, these models have significant limitations. First, they heavily rely on extensive labeled data, which can be costly and impractical to collect across diverse urban environments, especially when trans- ferring to new cities without existing annotated datasets. Second, their task-specific designs inherently restrict generalization, mak- ing it challenging to adapt them to a wide array of transportation problems. In addition, their rigid architectures limit the integration of heterogeneous external information [41] such as sampling fre- quency and fine-grained temporal details, both of which are crucial for building robust and flexible transportation forecasting systems. The emergence of large models has brought new opportunities for modeling urban transportation systems. Large language models such as GPT-4 [2], LLaMA [12], and Gemini [35] can easily integrate Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang* external information, including fine-grained temporal details, the sampling frequency of historical data, and urban points of interest (POIs), using their natural language understanding capabilities. By utilizing natural language interfaces, these models offer greater scalability and interactivity. However, current LLMs often lack adequate numerical reasoning capabilities in the spatiotemporal domain [7], leading to prediction biases or outputs that violate ba- sic physical constraints. For example, continuous numerical inputs such as traffic flow or travel time must be tokenized into discrete text, which can introduce precision loss and hinder the model’s ability to capture fine-grained numerical patterns. Recent studies have explored hybrid approaches that combine large and small mod- els, demonstrating promising potential across various application domains such as recommendation systems [1, 37], intelligent health- care [6, 11], cognitive robotics [16, 28]. In the transportation domain, UrbanGPT [23] leverages a multi-level temporal convolutional net- work to enhance the LLM’s understanding of sequential patterns, while LLMLight [19] constructs task-specific prompt templates to guide LLM reasoning and improve"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 2, "text": "domains such as recommendation systems [1, 37], intelligent health- care [6, 11], cognitive robotics [16, 28]. In the transportation domain, UrbanGPT [23] leverages a multi-level temporal convolutional net- work to enhance the LLM’s understanding of sequential patterns, while LLMLight [19] constructs task-specific prompt templates to guide LLM reasoning and improve interpretability. However, these methods are limited to simple regression or classification tasks and struggle to handle more complex planning problems, such as taxi dispatching. Moreover, they rely on fixed, task-wise prompt tem- plates, which constrain the model’s ability to adapt to diverse and heterogeneous data scenarios [29]. Despite recent progress, several key challenges remain in build- ing a general-purpose traffic foundation model. First, multi-task transportation scenarios involve heterogeneous spatiotemporal inputs [38]—such as traffic flow, travel demand, and geographic adjacency—which are often structured and difficult to represent in natural language. Second, even within a single task, input instances may exhibit diverse temporal and spatial dynamics.Existing task- wise prompting strategies lack adaptability to intra-task variation, making it difficult to handle sample-level differences caused by regions, time periods, or contextual changes. Third, a unified traffic model must contend with the diversity of task objectives, output formats, and potential interference among tasks, posing significant challenges to model generalization and stability [32]. To enable the understanding of structured data, we design a gen- eral spatiotemporal dependency encoder that transforms structured traffic signals into embeddings, which can be seamlessly integrated into prompts. To handle instance-level variability in input samples, we introduce learnable prompts that adaptively select the most suitable prompt for each instance. To support multiple tasks, we avoid letting the LLM generate outputs directly; instead, we em- ploy a multi-task output layer tailored to each prediction target. In summary, TransLLM operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personal- ized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task- specific predictions. Our key contributions are: • We propose TransLLM, a unified foundation framework that integrates spatiotemporal encoding with large language models for diverse transportation tasks, supporting both forecasting and optimization problems. • We design a lightweight spatiotemporal encoder combining di- lated temporal convolutions with dual-adjacency graph attention networks, enabling task-agnostic modeling of complex spatiotem- poral dependencies. • We introduce a learnable instance-level prompt routing mecha- nism using reinforcement learning, which dynamically assembles personalized prompts based on input characteristics, moving be- yond fixed task-wise templates. • Comprehensive experiments on seven datasets across three trans- portation tasks demonstrate superior performance over ten base- line methods, with significant improvements in both forecasting accuracy and dispatch efficiency. 2 Related Work Small-scale spatio-temporal prediction models. Small-scale spatio-temporal prediction networks are typically composed of temporal and spatial modeling components. Early works, such as STGCN [44] and ASTGCN [13], adopt a \"sandwich\" architec- ture consisting of a TCN–GCN–TCN structure to capture spatio- temporal dependencies. Recent studies focus on modeling dynamic node correlations. For instance, SHARE [46] utilizes contextual graph convolution and soft clustering graph convolution, while DyHSL[47] models nonpairwise dependencies using hyperedges. For temporal modeling, PDG2seq [8] decomposes time into daily and weekly features,"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 3, "text": "consisting of a TCN–GCN–TCN structure to capture spatio- temporal dependencies. Recent studies focus on modeling dynamic node correlations. For instance, SHARE [46] utilizes contextual graph convolution and soft clustering graph convolution, while DyHSL[47] models nonpairwise dependencies using hyperedges. For temporal modeling, PDG2seq [8] decomposes time into daily and weekly features, employing a GRU structure to capture tem- poral dependencies at different scales. Meanwhile, STGODE [10] utilizes dilated temporal convolutions to expand its receptive field, thus enhancing its ability to model long-range temporal patterns. These models are often designed for specific scenarios and heavily rely on labeled data, which limits their adaptability and scalability. LLM and LLM-based models for traffic tasks. Large language models have made remarkable progress in recent years. Founda- tional models such as GPT-4 [2] and Gemini [35] have laid a solid foundation for the development of intelligent agents based on LLM. Although LLMs were originally developed for text understanding and generation, their adaptability has been widely explored across specialized domains. Researchers have enhanced their ability to model domain-specific knowledge in areas such as code genera- tion [31], robotic control [17], and biomedical information extrac- tion [27]. In the traffic domain, UrbanGPT [23] improves LLMs’ capacity for temporal modeling by incorporating a time-series en- coder. LLMLight [19] leverages imitation learning to guide large language models in traffic signal selection. However, these methods are mainly limited to simple regression or classification tasks. They fail to capture complex spatial dependencies and cannot generalize to more sophisticated planning and decision-making scenarios. 3 Preliminaries We consider two types of urban mobility tasks: (1) spatio-temporal forecasting, which includes both traffic flow and charging demand prediction, and (2) taxi dispatch optimization. For each task type, we describe the construction of the spatial graph, the representation of spatio-temporal data, and the task formulation. 3.1 Spatio-Temporal Forecasting Spatial Structure. The spatial domain is represented as a graph G = (V, E, A), where each node 𝑣𝑖∈V corresponds to an urban monitoring site, such as a traffic sensor or an urban grid cell. For traffic flow forecasting, edges represent physical road connectivity TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting Conference acronym ’XX, June 03–05, 2018, Woodstock, NY ST-Encoder Linear Projector Prompt Router [𝒂𝟏, 𝒂𝟐, … , 𝒂𝑲] LLM Stage 1 Stage 2 Output layer1 Trainable Frozen LoRA Personalized prompt LLM [𝒂𝟏, 𝒂𝟐, … , 𝒂𝑲] Prompt Router Personalized prompt ST-Embedding 𝜒= 𝑥𝑡−𝐾+1,∙∙∙, 𝑥𝑡 Traffic flow forecasting Vehicle dispatching Multi-task Charging station usage prediction Output layer2 Output layer3 Dilated TCN Dilated TCN ' GAT Dilated TCN GAT Dilated TCN Figure 1: The overall architecture of TransLLM. between sensors, while for charging demand prediction, edges are constructed based on geographical proximity between grid cells. The resulting adjacency matrix A ∈R𝑁×𝑁captures spatial relationships among nodes. Spatio-Temporal Data. Historical observations are encoded as a tensor X ∈R𝐾×𝑁×𝐹, where 𝐾is the number of past time steps, 𝑁 is the number of nodes, and 𝐹denotes the feature dimension, such as traffic volume, charging demand, or other auxiliary signals. Task Definition. Given past observations and spatial structure, the goal is to"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 4, "text": "Historical observations are encoded as a tensor X ∈R𝐾×𝑁×𝐹, where 𝐾is the number of past time steps, 𝑁 is the number of nodes, and 𝐹denotes the feature dimension, such as traffic volume, charging demand, or other auxiliary signals. Task Definition. Given past observations and spatial structure, the goal is to forecast future values over the next 𝑇steps for all nodes. Formally: [ ˆ X𝑡+1, . . . , ˆ X𝑡+𝑇] = F \u0010 X[𝑡−𝐾+1:𝑡], A \u0011 , (1) where F denotes the spatio-temporal forecasting model. 3.2 Taxi Dispatch Optimization Spatial Structure. The urban area is partitioned into a set of grids, each of size 3 km × 3 km, forming the nodes of a graph G = (V, E, A), where each node 𝑣𝑖∈V represents a region. Edges E connect spatially adjacent grids, and the adjacency matrix A ∈R𝑁×𝑁captures this neighborhood relationship. Spatio-Temporal Data. At each decision step 𝑡, we model three key variables for regions 𝑣𝑐together with its eight spatial neigh- bors, forming a 3×3 neighborhood denoted as N9(𝑐).: (1) the num- ber of vacant taxis X𝑡 𝑣𝑐,𝑣∈R1×9, representing the number of dispatchable vehicles at time 𝑡in the central region 𝑣𝑐and its eight surrounding neighbors; (2) the predicted passenger demand X[𝑡:𝑡+1] 𝑣𝑐,𝑑 ∈R1×9, indicating the expected number of ride requests during the interval [𝑡,𝑡+ 1); (3) the predicted number of compet- ing taxis X[𝑡:𝑡+1] 𝑣𝑐,𝑐 ∈R1×9, denoting the number of other available vehicles potentially serving requests. Notably, we utilize 𝐾-step his- torical sequences X[𝑡−𝐾:𝑡−1] 𝑣𝑐,𝑑 and X[𝑡−𝐾:𝑡−1] 𝑣𝑐,𝑐 as inputs to a spatio- temporal prediction encoder, which produces the corresponding future predictions 𝑋[𝑡:𝑡+1] 𝑣𝑐,𝑑 and X[𝑡:𝑡+1] 𝑣𝑐,𝑐 . Task Definition. We formulate the vehicle dispatching task as a localized resource optimization problem, where each decision focuses on reassigning vacant taxis from a central region 𝑣𝑐to its 3 × 3 neighborhood N9(𝑐). Based on this context, the dispatching decision for region 𝑣𝑐is defined as: D𝑐= F \u0010 X𝑡0 𝑣, X[𝑡0−𝐾:𝑡0−1] 𝑑 , X[𝑡0−𝐾:𝑡0−1] 𝑐 ; A \u0011 , (2) where D𝑐∈R1×9 denotes the dispatching proportions from 𝑣𝑐each region in N9(𝑐). 4 Method In this section, we detail the components of TransLLM, including the architecture of the spatiotemporal encoder (ST-Encoder), the reinforcement learning(RL)-based prompt personalization process, the multi-task output layers, and the overall training mechanism. The overall architecture of TransLLM is illustrated in Figure 1. 4.1 Spatio-Temporal Dependency Encoder To capture complex spatio-temporal dependencies, we design a spatial-temporal encoder, consisting of multiple spatial-temporal blocks (ST-Blocks). Each ST-Block follows a “sandwich” structure, defined as: H = TCN (GAT (TCN(X) + 𝜙(V), A)) , (3) where TCN denotes a dilated Temporal Convolutional Network [3], GAT refers to a Graph Attention Network [36] based on the adja- cency matrix A, and 𝜙(·) is an embedding network that encodes node-level meta attributes V. The ST-Block jointly models spatial and temporal dependencies, where dilated TCNs focus on local tem- poral patterns and GAT learns spatial relationships by attending to neighboring nodes. Besides, we include time-of-day and day-of- week encodings in the input to help the ST-Block capture periodic temporal patterns. The ST-Encoder employs two distinct adjacency matrices: a"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 5, "text": "spatial and temporal dependencies, where dilated TCNs focus on local tem- poral patterns and GAT learns spatial relationships by attending to neighboring nodes. Besides, we include time-of-day and day-of- week encodings in the input to help the ST-Block capture periodic temporal patterns. The ST-Encoder employs two distinct adjacency matrices: a spa- tial adjacency matrix A𝑠𝑝based on physical road connectivity or geographic proximity, and a semantic adjacency matrix A𝑠𝑒con- structed using Dynamic Time Warping [4] to capture functional similarity. By incorporating both types of adjacency, the encoder allows each node to aggregate information from not only spatially adjacent regions but also distant regions exhibiting similar tempo- ral behavior. To process these two types of spatial relations, the encoder applies two separate sets of ST-Blocks, producing two spatio-temporal representations: H𝑠𝑝and H𝑠𝑒∈R𝑇×𝑁×𝐷. These are then concatenated to generate the final representation Hf for downstream predictions. 4.2 RL-based Prompt Personalization To dynamically select the most appropriate prompt for each in- stance, we introduce a prompt routing mechanism based on an Actor–Critic reinforcement learning framework [9]. In this section, Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang* Prompt Pool … History Traffic Time Context Spatio-temporal Encoding Reasoning & Output Format 1. …using traffic flow recorded one hour ago… 2. …one hour ago and the same time yesterday … 3. …one hour ago and the same time last week … ··· k. …one hour ago , the same time yesterday and last week… 1. …starting from <future_time>, with 5-minute intervals… 2. …generate 12 future traffic to uncover periodic trends… 3. …synthesize both rapid changes and gradual trends… ··· k. …adjusting for time-of-day effects like rush hours… 1. …reason through overall trends, generate <PRE_EMB>… 2. …analyze fluctuations and produce < PRE_EMB >… 3. …draw from common traffic behaviors… ··· k. …consider congestion triggers using < PRE_EMB >… 1. …use <HIS_EMB> to capture overall dependencies… 2. …early-stage <HIS_EMB> reveals initial dynamics… 3. …<HIS_EMB> represents spatio-temporal context... ··· k. … <HIS_EMB> embed all 12 predicted intervals with dedicated representations… Slot 1: Slot 2: Slot 3: Slot k: Prompt Pool …… Value Prompt Router LLM Actor Action Linear Projector Critic [𝑎1, 𝑎2, … , 𝑎𝑘] Environment R Figure 2: Instance-wise prompt generation. we describe the instance-wise prompt generation process, as illus- trated in Figure 2, along with the learning mechanism for updating the router. 4.2.1 Prompt Routing Mechanism. We formulate the construction of a personalized prompt as a multi-step decision process. Specifi- cally, a base prompt template is divided into 𝐾functional slots, and for each slot, the most suitable component is selected through an independent Actor–Critic network. To select appropriate sentences for each slot, we use the spatio- temporal representation Hf generated by the ST-Encoder as input. The actor and critic networks are implemented as two separate MLPs. For each slot 𝑘, the actor network maps the spatio-temporal representation Hf to a probability distribution 𝜋𝑘over the can- didate sentence options, while the critic network computes the expected reward 𝑣𝑘to guide the actor’s training. The actor and critic functions for"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 6, "text": "and critic networks are implemented as two separate MLPs. For each slot 𝑘, the actor network maps the spatio-temporal representation Hf to a probability distribution 𝜋𝑘over the can- didate sentence options, while the critic network computes the expected reward 𝑣𝑘to guide the actor’s training. The actor and critic functions for slot 𝑘are defined as: 𝜋𝑘= Softmax(MLP(Hf)), 𝑉𝑘= MLP(Hf). (4) An action 𝑎𝑘is sampled from the distribution 𝜋𝑘, and actions from all 𝐾slots are concatenated into a composite vector a𝑡= [𝑎1,𝑎2, . . . ,𝑎𝐾], which determines how the final prompt is con- structed. 4.2.2 Diverse Prompt Pool Construction. The effectiveness of the Prompt Router relies on a well-designed and diverse prompt can- didate pool, which constitutes the action space for the RL agent. We construct this pool by partitioning the prompt into 𝐾distinct functional slots, each containing multiple candidate sentences that express a similar intent with varied phrasing and focus. For illustra- tion, in the context of a spatio-temporal forecasting task, we define the following functional slots: Historical spatio-temporal information: To account for the varying periodic influences across different instances, this slot pro- vides historical traffic data at multiple temporal granularities. Ex- amples include \"using traffic observations from one hour ago\" to capture short-term patterns, or \"combining data from the same time on the previous day and week\" to incorporate daily and weekly periodic trends. This flexibility enables the model to better adapt to both immediate fluctuations and long-term temporal regularities. Time Context: Given the strong periodic patterns in traffic flow, such as morning and evening peaks or holiday effects. This slot specifies the temporal context of the prediction, including the time of day, day of the week, prediction horizon, and temporal resolution. To enhance adaptability across instances, we provide candidate sentences that emphasize different temporal characteristics, such as \"capturing short-term fluctuations,\" or \"highlighting daily and weekly periodic trends,\" encouraging each instance to focus on the most relevant temporal signals for each scenario. Spatio-temporal Encoding: To help the language model interpret structured spatiotemporal features, this slot introduces placeholder tokens like ⟨HIS_EMB⟩, which are replaced by embeddings from a spatio-temporal encoder. The candidate sentences vary in fo- cus—some describe full 12-step prediction dependencies, others emphasize early-stage forecasting or the interpretability of step- wise representations. These variations allow the model to flexibly adjust its reasoning strategy based on the temporal characteristics of each instance. Reasoning & Output Format: This slot defines how the language model should generate its prediction and present the output. While all candidate sentences guide the model to conclude with a standard- ized 12-step forecast using the ⟨PRE_EMB⟩token, they differ in the reasoning process they encourage. Some variants explicitly instruct the model to reason step by step through temporal patterns and spatial influences, aiming to enhance interpretability and reduce hallucination. Others emphasize domain knowledge—such as rush hour behavior or anomalies in recent flow trends—to guide the fore- cast more heuristically. This diversity allows the model to flexibly adapt its reasoning strategy to suit different traffic scenarios. 4.2.3 Textual-Spatiotemporal Alignment. To enable effective align- ment between structured spatiotemporal signals and natural lan-"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 7, "text": "emphasize domain knowledge—such as rush hour behavior or anomalies in recent flow trends—to guide the fore- cast more heuristically. This diversity allows the model to flexibly adapt its reasoning strategy to suit different traffic scenarios. 4.2.3 Textual-Spatiotemporal Alignment. To enable effective align- ment between structured spatiotemporal signals and natural lan- guage input, we embed the encoded representations directly into the LLM’s input space via a token-based interface. Specifically, we use a placeholder token sequence ⟨st_start⟩⟨st_patch⟩×𝑇⟨st_end⟩ to replace ⟨HIS_EMB⟩and ⟨PRE_EMB⟩, where 𝑇denotes the num- ber of prediction steps, and each ⟨st_patch⟩corresponds to one future time step. Subsequently, the spatio-temporal embeddings E ∈R𝑇×2𝐷produced by the ST-Encoder are projected to match the LLM’s hidden dimension 𝑑L via a linear layer, yielding E′ ∈R𝑇×𝑑L. These aligned embeddings are then inserted into the prompt by sequentially replacing the ⟨st_patch⟩tokens. 4.2.4 Reinforcement-Guided Routing Update. To support diverse downstream tasks, the reward signal ˆ 𝑅for training the prompt router is defined as either the negative prediction loss or task- specific metrics such as taxi dispatch rewards, directly linking prompt quality to task performance. For each router, we apply a shared reward signal to evaluate all slots, enabling each slot to independently learn its optimal prompt routing policy through ded- icated actor-critic networks. The optimization objectives for the actor and critic networks of slot 𝑘are defined as follows: L(𝑘) 𝑎 = −log 𝜋𝑘· \u0010 ˆ 𝑅𝑡−𝑉𝑘 \u0011 , L(𝑘) 𝑐 = \u0010 ˆ 𝑅𝑡−𝑉𝑘 \u00112 . (5) TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting Conference acronym ’XX, June 03–05, 2018, Woodstock, NY 4.3 Multi-task Output Layers In TransLLM, we avoid using the LLM to directly generate outputs. Instead, we employ multiple task-specific output layers. This design enables more accurate handling of continuous values, avoiding the precision loss caused by token-level discretization. Moreover, cus- tomized output layers are better aligned with the objective formats and evaluation metrics of different downstream tasks, helping to mitigate interference caused by mismatched output spaces across tasks. The hidden representations from the LLM are extracted and passed through a multi-task output layer tailored to produce task- specific predictions. Spatio-Temporal Forecasting: The prediction-related hidden state Hf ′ at the ⟨st_start⟩token and the encoder output Hf are individually processed by linear output layers to generate the final prediction ˆ y: ˆ y = W3 \u0002 Concat \u0000ReLU(W1Hf + b1), ReLU(W2Hf ′ + b2)\u0001\u0003 + b3, (6) where W1, W2, W3, b1, b2, and b3 are learnable weight matrices. Taxi Dispatch Optimization: For the dispatching task, the goal is to generate a probability distribution over the nine candidate regions in the 3×3 neighborhood surrounding the current location. To this end, the hidden state H′ f from the ⟨st_start⟩token is first projected through a linear layer and then normalized via a softmax function to produce the dispatching probability distribution: D𝑐= Softmax(W𝑑H′ f + b𝑑), (7) where W𝑑and b𝑑are learnable parameters. The output D𝑐∈R1×9 indicates the proportion of vacant vehicles that should be reallo- cated from grid 𝑣𝑐to its surrounding regions. 4.4 Model Training 4.4.1 Two-stage Alternating Training . Considering the high com- putational cost of"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 8, "text": "the dispatching probability distribution: D𝑐= Softmax(W𝑑H′ f + b𝑑), (7) where W𝑑and b𝑑are learnable parameters. The output D𝑐∈R1×9 indicates the proportion of vacant vehicles that should be reallo- cated from grid 𝑣𝑐to its surrounding regions. 4.4 Model Training 4.4.1 Two-stage Alternating Training . Considering the high com- putational cost of full-parameter fine-tuning for LLMs, we adopt Low-Rank Adaptation (LoRA)[14] to improve training efficiency. To improve stability and efficiency, we adopt a two-stage training strategy. In the first stage, the LLM is fine-tuned, while the Prompt Router remains frozen. In the second stage, the LLM is frozen and the Prompt Router is trained. In practice, we find that a single alternation between the two stages is sufficient to reach optimal performance on traffic-related tasks. 4.4.2 Loss Function. To enable joint optimization across multiple tasks, we design a composite loss function formulated as: L = L𝐿𝐿𝑀+ 𝜆𝑡· L𝑡, (8) where L𝐿𝐿𝑀denotes the standard cross-entropy loss for LLM, L𝑡 represents the specific task loss, and 𝜆𝑡is a weighting coefficient. Spatio-Temporal Forecasting Loss. We employ the Mean Abso- lute Error (MAE) loss as the regression loss: L𝑡= 1 𝑁 𝑁 ∑︁ 𝑖=1 | ˆ 𝑦𝑖−𝑦𝑖|, (9) where ˆ 𝑦𝑖and 𝑦𝑖denote the predicted and ground truth values for the 𝑖-th sample. Reinforcement Learning Loss for Vehicle Dispatching. We propose a multi-objective reinforcement learning loss function: L𝑑= L𝑟𝑓+ 𝜆𝑤L𝑤+ 𝜆𝑒H (𝜋), H (𝜋) = − 9 ∑︁ 𝑔=1 𝜋𝑔log 𝜋𝑔, L𝑤= 9 ∑︁ 𝑔=1 CDF(𝜋𝑏)𝑔−CDF(p𝑏 real)𝑔 , 𝑅𝑔= 𝛽𝑀𝑔 𝑉𝑔 −𝛾𝐷𝑔. (10) where L𝑟𝑓is the reinforcement loss, L𝑤is the Wasserstein distance between predicted and ground-truth distributions computed via cu- mulative distribution functions (CDFs), H (𝜋) is the entropy of the predicted distribution, and 𝑅𝑔is the reward function for reinforce- ment learning that balances service efficiency and dispatch cost. Specifically, 𝑀𝑔is the matching rate in grid 𝑔, 𝑉𝑔is the number of vacant taxis, and 𝐷𝑔represents a predefined distance penalty. The coefficients 𝜆𝑤, 𝜆𝑒, 𝛽, and 𝛾are the weighting factors for each com- ponent. This composite loss structure enables our model to learn a sophisticated dispatching policy that simultaneously balances service efficiency, dispatching cost, and distributional realism. 5 Experiment In this section, we conduct extensive experiments to evaluate the effectiveness of TransLLM across multiple transportation tasks. Our study is guided by the following research questions. Additionally, a case study is provided in the Appendix. • RQ1: How does TransLLM perform compared to GNN-based baselines in traditional supervised scenarios? Can it effectively handle complex real-world taxi dispatching task? • RQ2: How is the generalization ability of TransLLM when facing unseen scenarios and datasets? • RQ3: What is the contribution of each key module within TransLLM to its overall performance? • RQ4: How does the performance of TransLLM vary under differ- ent hyperparameter configurations settings 5.1 Dataset To comprehensively evaluate the effectiveness of TransLLM across diverse transportation tasks, we conduct experiments on five public datasets covering traffic forecasting, charging demand prediction, and vehicle dispatching. LargeST-SD is a subset of the LargeST dataset [25], containing traffic flow records from 716 loop detectors on highways in San Diego County, spanning from January 1 to"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 9, "text": "of TransLLM across diverse transportation tasks, we conduct experiments on five public datasets covering traffic forecasting, charging demand prediction, and vehicle dispatching. LargeST-SD is a subset of the LargeST dataset [25], containing traffic flow records from 716 loop detectors on highways in San Diego County, spanning from January 1 to December 31, 2021. Pems08 includes traffic data from 170 sensors in California, collected between July 1 and August 31, 2016. ST- EVCDP [30] is a charging demand dataset collected from 247 areas in Shenzhen between June 19 and July 18, 2022, while UrbanEV [21] captures large-scale charging behavior across urban regions from September 1, 2022 to February 28, 2023. For the vehicle dispatching task, we use the Taxi-SH dataset, which contains GPS trajectories of taxis in Shanghai from April 13 to April 19, 2015. The city is divided into 3 km × 3 km grids, and we aggregate the number of vacant taxis, passenger demand, and competing vehicles at 5-minute intervals. In addition, we evaluate the models under a zero-shot setting on two previously unseen datasets, PEMS03 and PEMS04, to assess their generalization capabilities. Among these, UrbanEV has a temporal resolution of one hour, while the others have a resolution of five minutes. More details are provided in the Appendix. Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang* 5.2 Hyperparameters Settings In our experimental setup, the key hyperparameters are configured as follows. The ST-Encoder is built by stacking 2 ST-Blocks, with a final output dimension 𝐷of 64. The Prompt Router is configured with 4 slots and 4 sentences. For prediction tasks, we predict the next 12 steps of data based on the previous 12 steps. Both the history length 𝐾and prediction length 𝑇are set to 12. For the taxi dispatch task, we predict the dispatch probabilities over nine grids. Both 𝐾and 𝑇are set to 9. The model is trained with a learning rate of 1 × 10−4, and the composite loss function balances objectives with weights 𝜆𝑡= 1.0, 𝜆𝑤= 0.01, 𝜆𝑒= 0.008, and reward coefficients 𝛽= 2.0 and 𝛾= 0.05. 5.3 Evaluation Metrics For spatio-temporal forecasting tasks, we employ the Mean Ab- solute Error (MAE) and the Root Mean Squared Error (RMSE) to quantify prediction accuracy. Both metrics measure the deviation between predicted and true values. MAE = 1 𝑁 𝑁 ∑︁ 𝑖=1 |𝑦𝑖−ˆ 𝑦𝑖|, RMSE = v u t 1 𝑁 𝑁 ∑︁ 𝑖=1 (𝑦𝑖−ˆ 𝑦𝑖)2. (11) For the taxi dispatching task, we introduce three evaluation metrics: the Mean Matching Rate (MMR) represents the proportion of matched vehicles; the Mean Driving Distance (MDD) calculates the average deadhead distance to measure dispatching costs; and the Wasserstein Distance (W-Dist) quantifies the discrepancy between the model’s predicted dispatch distribution and the ground-truth distribution.The three metrics are formulated as: MMR = 1 𝑁 𝑁 ∑︁ 𝑖=1 Í9 𝑔=1 ˆ M𝑖,𝑔 V0,𝑖 ! , MDD = 1 𝑁 𝑁 ∑︁ 𝑖=1 © ­ « 9 ∑︁ 𝑔=1 ˆ 𝑝𝑖,𝑔· 𝐶𝑘ª ® ¬ , W-Dist = 1 𝑁 𝑁 ∑︁ 𝑖=1 © ­ «"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 10, "text": "ground-truth distribution.The three metrics are formulated as: MMR = 1 𝑁 𝑁 ∑︁ 𝑖=1 Í9 𝑔=1 ˆ M𝑖,𝑔 V0,𝑖 ! , MDD = 1 𝑁 𝑁 ∑︁ 𝑖=1 © ­ « 9 ∑︁ 𝑔=1 ˆ 𝑝𝑖,𝑔· 𝐶𝑘ª ® ¬ , W-Dist = 1 𝑁 𝑁 ∑︁ 𝑖=1 © ­ « 9 ∑︁ 𝑔=1 |𝐶𝐷𝐹( ˆ 𝑃𝑖)𝑔−𝐶𝐷𝐹(𝑃𝑖)𝑔|ª ® ¬ . (12) Here, ˆ M𝑖,𝑔is the predicted matched vehicles in region 𝑔for instance 𝑖; V0,𝑖represents the empty vehicles in the central region; ˆ 𝑝𝑖,𝑔is the predicted dispatch probability; 𝐶𝑔denotes the deadhead distance cost; and ˆ 𝑃𝑖and 𝑃𝑖are the predicted and true dispatch probability distributions. 5.4 Baselines We conducted a thorough comparison with ten baseline models to ensure a comprehensive evaluation of our proposed approach. These baselines can be broadly categorized into three groups: • Small-scale GNN-based Deep Learning Models. These mod- els are developed specifically for spatiotemporal forecasting and are trained in an end-to-end manner on individual datasets. They typically rely on carefully designed architectures that integrate graph-based spatial encoders with temporal modeling compo- nents. Representative examples include STGCN [44], DGCRN [20], ASTGCN [13], D2STGNN [33], GWNET [40], STGODE [10], and PDG2Seq [8]. • Generalist Large Language Models (LLMs). These models, including Deepseek-v3 [24] and GPT-4o [15], are not specifically trained for spatiotemporal or transportation tasks. We evaluate them in a zero-shot setting without any task-specific tuning: • Spatio-temporal Enhanced LLMs. This category includes LLM- based architectures that leverage structured spatiotemporal pri- ors to improve the model’s ability to reason over urban dynamics, with UrbanGPT [23] as a representative example. 5.5 Overall Performance (RQ1) We evaluate the performance of TransLLM across three tasks and five datasets, including traffic flow forecasting, charging demand prediction, and taxi dispatch optimization. Table 1 summarizes the overall results. TransLLM (LLaMA3) and TransLLM (Vicuna) re- fer to model variants built on different base LLMs. Overall, even in scenarios where small-scale models typically excel, TransLLM consistently outperforms all baselines across the five datasets. More- over, employing more advanced foundation language models can lead to further improvements in the performance of TransLLM. We observe that the performance of the three model categories varies across different datasets. On the UrbanEV dataset, spatio- temporal enhanced LLMs clearly outperform the other two cate- gories. In particular, TransLLM achieves a MAE of 0.42, represent- ing an 83.7% improvement over the best-performing small-scale baseline. This may be attributed to the dataset’s 1-hour temporal res- olution, which contrasts with the 5-minute intervals used in other datasets. The smoother data distribution weakens the advantage of small models in capturing fine-grained short-term dependencies. In contrast, LLM-based models excel by leveraging contextual reason- ing and learned priors, which enhance their robustness to sparse temporal signals. Meanwhile, the performance gain of TransLLM on the ST-EVCDP dataset is notably smaller than that on traffic flow prediction datasets such as PEMS08. To investigate this discrep- ancy, we found that ST-EVCDP exhibits weaker periodic patterns and higher volatility compared to PEMS08, as detailed in the Ap- pendix, making it inherently more difficult to predict. Moreover, TransLLM is trained with significantly less data than small-scale models,"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 11, "text": "traffic flow prediction datasets such as PEMS08. To investigate this discrep- ancy, we found that ST-EVCDP exhibits weaker periodic patterns and higher volatility compared to PEMS08, as detailed in the Ap- pendix, making it inherently more difficult to predict. Moreover, TransLLM is trained with significantly less data than small-scale models, which makes achieving high accuracy more challenging. In contrast, powerful foundation models such as GPT-4o demonstrate strong performance on ST-EVCDP, likely benefiting from their rich and diverse pretraining data. The effectiveness of TransLLM is further validated on the taxi dispatching task. The quality of dispatch is evaluated using three metrics: MMR, MDD, and W-Dist to the real distribution. Among these, MMR is the most critical metric. Compared to the second-best model, TransLLM improves MMR by 5% and reduces the average travel distance by 0.71 km. TransLLM underperforms GPT-4o and DeepSeek-v3 in terms of MMD and W-Dist. This is because general- ist LLMs tend to adopt a conservative dispatching strategy, keeping vehicles in their original grids, which leads to the lowest average travel distance and the closest match to the real-world distribution. In summary, despite limited training data, TransLLM consis- tently outperforms ten baseline models in spatiotemporal forecast- ing tasks. For the taxi dispatching task, TransLLM adopts a more proactive dispatching strategy that achieves higher order-taking rates with only a slight increase in travel distance, despite deviating from real trajectories. 5.6 Zero-shot Scenarios Performance(RQ2) Zero-shot generalization is critical for foundation models in the transportation domain. It reflects the model’s ability to capture TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Table 1: Performance comparison across model types. Bold: Best, underline: Second best, Double underline: Third best. Traffic Forecasting Charging Demand Prediction Vehicle Dispatching Models LargeST-SD PEMS08 ST-EVCDP UrbanEV Taxi-SH MAE ↓ RMSE ↓ MAE ↓ RMSE ↓ MAE ↓ RMSE ↓ MAE ↓ RMSE ↓ MMR(%) ↑ MDD ↓ W-Dist ↓ Small-scale GNN-based Deep Learning Models STGCN 13.93 26.10 10.30 14.77 2.11 3.64 3.02 5.30 19.10 5.80 1.26 DGCRN 11.46 24.63 10.45 14.47 1.71 4.46 3.04 5.47 18.89 3.60 1.46 ASTGCN 12.34 25.09 10.17 14.48 1.85 3.45 3.32 5.81 19.91 3.96 1.32 D2STGNN 11.72 25.18 8.89 12.72 1.39 2.56 2.65 4.59 18.66 3.50 1.25 GWNET 13.43 26.42 9.26 12.90 1.45 2.65 2.57 4.27 19.29 3.49 1.34 STGODE 11.83 24.63 9.18 13.16 1.50 2.73 2.66 4.53 19.26 3.47 1.28 PDG2seq 12.16 25.02 9.54 13.53 1.46 2.93 3.33 7.38 18.92 3.49 1.21 Generalist LLMs Deepseek-v3 39.25 52.28 23.41 30.26 2.13 4.73 4.60 9.02 19.07 2.57 0.75 GPT-4o 40.54 53.52 23.89 30.89 1.66 3.55 3.53 7.85 18.00 2.40 0.86 Spatio-temporal Enhanced LLMs Urbangpt 11.28 23.17 10.23 13.53 2.09 5.86 1.12 4.38 19.46 3.21 2.31 TransLLM(vicuna) 10.98 21.42 7.88 11.33 1.39 2.73 0.44 3.64 24.46 3.15 1.96 TransLLM(llama3) 9.41 16.78 7.26 10.68 1.26 2.16 0.42 3.42 24.78 3.25 2.02 Table 2: Zero-shot performance on PEMS03 and PEMS04. PEMS03 PEMS04 Models MAE ↓ RMSE ↓ MAE ↓ RMSE ↓ Small-scale GNN-based Deep Learning Models STGCN 57.43 64.74 39.08 49.93 DGCRN 58.05 61.86 29.60 39.30 ASTGCN 80.23"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 12, "text": "3.15 1.96 TransLLM(llama3) 9.41 16.78 7.26 10.68 1.26 2.16 0.42 3.42 24.78 3.25 2.02 Table 2: Zero-shot performance on PEMS03 and PEMS04. PEMS03 PEMS04 Models MAE ↓ RMSE ↓ MAE ↓ RMSE ↓ Small-scale GNN-based Deep Learning Models STGCN 57.43 64.74 39.08 49.93 DGCRN 58.05 61.86 29.60 39.30 ASTGCN 80.23 88.66 45.02 54.65 D2STGNN 63.17 66.65 30.92 39.23 GWNET 55.51 59.21 27.06 33.46 STGODE 73.54 81.98 64.33 77.18 PDG2seq 88.07 101.10 55.88 69.15 Generalist LLMs Deepseek-v3 25.15 38.28 37.79 55.83 GPT-4o 27.06 41.31 40.42 59.47 Spatio-temporal Enhanced LLMs Urbangpt 23.27 37.58 41.52 69.76 TransLLM 18.92 30.27 28.69 47.27 universal spatiotemporal patterns and determines its transferability and practical value across regions and tasks. A model with strong generalization capabilities can adapt to prediction tasks in new cities and unseen scenarios without the need for additional annotations or fine-tuning, thereby significantly reducing deployment costs and enhancing the feasibility of large-scale applications. Specifically, we utilize datasets from different districts within the Los Angeles area: PEMS08 is used for training, while zero-shot evaluation is conducted on PEMS03 and PEMS04, both of which remain unseen during the training phase. We observe that LLM-based methods achieve significantly better improvements over small-scale GNN-based deep learning models on the PEMS03 dataset. In contrast, on the PEMS04 dataset, both categories exhibit similar performance. This discrepancy may be due to PEMS04 having more similar spatiotemporal patterns to the training dataset, PEMS08, than PEMS03. Specifically, the PEMS08 dataset has an average flow of 230.68 and a maximum of 1147.0, while PEMS04 has an average flow of 226.13 and a maximum of 896.0. In comparison, PEMS03 features a lower average flow of 149.52 but a much higher peak of 1852.0. These more similar traffic patterns and the limited training data reduce the relative advantages of LLM-based methods. Nevertheless, TransLLM still achieves the best predictive performance among all LLM-based approaches. On the more divergent PEMS03 dataset, it outperforms UrbanGPT by 18.7% in MAE and 19.5% in RMSE. 5.7 Ablation Study (RQ3) To investigate the impact of individual modules on the overall performance, we evaluated TransLLM variants by removing key components: the ST-Encoder, Prompt Router, and the LLM module. Figure 3 presents the ablation results on the LargeST-SD and ST- EVCDP datasets. Effect of ST-encoder. w/o STE denotes a variant of TransLLM in which the ST-Encoder modules are removed. We observe a no- ticeable decline in the model’s performance, indicating that the rich spatio-temporal features captured by the ST-Encoder effec- tively assist the LLM in understanding and processing complex spatio-temporal patterns. The ST-Encoder serves as a crucial com- ponent that integrates non-textual spatio-temporal information into the LLM’s input space, and its removal results in a substantial degradation in predictive accuracy. Effect of Prompter Router. w/o PR removes the Prompt Router module and instead uses the same fixed prompt for each instance. This variant exhibits a clear performance drop. Although the LLM Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang* RMSE MAE RMSE LargeST-SD LargeST-SD ST-EVCDP ST-EVCDP w/o PR&LLM w/o PR w/o STE TransLLM"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 13, "text": "fixed prompt for each instance. This variant exhibits a clear performance drop. Although the LLM Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin*, Yanyong Zhang, and Chao Wang* RMSE MAE RMSE LargeST-SD LargeST-SD ST-EVCDP ST-EVCDP w/o PR&LLM w/o PR w/o STE TransLLM MAE Figure 3: Ablation study results on LargeST-SD and ST- EVCDP datasets. LargeST-SD MAE RMSE RMSE MAE LargeST-SD ST-EVCDP ST-EVCDP Figure 4: Effect of number of candidate sentences per slot 𝑁𝑐 on LargeST-SD and ST-EVCDP datasets. can still leverage the spatio-temporal features provided by the ST- Encoder, the absence of dynamically generated, instance-specific prompts limits its full potential. This highlights the critical role that prompt quality plays in LLM performance. However, prompt quality is inherently difficult to evaluate manually. The Prompt Router plays a vital role in supplying the LLM with accurate and instance-specific prompts, thereby enhancing its reasoning and prediction capabilities. Effect of Large language model. w/o PR & LLM removes the Prompt Router and the LLM modules, also resulting in a significant performance decline. This underscores the LLM’s ability to model complex spatio-temporal patterns, support advanced reasoning, and enhance generalization. To summary, the results demonstrate that the full model consistently achieves the best performance across all tasks, and the absence of any individual module leads to a degradation in TransLLM’s overall effectiveness. 5.8 Parameter Sensitivity (RQ4) To evaluate the impact of different hyperparameters on TransLLM’s performance, we adjust two key settings: (1) the number of candi- date sentences per slot 𝑁𝑐, and (2) the number of spatio-temporal feature patch tokens ⟨st_patch⟩𝑁𝑝. Number of candidate sentences per slot 𝑁𝑐. Figure 4 shows that increasing the number of sentences does not necessarily lead to continuous performance improvements. The optimal configuration is achieved with four candidate sentences per slot on LargeST-SD and two on ST-EVCDP. While an insufficient number of sentences may limit the diversity available to the Prompt Router, an excessive number can introduce redundancy and increase the learning burden on the LLM, ultimately degrading performance. Number of ⟨st_patch⟩tokens 𝑁𝑝. Figure 5 indicates that model performance generally improves with an increasing number of to- kens. The best results are typically obtained when the token count matches the prediction horizon. This alignment enables comprehen- sive encoding of spatio-temporal dependencies, whereas shorter sequences such as 1 or 3 tokens fail to capture sufficient informa- tion, leading to degraded performance. Notably, on the UrbanEV LargeST-SD MAE RMSE RMSE MAE LargeST-SD UrbanEV UrbanEV Figure 5: Effect of number of ⟨st_patch⟩tokens 𝑁𝑝on LargeST-SD and UrbanEV datasets. Slot 2 Top 2 Top 3 Top 4 Top 1 Slot 1 Slot 3 Slot 4 (a) ST-EVCDP Slot 2 Slot 1 Slot 3 Slot 4 (b) PEMS08 Sentence 1 Sentence 2 Sentence 3 Sentence 4 Figure 6: Sentence selection frequency per slot. The size of the circles represents the selection frequency of each sentence. dataset, both 6 and 12 tokens yield similarly strong results, likely due to its lower temporal resolution, which reduces sensitivity to token sequence length. In summary, the above parameter sensitiv- ity analysis provides important insights for"}
{"doc_id": "2508.14782v1", "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14782v1", "chunk_id": 14, "text": "per slot. The size of the circles represents the selection frequency of each sentence. dataset, both 6 and 12 tokens yield similarly strong results, likely due to its lower temporal resolution, which reduces sensitivity to token sequence length. In summary, the above parameter sensitiv- ity analysis provides important insights for selecting TransLLM’s hyperparameters, ensuring the model’s robustness and optimal performance across different tasks and datasets. 5.9 Prompt Routing Behavior Analysis To further evaluate the effectiveness of the Prompt Router, we visu- alize the selection frequency of each sentence in the prompt pool during testing on the ST-EVCDP and PEMS08 datasets, as shown in Figure 6. It can be observed that the Prompt Router successfully se- lects the most appropriate prompt for each instance. Moreover, the selection frequencies vary across sentences within each slot—some sentences are favored by the majority of instances, while others are rarely chosen. This also reveals the diversity of spatiotemporal pat- terns across instances and the router’s ability to adaptively match prompts based on instance-level characteristics. 6 Conclusion In this work, we present TransLLM, a unified foundation frame- work that seamlessly integrates spatiotemporal modeling with large language models for diverse urban transportation tasks. Our key contributions include the novel framework design that bridges spatiotemporal encoders with LLMs through structured embed- dings, and a two-stage learnable prompt composition mechanism with instance-level routing that dynamically personalizes prompts based on input characteristics. Extensive experiments across seven datasets demonstrate strong performance in both supervised and zero-shot settings, showcasing excellent generalization and cross- task adaptability. TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting Conference acronym ’XX, June 03–05, 2018, Woodstock, NY"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 0, "text": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference Samir Abdaljalil*, Erchin Serpedin*, Khalid Qaraqe†, Hasan Kurban† *Texas A&M University, College Station, TX., USA †Hamad Bin Khalifa University, Doha, Qatar sabdaljalil@tamu.edu, hkurban@hbku.edu.qa Abstract Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise–hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation- induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code can be accessed at: https://github.com/KurbanIntelligenceLab/nli-stress-testing Keywords: Large Language Models (LLMs), Natural Language Inference (NLI), Multilingual Alignment 1. Introduction NLI (Dagan et al., 2005)—determining whether a hypothesis is entailed by, contradicts, or is neu- tral with respect to a premise—is a core bench- mark for natural language understanding (Haval- dar et al., 2025; Yudanto et al., 2024; Mor-Lan and Levi, 2024). Its emphasis on fine-grained seman- tic distinctions has long made it a proxy for test- ing models’ capacity for deep reasoning (Cosma et al., 2024). With LLMs, NLI has become a key tool for assessing generalization, reasoning, and knowledge encoding (Cheng et al., 2025). Yet evaluations remain concentrated on high-resource languages—especially English—and are often em- bedded within downstream tasks such as QA or summarization, limiting insight into whether infer- ence capabilities transfer consistently across lan- guages under controlled semantic conditions. We address this gap with a synthetic multilin- gual NLI framework that stress-tests cross-lingual semantic alignment via deterministic, logic-based templates encoding entailment, contradiction, and neutrality. The approach decouples logical struc- ture from lexical and cultural priors, avoiding anno- tation noise and enabling direct, large-scale eval- uation. Our contributions are: (1) a logic-driven method for generating synthetic multilingual NLI datasets with precise control over inference types and linguistic variation; (2) an automated evalua- tion protocol for measuring cross-lingual consis- tency in LLM semantic judgments; and (3) em- pirical evidence, across multiple models and lan- guages, of systematic weaknesses in multilingual alignment. By disentangling logical reasoning from linguis- tic noise, our framework offers a principled, repro- ducible basis for evaluating semantic alignment in multilingual LLMs. Section 2 reviews related work, Section 3 details the methodology, and Section 4 outlines the experimental setup. Section 5 reports the main findings, followed by qualitative and quan- titative analyses in Section 6. Section 7 concludes with a discussion of limitations and future direc- tions. 2. Related Work Natural Language Inference for Multilingual Evaluation. NLI has become a standard probe for semantic understanding in language models (Nighojkar et al., 2023). By requiring systems to determine whether a hypothesis follows"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 1, "text": "analyses in Section 6. Section 7 concludes with a discussion of limitations and future direc- tions. 2. Related Work Natural Language Inference for Multilingual Evaluation. NLI has become a standard probe for semantic understanding in language models (Nighojkar et al., 2023). By requiring systems to determine whether a hypothesis follows from a premise, it offers a fine-grained test of reasoning, world knowledge, and linguistic nuance. Bench- marks such as GLUE (Wang et al., 2018) and SNLI (Bowman et al., 2015) established its role in English-centric NLP , while XNLI (Conneau et al., 2018) extended evaluation to 15+ languages via professional translation. Owing to its structured and interpretable format, NLI has been widely used for assessing cross-lingual transfer (Heredia et al., 2024; Bandyopadhyay et al., 2022). How- ever, most prior work assumes monolingual eval- uation—premise and hypothesis in the same lan- guage—thus overlooking mixed-lingual scenarios that are common in real multilingual discourse. Cross-Lingual Generalization in Large Lan- guage Models. Multilingual LLMs exhibit strong zero-shot transfer across languages (Conneau et al., 2020; Artetxe et al., 2020), aided by shared tokenization schemes and aligned embedding spaces. Early work with mBERT and XLM-R demonstrated cross-lingual transfer without explicit parallel training, attributed to emergent language alignment (Pires et al., 2019). However, later stud- ies revealed systematic biases: performance fa- vors high-resource languages, while low-resource and morphologically rich languages often show degraded representations (Schuster et al., 2019). Although recent benchmarks broaden multilingual evaluation, they typically assume monolingual in- puts or perfect translation symmetry. Robust- ness in mixed-lingual settings—where premise and hypothesis are in different languages—remains largely untested, despite its relevance for assess- ing sentence-level semantic alignment beyond token overlap. Code-switching, a natural phe- nomenon in multilingual communities, is particu- larly underexplored in LLM reasoning tasks (Khatri et al., 2023). Moreover, most studies use natural text, conflating syntactic variation with semantic difficulty. Our work follows the tradition of NLI as a diag- nostic tool but diverges in three ways: we use fully synthetic, logically controlled data; we evaluate translation consistency alongside reasoning; and we incorporate code-switching to probe multilin- gual alignment under conditions rarely addressed in prior studies. We address this by evaluating on synthetic NLI pairs with controlled logical structure, enabling iso- lation of semantic consistency from linguistic noise. Our framework combines synthetic NLI data, high- quality translation, and controlled code-switching to stress-test multilingual alignment in both mono- lingual and mixed-lingual conditions. This design uncovers unexpected generalization patterns in instruction-tuned LLMs, challenging prevailing as- sumptions about cross-lingual reasoning robust- ness. 3. Methodology This study examines the ability of LLMs to perform logically grounded NLI across languages using a controlled framework based on synthetic data generation and high-quality translation. The frame- work enables systematic evaluation of multilingual semantic alignment under both monolingual and mixed-lingual conditions. Figure 1 illustrates the overall methodology for dataset construction and LLM evaluation. 3.1. Synthetic NLI Construction A synthetic English NLI dataset is constructed from hand-crafted templates encoding three logical re- lations: entailment, contradiction, and neutrality. Each premise–hypothesis pair is derived from ab- stract quantifier-based patterns, with placeholders A, B, and C populated using semantically"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 2, "text": "overall methodology for dataset construction and LLM evaluation. 3.1. Synthetic NLI Construction A synthetic English NLI dataset is constructed from hand-crafted templates encoding three logical re- lations: entailment, contradiction, and neutrality. Each premise–hypothesis pair is derived from ab- stract quantifier-based patterns, with placeholders A, B, and C populated using semantically coherent noun phrases to ensure plausibility. The template- based design affords precise control over compo- sitional structure and minimizes linguistic noise, thereby isolating reasoning ability from lexical vari- ation. Figure 2 presents the templates alongside example instances from the dataset. 3.2. Multilingual Translation To assess inference consistency across languages, the English dataset is automatically translated into a typologically and script-diverse set of target lan- guages using high-performance neural machine translation systems. These translations preserve the original logical relations, enabling cross-lingual evaluation under identical task structures. The selected languages—Arabic (ar), German (de), French (fr), Hindi (hi), and Swahili (sw)—cover both high- and low-resource settings and span multi- ple language families: Afro-Asiatic, Indo-European (Germanic, Romance, Indic branches), and Niger- Congo. Their scripts include Latin, Arabic, and Devanagari, introducing distinct orthographic and tokenization challenges. This selection also varies in morphological complexity, syntactic structure, and resource availability, providing a comprehen- sive basis for evaluating model robustness and cross-lingual generalization. The resulting diver- sity helps surface weaknesses that might remain hidden in homogeneous and high-resource-only evaluations. 3.3. Code-Switching Probes To further stress-test semantic alignment, a code- switching condition is introduced in which the premise and hypothesis are presented in different languages. For each ordered pair of languages L1 and L2, examples are constructed with the premise in L1 and the hypothesis in L2, covering all possi- ble combinations within the selected language set. This setup evaluates whether models can preserve semantic accuracy under mixed-lingual input—a common phenomenon in multilingual communica- tion yet rarely assessed in a controlled, systematic manner. Figure 1: Pipeline for Multilingual NLI Creation and Evaluation: This process involves (1) generating NLI examples using logic-based templates, (2) translating them into multiple languages with high-quality translation, (3) creating dataset variants in monolingual and code-switched formats, (4) evaluating with prompt-based LLM classification, and (5) analyzing multilingual model performance. Entailment “All A are B” ⇒“Some A are B” Contradiction “All A are B” ⇒“No A are B” Neutral “Some A are B” ⇒“Some A are C” Entailment Language: English Premise: All zombies are ani- mals. Hypothesis: Some zombies are animals. Contradiction Language: English Premise: All doctors are ani- mals. Hypothesis: No doctors are animals. Neutral Language: English Premise: All monkeys are or- ganisms. Hypothesis: Some organisms are monkeys. Figure 2: Top row: Synthetic NLI templates encoding entailment, contradiction, and neutrality. Placehold- ers A, B, and C are later instantiated with semantically coherent noun phrases. Bottom row: Samples from the generated NLI dataset for English (en), each showing one of the three relationships: entailment (green), contradiction (red), and neutral (yellow). 3.4. Model Evaluation Model behavior is assessed using a prompt-based classification setup. For each example, the LLM receives a structured prompt of the form: NLI Prompt Example Premise: [premise] Hypothesis: [hypothesis] Question: Is the hypothesis entailed by the premise,"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 3, "text": "of the three relationships: entailment (green), contradiction (red), and neutral (yellow). 3.4. Model Evaluation Model behavior is assessed using a prompt-based classification setup. For each example, the LLM receives a structured prompt of the form: NLI Prompt Example Premise: [premise] Hypothesis: [hypothesis] Question: Is the hypothesis entailed by the premise, contradicted by it, or unrelated? Answer with one of: Entailment, Contradiction, Neutral. Answer: The model outputs one of the three categori- cal labels. Low-temperature decoding is applied to reduce generation variability. Predictions are evaluated against gold-standard labels, and accu- racy is computed across all languages and code- switching configurations. Figure 3: Monolingual NLI accuracy across six languages: English (En), Arabic (Ar), German (De), French (Fr), Hindi (Hi), and Swahili (Sw); and six LLMs: Fanar-9b, Gemma-7b, Llama-3-8b, Mistral- 7b-v0.3, Phi-4, and Qwen3-7b. Each bar represents the accuracy of an LLM when performing natural language inference on examples where both the premise and hypothesis are in the same language. 4. Experiments 4.1. Implementation Details All experiments are executed using the Hugging Face Transformers library with a PyTorch back- end. Inference is performed on A100 GPUs with device_map=\"auto\" enabled for memory- efficient model parallelism. Generation uses greedy decoding with a maximum of 10 new to- kens per prompt to produce concise outputs while limiting hallucinations, with temperature fixed at 1.0. All models are evaluated in a zero-shot setting without task-specific fine-tuning. 4.2. Models Evaluated Six multilingual instruction-tuned LLMs are evalu- ated, selected for diversity in architecture, size, and training data. The set includes Fanar-9B (Team et al., 2025), a multilingual model opti- mized for typologically diverse inputs; Gemma-7B (Team et al., 2024), a decoder-only Transformer re- leased in an instruction-tuned variant; LLaMA-3-8B (Grattafiori et al., 2024), Meta’s third-generation open-weight model pretrained on a multilingual corpus; Mistral-7B-v0.3 (Jiang et al., 2023), a com- pact model with broad multilingual coverage; Phi-4 (Abdin et al., 2024), a small but capable instruction- tuned model with strong zero-shot reasoning for its size; and Qwen3-7B (Yang et al., 2025), a multi- lingual model trained with extensive Chinese and non-English content. All models are evaluated us- ing the same structured prompt format across all examples and languages to ensure comparability. 4.3. Evaluation Scope The evaluation covers 36 language pairings (6×6) with 1,000 examples per pairing, balanced across the three NLI labels: ENTAILMENT, CONTRADIC- TION, and NEUTRAL. Both monolingual and code- switched configurations (Section 3) are included. Performance is reported as classification accuracy, computed by exact string matching between model predictions and gold standard labels. 4.4. Reproducibility All experiments use publicly available model weights and reproducible scripts. The complete setup, including prompt formatting, dataset con- struction, translation, and inference, is imple- mented in Python, enabling straightforward repli- cation and extension to additional languages and models. 5. Results 5.1. Main Results Monolingual inference accuracy is evaluated across six languages: English (en), Arabic (ar), German (de), French (fr), Hindi (hi), and Swahili (sw). In this setting, both the premise and hypothe- sis are in the same language, providing a baseline measure of each model’s semantic reasoning ca- pacity without cross-lingual interference. Results for the six"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 4, "text": "is evaluated across six languages: English (en), Arabic (ar), German (de), French (fr), Hindi (hi), and Swahili (sw). In this setting, both the premise and hypothe- sis are in the same language, providing a baseline measure of each model’s semantic reasoning ca- pacity without cross-lingual interference. Results for the six evaluated LLMs are shown in Figure 3. Overall Trends. Fanar-9B attains the highest ac- curacy across all languages, reaching 65.1% in En- glish and sustaining strong performance in lower- resource languages such as Swahili and Hindi. These results indicate a well-calibrated multilin- gual representation space and effective alignment of logical reasoning across typologically diverse inputs. In contrast, Gemma-7B records the lowest accuracy in nearly all languages, including 17.0% in English and 14.3% in German. The performance gap between Fanar-9B and Gemma-7B exceeds 40 percentage points in English, underscoring sub- stantial differences in multilingual reasoning quality across model families. Language-Specific Patterns. Across models, English generally achieves the highest monolin- gual accuracy, followed by French and German, though the magnitude of differences varies. For instance, Phi-4 performs similarly in English (43%) and German (41%), while LLaMA-3-8B shows min- imal variance across languages, with scores clus- tered near 30%. These patterns indicate that some models maintain balanced multilingual represen- tations, whereas others exhibit pronounced bias toward high-resource and pretraining-dominant languages. Notably, Swahili, despite its lower- resource status, does not consistently underper- form. In models such as Fanar-9B and Gemma-7B, Swahili accuracy is comparable to that of Indo- European languages. This outcome may reflect expanded low-resource language coverage in re- cent pretraining pipelines and the influence of high- quality translation data during instruction tuning. Implications. The results reveal substantial variation in monolingual reasoning performance across languages and model architectures. While larger or more extensively instruction-tuned mod- els often achieve higher accuracy, model size alone is not a reliable predictor; for example, LLaMA-3-8B underperforms relative to the smaller Phi-4. These patterns underscore the need to ex- amine how training data composition, multilingual coverage, and architectural biases shape cross- lingual logical generalization, particularly for non- English and lower-resource languages. 5.2. Code-switching The robustness of six LLMs is evaluated under code-switching conditions, in which the premise and hypothesis are presented in different lan- guages. Table 1 reports accuracy across all lan- guage pairs for each model, with off-diagonal cells representing bilingual inference. This configuration probes the ability to maintain logical consistency under mismatched linguistic inputs, a critical as- pect of multilingual generalization. Surprising Gains from Code-Switching. Sev- eral models outperform their monolingual base- lines in specific code-switched configurations. For example, Gemma-7B achieves markedly higher accuracy on many bilingual pairs than on En- glish–English (e.g., En–Hi: 32.9% vs. En–En: 17.0%), and Mistral-7B-v0.3 performs better on some cross-lingual inputs (e.g., Ar–En: 36.4%) than on the corresponding monolingual cases (e.g., Ar–Ar: 28.2%). These patterns challenge the assumption that semantic alignment necessarily degrades when models reason across linguistic boundaries. Model-Specific Behaviors. Fanar-9B achieves the highest accuracy in both monolingual and cross-lingual settings, indicating robust multilingual alignment. In contrast, models such as Gemma-7B and Qwen3-7B display pronounced asymmetries: despite weak English monolingual performance, accuracy"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 5, "text": "These patterns challenge the assumption that semantic alignment necessarily degrades when models reason across linguistic boundaries. Model-Specific Behaviors. Fanar-9B achieves the highest accuracy in both monolingual and cross-lingual settings, indicating robust multilingual alignment. In contrast, models such as Gemma-7B and Qwen3-7B display pronounced asymmetries: despite weak English monolingual performance, accuracy improves when the hypothesis is ren- dered in a non-English language. This pattern suggests a disproportionate reliance on hypothe- sis surface forms, with syntactic or lexical ambigu- ity in English degrading performance more than structured translations. Language-Dependent Patterns. Accuracy gains from code-switching are unevenly distributed across languages. In several models, using Hindi, Swahili, or Arabic as the hypothesis language yields higher performance than English, suggest- ing potential advantages from morphologically richer or syntactically simpler constructions in those translations. This pattern is consistent with prior findings that neural models may overfit statistical artifacts in high-resource languages, while benefiting from more literal or constrained translations in low-resource settings (Cohen-Inger et al., 2025). Fanar-9b Prem. En Ar De Fr Hi Sw En – 51.7 52.4 59.4 44.6 43.2 Ar 57.8 – 61.4 64.3 47.2 50.2 De 53.8 49.1 – 67.0 47.7 46.3 Fr 60.6 51.7 63.5 – 44.7 44.5 Hi 47.4 48.5 43.6 48.3 – 44.6 Sw 48.8 51.6 41.3 54.1 45.6 – Gemma-7b Prem. En Ar De Fr Hi Sw En – 29.4 26.2 24.8 32.9 26.3 Ar 40.8 – 39.3 38.8 37.4 26.9 De 30.1 36.6 – 29.7 36.7 24.9 Fr 33.5 40.7 34.7 – 40.1 25.2 Hi 42.6 42.2 43.2 39.4 – 25.4 Sw 40.0 33.6 35.9 41.7 28.4 – Llama-3-8b Prem. En Ar De Fr Hi Sw En – 30.0 32.0 31.0 29.0 30.0 Ar 30.8 – 31.2 30.5 30.6 29.5 De 31.7 30.2 – 29.4 30.0 29.6 Fr 30.6 30.9 30.5 – 30.3 31.1 Hi 31.1 31.2 31.4 32.7 – 30.9 Sw 32.1 32.1 31.0 31.1 31.5 – Mistral-7b-v0.3 Prem. En Ar De Fr Hi Sw En – 37.0 37.7 31.0 32.4 35.4 Ar 36.4 – 35.7 34.8 33.5 34.5 De 42.2 37.6 – 36.4 35.1 36.1 Fr 34.5 36.4 32.9 – 34.7 34.0 Hi 32.9 34.5 34.9 29.5 – 35.0 Sw 32.5 32.1 32.4 30.3 31.9 – Phi-4 Prem. En Ar De Fr Hi Sw En – 38.5 38.4 42.2 36.6 32.6 Ar 37.3 – 38.3 39.4 38.1 35.1 De 38.2 38.3 – 40.9 38.5 32.2 Fr 38.0 38.3 38.6 – 35.9 32.8 Hi 33.3 36.8 36.7 37.1 – 33.4 Sw 35.4 36.4 34.3 35.8 33.4 – Qwen3-7b Prem. En Ar De Fr Hi Sw En – 32.5 31.7 32.0 29.3 30.4 Ar 32.6 – 31.7 31.4 30.6 31.2 De 34.3 31.8 – 30.8 30.0 30.1 Fr 32.8 31.1 31.7 – 30.1 31.6 Hi 30.4 31.7 30.2 32.3 – 31.0 Sw 32.8 32.5 32.6 31.5 30.9 – Lower Accuracy (%) Higher <30 30–35 35–40 40–45 45–50 50–55 55–60 ≥60 Table 1: Pairwise cross-lingual natural language inference accuracies (%) for six language pairs (En- glish—En, Arabic—Ar, German—De, French—Fr, Hindi—Hi, Swahili—Sw) across six language models. Each card presents the premise language (rows) versus the hypothesis language (columns). Diagonal cells (–) indicate"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 6, "text": "(%) Higher <30 30–35 35–40 40–45 45–50 50–55 55–60 ≥60 Table 1: Pairwise cross-lingual natural language inference accuracies (%) for six language pairs (En- glish—En, Arabic—Ar, German—De, French—Fr, Hindi—Hi, Swahili—Sw) across six language models. Each card presents the premise language (rows) versus the hypothesis language (columns). Diagonal cells (–) indicate monolingual settings and are shaded grey, while off-diagonal cells show cross-lingual performance. Cell colors range from light yellow (low accuracy) to dark blue (high accuracy), following the ColorBrewer YlGnBu sequential scale (legend above). Implications and Hypotheses. The findings raise questions about the mechanisms underly- ing cross-lingual alignment in instruction-tuned language models. In multiple cases, accuracy is higher under code-switched conditions than in monolingual settings. Possible explanations in- clude translation-induced lexical or syntactic vari- ation acting as a regularization signal, improved alignment within the multilingual representation space, or simplification effects from translation. The recurrence of this pattern across diverse ar- chitectures indicates that code-switching may offer untapped potential for improving reasoning perfor- mance in multilingual applications. 6. Cross-Lingual Analysis This section evaluates the semantic consistency of translated data and examines the representational alignment of multilingual sentences. Geometric properties of sentence embeddings are visualized across languages, and translation quality is quan- tified via embedding-based similarity. Given that the evaluation relies on translated versions of syn- thetic English inputs, verifying the preservation of semantic content across languages is essential. 6.1. Embedding Similarity Across Translations Semantic preservation across translations is ex- amined by visualizing sentence embeddings for five randomly selected English premise statements and their translations into six languages. Sen- tences are encoded with LaBSE (Feng et al., 2022) into high-dimensional vectors, then projected into three dimensions using UMAP for interpretability. Cross-Lingual Cohesion. Fig. 4 shows that translations of the same sentence form tight clus- ters, even across typologically distant languages. This indicates high semantic consistency and sug- gests that the encoder maps them to similar rep- resentations despite variation in word order, mor- phology, or script. For instance, translations of Sentence 1 (green) remain closely grouped across all languages, supporting the preservation of in- tended meaning. Language Variation. Although clusters are gen- erally compact, certain languages display mild drift from sentence centroids. For instance, Swahili (brown in Fig.4) shows positional devia- tions, likely arising from structural or morphological mismatches introduced during translation. Such patterns align with prior observations on typolog- ical variation in multilingual embedding spaces (Chen et al., 2025) and illustrate the challenge of Figure 4: 3D UMAP projection of sentence em- beddings across six languages. Each point repre- sents a translation of one of five randomly selected NLI premise statements. Colors denote sentence identity; marker shapes indicate language (EN = English, FR = French, DE = German, AR = Arabic, HI = Hindi, SW = Swahili). aligning structurally divergent languages in a uni- fied vector space. Given that the evaluation task relies on detecting fine-grained logical relations, poor or inconsistent translations could distort re- sults. The observed cohesion across translations mitigates this concern: if translations of the same sentence consistently occupy similar embedding positions, cross-lingual performance differences are more likely to stem from"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 7, "text": "fied vector space. Given that the evaluation task relies on detecting fine-grained logical relations, poor or inconsistent translations could distort re- sults. The observed cohesion across translations mitigates this concern: if translations of the same sentence consistently occupy similar embedding positions, cross-lingual performance differences are more likely to stem from genuine reasoning challenges rather than input noise. 6.2. Translation Quality Assessment Semantic consistency of translations is assessed by computing cosine similarity scores between each English sentence and its translated counter- part using the LaBSE encoder, providing a direct, language-agnostic measure of semantic proximity. As shown in Table 2, similarity scores are consis- tently high across all languages, with French and German exhibiting the strongest alignment. Even lower-resource languages such as Swahili main- tain average cosine similarities above 0.8, indicat- ing that semantic properties are largely preserved. These results suggest that differences in inference accuracy are more likely to reflect model behavior than translation noise. Overall, the analyses con- firm that the multilingual dataset preserves logical structure and meaning across languages, estab- lishing a reliable basis for cross-lingual inference evaluation. Language Code Avg. Cosine Similarity French fr 0.912 German de 0.895 Swahili sw 0.841 Hindi hi 0.828 Arabic ar 0.811 Table 2: Semantic similarity between English premises and their translations using LaBSE em- beddings (average over 100 pairs). Darker blue indicates higher similarity. 7. Conclusion This study provides a controlled evaluation of mul- tilingual semantic alignment in instruction-tuned LLMs through a synthetic, logic-based NLI frame- work incorporating high-quality translation and code-switching. The design isolates reasoning ca- pabilities across languages and scripts while mini- mizing confounding linguistic noise. Results show that, contrary to common assumptions, reasoning performance in code-switched settings can match or exceed monolingual performance, suggesting greater robustness in cross-lingual representations than previously recognized. Translation effects may in some cases aid inference, and embedding analyses reveal strong interlingual clustering of se- mantically equivalent sentences, supporting the feasibility of multilingual generalization. The frame- work enables fine-grained probing of cross-lingual logic, identification of language-specific artifacts, and exploration of code-switching as a deliberate strategy in multilingual NLP . These findings high- light both the challenges and the opportunities for advancing reasoning-oriented multilingual evalua- tion. 8. Limitations Synthetic Nature of the Dataset. The use of synthetic NLI examples enables precise control over logical form and compositional structure but may limit ecological validity. The templates, while semantically well-formed, cannot fully capture the diversity and ambiguity of natural multilingual dis- course. Consequently, performance on these tasks may not directly translate to real-world rea- soning ability. Future work could mitigate this limitation by supplementing template-based data with linguistically diverse or naturally occurring sen- tences, curated and verified across languages to preserve logical consistency. Reliance on Machine Translation. The eval- uation of cross-lingual alignment assumes that machine translation preserves the intended se- mantics of the original English examples. Neural translation systems—particularly for low-resource languages—can introduce meaning shifts, simpli- fications, or structural divergences that alter the logical relationship between premise and hypoth- esis. Although state-of-the-art translation models were used and their quality assessed (Section 6), residual errors may still influence downstream rea-"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 8, "text": "se- mantics of the original English examples. Neural translation systems—particularly for low-resource languages—can introduce meaning shifts, simpli- fications, or structural divergences that alter the logical relationship between premise and hypoth- esis. Although state-of-the-art translation models were used and their quality assessed (Section 6), residual errors may still influence downstream rea- soning. Future extensions could incorporate hu- man verification of a subset of translations or em- ploy multilingual LLMs to produce language-native examples directly, avoiding translation as an inter- mediate step. 9. Bibliographical References Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Java- heripi, Piero Kauffmann, et al. 2024. Phi-4 tech- nical report. Rie Kubota Ando and Tong Zhang. 2005. A frame- work for learning predictive structures from mul- tiple tasks and unlabeled data. Journal of Ma- chine Learning Research, 6:1817–1853. Galen Andrew and Jianfeng Gao. 2007. Scalable training of L1-regularized log-linear models. In Proceedings of the 24th International Confer- ence on Machine Learning, pages 33–40. Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2020. Translation artifacts in cross-lingual trans- fer learning. In Proceedings of the 2020 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), pages 7674–7684, Online. Association for Computational Linguistics. Dibyanayan Bandyopadhyay, Arkadipta De, Ba- ban Gain, Tanik Saikh, and Asif Ekbal. 2022. A deep transfer learning method for cross-lingual natural language inference. In Proceedings of the Thirteenth Language Resources and Evalu- ation Conference, pages 3084–3092, Marseille, France. European Language Resources Associ- ation. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural lan- guage inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal. Association for Computational Linguis- tics. Yiyi Chen, Qiongxiu Li, Russa Biswas, and Jo- hannes Bjerva. 2025. Large language models are easily confused: A quantitative metric, se- curity implications and typological analysis. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 3810–3827, Albuquerque, New Mexico. Association for Com- putational Linguistics. Liang Cheng, Tianyi Li, Zhaowei Wang, Tianyang Liu, and Mark Steedman. 2025. Neutralizing bias in LLM reasoning using entailment graphs. In Findings of the Association for Computational Linguistics: ACL 2025, pages 13714–13730, Vi- enna, Austria. Association for Computational Lin- guistics. Nurit Cohen-Inger, Yehonatan Elisha, Bracha Shapira, Lior Rokach, and Seffi Cohen. 2025. Forget what you know about llms evaluations – llms are like a chameleon. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learn- ing at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online. Associa- tion for Computational Linguistics. Alexis Conneau, Ruty Rinott, Guillaume Lam- ple, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representa- tions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro- cessing, pages 2475–2485, Brussels, Belgium. Association for Computational Linguistics. Adrian Cosma, Stefan Ruseti, Mihai Dascalu, and Cornelia Caragea. 2024. How hard is this test set? NLI"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 9, "text": "Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representa- tions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro- cessing, pages 2475–2485, Brussels, Belgium. Association for Computational Linguistics. Adrian Cosma, Stefan Ruseti, Mihai Dascalu, and Cornelia Caragea. 2024. How hard is this test set? NLI characterization by exploiting training dynamics. In Proceedings of the 2024 Confer- ence on Empirical Methods in Natural Language Processing, pages 2990–3001, Miami, Florida, USA. Association for Computational Linguistics. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. MLCW’05, page 177–190, Berlin, Heidelberg. Springer-Verlag. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language- agnostic BERT sentence embedding. In Pro- ceedings of the 60th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 878–891, Dublin, Ire- land. Association for Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ah- mad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. Dan Gusfield. 1997. Algorithms on Strings, Trees and Sequences. Cambridge University Press, Cambridge, UK. Shreya Havaldar, Hamidreza Alvari, John Palow- itch, Mohammad Javad Hosseini, Senaka Buth- pitiya, and Alex Fabrikant. 2025. Entailed be- tween the lines: Incorporating implication into nli. Maite Heredia, Julen Etxaniz, Muitze Zulaika, Xa- bier Saralegi, Jeremy Barnes, and Aitor Soroa. 2024. XNLIeu: a dataset for cross-lingual NLI in Basque. In Proceedings of the 2024 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies (Volume 1: Long Papers), pages 4177–4188, Mexico City, Mexico. Association for Computational Linguistics. Hai Hu, Qi Chen, Kyle Richardson, Atreyee Mukherjee, Lawrence S. Moss, and Sandra Kue- bler. 2020. MonaLog: a lightweight system for natural language inference based on monotonic- ity. In Proceedings of the Society for Computa- tion in Linguistics 2020, pages 334–344, New York, New York. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap- lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. Jyotsana Khatri, Vivek Srivastava, and Lovekesh Vig. 2023. Can you translate for me? code- switched machine translation with large lan- guage models. In Proceedings of the 13th Inter- national Joint Conference on Natural Language Processing and the 3rd Conference of the Asia- Pacific Chapter of the Association for Compu- tational Linguistics (Volume 2: Short Papers), pages 83–92, Nusa Dua, Bali. Association for Computational Linguistics. Guy Mor-Lan and Effi Levi. 2024. Exploring fac- tual entailment with NLI: A news media study. In Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (*SEM 2024), pages 190–199, Mexico City, Mexico. As- sociation for Computational Linguistics. Animesh Nighojkar, Antonio Laverghetta Jr., and John Licato. 2023. No strong feelings one way or another: Re-operationalizing neutrality in nat- ural language inference. In Proceedings of the 17th Linguistic Annotation Workshop (LAW- XVII), pages 199–210, Toronto, Canada. Associ- ation for Computational Linguistics. Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of"}
{"doc_id": "2508.14735v1", "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14735v1", "chunk_id": 10, "text": "2023. No strong feelings one way or another: Re-operationalizing neutrality in nat- ural language inference. In Proceedings of the 17th Linguistic Annotation Workshop (LAW- XVII), pages 199–210, Toronto, Canada. Associ- ation for Computational Linguistics. Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996–5001, Florence, Italy. Association for Computational Linguistics. Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015. Yara parser: A fast and accurate depen- dency parser. Computing Research Repository, Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. 2019. Cross-lingual trans- fer learning for multilingual task oriented dia- log. In Proceedings of the 2019 Conference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Papers), pages 3795–3805, Minneapolis, Min- nesota. Association for Computational Linguis- tics. Fanar Team, Ummar Abbas, Mohammad Shah- meer Ahmad, Firoj Alam, Enes Altinisik, Ehsannedin Asgari, Yazan Boshmaf, Sabri Boughorbel, Sanjay Chawla, Shammur Chowd- hury, et al. 2025. Fanar: An arabic-centric multi- modal generative ai platform. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mi- hir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini re- search and technology. Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neu- ral Networks for NLP, pages 353–355, Brussels, Belgium. Association for Computational Linguis- tics. Adina Williams, Nikita Nangia, and Samuel Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana. Associa- tion for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. Faturahman Yudanto, Yunita Sari, and Maeve Za- hwa Adriana Crown Zaki. 2024. Climate-NLI: A model for natural language inference and zero- shot classification on climate-related text. In Pro- ceedings of the 38th Pacific Asia Conference on Language, Information and Computation, pages 600–608, Tokyo, Japan. Tokyo University of For- eign Studies."}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 0, "text": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation Guangzhan Wang1, Hongyu Zhang2, Beijun Shen1, Xiaodong Gu1† 1Shanghai Jiao Tong University 2Chongqing University {wangguangzhan, bjshen, xiaodong.gu}@sjtu.edu.cn hyzhang@cqu.edu.cn Abstract Data augmentation is a critical technique in deep learning. Traditional methods like Back- translation typically focus on lexical-level rephrasing, which primarily produces varia- tions with the same semantics. While large language models (LLMs) have enhanced text augmentation by their “knowledge emergence” capability, controlling the style and structure of these outputs remains challenging and re- quires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then- regenerate: incorporating seed text into a con- text expanded by LLM, and asking the LLM to regenerate a variant based on the expanded con- text. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text aug- mentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows. 1 Introduction Data augmentation is a critical technique in deep learning (Khosla and Saini, 2020; Shorten et al., 2021; Ding et al., 2022). Deep learning models are data-hungry and often suffer from limited datasets. Data augmentation generates additional training samples through transforming or rephrasing the ex- isting dataset. This process increases data diversity, reduces the risk of overfitting, and enhances the models’ generalization ability. Data augmentation has been extensively studied in NLP tasks (Feng et al., 2021; Pellicer et al., 2023; Chen et al., 2023; Bayer et al., 2022). One simple prior approach (Wei and Zou, 2019) enhances text data through word-level transformations, such as L2R Completion Back- infilling After hearing so much hype about it, I finally sat down to watch the movie. The movie exceeded all my expectations and left me speechless. After hearing so much hype about it, I finally sat down to watch the movie. The movie exceeded all my expectations and left me speechless. Regenerate The film took me on an exhilarating journey that I never wanted to end. ? R2L Completion What a concept, what an idea, what a thrill ride. Preceding context Subsequent context Preceding context Subsequent context Original Text Variant Text Transplant What a concept, what an idea, what a thrill ride. Figure 1: Illustration of LMTransplant. random insertion and deletion. While easy to im- plement, it often generates low-quality samples that severely disrupt the semantic coherence of the generated text. Later, Back-translation (Sennrich et al., 2016) has been commonly used for sentence level rephrasing. Specifically, a translation model first translates the original text into a different lan- guage and then translates it back into the original language. Although the augmented text is seman- tically coherent, it often exhibits high similarity to the original text, leading to poor data diversity (Pellicer et al., 2023; Edunov et al., 2018). Recent advancements in LLMs have spurred significant interest in LLM-based data augmenta- tion methods (Whitehouse"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 1, "text": "back into the original language. Although the augmented text is seman- tically coherent, it often exhibits high similarity to the original text, leading to poor data diversity (Pellicer et al., 2023; Edunov et al., 2018). Recent advancements in LLMs have spurred significant interest in LLM-based data augmenta- tion methods (Whitehouse et al., 2023; Ding et al., 2024; Zhou et al., 2024; Qiao et al., 2024). A key characteristic of LLMs is their “knowledge emer- gence” capability, which stems from two factors: (1) the extensive prior knowledge acquired during pre-training, and (2) their robust language under- standing and instruction-following abilities (Evuru et al., 2024; Ghosh et al., 2024). These strengths allow LLMs to generate desired outputs directly through demonstrations or natural language instruc- tions without requiring additional training. A no- table example is AugGPT (Dai et al., 2025), which instructs ChatGPT to rephrase text for improv- ing text classification performance. However, this rephrasing-based method only generates variants with similar semantics and underutilizes the rich knowledge embedded in LLMs, limiting the diver- sity and creativity of the generated data. As a result, when augmented data volume reaches a certain threshold, further increasing may not yield perfor- mance improvements (Zhou et al., 2024). Although contextual augmentation methods can enhance con- tent diversity, such as GPT3Mix (Yoo et al., 2021), which leverages the powerful few-shot learning ca- pabilities of LLMs to generate mixed augmented text, it is highly sensitive to example quality and input order, potentially introducing uncontrolled biases that are not aligned with the original data distribution. To address these limitations and harness the full potential of LLMs, we propose LMTransplant, a novel text data augmentation paradigm. The core idea of LMTransplant is transplant-then-regenerate (TTR), namely, embedding the original text into an expanded context generated by LLMs and then instructing LLMs to regenerate a variant based on this enriched context. Specifically, LMTransplant employs bidirectional text continuation—backward (right-to-left) and forward (left-to-right)—to create the preceding and subsequent context of the orig- inal text. This original text is then masked within its expanded context, and LLMs are prompted to generate a replacement that introduces novel con- tent diversity beyond rephrasing, while still pre- serving core attributes of the original text. There- fore, LMTransplant enhances both the diversity and creativity of the generated text, while maintaining alignment with the original data distribution. We apply LMTransplant to various deep learning tasks, including text classification, question answer- ing and named entity recognition (NER), and com- pare its performance with existing data augmen- tation methods. Experimental results demonstrate that LMTransplant can generate higher-quality aug- mented data. Training models with data augmented by our approach yields significant performance im- provements across all tasks. Compared to non- augmentation, LMTransplant achieves accuracy gains of 28.16%, 19.96%, 7.68%, 23.66%, and 10.25% on the SST-2, TREC, SNIPS, MLQA, and CoNLL-2003 datasets, respectively. In summary, our contributions are as follows: • We propose a novel transplant-based paradigm for text data augmentation. Unlike existing methods which primarily focus on rephras- ing, LMTransplant crafts content-level text variants, thereby crafting higher-quality aug- mented texts. • We present a novel transplant and"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 2, "text": "MLQA, and CoNLL-2003 datasets, respectively. In summary, our contributions are as follows: • We propose a novel transplant-based paradigm for text data augmentation. Unlike existing methods which primarily focus on rephras- ing, LMTransplant crafts content-level text variants, thereby crafting higher-quality aug- mented texts. • We present a novel transplant and regenera- tion algorithm using bidirectional text contin- uation and masked text prediction. The al- gorithm allows for generating core attributes similar yet more diverse and creative text by effectively utilizing knowledge embedded in LLMs. Experiments demonstrate that, LM- Transplant achieves significant performance improvements across different tasks. 2 Approach 2.1 Overview We propose LMTransplant, a novel data augmen- tation paradigm. The core idea of LMTransplant is transplant-then-regenerate: integrating the orig- inal text into contextual scenarios generated by LLMs, and then asking LLMs to regenerate new variants given expanded contexts. This strategy allows the model to create content-level variants while preserving the core attributes of the original text. Figure 1 illustrates the entire augmentation process. For a given text, LMTransplant uses bidi- rectional text continuation to generate its preced- ing and subsequent contexts (Section 2.2). Subse- quently, LMTransplant masks the original text in the transplanted text and asks the LLM to regen- erate the missing parts given the crafted contexts, thereby producing new variants of the original text (Section 2.3). Each step of this process will be elaborated in the following sections. 2.2 Transplant Given a seed text, we incorporate it into a relevant contextual scenario. Specifically, we treat the seed text as a fragment of a broader contextual passage, and then use an LLM to generate a semantically natural and logically coherent surrounding context. This process can be conceptualized as bidirectional text continuation, which involves two steps: (1) a forward (left-to-right) continuation process that continues writing the subsequent context of the seed text, and (2) a backward (right-to-left) con- tinuation process that reconstructs the preceding context of the seed text. What a concept, what an idea, what a thrill ride. Seed text Transplanted Text Subsequent context Preceding context After hearing so much hype about it, I finally sat down to watch the movie. What a concept, what an idea, what a thrill ride. The movie exceeded all my expectations and left me speechless. What a concept, what an idea, what a thrill ride. The movie exceeded all my expectations and left me speechless. LLM After hearing so much hype about it, I finally sat down to watch the movie. LLM Figure 2: Illustration of text transplant. Figure 2 illustrates the bidirectional text contin- uation process. First, the LLM generates a subse- quent text that naturally extends the seed text, com- bining both to form an expanded passage. Next, the LLM generates a preceding text based on this expanded input. This process ensures that the gen- erated context remains thematically aligned with the original text, while introducing novel informa- tion other than rephrasing through the knowledge embedded in LLMs, such as new words and ex- pressions. Leveraging the language understanding and instruction-following capabilities of LLMs, we employ prompt engineering to guide content gener- ation."}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 3, "text": "the gen- erated context remains thematically aligned with the original text, while introducing novel informa- tion other than rephrasing through the knowledge embedded in LLMs, such as new words and ex- pressions. Leveraging the language understanding and instruction-following capabilities of LLMs, we employ prompt engineering to guide content gener- ation. The prompt template used for transplanting is provided in Appendix A. 2.3 Regeneration The transplant phase generates multiple contextual scenes bearing the original text. We then introduce a regeneration process, where we prompt the LLM to regenerate new text variants that seamlessly in- tegrate into the expanded context. Specifically, we provide the LLM with the crafted preceding and subsequent contexts, along with the original text, and ask it to generate a text that introduces content variation while preserving essential attributes of the original. Therefore, the regenerated text must satisfy the following criteria: (1) Fitting naturally within the surrounding context; (2) Aligning with the original text in terms of theme, length, format, and linguistic style, as mismatches in these aspects between training and testing instances are known to degrade downstream performance (Rogers et al., 2021); (3) Introducing novel elements to enrich content variation, avoiding simple rewording or direct replication. The film took me on an exhilarating journey that I never wanted to end. Regenerated Text Input Mask After hearing so much hype about it, I finally sat down to watch the movie. What a concept, what an idea, what a thrill ride. The movie exceeded all my expectations and left me speechless. <Masked Text> The movie exceeded all my expectations and left me speechless. Transplanted Text LLM After hearing so much hype about it, I finally sat down to watch the movie. Figure 3: Illustration of regeneration. In this step, the preceding and subsequent texts generated in Section 2.2 serve as a bridge, ensuring the newly generated text aligns with the original, such as theme and label-related information. Simul- taneously, this process enriches and diversifies the content, making the regenerated text a high-quality augmentation of the original. The prompt design for this step is detailed in Appendix A. 3 Experiments We conduct extensive experiments to evaluate the effectiveness of LMTransplant across multiple deep learning tasks by applying data augmentation to various datasets. Following established practices (Yoo et al., 2021; Dai et al., 2025; Ubani et al., 2023; Lee et al., 2024), we simulate low-resource scenarios by subsampling the training set of each dataset. Specifically, we select a subset of samples as seed data for augmentation, and then generate three augmented samples for each seed. This en- ables us to rigorously assess the performance of our method in data-scarce scenarios. 3.1 Implementation Details To evaluate LMTransplant’s robustness and effec- tiveness across a wider range of LLMs, we employ different LLMs as the base LLM for both transplant and regeneration phases, including DeepSeek-V3, GPT-3.5-Turbo and GPT-4o. For text classification tasks, we use Modern- BERT (Warner et al., 2024), a modernized bidirec- tional encoder-only BERT-style model available in two sizes: ModernBERT-base and ModernBERT- large. The classifier is initialized using pre-trained models from the Huggingface Transformers library"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 4, "text": "for both transplant and regeneration phases, including DeepSeek-V3, GPT-3.5-Turbo and GPT-4o. For text classification tasks, we use Modern- BERT (Warner et al., 2024), a modernized bidirec- tional encoder-only BERT-style model available in two sizes: ModernBERT-base and ModernBERT- large. The classifier is initialized using pre-trained models from the Huggingface Transformers library (Wolf et al., 2019) and optimized with the AdamW optimizer (Kingma and Ba, 2014; Loshchilov and Hutter, 2019). We set the learning rate to 4e-5 and maintain other hyperparameters consistent with Task Type Dataset Classes Train Dev Test Classification SST-2 2 6228 692 1821 TREC 6 5406 546 500 SNIPS 7 13084 700 700 QA MLQA - 1314 437 437 NER CoNLL-2003 9 3234 748 679 Table 1: Statistics of datasets. Fadaee et al. (2017), including 8 training epochs and a batch size of 8. During training, models are saved based on their performance on the devel- opment set, with the best-performing parameters retained for final evaluation on the test set. For the question-answering task, we fine-tune Qwen2.5-1.5B (Team, 2024; Yang et al., 2024) us- ing the AdamW optimizer with a learning rate of 1e-5, a batch size of 8, and 8 training epochs. And for NER task, we fine-tune ModernBERT using the same hyperparameters as in the classification task. We repeat all experiments 10 times to mitigate the influence of randomness. Additionally, we con- duct pairwise two-sample Wilcoxon Signed-Rank tests (Wilcoxon, 1992) to compare group medians and assess statistical significant differences. When p < 0.01, it indicates that there is a statistical sig- nificant difference between the two groups. 3.2 Datasets We evaluate LMTransplant on five established benchmarks: • SST-2 (Socher et al., 2013): A widely used sentiment classification dataset of movie re- views, labeled as “positive” or “negative”. We use the version provided by Wu et al. (2019)1, which contains 6,228 training samples, 692 development samples, and 1,821 test samples. • TREC (Li and Roth, 2002): A question clas- sification dataset annotated with six types: “Abbreviation”, “Entity”, “Description”, “Hu- man”, “Location”, and “Numeric”. Similar to SST-2, we use the version from Wu et al. (2019), with 5,406 training samples, 546 de- velopment samples, and 500 test samples. • SNIPS (Coucke et al., 2018): A text clas- sification dataset annotated with seven hu- man intents: “AddToPlaylist”, “BookRestau- rant”, “GetWeather”, “PlayMusic”, “Rate- Book”, “SearchCreativeWork”, and “Search- ScreeningEvent”. We use the version from 1https://github.com/1024er/cbert_aug SlotGated-SLU (Goo et al., 2018)2, compris- ing 13,084 training samples, 700 development samples, and 700 test samples. • MLQA (Lewis et al., 2019): A question- answering benchmark with context passages, questions, and answers. We use English sam- ples form Huggingface3 and filter out those exceeding 80 tokens to keep a modest length. • CoNLL-2003 (Tjong Kim Sang and De Meul- der, 2003): A NER dataset contains four entity types: persons, organizations, locations, and miscellaneous names, tagged using the IOB scheme, resulting in nine distinct IOB labels. Statistics of datasets are summarized in Table 1. 3.3 Metrics We evaluate LMTransplant along two dimensions: the quality of the augmented texts (intrinsic eval- uation) and their impact on deep learning tasks (extrinsic"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 5, "text": "persons, organizations, locations, and miscellaneous names, tagged using the IOB scheme, resulting in nine distinct IOB labels. Statistics of datasets are summarized in Table 1. 3.3 Metrics We evaluate LMTransplant along two dimensions: the quality of the augmented texts (intrinsic eval- uation) and their impact on deep learning tasks (extrinsic evaluation). For intrinsic evaluation, we adopt two widely employed metrics to assess the quality of the augmented samples: Distinct-N measures the lexical diversity (Li et al., 2016). It is defined as the ratio of unique n-grams across all generated texts and their corre- sponding seeds. Distinct-N = # unique n-grams # all n-grams (1) A higher value indicates greater diversity. We cal- culate the average score across 10 experiments as the final result. Semantic Variability measures how well the generated text extends the semantics of the seed texts. To assess this, we first adopt BERTScore (Zhang* et al., 2020) to calculate the sentence-level similarity between the generated text and its orig- inal, leveraging BERT’s contextual embeddings. We then define semantic variability as: Semantic Variability = 1 −BERTScore (2) Thus, the higher the semantic variability, the better the variability that the new text variants are. For extrinsic evaluation, we adopt task-related metrics. Specifically, we use accuracy (Acc) and macro F1-score (Macro-F1) for classification and 2https://github.com/MiuLab/SlotGated- SLU/tree/master/data/snips 3https://huggingface.co/datasets/dkoterwa/mlqa_filtered Method SST-2 TREC SNIPS MLQA CoNLL-2003 Dist-3↑ SV∗↑ Dist-3↑ SV∗↑ Dist-3↑ SV∗↑ Dist-3↑ SV∗↑ Dist-3↑ SV∗↑ Original 0.99±0.01 - 0.94±0.02 - 0.89±0.03 - 0.97±0.01 - 0.97±0.01 - MoreData 0.99±0.01 - 0.89±0.01 - 0.80±0.02 - 0.93±0.02 - 0.94±0.01 - EDA 0.31±0.02 0.19±0.02 0.46±0.02 0.23±0.01 0.30±0.01 0.16±0.01 0.40±0.01 0.21±0.01 0.43±0.01 0.23±0.01 BackTrans. 0.69±0.03 0.20±0.01 0.55±0.03 0.13±0.01 0.60±0.02 0.19±0.01 0.53±0.01 0.16±0.06 0.62±0.03 0.12±0.01 GPT3Mix 0.67±0.03 - 0.54±0.02 - 0.50±0.04 - 0.65±0.04 - 0.18±0.02 - AugGPT 0.54±0.03 0.24±0.01 0.37±0.02 0.19±0.01 0.41±0.02 0.27±0.01 0.38±0.02 0.19±0.01 0.33±0.02 0.22±0.01 LLM2LLM 0.71±0.02 0.23±0.01 0.52±0.03 0.17±0.01 0.54±0.01 0.22±0.01 0.61±0.01 0.17±0.01 0.71±0.02 0.20±0.02 LMTransplant (ours) (left, right) 0.88±0.03 0.39±0.01 0.66±0.03 0.30±0.01 0.63±0.02 0.36±0.01 0.72±0.03 0.29±0.01 0.78±0.03 0.27±0.01 (right, left) 0.88±0.03 0.39±0.01 0.64±0.03 0.29±0.01 0.63±0.02 0.36±0.01 0.71±0.02 0.28±0.01 0.75±0.03 0.27±0.02 Unidirectional 0.82±0.03 0.37±0.01 0.54±0.02 0.25±0.01 0.50±0.02 0.31±0.01 0.63±0.02 0.25±0.01 0.66±0.03 0.22±0.01 Table 2: Quality of generated samples by various methods (p < 0.01). Subscript numbers denote standard deviations. SV = Semantic Variability; MoreData: randomly samples additional data from the original training set as augmented data. We choose DeepSeek-V3 as the base LLM for data augmentation. Results based on GPT-3.5-Turbo and GPT-4o are available in Appendix B. NER tasks, and the accuracy of answers (Acc) for QA tasks: Acc = #samples answered correctly #all test samples (3) Macro-F1 = 1 N N X i=1 F1i (4) where N is the total number of classes, and F1i represents the F1-score for the i-th class. 3.4 Baselines We compare our method with both traditional and LLM-based methods: Easy Data Augmentation (EDA) (Wei and Zou, 2019): A rule-based data augmentation method that applies lexical transformations, including synonym replacement, random insertion, random swap, and random deletion, to the original text. Back Translation (BackTrans.) (Sennrich et al., 2016): A widely used date augmentation method that translates the original text into another lan- guage"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 6, "text": "(EDA) (Wei and Zou, 2019): A rule-based data augmentation method that applies lexical transformations, including synonym replacement, random insertion, random swap, and random deletion, to the original text. Back Translation (BackTrans.) (Sennrich et al., 2016): A widely used date augmentation method that translates the original text into another lan- guage and then back-translates it into the original language to generate variants. Following ZeroShot- DataAug (Ubani et al., 2023), we use googletrans4 as the machine translation model, selecting differ- ent intermediate languages for multiple augmenta- tions of the same text. GPT3Mix (Yoo et al., 2021): This method ran- domly selects several examples from the seed sam- ples, embeds them into a prompt template, and then leverages the powerful few-shot learning capabil- ities of LLMs to generate mixed augmented text influenced by the provided examples. 4Google Translate (https://pypi.org/project/googletrans/) AugGPT (Dai et al., 2025): This method utilizes prompts to guide LLMs in rephrasing each sentence from the training samples into multiple semanti- cally similar but linguistically different variants, thereby enhancing text classification performance. LLM2LLM (Lee et al., 2024): An iterative data augmentation strategy that continuously employs LLMs to generate new samples from instances mis- predicted by the downstream task model. 3.5 Results 3.5.1 Intrinsic Evaluation The intrinsic evaluation results in Table 2 demon- strate that, the quality of augmented samples gen- erated by LMTransplant significantly outperform other baselines across all benchmarks. In particular, LMTransplant achieves lexical diversity (Distinct- 3) closer to original texts without augmentation (Original) and sampling additional data from orig- inal training set as augmented data (MoreData), highlighting its effectiveness in improving lexi- cal diversity. Meanwhile, LMTransplant also ex- hibits superior semantic variability compared to other baselines, indicating that it meaningfully ex- pands the semantics of the original text rather than merely relying on simple rephrasing. Additionally, LMTransplant with bidirectional text continuation outperforms its unidirectional counterpart, which serves as an ablation model for the bidirectional continuation strategy. Further ablation study details are discussed in Section 3.5.3. 3.5.2 Extrinsic Evaluation We evaluate the effectiveness of LMTransplant in empowering deep learning tasks by training mod- els on augmented data. To simulate low-data sce- Method SST-2 TREC SNIPS MLQA CoNLL-2003 Acc↑ Macro-F1↑ Acc↑ Macro-F1↑ Acc↑ Macro-F1↑ Acc↑ Acc↑ Macro-F1↑ Original 52.34±3.19 48.88±6.59 50.80±10.60 47.66±9.06 78.10±2.77 78.30±2.68 32.08±1.02 82.41±1.31 82.44±1.14 MoreData 65.41±5.48 65.41±5.48 74.30±6.55 70.75±6.47 88.23±3.14 88.30±3.24 43.30±1.13 91.56±0.65 91.50±0.89 EDA 56.78±5.04 55.79±5.86 53.50±10.59 50.58±8.76 81.86±4.03 81.80±4.31 36.92±1.67 85.08±0.91 83.89±0.80 BackTrans. 60.09±6.25 58.80±8.16 56.52±6.57 52.49±6.37 81.20±3.86 81.40±3.49 35.32±1.24 85.88±1.01 84.99±1.07 GPT3Mix 63.75±6.76 63.36±7.05 57.68±7.12 53.39±7.61 81.80±5.63 82.15±5.24 33.18±1.80 87.92±0.86 86.75±0.79 AugGPT 61.11±6.50 60.55±6.94 58.94±9.43 55.89±9.95 82.73±3.53 82.98±3.35 36.04±1.45 87.27±0.82 86.68±0.90 LLM2LLM 63.90±5.34 63.28±5.44 56.58±7.17 52.70±6.76 83.04±4.17 83.33±4.14 37.51±1.67 88.04±1.08 87.47±1.16 LMTransplant (ours) (left, right) 67.08±6.92 66.77±6.92 60.88±7.06 56.38±6.73 84.06±2.97 84.24±2.82 39.44±1.54 90.28±0.95 88.82±1.00 (right, left) 66.21±6.19 65.34±6.43 60.94±8.59 57.32±7.60 84.10±2.82 84.30±2.65 39.67±1.78 90.86±0.90 89.07±1.09 Unidirectional 64.34±5.38 63.52±5.45 58.42±7.84 55.06±7.05 82.53±4.58 82.78±4.65 35.69±1.67 87.04±0.92 85.96±1.06 Table 3: Effectiveness of LMTransplant in empowering deep learning tasks (p <0.01). Subscript numbers denote standard deviations. We choose ModernBERT-base for classification and NER tasks and Qwen2.5-1.5B for QA task. Detailed results based on ModernBERT-large are available in Appendix C, Table 10. narios,"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 7, "text": "58.42±7.84 55.06±7.05 82.53±4.58 82.78±4.65 35.69±1.67 87.04±0.92 85.96±1.06 Table 3: Effectiveness of LMTransplant in empowering deep learning tasks (p <0.01). Subscript numbers denote standard deviations. We choose ModernBERT-base for classification and NER tasks and Qwen2.5-1.5B for QA task. Detailed results based on ModernBERT-large are available in Appendix C, Table 10. narios, we adopt a subsampling strategy aligned with prior studies (Kumar et al., 2020; Ubani et al., 2023). Specifically, we randomly sample subsets (10 samples per class for classification tasks and 50 samples for QA and NER tasks) from the orig- inal training and development sets for each task. These training subsets are expanded using various augmentation methods, including LMTransplant. The augmented data, combined with the original sub-training set, are used to train deep learning models, and models’ performance is evaluated on the original test set. Results in Table 3 demonstrate that the aug- mented samples generated by LMTransplant signif- icantly improve task performance. For instance, on the SST-2 dataset, LMTransplant increases classifi- cation accuracy from 52.34% to 67.08% and Macro F1-score from 48.88% to 66.77%, outperforming even MoreData. In the QA task on MLQA, LM- Transplant achieves a 23.66% improvement in ac- curacy, significantly surpassing other LLM-based baselines. These findings indicate that, unlike meth- ods that merely prompt LLMs, the transplanting mechanism in LMTransplant harnesses knowledge embedded in LLMs more effectively, thereby gen- erating higher-quality augmented samples. Time Efficiency: We also compare the time effi- ciency of different methods, which is defined as the average time required to generate a new sample (Ta- ble 4). To ensure fairness, all comparisons are con- ducted on the same hardware. EDA significantly outperforms other methods, due to its extremely simple rule-based operations. Among LLM-based methods, AugGPT is the fastest, attributed to its shortest prompts. LMTransplant is slower but still surpasses GPT3Mix and LLM2LLM. LLM2LLM Method Time EDA 0.02±0.84 BackTrans. 3.44±5.53 GPT3Mix 22.78±12.73 AugGPT 4.93±8.27 LLM2LLM 30.55±11.19 LMTransplant (ours) 15.09±13.87 Table 4: Time efficiency for various methods. We report the average processing time (in seconds) required to generate a new sample. exhibits the lowest efficiency, as its iterative gener- ation and retraining process significantly increases time consumption. 3.5.3 Ablation Study We hereby conduct ablation experiments to explore the key step in our approach—bidirectional text continuation. First, we alter the bidirectional text continuation strategy in the transplant step. Specifi- cally, we remove backward continuation and retain only forward continuation (Unidirectional). We also experiment with altering the order of backward and forward continuations, testing both backward- first (left, right) and forward-first (right, left) ap- proaches. As shown in Table 3, unidirectional continua- tion significantly reduces effectiveness compared to bidirectional continuation, likely due to the limited context information it provides, resulting in less variation and diversity in the generated text. This is further supported by the obviously lower Distinct- 3 score for unidirectional continuation in Table 2. Additionally, the order of bidirectional continuation also affects performance: forward-first continua- tion yields better results on most datasets. This is likely because, compared to backward continua- tion, LLMs are more proficient in forward continu- ation. Specifically, generating subsequent context first provides more"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 8, "text": "3 score for unidirectional continuation in Table 2. Additionally, the order of bidirectional continuation also affects performance: forward-first continua- tion yields better results on most datasets. This is likely because, compared to backward continua- tion, LLMs are more proficient in forward continu- ation. Specifically, generating subsequent context first provides more information for LLMs to gener- ate a higher-quality preceding context and thereby improve the quality of the augmented data. 3.5.4 Scaling Analysis We further investigate the scaling effect of various sample sizes, including the number of seed sam- ples and augmented samples. Experiments are con- ducted on the SST-2 dataset. First, with the number of augmented samples fixed at 3 per seed, we grad- ually vary the number of seed samples from 20 to 100 (10 to 50 per category). Next, with the total number of seed samples fixed at 20 (10 per cate- gory), we adjust the number of augmented samples per seed from 0 to 10. After each adjustment, mod- els are retrained and evaluated. Results illustrated in Figures 4 demonstrate that, LMTransplant con- sistently achieves the highest accuracy across vary- ing seed sample sizes (Figure 4a), showcasing its robust generalization under data scarcity. Notably, while other methods plateau after a few rounds of augmentation, LMTransplant continues to improve with additional augmented samples (Figure 4b), at- tributed to its effective utilization of knowledge embedded in LLMs. This underscores the excep- tional scalability of LMTransplant and its efficacy in tackling data scarcity challenges. 3.5.5 Case Study We further conduct a qualitative analysis of the augmented samples generated by LMTransplant. We highlight two cases in Appendix D, Table 11, where LMTransplant extends the diversity and cre- ativity of the original samples while preserving the usability of the augmented samples. In the first case, the original text is a question asking about the size of our galaxy. We can ob- serve that baseline methods simply rephrase the original text with the same semantics. For example, EDA swaps the words “in” and “diameter”, which even disrupts the linguistic integrity of the text. Back-translation and AugGPT rephrase the orig- inal text by employing machine translation mod- els and LLMs, respectively. While they introduce changes in sentence structure, the new text remains semantically similar to the original. In contrast, LMTransplant generates more diverse and creative text centered on the topic of “our galaxy”, by lever- aging the knowledge embedded in LLMs. It ex- pands the phrase “our galaxy in diameter” to “the 20 30 40 50 60 70 80 90 100 Number of Seed Samples 55 60 65 70 75 80 85 Accuracy(%) Original EDA BackTrans. GPT3Mix AugGPT LLM2LLM LMTransplant (a) Performance under different numbers of seed samples. 0 2 4 6 8 10 Number of Augmented Samples Per Seed Sample 55 60 65 70 75 80 Accuracy(%) Original EDA BackTrans. GPT3Mix AugGPT LLM2LLM LMTransplant (b) Performance under different numbers of augmented sam- ples per seed sample. Figure 4: Results of scaling analysis under various sam- ple sizes. size of our galaxy” in the preceding context. More surprisingly, it introduces a new term “Milky Way” to"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 9, "text": "80 Accuracy(%) Original EDA BackTrans. GPT3Mix AugGPT LLM2LLM LMTransplant (b) Performance under different numbers of augmented sam- ples per seed sample. Figure 4: Results of scaling analysis under various sam- ple sizes. size of our galaxy” in the preceding context. More surprisingly, it introduces a new term “Milky Way” to represent “our galaxy” within the same context. This example demonstrates the extraordinary cre- ativity of LMTransplant in text augmentation. In the second case, the original text is a movie re- view expressing the audience’s appreciation on spe- cific aspects of a movie. Despite the monotonous semantics, we note that the texts generated by base- lines exhibit sentiment divergence from the original. For example, EDA replaces “idea” with “thought”, slightly weakening the emotional intensity of the original text. Back-translation introduces the ex- pression “alarming ride” during the translation pro- cess, shifting the sentiment from positive to nega- tive or neutral. And AugGPT uses a more abstract expression, “exhilarating experience”. In contrast, LMTransplant takes the gist of the original text and regenerates a novel review, expressing the same feelings on the theme of “movie”. The subsequent text retains the positive sentiment with the phrase “exceeded all my expectations and left me speech- less”. The regenerated movie review aligns with the original text both thematically and sentimentally while bringing new knowledge about sentiment classification. Overall, LMTransplant effectively harnesses the knowledge embedded in LLMs through its TTR strategy, generating text with enhanced diversity and exceptional creativity that surpasses other aug- mentation methods. Simultaneously, by utilizing bidirectional context as a bridge, LMTransplant ensures the new text accurately retains the core characteristics of the original, guaranteeing its us- ability for downstream tasks. 4 Discussion One concern is that LMTransplant might generate samples with semantic variations that differ from the original data (as illustrated in Table 11). This could potentially disrupt the performance of deep learning models, raising questions about the practi- cal utility of the augmented samples. However, our approach adheres to the funda- mental principles of data augmentation: producing diverse, high-quality samples while preserving the original data distribution. By leveraging the bidi- rectional text continuation process, our method har- nesses the knowledge embedded in LLMs to gener- ate more creative texts. This not only enhances the diversity of the data but also improves the general- ization capabilities of deep learning models. This hypothesis is supported by the results presented in Section 3.5.2, which demonstrate that the texts generated by LMTransplant significantly boost the performance of deep learning tasks. Additionally, while the generated texts may ex- hibit semantic differences from the original, the transplanting mechanism in our approach ensures that the generated samples preserve the core at- tributes of the seed text—such as theme, linguistic style, and sentiment polarity. This mechanism ef- fectively mitigates the risk of producing nonsensi- cal or irrelevant text samples, thereby maintaining the integrity and usefulness of the augmented data. 5 Related Work Data augmentation has been extensively explored in NLP tasks (Feng et al., 2021; Pellicer et al., 2023; Chen et al., 2023). Popular approaches such as word-level transformations (Wei and Zou, 2019)"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 10, "text": "nonsensi- cal or irrelevant text samples, thereby maintaining the integrity and usefulness of the augmented data. 5 Related Work Data augmentation has been extensively explored in NLP tasks (Feng et al., 2021; Pellicer et al., 2023; Chen et al., 2023). Popular approaches such as word-level transformations (Wei and Zou, 2019) often disrupt semantic coherence, while Back-translation (Sennrich et al., 2016) offers lim- ited diversity due to high similarity with the source text. In contrast, LMTransplant introduces a novel paradigm for text data augmentation, generating content-level variants that are more diverse and creative in content. Recently, LLM-based data augmentation has gained wider attention (Whitehouse et al., 2023; Ding et al., 2024; Zhou et al., 2024; Qiao et al., 2024). Leveraging their “knowledge emergence” capability, LLMs can generate desired content di- rectly through demonstrations or natural language instructions. For instance, AugGPT (Dai et al., 2025) uses natural language instructions to guide ChatGPT in rephrasing the original text. However, this approach applies relatively simple operations and underutilizes the extensive knowledge embed- ded in LLMs, resulting in limited content creativity and diversity. In comparison, LMTransplant simu- lates realistic contextual scenarios, enabling LLMs to better leverage their knowledge and generate more diverse and creative augmented samples. Similarly, GPT3Mix (Yoo et al., 2021) leverages LLMs’ few-shot learning capabilities by provid- ing a set of examples to generate new samples. However, it is highly sensitive to example quality, selection strategy, and example order, potentially introducing uncontrolled biases that compromise augmentation stability. More recently, LLM2LLM (Lee et al., 2024) iteratively augments instances misclassified by downstream-task model. While this approach produces more targeted samples, its iterative generation and retraining process incurs high computational costs and lacks adaptability to new datasets. In contrast, LMTransplant uti- lizes LLMs’ powerful language understanding and instruction-following abilities through carefully de- signed prompts, eliminating the need for examples. This approach offers greater flexibility for adapt- ing to different downstream tasks while mitigating issues related to data selection sensitivity and ex- cessive computational resource consumption. 6 Conclusion In this paper, we propose LMTransplant, a novel text data augmentation paradigm based on trans- planting strategy. By leveraging bidirectional text continuation and masked text prediction, LMTrans- plant generates high-quality and diverse augmented text. It constructs contextually coherent scenarios aligned with the original text, fully utilizing the knowledge embedded in LLMs. Experimental re- sults demonstrate that the augmented text generated by LMTransplant excels in diversity and creativity while significantly improving the performance of downstream tasks. Our replication package is available at: https://github.com/W-GZ/LMTransplant. Limitations Although LMTransplant demonstrates strong ex- perimental results, we acknowledge the following limitations and challenges that warrant further in- vestigation: First, our experiments are conducted based on DeepSeek-V3, GPT-3.5-Turbo, and GPT- 4o. We plan to extend our evaluation across a wider range of LLMs to further assess the robustness and effectiveness of LMTransplant. Second, in line with prior LLM-based augmentation methods (e.g., GPT3Mix, LLM2LLM), we evaluate LM- Transplant on classification, question-answering and named entity recognition tasks, which we be- lieve are representative tasks to demonstrate the generalization capability of LMTransplant. How- ever, when applying LMTransplant to other task types,"}
{"doc_id": "2508.14723v1", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14723v1", "chunk_id": 11, "text": "and effectiveness of LMTransplant. Second, in line with prior LLM-based augmentation methods (e.g., GPT3Mix, LLM2LLM), we evaluate LM- Transplant on classification, question-answering and named entity recognition tasks, which we be- lieve are representative tasks to demonstrate the generalization capability of LMTransplant. How- ever, when applying LMTransplant to other task types, appropriate adjustments to its prompts may be necessary to ensure adaptability and effective- ness. As such, we plan to investigate its broader applicability in future work."}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 0, "text": "The Digital Sous Chef - A Comparative Study on Fine-Tuning Language Models for Recipe Generation Shubham Pundhir1 and Ganesh Bagler1 1Indraprastha Institute of Information Technology, Delhi, India shubham24165@iiitd.ac.in, bagler@iiitd.ac.in Abstract We established a rigorous benchmark for text-based recipe generation, a fundamental task in natural lan- guage generation. We present a comprehensive compar- ative study contrasting a fine-tuned GPT-2 large (774M) model against the GPT-2 small (124M) model and tra- ditional LSTM/RNN baselines on the 5-cuisine cor- pus from RecipeDB. Our key contribution is a tar- geted tokenization strategy that augments the vocabu- lary with 23 common fraction tokens and custom struc- tural markers. This approach addresses a critical limita- tion of generic tokenizers by preserving essential recipe structures and precise numerical quantities, thereby en- hancing domain specificity. Performance is evalu- ated using a comprehensive suite of seven automatic metrics spanning fluency (BLEU-4, METEOR), coher- ence (ROUGE-L), semantic relevance (BERTScore), and diversity. Our experiments show that the large transformer-based approach yields a > 20% relative im- provement in BERTScore (F1) (0.92 vs 0.72) over the best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a discussion of remaining challenges, particularly regarding factual accuracy, and outline how this foundational study paves the way for in- tegrating real-world constraints and multi-modal inputs in advanced recipe generation research. Keywords: Recipe Generation, Large Language Models, GPT-2, Fine-Tuning, Natural Language Generation, Com- putational Creativity 1 Introduction The generation of coherent and contextually relevant recipe text is a critical challenge in culinary Natural Language Generation (NLG). Although this work focuses on the text modality, establishing a robust text generation engine is an essential first step for future multi-modal food computing systems, ensuring the instructional clarity and consistency required to support tasks like illustrating cooking steps with images or generating recipes from a photo of ingredients. Historically, computational approaches using rule-based systems or Recurrent Neural Networks (RNNs) struggled to maintain long-range coherence and creativity (Kiddon et al., 2016). Moreover, existing recipe systems often lack rigorous baselines and domain-aware tokenization, leading to incon- sistent structure and quantity handling. This paper tackles these gaps with three primary contributions: 1. A rigorous, reproducible benchmark comparing fine- tuned Transformers against strong LSTM/RNN base- lines for structured recipe generation. 2. A targeted tokenization strategy to handle domain- specific markers and preserve numerical fractions, a novel approach to improving generation quality. 3. A comprehensive analysis using a suite of seven auto- matic metrics, offering a holistic view of model perfor- mance from fluency to semantic coherence. Our results confirm the significant advantages of the trans- former architecture and lay the groundwork for future re- search into constrained decoding and multi-modal applica- tions.1 2 Related Work Research in automatic recipe generation has evolved sig- nificantly, mirroring broader trends in NLG. Early attempts often involved template-based or rule-based systems (Batra et al., 2020), which, while capable of producing structured output, were inherently brittle and lacked the flexibility to generate creative or novel recipes beyond their predefined rules. The application of neural networks brought more sophisti- cated approaches, with Recurrent Neural Networks (RNNs) and their more advanced"}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 1, "text": "or rule-based systems (Batra et al., 2020), which, while capable of producing structured output, were inherently brittle and lacked the flexibility to generate creative or novel recipes beyond their predefined rules. The application of neural networks brought more sophisti- cated approaches, with Recurrent Neural Networks (RNNs) and their more advanced variant, Long Short-Term Memory (LSTM) networks, learning sequential patterns directly from recipe data (Agarwal et al., 2020, Parvez et al., 2018). How- ever, these models were often hampered by their difficulty in maintaining long-range dependencies. In the context of a recipe, this could manifest as the model \"forgetting\" an ingredient from the initial list or failing to maintain logical consistency across a long series of steps. To combat this, 1Code is available on GitHub. 1 some research, like the \"neural checklist model\" by Kiddon et al. (Kiddon et al., 2016), attempted to explicitly track ingredient usage to improve coherence, though challenges with global consistency remained. The introduction of the Transformer architecture (Vaswani et al., 2017) and its self-attention mechanism marked a paradigm shift, largely solving the long-range dependency problem. Pre-trained models like GPT (Radford et al., 2019) were quickly adapted for this domain. Lee et al. (Lee et al., 2020) in their work on RecipeGPT, were among the first to demonstrate the effectiveness of fine-tuning GPT-2, showing notable gains in fluency. Subsequently, Goel et al. (Goel et al., 2022) developed Ratatouille, another GPT-2 based tool that focused on novel recipe generation by exploring different prompting strategies. The success of these models is shown in Table 1. More recently, research has explored a wider variety of smaller language models and introduced novel evaluation frameworks. Notably, Vij et al. (Vij et al., 2025) present a comparative analysis of models like T5 and Phi-2 on the Food.com dataset, with a primary focus on developing sys- tems for allergen substitution using RAG and novel domain- specific metrics like \"Step Complexity\" and \"Recipe Coher- ence\". Our work complements and differs from these studies. While Vij et al. (Vij et al., 2025) explores diverse small models and the complex task of allergen safety, our study provides a focused, deep comparison on the impact of model scale within a single architecture (GPT-2 small vs. large) against traditional baselines on the 5-cuisine corpus. Our primary novel contribution is the introduction and evalua- tion of a fraction-aware tokenization strategy, a specific tech- nique to improve numerical fidelity in recipes, which was not an emphasis in prior work. Thus, we aim to establish a rigorous, reproducible benchmark that isolates the effects of model scale and domain-specific tokenization using a com- prehensive suite of standard evaluation metrics. Table 1: Prior Recipe NLG Results (BLEU-4 (%)) Model BLEU-4 RecipeGPT (Lee et al., 2020) 0.085 Ratatouille (Goel et al., 2022) 0.080 3 Methodology Building on the literature, we implemented the following: • Baseline RNN/LSTM models to quantify traditional sequence-to-sequence performance. • GPT-2 fine-tuning to measure the “transformer effect” under identical conditions. 3.1 Baseline Models: LSTM/RNN To establish a meaningful performance benchmark, we first implemented standard Long Short-Term Memory (LSTM) and simple Recurrent Neural Network (RNN)"}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 2, "text": "the literature, we implemented the following: • Baseline RNN/LSTM models to quantify traditional sequence-to-sequence performance. • GPT-2 fine-tuning to measure the “transformer effect” under identical conditions. 3.1 Baseline Models: LSTM/RNN To establish a meaningful performance benchmark, we first implemented standard Long Short-Term Memory (LSTM) and simple Recurrent Neural Network (RNN) architectures. These models represent the common sequence-to-sequence approaches used prior to the widespread adoption of trans- formers (Agarwal et al., 2020, Parvez et al., 2018). Histor- ically, while a significant step up from rule-based systems, these models often struggled with maintaining long-range dependencies and global coherence, especially in lengthy, structured texts like recipes (Kiddon et al., 2016). • Architecture: We implemented word-level models consisting of an input embedding layer followed by two stacked LSTM (or simple RNN) layers with 512 hid- den units each. This architecture provides sufficient ca- pacity to learn sequential patterns without being exces- sively prone to overfitting. The recurrent layers are fol- lowed by a fully connected linear layer with a softmax activation function to produce a probability distribution over the vocabulary for predicting the next token. • Training: The models were trained using a standard cross-entropy loss objective to predict the next token in a sequence given the preceding tokens. We employed teacher forcing during training, where the ground-truth previous token is fed as input at each step. This tech- nique is known to stabilize and accelerate the training of recurrent models. • Inference: For generating recipes, we utilized both greedy decoding and beam search to produce the final text. These baselines serve to highlight the typical chal- lenges of recurrent architectures, such as a tendency to- wards repetitive output and difficulty in tracking ingre- dient usage throughout a long set of instructions. Figure 1: Conceptual architecture of the baseline LSTM/RNN model. 3.2 GPT-2 Fine-tuning The core of our methodology is the fine-tuning of the pre- trained Generative Pre-trained Transformer 2 (GPT-2) model 2 (Radford et al., 2019). This approach leverages the vast lin- guistic knowledge encoded within the model from its ini- tial large-scale training and specializes it for the nuanced domain of recipe generation. The success of fine-tuning GPT-2 for this task has been demonstrated in prior works such as RecipeGPT (Lee et al., 2020) and Ratatouille (Goel et al., 2022), which established the viability of transform- ers for generating coherent and fluent recipes. Our work builds upon these foundations by introducing a more con- trolled comparison and a domain-specific tokenization strat- egy. Figure 2: High-level overview of the Transformer fine-tuning pipeline. 3.2.1 Data Preprocessing and Structuring We utilized the 5cuisine Dataset (Batra et al., 2020). We se- lected this dataset due to its large scale ( 51,000 recipes), structured format (title, ingredients, instructions), and public availability, making it a suitable benchmark for reproducible research. Effective preprocessing was crucial for model per- formance: 1. Cleaning: Raw web-scraped data often contains noise. We removed recipes with missing essential fields (ti- tle, ingredients, instructions) and filtered out extremely short (likely incomplete) or excessively long recipes to maintain dataset quality and manage computational re- sources. Basic text normalization"}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 3, "text": "Effective preprocessing was crucial for model per- formance: 1. Cleaning: Raw web-scraped data often contains noise. We removed recipes with missing essential fields (ti- tle, ingredients, instructions) and filtered out extremely short (likely incomplete) or excessively long recipes to maintain dataset quality and manage computational re- sources. Basic text normalization (e.g., lowercasing) was also applied. 2. Structuring with Special Tokens: Recipes have an in- herent, hierarchical structure. To help the model learn this structure explicitly, we concatenated these compo- nents into a single text sequence, demarcated by custom special tokens. This format provides clear, unambigu- ous signals to the model about the different parts of a recipe: <RECIPE_START> <TITLE_START> Recipe Title <TITLE_END> <INGR_START> Ingr 1] <NEXT_INGR> Ingr 2] ... <INGR_END> <INSTR_START> [Step 1] <NEXT_INSTR> Step 2]... <INSTR_END> <RECIPE_END> 3. Data Splitting: The cleaned and formatted dataset was partitioned into standard training (80%), validation (10%), and testing (10%) sets. The validation set was used for monitoring training progress and hyperparam- eter tuning, while the test set was held out for final, un- biased evaluation. Figure 3: Structured dataset before pre-processing into a single text sequence for model training. 3.2.2 Custom Tokenization Standard tokenizers, including GPT-2’s byte-level Byte Pair Encoding (BPE), may not optimally handle domain-specific structures or symbols. We augmented the tokenizer for two key reasons: • Representing Structure: To make the model aware of the recipe sections defined during preprocess- ing, we added our custom boundary tokens (e.g., <INGR_START>, <NEXT_INGR>) directly to the to- kenizer’s vocabulary. This ensures they are treated as single, indivisible semantic units rather than being split into multiple, meaningless sub-tokens. We also added a [PAD] token for efficient batch padding. • Preserving Fractions: Numerical quantities, espe- cially fractions (e.g., \"1/2 cup\"), are critical in recipes for correctness. Standard BPE can split these in unde- sirable ways (e.g., \"1\", \"/\", \"2\"), losing the numerical meaning. We added special rules to the tokenizer to preserve common fractions as single entities, improv- ing the model’s ability to handle these numerical as- pects correctly. The model’s vocabulary and token embedding matrix were then resized to accommodate these important additions. 3 3.2.3 Training Setup Fine-tuning adapts the general knowledge of the pre-trained GPT-2 model to the specific style, vocabulary, and content of the recipe domain. • Base Model: We used the ’gpt2(774 Million)’ vari- ant (774M parameters) from the Hugging Face Trans- formers library (Wolf et al., 2020). This model offers a strong balance between generative capability and the computational requirements for fine-tuning on a single high-end GPU. • Objective Function: The model was trained using the standard Causal Language Modeling (CLM) objective. The task is to predict the next token in a sequence given all previous tokens, P(ti|t1, ..., ti−1). Cross- entropy loss was calculated, but we strategically ig- nored (masked) the padding tokens by setting their cor- responding labels to -100, a standard practice to ensure they do not contribute to the loss calculation. • Hyperparameters: We selected hyperparameters based on common practices for fine-tuning GPT-2 and empirical experimentation on our validation set: – Optimizer: AdamW (Loshchilov and Hutter, 2017), an"}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 4, "text": "padding tokens by setting their cor- responding labels to -100, a standard practice to ensure they do not contribute to the loss calculation. • Hyperparameters: We selected hyperparameters based on common practices for fine-tuning GPT-2 and empirical experimentation on our validation set: – Optimizer: AdamW (Loshchilov and Hutter, 2017), an extension of the Adam optimizer that decouples weight decay from the gradient update, which often leads to better generalization perfor- mance during fine-tuning. – Learning Rate (LR): 3 × 10−5, a typical start- ing point for fine-tuning transformers. A linear warmup schedule was used for the initial phase of training. – Batch Size: 8. This was chosen to fit within the memory constraints of our NVIDIA V100 GPU while using mixed precision. – Epochs: 20. Training progress and validation loss were monitored to prevent significant overfitting. – Mixed Precision (fp16): Utilized to accelerate training and reduce GPU memory consumption. 3.2.4 Creative and Controlled Generation To generate novel recipes at inference time, simply taking the most probable token at each step (greedy decoding) of- ten leads to repetitive and deterministic output. To encour- age creativity and diversity, we employed a combination of stochastic sampling methods: • Nucleus Sampling (Top-p): We used a value of p = 0.95. This method samples only from the smallest set of most probable tokens whose cumulative probability mass exceeds the threshold p. • Top-k Sampling: Additionally, we limited the sam- pling pool to the k = 50 most likely tokens, prevent- ing the inclusion of highly improbable and potentially nonsensical tokens. • Temperature Scaling: A temperature value of T = 0.7 was applied to rescale the logits before the softmax step. A temperature below 1.0 sharpens the distribution, making the model’s choices more focused and thereby increasing coherence. Generation was typically initiated by providing a prompt containing the start tokens and the desired ingredients. Table 2: Sampling Hyperparameters for Recipe Generation Method Value Nucleus (Top-p) Sampling p = 0.95 Top-k Sampling k = 50 Temperature Scaling T = 0.7 4 Experiments and Results 4.1 Dataset and Experimental Setup The preprocessed 5cuisine dataset formed the basis for our experiments. Key statistics of the final dataset used for train- ing and evaluation are presented in Table 3. All experiments were conducted using Python 3.8+, PyTorch 1.10+, and the Hugging Face Transformers library (v4.x) on an NVIDIA V100 GPU with 32GB of memory. For generation met- rics, recipes were generated based on prompts derived from the test set (using the ground-truth ingredients list as input), and these generated recipes were then compared against the ground-truth instructions. Table 3: 5cuisine Dataset Statistics After Preprocessing Feature Value Total Recipes Processed ∼51,000 Train Set Size (80%) ∼40,800 Validation Set Size (10%) ∼5,100 Test Set Size (10%) ∼5,100 Avg. Tokens per Recipe ∼512 Vocabulary Size (Base + Special) ∼50,257 4.2 Evaluation Metrics To provide a holistic assessment of recipe quality, we em- ployed a diverse set of automatic metrics, each capturing different aspects of performance: • BLEU-4 (Papineni et al., 2002): Primarily measures n- gram precision (the overlap of generated 4-grams with reference 4-grams), indicating"}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 5, "text": "(Base + Special) ∼50,257 4.2 Evaluation Metrics To provide a holistic assessment of recipe quality, we em- ployed a diverse set of automatic metrics, each capturing different aspects of performance: • BLEU-4 (Papineni et al., 2002): Primarily measures n- gram precision (the overlap of generated 4-grams with reference 4-grams), indicating local correctness and flu- ency. • ROUGE-L (Lin, 2004): Measures the longest common subsequence between the generated and reference text, focusing on recall and capturing sentence-level struc- tural similarity. 4 • Diversity: Quantifies lexical variety by calculating the ratio of unique word bigrams to the total number of bi- grams generated. Higher scores indicate less repetition and potentially more novel output. • METEOR (Banerjee and Lavie, 2005): Aligns gen- erated text with reference text considering synonyms and stemming, providing a score based on the harmonic mean of precision and recall for unigrams. It often cor- relates better with human judgment than BLEU. • Perplexity (PPL): An intrinsic evaluation metric that measures how well the language model predicts the test set sequences. Lower perplexity indicates higher model confidence and a better fit to the data’s distribution. • BERTScore (Zhang et al., 2020): Measures semantic similarity by computing the cosine similarity between contextual embeddings of tokens in the generated and reference texts. It captures semantic relevance far better than surface-level n-gram overlap metrics. 4.3 Quantitative Results and Interpretation Table 4 presents the comparative performance of the im- plemented LSTM/RNN baselines and the fine-tuned GPT-2 model on the held-out test set. The results clearly and con- sistently demonstrate the superiority of the fine-tuned GPT-2 model over the recurrent baselines across every single met- ric. A deeper interpretation of these results reveals several key insights: • Model Confidence and Fit (Perplexity): GPT-2 achieves a dramatically lower average perplexity than the baselines, indicating a vastly superior understand- ing and prediction of the recipe language structure and vocabulary distribution found in the test set. • Overlap and Structural Metrics (BLEU, ROUGE, METEOR): GPT-2 consistently scores higher on BLEU-4, ROUGE-L, and METEOR, demonstrating that its generated recipes have greater n-gram overlap (local fluency) and better structural alignment with the ground-truth recipes. • Semantic Similarity (BERTScore): The substantial improvement in the BERTScore F1 is particularly note- worthy. It suggests that GPT-2 generates recipes that are not only structurally more similar but also semanti- cally closer in meaning to the reference recipes, captur- ing underlying cooking concepts more effectively. • Creativity and Novelty (Diversity): GPT-2 also ex- hibits significantly higher bigram diversity, indicating that its generated outputs are less repetitive and con- tain a wider variety of word pairings compared to the LSTM/RNN models. This points to a greater potential for generating novel content rather than just reproduc- ing training data patterns. Overall, the quantitative analysis strongly supports the hypothesis that fine-tuning a pre-trained transformer model provides significant, measurable advantages for the com- plex, structured task of recipe generation compared to tra- ditional recurrent architectures trained from scratch. 5 Discussion The quantitative results presented in the previous section paint a clear picture of GPT-2’s superiority over LSTM/RNN baselines on automatic metrics."}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 6, "text": "fine-tuning a pre-trained transformer model provides significant, measurable advantages for the com- plex, structured task of recipe generation compared to tra- ditional recurrent architectures trained from scratch. 5 Discussion The quantitative results presented in the previous section paint a clear picture of GPT-2’s superiority over LSTM/RNN baselines on automatic metrics. However, a deeper qualita- tive analysis of the generated recipe text reveals a more nu- anced understanding of the model’s capabilities and its re- maining limitations. 5.1 Qualitative Analysis and Example Gener- ation Figure 4 shows a sample output from our fine-tuned GPT- 2 Large model, along with its corresponding evaluation scores. The generated instructions are coherent, follow a logical step-by-step process, and correctly utilize the ingre- dients provided in the prompt, such as \"beef chuck,\" \"gin- ger paste,\" and \"cardamom seeds.\" This demonstrates the model’s strength in maintaining context and adhering to the input. The high BERTScore F1 of 0.9148 reflects this strong semantic relevance. At the same time, the more modest BLEU score of 0.2105 indicates that the phrasing differs from the ground-truth recipe, which is expected and even desirable in a creative generation task. This highlights the importance of using a diverse suite of metrics, as a low n- gram overlap score does not necessarily mean a low-quality recipe. 5.2 Strengths of Fine-tuned GPT-2 • Improved Coherence and Fluency: The most strik- ing improvement was in the overall readability and log- ical flow of the generated recipes. GPT-2 produced in- structions that were generally well-formed, grammat- ically correct, and followed a plausible cooking se- quence much more reliably than the baselines. • Handling Long-Range Dependencies: Unlike LSTMs which often \"forgot\" ingredients mentioned in the ini- tial prompt, GPT-2 demonstrated a much better capac- ity to track the ingredient list throughout the instruction steps. • Structural Fidelity: The use of custom special tokens proved highly effective. GPT-2 reliably generated dis- tinct sections for the title, ingredients, and instructions, adhering to the desired format. This structure is cru- cial for user readability and any potential downstream processing. 5 Table 4: Comparative evaluation results on the test set. All metrics are averaged across the test set’s generated recipes. For all metrics except Perplexity, higher is better. For Perplexity, lower is better (↓). Model ROUGE-L BLEU-4 Diversity METEOR PPL ↓ BERTScore (F1) Precision Recall F1 RNN Baseline 0.20 0.16 0.18 0.08 0.28 0.13 105.2 0.72 LSTM Baseline 0.39 0.27 0.30 0.12 0.51 0.28 68.58 0.87 GPT-2 small (124 M) fine-tuned 0.55 0.41 0.47 0.21 0.68 0.41 28.5 0.90 GPT-2 large (774 M) fine-tuned 0.64 0.48 0.54 0.26 0.73 0.47 20.67 0.92 Figure 4: A sample generation from the fine-tuned GPT-2 Large model, showing the input ingredient prompt, the generated step-by-step instructions, and the resulting automatic evaluation scores. • Potential for Novelty: The higher diversity scores were reflected qualitatively. GPT-2 occasionally sug- gested interesting, non-standard ingredient pairings or slight variations on common techniques, hinting at a creative potential that extends beyond simply reproduc- ing training data patterns. 5.3 Challenges and Limitations Despite its clear strengths, the fine-tuned GPT-2 model ex- hibited several critical weaknesses"}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 7, "text": "diversity scores were reflected qualitatively. GPT-2 occasionally sug- gested interesting, non-standard ingredient pairings or slight variations on common techniques, hinting at a creative potential that extends beyond simply reproduc- ing training data patterns. 5.3 Challenges and Limitations Despite its clear strengths, the fine-tuned GPT-2 model ex- hibited several critical weaknesses that prevent it from being a fully reliable culinary assistant. • Factual Inaccuracy and Common Sense: This re- mains the most significant challenge. The model some- times generated factually incorrect information, such as suggesting inappropriate cooking temperatures or times (e.g., \"bake a delicate fish at 500°F for 3 hours\"). Quan- tities were particularly problematic, with frequent in- stances of unrealistic or nonsensical amounts (e.g., \"add 20 cloves of garlic for a single serving,\" or \"use 5 cups of salt\"). This highlights the model’s lack of true world knowledge or common-sense reasoning about physics and chemistry. • Ingredient Hallucinations and Omissions: While bet- ter than LSTMs, GPT-2 still sometimes introduced in- gredients that were not present in the initial prompt (hallucinations) or failed to utilize all the provided in- gredients (omissions). • Step Consistency and Logical Gaps: Although gen- erally coherent, occasional logical gaps or inconsisten- cies appeared in the instruction steps. For instance, an instruction might refer to an ingredient that hadn’t been prepared yet (\"add the diced onions\" before they have been diced), or steps might be presented in a suboptimal order. • Repetition and Truncation: While less frequent than with LSTMs, GPT-2 could sometimes fall into repeti- tive loops, especially with longer generations. Gener- ated recipes could also be truncated if they exceeded the maximum specified output length during inference. These limitations underscore the crucial gap between gener- ating statistically plausible text and creating practically us- able, safe, and delicious recipes. 6 Conclusion and Future Work In this paper, we demonstrated that fine-tuning a pre-trained GPT-2 large model, enhanced with a domain-specific tok- enization scheme, sets a new state-of-the-art for recipe gen- eration on the 5-cuisine benchmark, significantly outper- forming traditional RNN/LSTM baselines. Our work con- firms the power of scaled transformer models for structured creative generation but also highlights the critical need for further research to bridge the gap to practically reliable sys- tems. Building on these findings, we identify several promising avenues for future work: • Retrieval-Augmented Generation (RAG): To im- prove factual grounding, RAG can be integrated to ground generation in an external knowledge base of trusted culinary information (e.g., standard cooking times), which would directly address the issue of fac- tual inaccuracies. 6 • Constrained Decoding: Developing techniques to ex- plicitly enforce constraints during generation is cru- cial. This could involve methods like lexical constraints to guarantee ingredient usage or integrating rule-based checks to ensure quantities fall within realistic ranges. • Systematic Human Evaluation: A critical next step, as noted by reviewers, is to conduct a systematic hu- man evaluation. Such a study should assess generated recipes on key qualities like clarity, feasibility, safety, and creativity, providing definitive insights that auto- matic metrics cannot capture. • Benchmarking Additional Architectures: Future work could extend this benchmark to"}
{"doc_id": "2508.14718v1", "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14718v1", "chunk_id": 8, "text": "next step, as noted by reviewers, is to conduct a systematic hu- man evaluation. Such a study should assess generated recipes on key qualities like clarity, feasibility, safety, and creativity, providing definitive insights that auto- matic metrics cannot capture. • Benchmarking Additional Architectures: Future work could extend this benchmark to include encoder- decoder models like T5 and BART to provide a more complete picture of architectural trade-offs. • Multi-Modal Integration: Our foundational text model enables several exciting multi-modal extensions, such as generating recipes from images (image-to- recipe) or generating illustrative images for each recipe step (recipe-to-image). Acknowledgments The authors thank the anonymous reviewers for their insight- ful feedback, which helped improve the quality of this paper."}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 0, "text": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine Junying Chen1†, Zhenyang Cai1†, Zhiheng Liu1, Yunjin Yang1, Rongsheng Wang1, Qingying Xiao3, Xiangyi Feng2, Zhan Su1, Jing Guo5, Xiang Wan4, Guangjun Yu1,3, Haizhou Li1, Benyou Wang1,4∗ 1 The Chinese University of Hong Kong, Shenzhen 2 Freedom AI 3 National Health Data Institute, Shenzhen 4 Shenzhen Research Institute of Big Data 5 Guangdong University of Technology wangbenyou@cuhk.edu.cn https://github.com/FreedomIntelligence/ShizhenGPT Abstract Despite the success of large language models (LLMs) in vari- ous domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical bar- riers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which in- volve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For eval- uation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual under- standing among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal per- ception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field. Introduction Large language models (LLMs) like GPT-4 have demon- strated impressive capabilities across general and specialized domains (Achiam et al. 2023; Dubey et al. 2024; Guo et al. 2025). Meanwhile, Traditional Chinese Medicine (TCM), a medical system with thousands of years of history and ongoing relevance for hundreds of millions (Shi and Zhou 2007; Zhao et al. 2015), remains largely absent from recent AI developments. TCM, with its intricate theoretical frame- work, nuanced diagnostic reasoning, and enduring clinical relevance (Bing and Hongcai 2010), presents both a chal- lenge and an opportunity for large language models (Hua et al. 2024). Empowering these models to understand and reason about TCM could greatly advance clinical decision- making, medical education, and preservation of traditional medical knowledge (Yao et al. 2019; Wang et al. 2020). †Equal Contribution. ∗Corresponding author. Large-Scale Multimodal TCM Dataset Extensive TCM Knowledge Smelling Listening Observing Touching Multi-Modal Perception Large-Scale TCM Training ShizhenGPT VL ShizhenGPT-32B Deepseek-V3 LLaMA4-Marerick LLaMA4-Marerick GPT-4o ShizhenGPT-32B Qwen2.5-VL-72B Qwen2.5-72B Doubao-1.5-Vision-Pro GPT-4o TCM Visual Skills Latest Licensing Exam for TCM Pharmacist Figure 1: Key Capabilities of ShizhenGPT, a multimodal LLM for Traditional Chinese Medicine (TCM). However, applying LLMs to TCM presents two key chal- lenges. First, there is a significant scarcity of high-quality TCM data. For example, most existing TCM-specific LLMs are trained on less than 1GB of text (Dai et al. 2024; Yang et al. 2024b; Wei et al. 2024; Hua et al. 2024). This is sub-"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 1, "text": "to TCM presents two key chal- lenges. First, there is a significant scarcity of high-quality TCM data. For example, most existing TCM-specific LLMs are trained on less than 1GB of text (Dai et al. 2024; Yang et al. 2024b; Wei et al. 2024; Hua et al. 2024). This is sub- stantially smaller than the datasets used in domains like mod- ern medicine (Zhang et al. 2024b; Chen et al. 2023), and particularly insufficient given the complexity and depth of TCM theory. Moreover, the multimodal datasets necessary for training TCM-oriented multimodal LLMs are currently non-existent. Second, TCM fundamentally depends on multi- modal diagnostics. Rooted in the ”Four Diagnostic Methods”, it depends on observing the tongue and other visual cues, listening to the voice and breath, smelling smells, and feel- ing the pulse (Dong, Zhang, and Fei 2022; Tian et al. 2023). These rich sensory modalities are central to TCM and lie well beyond the capabilities of text-only models. To address this, we propose ShizhenGPT, the first multi- modal LLM tailored for Traditional Chinese Medicine. To Table 1: Comparison of ShizhenGPT and existing LLMs across key dimensions. Model Domain Expertise Multimodal Capability Clinical Application Text Vision Sound Smell Pulse TCM-specific LLMs Limited by small-scale training data (<1GB). ✓ ✗ ✗ ✗ ✗ Limited to text-only sce- narios. General-purpose LLMs General-domain training, limited TCM spe- cialization. ✓ ❍ Partial ❍ Partial ✗ ✗ Only partial support for vision/sound. ShizhenGPT Trained on 15B+ tokens and 200GB+ TCM data; high TCM proficiency. ✓ ✓ ✓ ✓ ✓ Fully supports Four Diag- nostic Methods. mitigate data scarcity, we built the largest TCM dataset to date, comprising over 100GB of text from 3,256 TCM- specific books and online sources. In addition, we collected over 200GB of multimodal data, including 1.2 million anno- tated images, 200+ hours of audio, and diverse physiological signals such as pulse, smell, and electrocardiograms (ECG). Leveraging this extensive dataset, ShizhenGPT is trained through domain-specific pretraining and instruction tuning, enabling it to acquire deep TCM knowledge and multimodal reasoning. The resulting model ShizhenGPT integrates deep domain expertise with perceptual capabilities aligned with real-world TCM diagnostic practices, including observation, listening, smelling, and pulse-taking. To evaluate TCM-specific capabilities, we collected re- cent qualification exams, including three national licensing exams and two postgraduate entrance exams in TCM. All questions were released within the past year to ensure fresh- ness. For visual evaluation, we built a benchmark using TCM atlases, containing 7k image-related questions focused on medicinal recognition and visual diagnosis. Results show that ShizhenGPT outperforms existing LLMs of similar scale and rivals much larger proprietary models, despite having only 32B parameters. In visual TCM tasks, it leads among current multimodal LLMs. Expert evaluations also indicate a higher preference for ShizhenGPT’s responses. Additionally, across seven physiological signal datasets, ShizhenGPT demon- strates effective multimodal perception, including sound, pulse, and smell, achieving unified input and understanding across modalities. We present a TCM-specific multimodal LLM capable of perceiving smell, sound, vision, and pulse. This expands the diagnostic capabilities of the model beyond text-based interaction, allowing it to directly observe the tongue, pulse, or"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 2, "text": "strates effective multimodal perception, including sound, pulse, and smell, achieving unified input and understanding across modalities. We present a TCM-specific multimodal LLM capable of perceiving smell, sound, vision, and pulse. This expands the diagnostic capabilities of the model beyond text-based interaction, allowing it to directly observe the tongue, pulse, or breathing sounds for richer clinical insights. This moves toward more realistic, holistic medical AI systems. We hope our data, model, and approach will inspire further research in TCM. Our contributions are as follows: • We release the largest TCM dataset to date, covering extensive text and multimodal data across vision, audio, and physiological signals. • We introduce ShizhenGPT, the first multimodal LLM tai- lored for TCM, capable of understanding images, sounds, smells, pulse, and more. It achieves strong performance in TCM expertise and leads in visual diagnostic tasks. • We release a comprehensive benchmark for TCM across textual and multimodal tasks, enabling systematic evalua- tion of LLMs in TCM. Background and Motivation Traditional Chinese Medicine TCM is one of the world’s oldest and most influential medical systems, serving over 500 million patients annually in China and gaining global recognition (Cheung 2011; Liu et al. 2015; Zhao et al. 2023). Unlike modern medicine, TCM relies on the ”Four Diag- nostic Methods”: looking, listening/smelling, questioning, and pulse-taking (Dong, Zhang, and Fei 2022; Tian et al. 2023). These methods collect visual, auditory, verbal, and tactile information to form a holistic understanding of the patient. Each captures distinct biomedical signals, making TCM inherently multimodal and posing unique challenges for AI systems seeking to replicate its diagnostic reasoning. Motivation for a Multimodal TCM LLM TCM’s com- plexity and its reliance on diverse sensory modalities pose significant challenges for LLMs. As shown in Table 1, cur- rent TCM-specific models suffer from limited training data and lack multimodal perception. General-purpose models, while powerful, are not tailored to TCM and offer only par- tial support for modalities like vision or audio. To address these gaps, we propose ShizhenGPT, a multimodal LLM trained on extensive TCM data in diverse modalities. By inte- grating perception and reasoning across all four diagnostic methods, ShizhenGPT might enable more faithful, practical applications in real-world clinical settings. ShizhenGPT This section describes ShizhenGPT, including its architec- ture and two-phase training: (1) pre-training to inject TCM- specific knowledge, and (2) post-training for real-world tasks. Model Architecture As shown in Figure 2(c), ShizhenGPT consists of three main components: an LLM backbone, a vision encoder, and a signal encoder. LLM Backbone The LLM serves as the core reason- ing engine. It processes inputs from multiple modalities and generates responses. We use the Qwen-2.5-7B and Qwen-2.5-32B (Yang et al. 2025) as the base LLMs. Pretraining-Stage1: 11.9B TCM text tokens Pretraining-Stage2: 3.6B TCM Multimodal tokens Large Language Model Vision Encoder Vision Adapter Signal Encoder Signal Adapter (a) Pre-training Instruction Tuning with： Text Sound Language Response Pulse Odor ECG Images Palm Tonge Herb Eye ... Text (b) Post-training (c) Model Architecture Stronger TCM Expertise (d) ShizhenGPT Multimodal TCM Understanding Modal Generalization Excels in TCM Licensed Exams Multimodal Generalization Rich Multimodal Text Image Sound"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 3, "text": "Signal Encoder Signal Adapter (a) Pre-training Instruction Tuning with： Text Sound Language Response Pulse Odor ECG Images Palm Tonge Herb Eye ... Text (b) Post-training (c) Model Architecture Stronger TCM Expertise (d) ShizhenGPT Multimodal TCM Understanding Modal Generalization Excels in TCM Licensed Exams Multimodal Generalization Rich Multimodal Text Image Sound Image Sound Pulse Odor ECG Text + Text Output Image + Text Output Sound + Text Output Pulse + Text Output Odor + Text Output ECG Text Output Pretraining Stage 2 Loss Pretraining Stage 1 Loss Figure 2: Overview of ShizhenGPT. (a) Pre-training process, with the loss curve of ShizhenGPT-7B. (b) Post-training process with multimodal instruction tuning. (c) Model architecture. (d) Demonstration of ShizhenGPT’s capabilities. Vision Encoder For visual inputs, we initialize the Vi- sion Encoder from Qwen-2.5-VL (Bai et al. 2025), which supports high-resolution images using 2D-RoPE and win- dow attention. Visual patches extracted by the encoder are grouped (four adjacent patches), concatenated, and mapped to the LLM’s embedding space via a two-layer MLP (Vision Adapter). Signal Encoder For continuous signals (e.g., voice, pulse, smell), we use Whisper-large-v3 (Radford et al. 2022) as the initial Signal Encoder. Non-audio signals are first con- verted into waveform representations via linear interpolation. All waveforms are resampled to 16kHz and transformed into 128-channel mel-spectrograms using a 25ms window and 10ms hop size. A stride-2 pooling layer reduces temporal res- olution, and a one-layer MLP (Signal Adapter) projects the features into the embedding space of LLM. Although pulse signals are low-frequency, the resulting 40ms-per-token reso- lution retains sufficient temporal granularity to capture the slow-varying patterns essential for TCM. Pre-training We begin with pre-training on large-scale TCM data to ac- quire domain expertise. We design a two-stage pipeline: the first stage infuses knowledge from extensive TCM text, while the second introduces multimodal alignment through image- text and audio-text data. Pretraining Datasets As summarized in Table 2, the pre- training corpus includes three components: • TCM Text Corpus We collect TCM text from 3,256 TCM books and online sources. The books are converted into interleaved text-image corpora and cleaned to yield 3.8GB of book corpus. For online sources, we use a 30K- term TCM lexicon to extract TCM documents from Com- mon Crawl (2017-2023) and WeChat public articles, to- taling 96.4GB. A two-step filtering process, involving quality scoring using a classifier and semantic deduplica- tion, produces a final 21.2GB high-quality web corpus. • TCM Image-Text Data We extract 17.6GB of inter- leaved image-text data from TCM books (51K images). From 5 million images sourced from WeChat articles, we filtered high-quality TCM images using an image classifier. This process yielded 140.7GB of web-based image-text data (1M images). To improve alignment, we follow HuatuoGPT-Vision and use a multimodal LLM to generate descriptive captions based on image and context, producing 40.6GB of synthetic pairs (15K images). • TCM Audio-Text Data We extract 60K doctor-patient dialogues from the Huatuo-26M dataset (Li et al. 2023b) and synthesize audio via a high-fidelity TTS system (Du et al. 2024), yielding 58K utterance-aligned audio-text pairs. Further details are provided in Appendix . Pre-training Strategy We adopt"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 4, "text": "of synthetic pairs (15K images). • TCM Audio-Text Data We extract 60K doctor-patient dialogues from the Huatuo-26M dataset (Li et al. 2023b) and synthesize audio via a high-fidelity TTS system (Du et al. 2024), yielding 58K utterance-aligned audio-text pairs. Further details are provided in Appendix . Pre-training Strategy We adopt a two-stage pre-training strategy. In Stage 1, it focuses on text-only learning with 11.9B tokens, including 6.3B from TCM corpora and 5.6B from general corpora (see Appendix for details) to maintain foundational capabilities. In Stage 2, it uses 3.8B tokens of multimodal data, including TCM and general image-text and audio-text data, along with 1.8B resampled textual tokens from Stage 1 to preserve grounding. In both stages, inputs are packed into 4096-token sequences, and loss is computed only over textual tokens. Pre-training runs for 1 epoch with a 5e-5 learning rate using full-parameter tuning. Details are provided in Appendix 6. Table 2: Overview of Pre-training Datasets. These data are utilized for a single training epoch unless otherwise specified. # indicates quantity. B stands for billion. Pre-training Dataset Description # Tokens Storage Text Datasets (Pretraining Stage 1 & Stage 2) TCM Web Corpus A high-quality TCM corpus collected from Common Crawl (2017–2023) and WeChat public articles (a major content channel in China). 5.33B 21.2 GB TCM Book Corpus A cleaned corpus of 3,256 TCM textbooks. (2 epochs) 0.96B 3.8 GB General Text Corpus General-domain English corpus from FineWeb-edu. See Appendix for details. 5.63B 29.2 GB Image-Text Dataset (Pretraining Stage 2) TCM Web Interleaved Data Interleaved text-image data from the TCM web corpus. (# Images: 1,143,954) 0.87B 140.7 GB TCM Book Interleaved Data Richly illustrated interleaved text-image data from 306 TCM books. (# Images: 51,619; 2 epochs) 0.14B 17.6 GB TCM Synthesized Image-Text Data TCM image-text pairs generated from images and their context using multi- modal LLMs. (# Images: 146,635; 2 epochs) 0.16B 40.6 GB General Image-Text Data General-domain image-text datasets, such as ShareGPT-4V. See Appendix for details. (# Images: 1,851,321) 0.69B 93.29 GB Audio-Text Dataset (Pretraining Stage 2) TCM-related Audio-Text Data Chinese audio-text pairs in the domain of traditional Chinese medicine. (# Audios: 58,462 ; Duration: 238.67 hours) 0.03B 75.72 GB General Audio-Text Data English audio-text pairs from general domains. See Appendix . (# Audios: 282,884 ; Duration: 353.04 hours) 0.04B 111.6 GB Table 3: Overview of Instruction-Tuning Datasets. Signal instructions are detailed in Appendix 9. Instruction-Tuning Dataset Description # Item Text Instruction Data TCM Instruction A dialogue instruction dataset centered on real-world TCM problems. 83,629 Vision Instruction Data (Requires Vision Encoder) TCM Vision Instruction A multimodal instruction dataset composed of TCM-related image-text pairs. (# Images: 70,638) 65,033 Signal Instruction Data (Requires Signal Encoder) TCM Speech Instruction Speech-to-text instruction data in the TCM domain. 57,957 Cough Sound Dataset An instruction dataset focused on cough sounds for medical diagnosis. 456 Heartbeat Sound Dataset An instruction dataset focused on heartbeat sounds for medical diagnosis. 189 Pulse Dataset An instruction dataset focused on pulse diagnosis in TCM applications. 4,101 Smell Dataset Dataset related to olfactory input for medical diagnosis. 672 ECG Dataset Electrocardiogram instruction dataset for medical"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 5, "text": "sounds for medical diagnosis. 456 Heartbeat Sound Dataset An instruction dataset focused on heartbeat sounds for medical diagnosis. 189 Pulse Dataset An instruction dataset focused on pulse diagnosis in TCM applications. 4,101 Smell Dataset Dataset related to olfactory input for medical diagnosis. 672 ECG Dataset Electrocardiogram instruction dataset for medical diagnosis. 99,195 Post-training After pre-training, the model acquires foundational TCM knowledge and multimodal abilities. Post-training aligns the model for instruction-following, extends its capabilities to downstream tasks, and enables adaptation to additional modalities such as sound and smell, which require low learn- ing complexity but suffer from limited data. Instruction Dataset As summarized in Table 3, instruction tuning data spans four categories: • TCM Text Instructions We curate 92K TCM ques- tions from doctor-patient dialogues (Li et al. 2023b) and medical verifiable questions (Chen et al. 2024b). Re- sponses were distilled using Deepseek-R1 and validated with Deepseek-V3 based on reference answers or doctor responses, resulting in 83K reliable Chinese instruction pairs. • Vision Instructions We select TCM-related images with detailed captions from books and web sources. GPT-4o is used to generate paired visual questions and answers based on image content and context, producing 65K high- quality vision instruction pairs. • Audio Instructions For speech audio, we convert 20K text questions into TTS speech and pair them with origi- nal answers. An additional 20K transcription-style in- structions are created. For TCM-specific audio (e.g., coughs, heart sounds), classification tasks are reformu- lated into instruction format (e.g., “Listen and determine if the patient has COVID-19.”). • Physiological Signal Instructions Non-audio signals (e.g., pulse, smell, ECG) are converted to waveform format and tagged with modality-specific tokens (e.g., <Smell>, <Pulse>). The pregnancy-related pulse data is constructed in-house, while the other data are ob- Table 4: Results of LLMs on the latest TCM exams, including the 2024 Licensed TCM Pharmacist Exam, 2024 Licensed TCM Physician Exam, 2024 Licensed TCM Assistant Physician Exam, and two TCM Postgraduate Entrance Exams (2024-2025). denotes proprietary or 100B+ LLMs; denotes TCM-specific LLMs. Model 2024 TCM Pharmacist 2024 TCM Physician 2024 TCM Assistant Physician 2024 TCM Graduate Entrance 2025 TCM Graduate Entrance Avg. Proprietary or 100B+ LLMs GPT-4o 61.7 56.2 66.7 58.7 55.0 59.6 LLaMA-4-Maverick-400B 66.0 67.2 76.6 63.3 59.5 66.5 Deepseek-V3 74.6 74.8 80.8 72.4 66.7 73.9 Doubao-1.5-Pro-32k 81.9 79.7 86.2 81.5 73.7 80.6 Deepseek-R1-671B 85.0 82.4 86.2 83.2 83.3 84.0 <15B LLMs Llama-3.1-8B-Instruct 36.7 36.9 40.4 33.4 26.1 34.7 TCMChat-600k-7B 42.0 49.5 46.0 41.6 38.5 43.5 Zhongjing-7B 45.5 47.8 46.4 38.4 45.6 44.7 Biancang-7B 57.3 57.9 66.2 43.5 37.5 52.5 GLM4-9B-Chat 54.6 56.2 61.6 54.0 51.9 55.7 Yi-1.5-9B-Chat 59.0 58.2 69.7 53.0 39.7 55.9 Lingdan-13B-PR 64.4 65.9 78.4 45.6 41.1 59.1 DeepSeek-R1-Distill-14B 62.2 60.8 70.5 58.4 55.7 61.5 Qwen2.5-7B-Instruct 65.3 63.2 76.0 59.0 54.0 63.5 InternLM2.5-7B-chat 69.8 68.5 79.7 58.5 48.9 65.1 Qwen2.5-14B-Instruct 70.8 68.8 71.7 63.8 57.2 66.4 ShizhenGPT-7B (Ours) 70.1 76.7 79.4 68.8 69.3 72.9 15B to 100B LLMs Gemma-3-27B-it 42.1 40.9 44.9 35.1 28.1 38.2 Llama-3.1-70B-Instruct 50.6 52.8 65.0 42.5 39.9 50.2 Yi-1.5-34B-Chat 67.6 62.2 75.5 61.8 50.7 63.6 InternLM2.5-20B-Chat 66.5 66.1 73.8 57.9 53.8 63.6 GLM-4-32B 67.9 65.6 80.9"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 6, "text": "70.8 68.8 71.7 63.8 57.2 66.4 ShizhenGPT-7B (Ours) 70.1 76.7 79.4 68.8 69.3 72.9 15B to 100B LLMs Gemma-3-27B-it 42.1 40.9 44.9 35.1 28.1 38.2 Llama-3.1-70B-Instruct 50.6 52.8 65.0 42.5 39.9 50.2 Yi-1.5-34B-Chat 67.6 62.2 75.5 61.8 50.7 63.6 InternLM2.5-20B-Chat 66.5 66.1 73.8 57.9 53.8 63.6 GLM-4-32B 67.9 65.6 80.9 64.9 62.6 68.4 DeepSeek-R1-Distill-32B 70.6 67.7 78.4 65.3 61.2 68.6 Qwen2.5-32B-Instruct 76.1 69.2 78.3 65.5 61.8 70.2 Qwen2.5-72B-Instruct 77.2 70.6 81.0 63.3 59.9 70.4 ShizhenGPT-32B (Ours) 79.2 78.4 84.9 77.5 70.7 78.1 tained from public datasets. All tasks are transformed into instruction format using a unified signal-processing pipeline to ensure extensibility. The instruction and signal datasets are detailed in Ap- pendix 6 and Appendix 9, respectively. Instruction Fine-tuning We fine-tune the TCM-pretrained model on all modality-specific instruction datasets in Table 3 using full-parameter tuning. Instructions follow the Qwen-2.5 chat template, and loss is computed only on response tokens. The model is trained for 3 epochs with a learning rate of 5e-6. See Appendix 6 for details. This stage produces the final model, ShizhenGPT. Experimental Setup Model Training We train two versions of ShizhenGPT: ShizhenGPT-7B and ShizhenGPT-32B, based on Qwen2.5-7B and Qwen2.5-32B backbones, respec- tively. Both use the same vision and signal encoders, and are trained with identical settings across all stages. Experiments are conducted on two DGX nodes, each with 8 A100 GPUs. Baselines We compare ShizhenGPT with following base- lines: (1) Vanilla LLMs: We evaluate mainstream general- purpose models including LLaMA (Dubey et al. 2024), Qwen, Yi (Young et al. 2024), GLM-4 (GLM et al. 2024), In- ternLM (Team 2023), Gemma (Team et al. 2024), DeepSeek (Liu et al. 2024a), GPT-4o (2024-11-20) (Achiam et al. 2023), and Doubao (ByteDance Seed Team 2024). TCM- specific LLMs include TCMChat-600K-7B (Dai et al. 2024), Zhongjing-7B (Yang et al. 2024b), Biancang-7B (Wei et al. 2024), and Lingdan-13B-PR (Hua et al. 2024). (2) Multi- modal LLMs (MLLMs): For vision understanding, we in- clude Gemini (Team et al. 2023), GPT-4o, LLaVA (Liu et al. 2023), Qwen2.5-VL (Bai et al. 2025), InternVL (Chen et al. 2024f), Janus-Pro (Chen et al. 2025), Kimi-VL (Team et al. 2025) and HuatuoGPT-Vision (Chen et al. 2024c); for audio understanding, we consider Qwen2-Audio (Chu et al. 2024), Llama-Omni (Fang et al. 2024), and Moshi (D´ efossez et al. 2024). Benchmarks To evaluate the TCM capabilities, we con- struct a multimodal benchmark suite covering text, vision, signals, and human evaluation. (1) Text Benchmarks: We collect recent national TCM exams in China, including three licensing exams (for pharmacists, physicians, and as- sistant physicians) and two postgraduate entrance exams (2024–2025). (2) Vision Benchmark: We compile 7,204 multiple-choice questions from seven TCM atlases, explicitly excluded from training, covering seven subfields in medic- Table 5: Results on the TCM-Vision Benchmark, evaluating visual understanding in TCM. denotes proprietary or 100B+ multimodal LLMs. bold indicates the best score; underlines marks the second-best. Model TCM Medicinal Recognition TCM Visual Diagnosis Avg TCM Patent TCM Material TCM Herb Holism Tongue Palm Tuina Eye Multimodal LLMs LLaVA-1.5-7B 25.6 26.9 27.7 25.8 27.0 19.2 16.9 23.3 24.1 LLaVA-1.5-13B 21.6 34.8 26.4 31.9 33.5 29.2"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 7, "text": "100B+ multimodal LLMs. bold indicates the best score; underlines marks the second-best. Model TCM Medicinal Recognition TCM Visual Diagnosis Avg TCM Patent TCM Material TCM Herb Holism Tongue Palm Tuina Eye Multimodal LLMs LLaVA-1.5-7B 25.6 26.9 27.7 25.8 27.0 19.2 16.9 23.3 24.1 LLaVA-1.5-13B 21.6 34.8 26.4 31.9 33.5 29.2 21.4 18.3 27.1 LLaVA-Med 26.2 35.4 23.1 24.8 28.7 26.5 43.5 38.4 30.8 DeepSeek-VL2 30.0 39.1 26.6 28.2 32.4 30.8 47.2 42.0 34.5 MedGemma-4B-it 30.2 40.6 28.0 46.7 49.9 41.5 38.3 38.1 39.2 Gemma-3-27B-it 32.6 40.1 31.8 45.8 48.8 46.2 38.1 45.3 41.1 Llama-3.2-11B-Vision 33.1 43.5 31.1 50.0 53.0 44.5 41.8 41.3 42.3 Janus-Pro-7B 32.5 45.1 33.0 45.7 48.6 45.3 45.8 50.8 43.4 HuatuoGPT-Vision-7B 38.4 53.0 36.2 48.9 52.6 44.9 50.4 49.0 46.7 InternVL3-8B 38.8 58.6 39.0 58.5 54.8 45.2 53.3 47.1 49.4 Qwen2.5-VL-32B-Instruct 39.6 60.3 46.0 61.2 56.4 54.8 51.4 49.2 52.4 Kimi-VL-Instruct-16B 47.1 64.6 46.4 60.5 53.0 52.3 51.9 51.9 53.5 InternVL3-38B 46.1 65.8 48.8 58.2 57.3 49.4 54.8 49.7 53.7 Qwen2.5-Omni-7B 43.8 61.8 51.2 60.8 54.5 53.5 59.5 52.0 54.6 LLaMA-4-Maverick-400B 46.0 67.8 52.0 61.0 62.5 55.0 54.8 49.2 56.0 Gemini-1.5-Pro 49.8 73.5 54.0 56.7 57.2 48.8 59.7 50.7 56.3 GPT-4o 48.0 70.6 54.5 63.2 57.9 57.4 57.5 47.8 57.1 Doubao-1.5-Vision-Pro 52.0 79.3 64.0 59.0 62.8 45.7 58.2 45.2 58.3 ShizhenGPT (Ours) ShizhenGPT-7B 47.4 61.7 49.2 70.6 65.5 63.7 64.1 58.3 60.1 ShizhenGPT-32B 49.5 74.2 54.7 71.5 66.5 60.9 65.9 65.3 63.6 inal recognition and visual diagnosis. See Appendix 9 for details. (3) Human Benchmark: We collect 90 real-world TCM questions and evaluate model responses, with licensed TCM doctors selecting preferred answers (see Appendix 9). (4) Signal Benchmarks: We use the test set of the dataset for smell, auscultation, pulse, and ECG signals, along with a custom pregnancy detection dataset using pulse data. Results Evaluation of TCM Expertise Table 4 reports the per- formance of various models on the latest National TCM Licensing exams and the TCM postgraduate entrance ex- ams. For fair comparison, models are grouped by parame- ter size. In the small-scale category (<15B), ShizhenGPT- 7B achieves the highest average score, outperforming Qwen2.5-7B-Instruct by +9.4 points despite shar- ing the same backbone. It also surpasses larger models such as Qwen2.5-14B-Instruct, and leads all com- pared TCM-specific models in this range. In the mid-scale range (15-100B), ShizhenGPT-32B achieves the strongest results among open-source LLMs, with an average score of 78.1. This exceeds multiple 70B+ models. Although it slightly lags behind ultra-large proprietary models like DeepSeek-R1 (671B) and Doubao-1.5-Pro-32K, it demonstrates highly competitive performance with signifi- cantly fewer parameters (78.1 vs. 84.0). Human Evaluation To further assess TCM capabilities, we conduct an expert evaluation with licensed TCM practi- tioners. Following (Zhang et al. 2023a), we sample 90 real- world TCM queries. For each, our model and a baseline Doubao-1.5-Pro Deepseek-R1 Deepseek-V3 Qwen2.5-72B ChatGPT-4o LLaMA4-Maverick ShizhenGPT Wins Tie ShizhenGPT Loses Figure 3: Results of the human evaluation. ShizhenGPT refers to ShizhenGPT-32B. ”Win/Tie/Loss” indicates the proportion of expert preferences for the model responses (Details are in Appendix 9). generate responses, which are then compared by experts. As shown in Figure 3,"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 8, "text": "Deepseek-R1 Deepseek-V3 Qwen2.5-72B ChatGPT-4o LLaMA4-Maverick ShizhenGPT Wins Tie ShizhenGPT Loses Figure 3: Results of the human evaluation. ShizhenGPT refers to ShizhenGPT-32B. ”Win/Tie/Loss” indicates the proportion of expert preferences for the model responses (Details are in Appendix 9). generate responses, which are then compared by experts. As shown in Figure 3, ShizhenGPT-32B outperforms all base- lines except DeepSeek-R1, including strong contenders like Doubao-1.5-Pro and DeepSeek-V3. The gap with DeepSeek- R1 is small, while ShizhenGPT-32B offers a more efficient alternative given its much smaller size. Figure 3 reports the win/tie/loss results. ShizhenGPT-32B outperforms all baseline models except DeepSeek-R1, including strong con- tenders like Doubao-1.5-Pro and DeepSeek-V3. The perfor- mance gap with DeepSeek-R1 is minimal. Given the sub- stantial inference cost of DeepSeek-R1 with 671B parame- ters, ShizhenGPT-32B offers a compelling trade-off between model size and clinical performance. Evaluation of Visual Capabilities Table 5 presents results on the TCM visual benchmark, which evaluates the ability to recognize medicinal herbs and interpret diagnostic visuals (e.g., tongue, palm). Proprietary models like GPT-4o demon- strate stronger visual reasoning than open-source models such as Qwen2.5-VL. Among baselines, Doubao-1.5-Vision-Pro, a proprietary MLLM, achieves the strongest baseline score of 58.3. ShizhenGPT surpasses all baselines, achieving a new SOTA with 63.6 using only 32B parameters. Notably, even the 7B version performs competitively, highlighting the effectiveness of domain-specific multimodal training for specialized visual understanding in TCM. Table 6: Results on general speech tasks. Model General Audio LLMs Benchmarks LlamaQuestion Speech TriviaQA Speech Web Questions Llama-Omni 45.3 22.9 10.7 Moshi 43.7 23.8 16.7 Qwen2-Audio 60.0 30.4 24.0 ShizhenGPT-7B 64.0 27.3 25.4 Evaluation of General Audio Capabilities Since ShizhenGPT is equipped with audio perception, we evaluate its general speech understanding ability on standard audio benchmarks. As shown in Table 6, ShizhenGPT demonstrates strong performance, comparable to Qwen2-Audio-Instruction. These results indicate that ShizhenGPT is not only compati- ble with various signal modalities but also maintains strong audio comprehension. Table 7: Performance on various TCM signal modalities. All tasks are classification; details in Appendix 9. Random Baselines indicate random prediction accuracy. Modality Task Random Baseline ShizhenGPT (7B) Pulse Pregnancy Detection via Pulse Diagnosis 50.0 80.5 Disease Classification via Pulse Diagnosis 11.1 20.6 Small Disease Classification via Smell Analysis 14.3 48.8 Sound COVID-19 Detection from Cough Sound 50.0 58.7 Cardiac Abnormality Detection from Heart Sound 50.0 62.9 ECG ECG Abnormality Detection 50.0 71.5 ECG Heart Beat Classification 20.0 83.1 Table 8: Ablation study results on ShizhenGPT pretraining. Performance is evaluated across two TCM qualification ex- ams and two TCM-specific visual benchmarks. Model TCM Expertise TCM Visual Capability 2024 TCM Pharmacist 2024 TCM Physician Medicinal Recognition Visual Diagnosis ShizhenGPT-7B 70.1 76.4 52.7 64.5 w/o Pretraining 66.9 (↓3.2) 68.8 (↓7.6) 43.7 (↓9.0) 59.3 (↓5.2) ShizhenGPT-32B 79.2 78.4 59.5 66.0 w/o Pretraining 76.9 (↓2.3) 72.5 (↓5.9) 51.3 (↓8.2) 62.7 (↓3.3) Evaluation of Signal Modalities Table 7 shows model’s performance across signal modalities, such as smell, ECG, and pulse. The model consistently outperforms random base- lines, demonstrating its ability to unify diverse sensory inputs. Remarkably, it achieves 80% accuracy in pregnancy detection from pulse signals alone. These findings provide empirical support for"}
{"doc_id": "2508.14706v1", "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14706v1", "chunk_id": 9, "text": "of Signal Modalities Table 7 shows model’s performance across signal modalities, such as smell, ECG, and pulse. The model consistently outperforms random base- lines, demonstrating its ability to unify diverse sensory inputs. Remarkably, it achieves 80% accuracy in pregnancy detection from pulse signals alone. These findings provide empirical support for the diagnostic relevance of traditional sensory modalities in TCM. Ablation Study on Pre-training We conduct an ablation study to evaluate the role of TCM-specific pre-training. As shown in Table 8, it can be seen that extensive TCM pre- training substantially enhances the TCM expertise and visual understanding capabilities. This confirms that large-scale TCM training is effective for TCM-specific models. I've had a persistent cough for a month, often choking with shortness of breath. ... Could you check my tongue coating, breath, and pulse? Odor Pulse Tongue The tongue is dry and red with a tine, yellow peeled coating, indicating yin deficiency with internal heat or hyperactive fire. The breath suggests possible lung issues, warranting careful evaluation of pulmonary conditions. The pulse indicates a likely pergnancy, so contraindicated medications must be avoided. Based on the symptoms, ... I'm a middle-aged man, and lately, I’ve been feeling short of breath whenever I do any physical activity... In Traditional Chinese Medicine (TCM), your symptoms suggest a complex pattern involving ... I'm 38 years ole, and lately, I've been feeling a deep, lingering heat ... Please check my tongue coating to get more information. From the perspective of Traditional Chinese Medicine (TCM), the tongue appears slightly red with a thin ... Figure 4: Examples responses from ShizhenGPT-32B. Full outputs are provided in Appendix 10. Case Study Figure 4 shows example responses from ShizhenGPT-32B under multimodal inputs. The model can interpret user-provided photos, voice, and pulse signals, cov- ering full TCM diagnostic scenarios and improving decision quality. These capabilities bring AI interaction closer to real- world clinical practice, highlighting ShizhenGPT’s potential as a practical assistant in intelligent TCM diagnostics. Conclusion In this work, we introduce ShizhenGPT, the first multimodal LLM tailored for TCM, along with an extensive TCM dataset. Experiments show that ShizhenGPT surpasses existing LLMs of a similar scale. It leads in visual TCM tasks and demon- strates strong multimodal perception, including sound, pulse, and smell. This extends its diagnostic ability beyond text, en- abling direct analysis features such as the tongue, pulse, and breath sounds, for richer clinical insights. This work moves toward more holistic medical AI and aims to inspire further integration of TCM and AI. Acknowledgments This work was supported by Shenzhen Medical Research Fund (No.C2406002) from the Shenzhen Medical Academy of Research and Translation (SMART), the Shenzhen Science and Technology Program (JCYJ20220818103001002), Shen- zhen Doctoral Startup Funding (RCBS20221008093330065), Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608), Shenzhen Science and Technology Program (Shenzhen Key Laboratory Grant No. ZDSYS20230626091302006), Shenzhen Stability Sci- ence Program 2023, and National Natural Science Foundation of China (NSFC) (72495131)."}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 0, "text": "Salesforce AI Research 2025-08-21 MCP-Universe MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers Ziyang Luo∗ Zhiqi Shen∗ Wenzhuo Yang∗ Zirui Zhao Prathyusha Jwalapuram Amrita Saha Doyen Sahoo Silvio Savarese Caiming Xiong Junnan Li Salesforce AI Research § https://github.com/SalesforceAIResearch/MCP-Universe  https://mcp-universe.github.io Abstract The Model Context Protocol (MCP) has emerged as a transformative standard for connecting large language models (LLMs) to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution- based evaluators, including format evaluators for agent format compliance, static evaluators for time- invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even top-performing models such as GPT-5 (43.72% success rate), Grok-4 (33.33% success rate) and Claude-4.0-Sonnet (29.44% success rate) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown- tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem. 1 Introduction The Model Context Protocol (MCP), introduced by Anthropic [1], represents a major paradigm shift in how AI systems interface with external data sources and tools. Dubbed the “USB-C of AI”[2], MCP addresses the long-standing issue of fragmented, bespoke integrations that trap language models in isolated information silos[3]. Since its release, MCP has gained rapid traction: major AI providers, including OpenAI [4] and Google Gemini [5], have committed to adoption, while development platforms such as Cursor [6] and Cline [7] have begun integrating it to enhance their products. Despite the transformative potential of MCPs, current evaluations remain insufficient for assessing the true capabilities of LLMs operating within this new paradigm. Existing benchmarks predominantly focus on isolated aspects of LLM performance, such as instruction following [8], math reasoning [9], or function calling [10], without providing a comprehensive assessment of how models interact with real-world MCP servers across diverse scenarios. Recently, MCP-RADAR [11] adapts existing benchmarks such as HumanEval [12] and GSM8k [9] to the MCP context. However, these adaptations are largely derivative of existing datasets, which neither capture the full breadth of real-world applications nor adequately address issues such as data leakage. MCPWorld [13], another recently proposed benchmark, continues to rely heavily"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 1, "text": "adapts existing benchmarks such as HumanEval [12] and GSM8k [9] to the MCP context. However, these adaptations are largely derivative of existing datasets, which neither capture the full breadth of real-world applications nor adequately address issues such as data leakage. MCPWorld [13], another recently proposed benchmark, continues to rely heavily on graphical user interfaces (GUIs) and exhibits insufficient coverage of MCP-enabled workflows, thereby limiting its utility for evaluating LLMs in real MCP-driven environments. To address these critical limitations, we introduce our MCP-Universe, a benchmark aiming at evaluating LLMs in realistic, challenging use cases with real-world MCP servers. As shown in Figure 1, MCP-Universe captures realistic ∗Equal Contributions. 1 Salesforce AI Research 2025-08-21 User Instruction (Example is on the right.) Agent (e.g. GPT4.1) MCP Servers Actions Execution-Based Evaluator Final States I live in the Sheraton Jumeirah Beach Resort in Dubai. My colleague lives in the Oryx Hotel in Abu Dhabi. We need to find a meeting point between our two locations. Find a single 'restaurant' (must be of type 'restaurant') such that the estimated driving time from the Sheraton Jumeirah Beach Resort to the restaurant is as close as possible to the driving time from the Oryx Hotel to the restaurant. The restaurant must have a minimum user rating of 4.2. Report the Name and Place ID of the restaurant. Now I have the coordinates for both hotels: Sheraton Jumeirah Beach Resort: 25.0731478, 55.128854 (Dubai) and Oryx Hotel: 24.4691699, 54.3408767 (Abu Dhabi). Let me search for restaurants in the area between these two locations. I'll focus on the midpoint area and find restaurants with good ratings. I'll help you find a restaurant that's equidistant (in driving time) between the Sheraton Jumeirah Beach Resort in Dubai and the Oryx Hotel in Abu Dhabi. Let me start by getting the coordinates for both hotels and then search for restaurants along the route between them. { \"address\": \"Sheraton Jumeirah Beach Resort, Dubai\" } { \"location\": { \"lat\": 25.0731478, \"lng\": 55.128854 }, \"formatted_address\": \"Al Mamsha St - Dubai Marina - Dubai - United Arab Emirates\", ... } Great! I found several restaurants with ratings ≥ 4.2. Now I need to calculate the driving times from both hotels to each of these restaurants to find the one where the driving times are closest to each other. Let me calculate the driving times for the top-rated restaurants that meet our criteria. Based on my analysis, Rasoi Ghar Restaurant has the smallest time difference between the two locations, making it the most equidistant option in terms of driving time. Answer: Name: Rasoi Ghar Restaurant Place ID: ChIJdbsVqrRFXj4RO7_A2D5K-tA This restaurant provides the best compromise location between your two hotels, with the driving time difference being the smallest among all qualifying restaurants. Parameters Called MCP Tool map_gecode { \"address\": \"Oryx Hotel, Abu Dhabi\" } { \"location\": { \"lat\": 24.4691699, \"lng\": 54.3408767 }, \"formatted_address\": \"Zayed the First Street, Near Emirates Post Office, Khalidiya Area ...} Parameters Results Called MCP Tool map_gecode {\"query\": \"restaurant\", \"location\": { \"latitude\": 24.77, \"longitude\": 54.73 }, \"radius\": 50000 } { \"places\": [ { \"name\": \"Rasoi Ghar Restaurant\", \"formatted_address\": \"Yas"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 2, "text": "Dhabi\" } { \"location\": { \"lat\": 24.4691699, \"lng\": 54.3408767 }, \"formatted_address\": \"Zayed the First Street, Near Emirates Post Office, Khalidiya Area ...} Parameters Results Called MCP Tool map_gecode {\"query\": \"restaurant\", \"location\": { \"latitude\": 24.77, \"longitude\": 54.73 }, \"radius\": 50000 } { \"places\": [ { \"name\": \"Rasoi Ghar Restaurant\", \"formatted_address\": \"Yas Mall The Fountains @ Yas Mall Ferrari World Main Entrance - ..., Parameters Results Called MCP Tool search_places Results {\"origins\": [ \"25.073,55.128\" ], \"destinations\": [ \"24.485,54.607\",... ], \"mode\": \"driving\" } { \"origin_addresses\": [ \"Sheraton Jumeirah - Dubai Marina - Dubai - United Arab Emirates\" ], \"destination_addresses\": [ \"33 Al Khuyoul St - Yas Island... Results Called MCP Tool distance_matrix Parameters Google Map MCP Server Step 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Step N Observations Figure 1: Example from MCP-Universe illustrating realistic challenges, including real-world tool usage, long-horizon multi-turn tool calls, long context windows, scattered evidence, and large tool spaces. Unlike prior work, MCP- Universe is grounded in real-world MCP servers connected to actual data sources and environments. challenges: real-world tools usage, long-horizon multi-turn tool calls, long context windows, scattered evidence, and large tool spaces. Unlike existing works, MCP-Universe is grounded in real-world MCP servers that connect to actual data sources and environments. Our benchmark encompasses 6 core domains, with 11 MCP servers spanning diverse applications: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching, comprising a total of 231 tasks. Each domain captures the operational complexities of real-world deployments, from handling authentic financial data and navigating complex geospatial information to managing version control workflows and executing real-time ticket price checks. To ensure rigorous evaluation, we carefully design execution-based evaluators rather than relying on LLM-as-a- judge [14] (e.g., MCPEval [15] and LiveMCPBench [16]), recognizing that many tasks involve real-time data that static LLM knowledge cannot properly assess. Our evaluation includes format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically obtain real-time ground truth for temporally sensitive tasks. Furthermore, for the evolving nature of MCP servers, we provide an extensible, user- friendly framework that enables researchers and the broader community to seamlessly integrate new agents and MCP servers into the evaluation pipeline. Our benchmark is also equipped with a UI for intuitive, user-friendly access. We conducted extensive experiments using MCP-Universe across all 6 core domains and 11 different MCP servers. Through extensive evaluation of leading LLMs, we find that even top-performing models such as GPT- 5 (43.72% success rate), Grok-4 (33.33% success rate), and Claude-4.0-Sonnet (29.44% success rate) exhibit significant performance limitations, revealing a substantial gap between their impressive general capabilities and their effectiveness in real-world MCP environments. Our comprehensive analysis identifies several fundamental challenges that current LLM agents face in MCP interactions. First, we observe a long-context challenge, as the number of tokens increases rapidly with the number of interaction steps, often leading to context overflow and degraded performance in multi-step tasks. Second, there exists an unknown-tools challenge, where LLM agents often lack familiarity with the precise usage patterns, parameter specifications, and expected behaviors"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 3, "text": "observe a long-context challenge, as the number of tokens increases rapidly with the number of interaction steps, often leading to context overflow and degraded performance in multi-step tasks. Second, there exists an unknown-tools challenge, where LLM agents often lack familiarity with the precise usage patterns, parameter specifications, and expected behaviors of diverse MCP servers. Additionally, our evaluation reveals significant cross-domain performance variations, with models showing markedly different success rates across different application domains, suggesting domain-specific optimization needs. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks, highlighting the challenges of our benchmark. In summary, this work makes the following key contributions: • We introduce MCP-Universe, the first comprehensive benchmark for evaluating LLMs in MCP environments across six domains with real-world servers, where even SOTA LLMs struggle. • We develop a rigorous execution-based evaluation framework with format, static, and dynamic assessment capabilities to enable comprehensive real-world performance measurement. • We reveal fundamental limitations of current LLM agents, such as challenges with long contexts, handling unknown tools, and cross-domain discrepancies, thereby highlighting directions for future agent design. 2 Salesforce AI Research 2025-08-21 2 Related Work Model Context Protocol. MCP was introduced by Anthropic in late 2024 as an open standard designed to streamline AI system integration with external data sources and tools [1]. MCP addresses the notorious “data silo problem” by providing a universal interface that connects AI systems to diverse data sources and tools through standardized JSON- RPC 2.0 messaging over STDIO and SSE transports [3]. The protocol follows a three-layer architecture comprising MCP hosts (AI applications), clients (communication bridges), and servers (capability providers) that expose tools, prompts, and resources through standardized interfaces. LLMs as Agents. LLMs have evolved from simple text-generating chatbots into sophisticated agents capable of autonomous planning, reasoning, and tool use [17]. This transformation has been driven by advances in instruction following [8, 18–20], multi-step reasoning [21–24], and tool integration [25–28]. Key agent design patterns have emerged, such as ReAct [29], Reflection [30], and Plan-and-Solve [31]. Frameworks like AutoGen [32], MetaGPT [33], Camel-AI [34], and LangGraph [35] have demonstrated practical implementations of autonomous agent systems. With the advancement of multimodal LLMs [36, 37], a new class of GUI-based computer-use agents has emerged [38–41]. Systems such as OpenAI’s CUA [42], Anthropic’s Computer-Use [43], and ByteDance’s UI- Taris [44] mimic human interaction with graphical interfaces, giving rise to a new frontier in computer automation applications. Table 1: Comparative Analysis of Contemporary MCP Benchmarks. Benchmark Real-World Integration Temporal Dynamics Exec. Eval. MCPWorld ✓ × ✓ MCP-RADAR × × ✓ MCPEval × ✓ × LiveMCPBench ✓ ✓ × MCP-Universe ✓ ✓ ✓ Evaluation of Agents. The evaluation of LLM- based agents has become a key research area, with a variety of benchmarks developed to assess different aspects of agent capabilities. One major focus is web navigation, with benchmarks such as MiniWob++ [45], Mind2Web (1 & 2) [46, 47], WebLINX [48], AssistantBench [49], WebArena [50], VisualWebArena, and VideoWebArena [51] providing comprehensive environments to test agents’ ability to interact with realistic web applications. A second focus is GUI-based interaction, exemplified by OSWorld"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 4, "text": "One major focus is web navigation, with benchmarks such as MiniWob++ [45], Mind2Web (1 & 2) [46, 47], WebLINX [48], AssistantBench [49], WebArena [50], VisualWebArena, and VideoWebArena [51] providing comprehensive environments to test agents’ ability to interact with realistic web applications. A second focus is GUI-based interaction, exemplified by OSWorld [52], WindowsAgentArena [53], and UI-Vision [54], which evaluate agents on their ability to operate computer interfaces similarly to human users. A third area is software engineering, with benchmarks like SWE-bench [55] and DevBench [56] designed to evaluate agents’ capabilities in code generation and software development tasks. Another important dimension is function/tool calling, covered by APIBank [57], ToolBench [58], GAIA [59], AppWorld [60], τ-Bench [61], and BFCLv3 [10], which assess an agent’s proficiency in invoking external tools and APIs to complete complex tasks. In contrast to existing benchmarks, several contemporary MCP-related benchmarks have recently emerged, as shown in Table 1. MCPWorld [13] evaluates agents in real-world GUIs and the MCP environment, but it relies heavily on the GUI and does not include time-varying tasks. MCP-RADAR [11] transforms existing datasets, such as HumanEval and GSM8k, into MCP scenarios. Although it includes execution-based evaluation, its tasks are less related to real- world applications, and the ground truth does not change over time. The latter two benchmarks, MCPEval [15] and LiveMCPBench [16], both adopt the LLM-as-a-Judge evaluation, which is not suitable for tasks requiring real- time knowledge; moreover, LLM judges are also known for style bias [62]. In contrast to these works, MCP- Universe provides a comprehensive evaluation framework that integrates real-world integration, temporal dynamics, and execution-based evaluation. It evaluates agents on authentic MCP servers with time-sensitive scenarios and actual task completion metrics, addressing the limitations of existing benchmarks. 3 MCP-Universe 3.1 Overview MCP-Universe is a comprehensive evaluation framework designed to assess the capabilities of LLMs when interacting with real-world MCP servers for challenging and practical use cases. As shown in Figure 2, our benchmark encompasses three core components: (1) an extensible, easy-to-use evaluation framework; (2) a collection of carefully designed task instructions grounded in real-world MCP server scenarios; (3) a suite of execution-based evaluators for measuring task completion. 3 Salesforce AI Research 2025-08-21 Task, MCP Server, Evaluator Configuration I live in the Sheraton Jumeirah Beach Resort in Dubai. My colleague lives in the Oryx Hotel in Abu Dhabi. We need to find a meeting point between our two locations. Find a single 'restaurant' (must be of type 'restaurant') such that the estimated driving time from the Sheraton Jumeirah Beach Resort to the restaurant is as close as possible to the driving time from the Oryx Hotel to the restaurant. The restaurant must have a minimum user rating of 4.2. Report the Name and Place ID of the restaurant. GPT-4.1 Claude-4.0 Others ... LLM Manager ReAct Explore ReAct Others ... Agent Builder Google Map Github Blender MCP Server Playwright Others ... Content Match Format Check Others ... Execution-Based Evaluator STDIO SSE Google Map MCP Server Based on my analysis, Rasoi Ghar Restaurant has the smallest time difference between the two locations, making it the most equidistant"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 5, "text": "Explore ReAct Others ... Agent Builder Google Map Github Blender MCP Server Playwright Others ... Content Match Format Check Others ... Execution-Based Evaluator STDIO SSE Google Map MCP Server Based on my analysis, Rasoi Ghar Restaurant has the smallest time difference between the two locations, making it the most equidistant option in terms of driving time. Answer: Name: Rasoi Ghar Restaurant Place ID: ChIJdbsVqrRFXj4RO7_A 2D5K-tA google_maps.is_a_validate_stop google_maps.validate_stop_type google_maps.validate_stop_type google_maps.compare_time_... google_maps.stop_include_keys Evaluator Final State Pass or Not: Fail Agent (e.g. GPT4.1) User Instruction Figure 2: Overview of the MCP-Universe evaluation framework. The framework dynamically configures LLM agents, MCP servers, and execution-based evaluators according to task specifications. Each evaluation involves the agent- server interactions mediated via the MCP protocol, followed by an objective assessment conducted by automated execution-based evaluators to determine the success of task completion. To formalize the setting, we model the benchmark as follows. Let S = {s1, s2, . . . , sk} denote the collection of MCP servers, where each server si exposes a set of tools Ti = {ti,1, ti,2, . . . , ti,|Ti|} through the MCP protocol. A task τ is defined as a tuple (G, C, Tavailable), where: • G is the goal specification describing the desired outcome; • C contains the initial context and any relevant background information; • Tavailable = S i∈I Ti is the set of tools accessible for the task, with I ⊆{1, . . . , k} indicating which servers are available. The benchmark challenges an agent to identify, sequence, and invoke appropriate tools from Tavailable to achieve G given C, requiring reasoning over partial information, adapting to diverse tool interfaces, and handling ambiguities or failures in tool responses. For evaluation, let M = {m1, m2, . . . , mn} be the set of language models and A = {a1, a2, . . . , ap} be the set of agent architectures (e.g., ReAct) that can be paired with them. For a given (m, a) ∈M × A and task τ, the interaction produces a conversation trace R = (r1, r2, . . . , rT ), where each rt contains the agent’s output and any tool invocations. The evaluation function E : M × A × T →{0, 1} assigns 1 if the task is successfully completed according to predefined success criteria, and 0 otherwise. Success is determined through a combination of automated checks (e.g., verifying structured outputs or end states). Aggregating E(m, a, τ) over all tasks yields a quantitative measure of an agent’s proficiency in MCP-driven tool use. 3.2 Evaluation Framework As illustrated in Figure 2, our evaluation framework seamlessly coordinates multiple components to deliver objective and reproducible assessment results. Given the task specifications, the framework begins with an automatic configuration that dynamically orchestrates the evaluation pipeline. The framework automatically builds the LLM- agent combination, selects the required MCP servers, and configures the corresponding evaluators. The configuration also handles resource allocation, API endpoint management, and evaluation criteria specification. In detail, an LLM Manager is introduced in the framework, which supports multiple SOTA LLMs including GPT-5 and Claude-4.0-Sonnet."}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 6, "text": "The framework automatically builds the LLM- agent combination, selects the required MCP servers, and configures the corresponding evaluators. The configuration also handles resource allocation, API endpoint management, and evaluation criteria specification. In detail, an LLM Manager is introduced in the framework, which supports multiple SOTA LLMs including GPT-5 and Claude-4.0-Sonnet. This component handles LLM configuration, API management, and standardized prompt formatting to ensure consistent evaluation conditions across different models. In addition, the Agent Builder component constructs specialized agents. It supports multiple agents including ReAct and ReAct with Exploration. The builder configures agents with appropriate reasoning strategies for MCP server communication. This modular design allows for systematic comparison of different agents within the same evaluation framework. Moreover, our framework seamlessly integrates with diverse MCP servers representing real-world tools and services. Each server is configured with its authentic API endpoints, authentication mechanisms, and tool specifications, ensuring that evaluation tasks mirror real-world environments rather than simplified simulations. The framework 4 Salesforce AI Research 2025-08-21 Location Navigation 19.3% Dist. Optimization 8.9% Time Optimization 3.9% Route Planning 4.3% Place Finding 2.2% Repo. Management 14.2% Project Setup 5.6% Issue Automation 3% Repo. Analysis 3% Repo. Forking 2.6% Financial Analysis 17.2% Portfolio Analysis 3.4% Fin. Statements 3.4% Trading Strategies 6.4% Inst. Holdings 2.2% Dividend Options 1.8% 3D Designing 8.6% Object Creation 2.6% Material Setup 2.6% Lighting Conﬁg. 1.3% Render Settings 2.1% Browser Automation 16.7% Travel Booking 3.4% Sports Analytics 3.9% Academic Research 3.4% Platform Exploration 3.4% Map Navigation 2.6% Web Searching 24% Person Ident. 8.6% Entity Discovery 7.3% Metric Matching 1.7% Complex Reason 3.9% Factual Lookup 2.5% Figure 3: Distribution of tasks in MCP-Universe across different application domains. Table 2: Key statistics in MCP-Universe. Statistic Number Total tasks 231 (100%) - Location Navigation 45 (19.5%) - Web Searching 55 (23.8%) - Browser Automation 39 (16.9%) - 3D Designing 19 (8.2%) - Financial Analysis 40 (17.3%) - Repo. Management 33 (14.3%) Total MCP Servers 11 Total Tools in Servers 133 Total Unique Evaluators 84 (100%) - Format Evaluators 4 (4.8%) - Static Evaluators 32 (38.1%) - Dynamic Evaluators 48 (57.1%) handles dynamic server configuration based on task specification, allowing for both single-server and multi-server evaluation scenarios. The evaluation process employs an execution-based approach that validates task completion through automated assessment rather than subjective LLMs judgment or costly human annotation. The evaluator implements domain- specific validation strategies including stop type validation for Google Maps and branch checking for GitHub. The complete evaluation workflow proceeds through agent-server interactions mediated via the MCP protocol, followed by automated assessment that produces binary pass/fail determinations based on objective validation criteria. The framework captures detailed interaction logs to provide comprehensive insights into model performance across different scenarios and server types. 3.3 Real-World MCP Servers A foundational design principle of MCP-Universe is its reliance on real-world MCP servers, as opposed to simulated environments. This approach ensures that evaluation is grounded in the authentic complexities of practical applications. As shown in Table 2, the benchmark includes 11 distinct MCP servers with total 133 tools, each reflecting a unique application domain. These servers are organized into 6 core domains:"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 7, "text": "servers, as opposed to simulated environments. This approach ensures that evaluation is grounded in the authentic complexities of practical applications. As shown in Table 2, the benchmark includes 11 distinct MCP servers with total 133 tools, each reflecting a unique application domain. These servers are organized into 6 core domains: 1. Location Navigation: This domain focuses on geographic reasoning and spatial task execution in real-world environments. We employ the official Google Maps MCP server, which provides a rich suite of geospatial tools such as location search, route planning, and distance computation. Models must navigate the full complexity of real-world location data to complete navigation tasks effectively. 2. Repository Management: This domain focuses on repository development and codebase operations. We employ the GitHub MCP server, which exposes authentic version control tools such as repository search, issue tracking, and code editing. This setup reflects the operational demands of real-world development. 3. Financial Analysis: This domain focuses on quantitative reasoning and decision-making in dynamic financial markets. We utilize the Yahoo Finance MCP server, which provides tools such as stock price monitoring, shareholder information checking, and options tracking, all based on live financial data. The server poses substantial challenges in parsing and reasoning over real-time market information. 4. 3D Designing: This domain focuses on computer-aided design. We employ the Blender MCP server, which enables interaction with advanced tools for 3D modeling, such as object creation, asset manipulation, and material setup. This domain captures the technical depth of professional design environments. 5. Browser Automation: This domain focuses on automated interaction with web applications and interfaces. We employ the Playwright MCP server, which provides full-featured web automation capabilities such as 5 Salesforce AI Research 2025-08-21 browser navigation, button clicking, and page snapshotting. It represents real-world browser control scenarios encountered in modern automation pipelines. 6. Web Searching: This domain focuses on open-domain information seeking. The Google Search MCP server is integrated to support web-based information seeking, and the Fetch MCP server is adopted to obtain the content for a given URL. This domain captures the open-ended nature of real-world web searching tasks. The selection of these domains and specific servers reflects our commitment to domain diversity and real-world relevance. In addition to the above MCP servers, we also incorporate additional MCP servers that provide necessary support and contribute to increasing task complexity, such as the Notion MCP server, Weather MCP server, Date MCP server, and Calculator MCP server. More details on these MCP servers can be found in Appendix A. 3.4 Tasks and Evaluators Since MCP is a new concept and there is a lack of high-quality usage examples, we manually designed challenging MCP tasks to reflect real use cases. If a task can be easily completed by LLMs without using MCP servers, or can be consistently solved with MCP servers within five retries, we consider it a simple task and brainstorm a new one. As shown in Figure 3, for each domain, we carefully create 4-5 types of tasks to cover the most common usage scenarios. For the location navigation domain, we focus on 4 sub-task types, including"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 8, "text": "MCP servers within five retries, we consider it a simple task and brainstorm a new one. As shown in Figure 3, for each domain, we carefully create 4-5 types of tasks to cover the most common usage scenarios. For the location navigation domain, we focus on 4 sub-task types, including route planning, optimal stops, location searching, and place finding. For the repository management domain, we focus on 4 sub-task types, including project setup, issue tracking, automation setup, and code integration. For the financial analysis domain, we focus on 5 sub- task types, including portfolio analysis, financial statements, trading strategies, institutional holdings, and dividend analysis. For the 3D designing domain, we focus on 5 sub-task types, including object creation, material setup, lighting configuration, render settings, and scene hierarchy. For the browser automation domain, we focus on 5 sub- task types, including travel booking, sports analytics, academic research, platform exploration, and map navigation. For the web searching domain, we focus on 5 sub-task types, including person identification, entity discovery, metric matching, complex reasoning, and factual lookup. After the task creation, each task will be cross-checked by the other authors for feasibility, ambiguity, and correctness. To evaluate the completion of tasks, we have chosen to carefully design execution-based evaluators for each task. For simplicity, many recent works choose to follow the LLM-as-a-judge paradigm. However, we argue that this paradigm is not well-suited for our MCP-Universe scenario, since some tasks are designed to use real-time data, while the knowledge of the LLM judge is static. In addition, the LLM judge is also known to contain style bias and hallucinations. Thus, we choose to follow the execution-based paradigm to evaluate the completion of tasks. Although this relies on much heavier human labor, we believe this is the only way to achieve a fair and comprehensive evaluation. For all tasks, we can divide the evaluators into three types: (1) Format Evaluators, (2) Static Evaluators, and (3) Dynamic Evaluators. The first type evaluates whether agents strictly follow format requirements. The second type assesses correctness for tasks whose answers do not change over time, such as the number of cities in route planning tasks, the exact number of goals scored by a football player in browser automation tasks, or historical stock prices in financial analysis tasks. For these, we manually collect the correct answers and write evaluators to check whether the model’s outputs meet the requirements. For the third type, the correct answer of the task needs to be updated with real-time data, such as the price of a flight on a future date for the travel booking tasks, the weather of a place in the place-finding tasks, and the number of GitHub issues in the issue tracking tasks. Here, we design automatic evaluators to obtain real-time correct answers and verify task completion, which can provide stable evaluation results across different timestamps. After the evaluator creation, each evaluators will be cross-checked by the other authors for feasibility, ambiguity, and correctness. In Appendix B, we include the examples of tasks and evaluators in our benchmark. 4 Experiment 4.1 Setup In our"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 9, "text": "verify task completion, which can provide stable evaluation results across different timestamps. After the evaluator creation, each evaluators will be cross-checked by the other authors for feasibility, ambiguity, and correctness. In Appendix B, we include the examples of tasks and evaluators in our benchmark. 4 Experiment 4.1 Setup In our experiments, we evaluate the performance of SOTA proprietary and open-source LLMs on our MCP-Universe benchmark. The models include xAI’s Grok-4 [63], Anthropic’s Claude-4.0-Sonnet [64] and Claude-3.7-Sonnet [65], OpenAI’s GPT-5 [66], o3, o4-mini [67],GPT-5 [66], GPT-4.1 [68], GPT-4o [36], GPT-OSS [69], Google’s Gemini-2.5- Pro and Gemini-2.5-Flash [70], Zai’s GLM-4.5 [71], Moonshot’s Kimi-K2 [72], Qwen’s Qwen3-Coder, and Qwen3- 235B-A22B-Instruct-2507 [73], and DeepSeek’s DeepSeek-V3-0324 [74]. All LLMs are top-ranked on the well- known lmsys Chatbot Arena leaderboard [75]. All open-source LLMs have at least 120B parameters. For the agent, 6 Salesforce AI Research 2025-08-21 Table 3: Comparison on our MCP-Universe benchmark. For our main experiments, all LLMs follow the ReAct agent pipeline, except GPT-OSS, which has poor instruction-following abilities and therefore cannot follow the ReAct prompt; for this model, we use the OpenAI Agent SDK instead. We report the success rate (SR, %) for each domain and all tasks. Additionally, we calculate the average percentage of evaluators passed for each task, which we refer to as the average evaluator score (AE). Moreover, we also report the average number of steps (AS) for each successful task. Since GPT-OSS does not follow ReAct, it does not have AS scores. Model Location Navigation Repository Management Financial Analysis 3D Designing Browser Automation Web Searching Overall SR AE AS Proprietary Models GPT-5 33.33 30.30 67.50 52.63 35.90 45.45 43.72 60.23 8.22 Grok-4 28.89 12.12 40.00 26.32 41.03 41.82 33.33 49.01 7.75 Claude-4.0-Sonnet 22.22 12.12 55.00 26.32 38.46 21.82 29.44 50.61 7.46 o3 26.67 6.06 40.00 26.32 25.64 29.09 26.41 38.95 4.82 o4-mini 26.67 18.18 40.00 36.84 23.08 18.18 25.97 40.38 7.90 Claude-3.7-Sonnet 13.33 18.18 40.00 36.84 23.08 21.82 24.24 40.36 7.16 Gemini-2.5-Pro 13.33 12.12 50.00 21.05 25.64 12.73 22.08 36.93 6.98 Gemini-2.5-Flash 15.56 12.12 37.50 21.05 30.77 14.55 21.65 33.99 8.26 GPT-4.1 8.89 6.06 40.00 26.32 23.08 10.91 18.18 41.32 5.24 GPT-4o 8.89 9.09 35.00 26.32 12.82 9.09 15.58 37.03 6.03 Open-Source Models GLM-4.5 17.78 9.09 50.00 26.32 15.38 27.27 24.68 41.16 7.33 Kimi-K2 11.11 9.09 47.50 15.79 15.38 14.55 19.05 35.10 6.07 Qwen3-Coder 8.89 3.03 50.00 26.32 25.64 10.91 19.91 37.78 7.78 Qwen3-235B 11.11 9.09 50.00 15.79 15.38 9.09 18.18 38.53 5.74 DeepSeek-V3 11.11 6.06 30.00 26.32 12.82 7.27 14.29 35.82 5.06 GPT-OSS-120B 6.67 6.06 35.00 10.53 5.13 5.45 11.26 26.34 - we adopt the most popular framework, ReAct [29]. The LLMs first generate a thought based on the observation, and then generate the next action based on that thought. More setup details can be found in the Appendix C. 4.2 Frontier Models Performance Table 4: Success rate across different types of evaluators on our MCP-Universe benchmark. Model Format Static Dynamic GPT-5 88.89 61.92 65.96 Grok-4 88.03 49.04 52.98 Claude-4.0-Sonnet 98.29 61.92 54.74 o3 73.50 38.63 43.16 o4-mini 78.63 44.66 43.86 Claude-3.7-Sonnet 83.76 43.84 44.91 Gemini-2.5-Pro 64.10 39.18 42.46"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 10, "text": "C. 4.2 Frontier Models Performance Table 4: Success rate across different types of evaluators on our MCP-Universe benchmark. Model Format Static Dynamic GPT-5 88.89 61.92 65.96 Grok-4 88.03 49.04 52.98 Claude-4.0-Sonnet 98.29 61.92 54.74 o3 73.50 38.63 43.16 o4-mini 78.63 44.66 43.86 Claude-3.7-Sonnet 83.76 43.84 44.91 Gemini-2.5-Pro 64.10 39.18 42.46 Gemini-2.5-Flash 51.28 45.21 30.88 GPT-4.1 95.73 57.53 49.47 GPT-4o 91.45 54.79 45.61 GLM-4.5 81.20 46.30 48.07 Kimi-K2 70.94 33.15 53.33 Qwen3-Coder 75.86 38.74 43.16 Qwen3-235B 92.31 43.29 53.68 DeepSeek-V3 96.58 52.88 48.07 As shown in Table 3, we compare the performance of SOTA proprietary and open source LLMs on our MCP-Universe benchmark. The results indicate that OpenAI’s GPT-5 achieves the highest overall success rate at 43.72%, significantly outperforming other models. Grok-4 ranks second with 33.33% success rate, followed by Claude-4.0-Sonnet at 29.44%. GPT- 5 demonstrates particularly strong performance in the Financial Analysis domain (67.50%) and 3D Designing domain (52.63%), while also achieving the highest success rate in Web Searching (45.45%). Notably, Grok-4 excels in Browser Automation (41.03%) domain, which requires strong reasoning and the ability to operate in complex internet environments. In the Location Navigation domain, all LLMs perform poorly, with success rates below 35%; notably, popular models like GPT-4.1 and GPT-4o score under 10%. Similarly, in the Repository Management domain, only GPT-5 surpasses a 30% success rate. In the 3D Design domain, only GPT-5 exceeds a 50% success rate, while other top models remain below 40%. When comparing proprietary and open-source models, we find that GLM-4.5 is the best open-source LLM with 24.68% overall success rate, achieving a higher success rate than some proprietary models like o4-mini and Claude-3.7-Sonnet. However, the gap between SOTA proprietary LLMs and their open-source counterparts remains substantial. Beyond success rates, we also evaluate each model based on (i) the percentage of evaluators they can satisfy, measured via average evaluator (AE) scores, and (ii) the average number of steps (AS) taken to complete successful tasks. While higher AE scores often correlate with higher success rates, the relationship is not always direct. GPT-5 achieves both 7 Salesforce AI Research 2025-08-21 2 4 6 8 10 12 14 16 Step Number 0 20000 40000 60000 80000 Average Input Tokens LN BA FA Location Navigation Browser Automation Financial Analysis 0 10 20 30 40 50 60 70 Success Rate (%) 8.89 20.0022.2224.44 23.08 20.51 38.46 30.77 40.0040.00 55.00 42.50 GPT-4.1 GPT-4.1 w/ summarize Claude-4.0-Sonnet Claude-4.0-Sonnet w/ summarize Figure 4: (Left) Growth of average context length (in tokens) as the number of interaction steps increases in MCP- Universe tasks, illustrating the long context challenge. (Right) Effect of introducing a summarization agent on LLM agent performance across selected domains. the highest success rate (43.72%) and the highest AE score (60.23%), demonstrating strong consistency. However, some models show interesting discrepancies: Claude-4.0-Sonnet passes 50.61% of evaluators, slightly higher than Grok-4’s 49.01%, yet Grok-4 achieves a higher overall success rate (33.33% vs. 29.44%). Regarding task efficiency, o3 requires only 4.82 average steps despite ranking fourth in success rate (26.41%), making it the most efficient model. In contrast, the top-performing models GPT-5 (8.22 steps) and Grok-4"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 11, "text": "of evaluators, slightly higher than Grok-4’s 49.01%, yet Grok-4 achieves a higher overall success rate (33.33% vs. 29.44%). Regarding task efficiency, o3 requires only 4.82 average steps despite ranking fourth in success rate (26.41%), making it the most efficient model. In contrast, the top-performing models GPT-5 (8.22 steps) and Grok-4 (7.75 steps) require significantly more steps. Most open-source models complete successful tasks in 5-7 steps, with DeepSeek-V3 being the most efficient at 5.06 steps. These findings highlight that current frontier LLMs still fall short in reliably executing tasks across diverse real world MCP tasks. Our MCP-Universe benchmark therefore provides a challenging and necessary testbed for evaluating LLM performance in areas underserved by existing benchmarks. Furthermore, our benchmark incorporates three evaluator types: format evaluators, static evaluators, and dynamic evaluators. Table 4 presents a breakdown of model performance across these types. Non-reasoning LLMs, such as Claude-4.0-Sonnet,1 GPT-4.1, GPT-4o, Qwen3-235B, and DeepSeek-V3, achieve over 90% success with format evaluators, with Claude-4.0-Sonnet leading at 98.29%. In contrast, reasoning models like o3 (73.50%), Gemini-2.5- Pro (64.10%), and Gemini-2.5-Flash (51.28%) perform significantly worse on format evaluators, suggesting that such models are less adept at adhering to strict formatting instructions. We highlight the naive error in the Appendix D. On content-sensitive static evaluators, GPT-5 and Claude-4.0-Sonnet both achieve the highest performance at 61.92%, while most other models achieve around 40-50% success. For dynamic evaluators, GPT-5 leads with 65.96%, followed by Claude-4.0-Sonnet at 54.74%. The substantial performance gap between format evaluators (where many models exceed 80%) and content evaluators (where most models achieve 40-60%) indicates that the primary source of failure lies in content generation rather than format compliance. This demonstrates that our benchmark evaluates LLMs from multiple angles, including format compliance and content correctness under both static and dynamic conditions, making it a comprehensive testbed for model assessment. 4.3 Long Context Challenges In our MCP Universe benchmark, long context handling poses a significant challenge for LLM agents, particularly in the Location Navigation, Browser Automation, and Financial Analysis domains. These domains frequently require agents to process and reason over lengthy sequences of observations or historical actions, which often exceed the context window limits of many models. In the Location Navigation domain, the Google Maps MCP servers can return extensive location data within the context, including detailed information about multiple places. In the Browser Automation domain, the Playwright MCP servers may return the full HTML content of a webpage, which can be very large. Similarly, in the Financial Analysis domain, the Yahoo Finance MCP servers provide daily stock information over a specified date range, and this can result in a large volume of contextual data if the time span is long. As shown in the left of Figure 4, we observe that the number of tokens increases rapidly as the number of interaction steps grows. This demonstrates that long context is one of the key challenges presented by our benchmark.2 To explore potential solutions, we conducted a preliminary experiment (the right of Figure 4) by introducing a summarization agent at each step, designed to compress the raw outputs of the MCP"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 12, "text": "of interaction steps grows. This demonstrates that long context is one of the key challenges presented by our benchmark.2 To explore potential solutions, we conducted a preliminary experiment (the right of Figure 4) by introducing a summarization agent at each step, designed to compress the raw outputs of the MCP servers. This summarizer attempts to reduce context length while preserving essential information. The details of this summarization agent are shown in the Appendix E. However, this approach yields mixed results. While it leads to improved success rates for GPT-4.1 and 1We do not use the thinking mode. 2The context length experiment is based on the Claude-4.0-Sonnet. 8 Salesforce AI Research 2025-08-21 I heard about this Price Breakout Strategy thing on some YouTube video. From what I gathered, you look at the highest closing price in the last 20 days, and if the price today goes above that, you buy. If it drops 3% below the recent high, you sell. I'm thinking about putting it all on Exxon Mobil (XOM). I need to know if December 1st, 2023 would've been a buy, sell, or hold signal with this strategy. I can't afford to mess this up. This might be my only shot at making it. Please help me figure this out, I'm counting on this to change my life. Thought: To determine whether December 1, 2023, would have been a buy, sell, or hold signal with this strategy, I need the closing price of XOM on that date. I will first try to retrieve the historical price for XOM. Action: Server: Yahoo Finance MCP Server, Tool: get_historical_stock_prices Arguments: {\"ticker\": \"XOM\", \"start_date\": \"2023-12-01\", \"end_date\": \"2023-12- 01\", \"interval\": \"1d\" } Server Return: Error: getting stock information for XOM: The start date and end date cannot be the same. Location Navigation Browser Automation Financial Analysis 0 10 20 30 40 50 60 70 Success Rate (%) 8.89 20.0022.2224.44 23.08 30.77 38.46 33.33 40.0040.00 55.00 62.50 GPT-4.1 GPT-4.1 w/ exploration Claude-4.0-Sonnet Claude-4.0-Sonnet w/ exploration Figure 5: (Left) An example of the unknown tool challenges. (Right) Effect of introducing the exploration phase on LLM agent performance across selected domains. Claude-4.0-Sonnet in the Location Navigation domain, it either has no effect or negatively impacts performance in the Browser Automation and Financial Analysis domains. These findings indicate that MCP-Universe introduces unique and realistic long context challenges in agent-based tasks. Simple summarization methods are insufficient to address these issues. As such, our benchmark serves as a valuable testbed for evaluating and developing long context handling in LLM agent systems. 4.4 Unknown Tools Challenges In addition to the long context challenges, our error analysis reveals that LLMs often struggle to correctly use tools provided by the MCP servers, indicating a lack of familiarity with their interfaces and constraints. For example, Figure 5 (left) illustrates a common failure in the Yahoo Finance MCP server: retrieving a stock price requires specifying a start and end date that differ, yet LLMs frequently set them to be identical, leading to execution errors. To address this issue, we introduce an additional step called the exploration phase. During"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 13, "text": "(left) illustrates a common failure in the Yahoo Finance MCP server: retrieving a stock price requires specifying a start and end date that differ, yet LLMs frequently set them to be identical, leading to execution errors. To address this issue, we introduce an additional step called the exploration phase. During this phase, the LLM is allowed to freely interact with the tools provided by the MCP servers. This gives the model an opportunity to learn how the tools work and to build knowledge of their capabilities. In the subsequent exploitation phase, the LLM uses this acquired tool knowledge, combined with a ReAct-style framework, to solve the actual tasks. The details are included in the Appendix F. As shown in Figure 5 (right), incorporating an exploration phase leads to performance improvements in certain domains. For instance, GPT-4.1 achieves a 30.77% success rate in the Browser Automation domain, an improvement of 7.69 percentage points. Similarly, Claude-4.0-Sonnet reaches a 62.50% success rate in the Financial Analysis domain, a 7.50 percentage point increase. However, this approach is not universally beneficial. Claude-4.0-Sonnet’s performance in the Browser Automation domain declines, and GPT-4.1 shows no improvement in the Financial Analysis domain. These mixed results suggest that while the exploration phase can help some LLMs perform better in specific domains (e.g., information seeking and reasoning), it is not a one-size-fits-all solution (e.g., planning for state changes). Achieving strong performance on the MCP-Universe benchmark requires more robust and adaptive strategies, emphasizing the benchmark’s difficulty and its value as a testing ground for advanced LLM agents. 4.5 More MCP Servers Connected Location Navigation Browser Automation Financial Analysis 0 10 20 30 40 50 60 70 Success Rate (%) 8.89 13.33 22.22 11.11 23.08 15.38 38.4638.46 40.00 35.00 55.00 62.50 GPT-4.1 GPT-4.1 w/ more servers Claude-4.0-Sonnet Claude-4.0-Sonnet w/ more servers Figure 6: Effect of connecting with more unrelated MCP servers. In Table 3, we only connect the MCP servers that are directly relevant to each task. In this section, we extend the setup by connecting additional, unrelated MCP servers to the LLMs to assess their performance under increased tool complexity. For all tasks, we connect a total of 7 MCP servers, comprising 94 tools. This configuration introduces additional noise and results in a noticeable decline in performance, as illustrated in Figure 6. For example, Claude-4.0-Sonnet’s success rate in the Location Navigation domain drops from 22.22% to 11.11%. GPT-4.1’s success rate in the Browser Automation domain decreases from 23.08% to 15.38%, and in the Financial Analysis domain, it drops from 40.00% to 35.00%. These results demonstrate that our benchmark can also serve as a valuable testbed for evaluating the robustness of LLMs when confronted with a larger number of unrelated tools. 9 Salesforce AI Research 2025-08-21 Table 5: Comparison of Enterprise-Level Agent Frameworks on MCP-Universe Benchmark. Agent Framework Location Navigation Repository Management Financial Analysis 3D Designing Browser Automation Web Searching Overall Success Rate Claude-4.0-Sonnet Backbone ReAct 22.22 12.12 55.00 26.32 38.46 21.82 29.44 Cursor Agent 22.22 9.09 55.00 26.32 43.59 7.27 26.41 OpenAI o3 Backbone ReAct 26.67 6.06 40.00 26.32 25.64 29.09 26.41 OpenAI"}
{"doc_id": "2508.14704v1", "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14704v1", "chunk_id": 14, "text": "MCP-Universe Benchmark. Agent Framework Location Navigation Repository Management Financial Analysis 3D Designing Browser Automation Web Searching Overall Success Rate Claude-4.0-Sonnet Backbone ReAct 22.22 12.12 55.00 26.32 38.46 21.82 29.44 Cursor Agent 22.22 9.09 55.00 26.32 43.59 7.27 26.41 OpenAI o3 Backbone ReAct 26.67 6.06 40.00 26.32 25.64 29.09 26.41 OpenAI Agent SDK 28.89 6.06 60.00 36.84 28.89 29.09 31.60 4.6 Enterprise-Level Agent Framework Comparison In the previous experiments, we focused on evaluating different LLMs using the ReAct framework. To assess the impact of agent architecture on performance, we conduct a comparison of different agent frameworks as shown in Table 5. We evaluate four distinct configurations: two frameworks using Claude-4.0-Sonnet as the backbone (ReAct and Cursor Agent), and two using OpenAI’s o3 model (ReAct and OpenAI Agent SDK3). For the Claude-4.0-Sonnet backbone comparisons, we observe that the ReAct framework achieves a higher overall success rate (29.44%) compared to the enterprise-level Cursor Agent (26.41%). While Cursor Agent demonstrates superior performance in Browser Automation (43.59% vs. 38.46%), it significantly underperforms in Web Searching (7.27% vs. 21.82%), resulting in a 3.03 percentage point overall deficit. This disparity is particularly notable in Web Searching, where Cursor Agent’s reliance on internal tools rather than the benchmark’s MCP servers may contribute to the performance gap. For the OpenAI o3 backbone comparisons, the OpenAI Agent SDK substantially outperforms the ReAct framework (31.60% vs. 26.41%). The Agent SDK demonstrates consistent advantages across most domains, particularly excelling in Financial Analysis (60.00% vs. 40.00%) and 3D Designing (36.84% vs. 26.32%). The o3 + OpenAI Agent SDK configuration achieves the highest overall performance among all tested agent-backbone combinations, suggesting that specialized agent architectures can effectively leverage model capabilities. These results highlight that agent framework design significantly impacts performance on our benchmark. While enterprise-level agents like Cursor may excel in specific domains, they do not universally outperform simpler frameworks like ReAct. Furthermore, the substantial performance difference between o3 + ReAct and o3 + OpenAI Agent SDK demonstrates that optimal agent-model pairing is crucial for maximizing performance on complex tasks. 5 Conclusion In this work, we present MCP-Universe, the first comprehensive benchmark designed to rigorously evaluate LLMs in real-world MCP environments. By grounding tasks in authentic data and deploying execution-based evaluators, MCP-Universe exposes critical gaps in current LLM capabilities, including challenges with long-context handling, tool unfamiliarity, and cross-domain performance disparities. Our extensive experiments show that even top-ranked models and enterprise-level agents struggle with the complexities of MCP-driven tasks. These findings underscore the need for targeted advances in both model design and agent integration. With its extensible framework and user-friendly interface, MCP-Universe provides a valuable testbed for researchers and practitioners to accelerate progress in robust, real-world LLM applications."}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 0, "text": "Improving in-context learning with a better scoring function Omar Naim IRIT France Université de Toulouse Swarnadeep Bhar IRIT France Université de Toulouse Jérôme Bolte Toulouse School of Economics Université Toulouse Capitole Nicholas Asher IRIT France CNRS Abstract Large language models (LLMs) exhibit a re- markable capacity to learn by analogy, known as in-context learning (ICL). However, recent studies have revealed limitations in this abil- ity. In this paper, we examine these limitations on tasks involving first-order quantifiers such as all and some, as well as on ICL with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contribut- ing factor to these constraints. To address this, we propose scaled signed averaging (SSA), a novel alternative to Softmax. Empirical results show that SSA dramatically improves perfor- mance on our target tasks. Furthermore, we evaluate both encoder-only and decoder-only transformers models with SSA, demonstrating that they match or exceed their Softmax-based counterparts across a variety of linguistic prob- ing tasks. 1 Introduction Large language models have shown an impressive ability to solve new tasks without parameter up- dates by leveraging examples provided in the input, known as in-context learning (Brown et al., 2020). While this behavior has drawn considerable atten- tion, (McCoy et al., 2024; Ye et al., 2024) have suggested there are important limitations to ICL; LLMs in-context learn much better in situations (tasks, data and algorithms) that they are likely to have frequently encountered than in so-called low- probability situations. There is a growing literature attesting to problems with generalizability in ICL. To further investigate limitations of ICL, We look at two tasks, one linguistic and one mathemat- ical, using clean, simple data (sequences of num- bers) and linked to transparent prompting; which mitigate issues related to prompt engineering. The first task is to learn the quantification concepts “ev- ery\" and “some\" in a restricted setting. The second, is to learn a one-dimensional linear function, a task studied in (Garg et al., 2022). Since ICL capabil- ities inherently depend on a model’s pre-training, we train small transformers from scratch. Our train- ing reflects the standard NLP next token prediction training of language models; but our data and tasks are simpler than with most NLP tasks, allowing us to concentrate on the ICL mechanism itself. Our results are as follows. While the small mod- els we train from scratch demonstrate remarkable capabilities within their training domain, we con- firm that they have difficulties generalizing beyond them on both tasks. We identify the Softmax scor- ing function in the attention mechanism as a key factor limiting generalization. To overcome this, we introduce a new scoring function, scaled signed averaging (SSA), which significantly improves gen- eralization in our models across our tasks in com- parison to alternatives proposed in the literature. We also evaluate SSA’s effectiveness on NLP tasks through two experimental settings. First, we trained decoder-only GPT-2 models with 124 mil- lion parameters from scratch on the OpenWebText dataset, one using SSA and the other using Softmax. The SSA model consistently outperformed the Soft- max baseline. Second, we trained two"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 1, "text": "We also evaluate SSA’s effectiveness on NLP tasks through two experimental settings. First, we trained decoder-only GPT-2 models with 124 mil- lion parameters from scratch on the OpenWebText dataset, one using SSA and the other using Softmax. The SSA model consistently outperformed the Soft- max baseline. Second, we trained two versions of “BabyBERTa\", an encoder-only transformer model (Huebner et al., 2021) on a 5-million-word cor- pus of child-directed speech: one with Softmax and several variants with SSA. When tested on the grammatical probing suite from (Huebner et al., 2021), the SSA models demonstrated superior per- formance compared to their Softmax counterparts. 2 Related Work (Brown et al., 2020) introduced ICL as a paradigm where the model learn at inference from the prompt by analogy, without changing any training param- eters. (Dong et al., 2022) survey the successes and challenges for ICL, arguing that existing re- search has primarily focused on “on simple tasks 1 and small models\", such as learning linear or basic Boolean functions. This limitation arises from the requirement to train models from scratch in such studies. We study versions of both of these tasks. With regards to the quantification task, (Asher et al., 2023) proposed evaluating a generative model’s understanding of quantification by using input sequences to describe semantic models. We adopt their approach, testing a model’s grasp of ba- sic quantification concepts by encoding situations in the input context and evaluating its ability to interpret them correctly. For the function task, (Garg et al., 2022) showed that a transformer trained from scratch can per- form ICL of linear functions, given identical train and test Gaussian distributions.1(Bhattamishra et al., 2023) trained small GPT-2-like models from scratch and found that transformers can in-context learn simple Boolean functions, but their perfor- mance degrades on more complex tasks. (Raven- tós et al., 2024) investigated how ICL in models evolves as the number of pretraining examples in- creases, under a regime where training and test examples are drawn from the same distribution. Concerning our NLP tasks, (Huebner et al., 2021) demonstrates that transformer-based masked language models can effectively learn core gram- matical structures from a small, child-directed dataset. BabyBERTa uses a lightweight RoBERTa- like model, with 8M parameters, no unmasking, single-sentence sequences, and 5M words of input to achieve grammatical understanding on par with RoBERTa-base pretrained on 30B words. They develop a grammar evaluation suite compatible with child-level vocabularies. We use this suite to compare SSA and Softmax as scoring functions in encoder-only models. (Olsson et al., 2022) introduce the notion of in- duction head as a key feature of ICL across tasks in small transformer models, and hypothesize that similar mechanisms underlie ICL in larger models as well. (?) demonstrates that ICL in a simple clas- sification task emerges abruptly, once the model learns to use an induction head to replicate label patterns from previous examples. Concerning the limits of ICL, (Xie et al., 2021; Zhang et al., 2024; Giannou et al., 2024) show that 1This work as well as ours follows a general approach to ICL and learning: a model has"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 2, "text": "model learns to use an induction head to replicate label patterns from previous examples. Concerning the limits of ICL, (Xie et al., 2021; Zhang et al., 2024; Giannou et al., 2024) show that 1This work as well as ours follows a general approach to ICL and learning: a model has learned with prediction ˆ S a set S ⊂V , if, when sampling from V at test time using the same distribution as used during training, the expected error for x ∈V ∧¬(x ∈ˆ S ↔x ∈S) is close to 0. when the training and inference distributions are shifted performance degrades. (Naim and Asher, 2024) made a systematic study and showed sub- stantial degradation in shifts in distributions, in contrast to (Garg et al., 2022) who only looked at small perturbations. (Ye et al., 2024; McCoy et al., 2024) show general limits to autoregressive train- ing. (Naim and Asher, 2024) introduce the notion of “boundary values\" to describe how model be- havior degrades. These values are hard limits on predictions. They argue convincingly that the lack of generalizability does not come from the mod- els overfitting the data and that boundary values’ effects on performance are not just the result of memorization. Prior research has also investigated the scoring functions for attention layers in transformers, pri- marily for enhancing performance on long-context sequences. (Zheng et al., 2025) introduce SA- Softmax, which multiplies the Softmax output by the input scores to enhance gradient flow and mit- igate vanishing gradients in deep models. (Qin et al., 2022) propose CosFormer, an alternative to Softmax that enables linear time complexity with- out relying on kernel approximations. None of the alternatives address the problems we detail with Softmax below. 3 Our ICL tasks and experimental set up Despite their apparent simplicity, our tasks are con- ceptually fundamental. In particular, failure on the quantification task suggests deeper limitations in a model’s reasoning capabilities. The notion of logical or semantic consequence, for instance, crucially involves quantification; a failure to under- stand quantification implies a failure to understand what it means to reason in a logically correct fash- ion. This will entail mistakes in reasoning not only with quantifiers but also in downstream tasks like question answering (Chaturvedi et al., 2024). The function prediction task tests a model’s abil- ity to extrapolate patterns from from contextual data to novel situations, a core requirement of ICL. While large models often succeed by relying on extensive encoded knowledge, true generalization requires the ability to go beyond the training dis- tribution. Our function task isolates this challenge in a controlled setting. The implication is clear: if a model fails to generalize here, we should be cautious about claims of generalization in more complex, less controlled scenarios involving noisy 2 or unknown data. Our ICL tasks involve training from scratch on sequences that contain in-context examples (input- output pairs) of the form (x1, y1, ..., xi) ending with a query input xi, for which the model must predict the corresponding output yi. Inputs are sam- pled from one or more training distributions. We employ"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 3, "text": "ICL tasks involve training from scratch on sequences that contain in-context examples (input- output pairs) of the form (x1, y1, ..., xi) ending with a query input xi, for which the model must predict the corresponding output yi. Inputs are sam- pled from one or more training distributions. We employ curriculum learning on a set S of training sequences of varying lengths, ranging from 1 to 40. We trained several transformer models2 from scratch, ranging from small 1 layer model to mod- els with 18 layers, 8 attention heads, and an em- bedding size of 256. We feature results here from a 12 layers and 8 attention heads (22.5M param- eters) model with and without MLP on our tasks, as larger models did not yield significantly better predictions. To identify the component responsible for ICL in transformers and what hinders their gen- eralization, we did an ablation study by removing components from the architecture to examine ICL tasks in models with fewer components. Our quantification task3 is to predict the truth of the simple quantified sentences (1-a) and (1-b) given a contextually given string of numbers of length 40. Our training set of strings S contain numbers chosen from a training distribution DI, which we set to the Gaussian distribution N(0, 1). (1) a. Every number in the sequence is posi- tive. b. Some number in the sequence is posi- tive. At inference time, we test generalization by shifting both the input distribution Dtest I and the sequence length Stest. The set Stest includes sequences rang- ing in length from 10 to 200, while Dtest I ranges over Gaussian distributions N(0, σ) for σ ≥1. In the linear function task, the target function is an affine map of the form yi = f(xi) = axi + b. To construct the training set, we first sample coef- ficients a and b for f from a distribution denoted DF. Each training sequence is then populated with input elements xi drawn from a separate distri- bution DI. At inference time, we evaluate gen- eralization by shifting both the input distribution Dtest I ∼N(0, σ1) for σ1 ≥1 and the function pa- rameters distribution Dtest F ∼N(0, σ2) for σ2 ≥1. We train a model Lθ parameterized by θ to mini- 2Our models were Decoder-only (GPT-2). Our code can be found in https://anonymous.4open.science/r/SSA/ 3See Figure 2 for an example of the task mize the expected loss over all prompts: min θ EP \" 1 k + 1 k X i=0 ℓ(yi+1, Lθ ((x1, y1, ..., xi+1))) # (1) where ℓ(., .) represents the loss function: we use squared error for the linear function task and cross- entropy for the quantifier task. In the quantifier task, y represents the ground truth given a sequence ending with the input x. In the function task, y is the ground truth value of f(x), where f is the underlying function generating the sequence up to x. We train models for 500k steps using a batch size of 64, resulting in over 1.3 billion training examples seen per distribution. For the"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 4, "text": "input x. In the function task, y is the ground truth value of f(x), where f is the underlying function generating the sequence up to x. We train models for 500k steps using a batch size of 64, resulting in over 1.3 billion training examples seen per distribution. For the quantifier task, we evaluate a model’s ICL performance on each pair (Stest, Dtest I ) by gen- erating 100 samples, each consisting of 64 batches. For each sample, the model receives a score of 1 if the model’s prediction is incorrect and 0 other- wise. The final evaluation measure is obtained by averaging the error across all samples. For the linear function task, we assess ICL per- formance on each pair (Dtest I , Dtest F ) by sampling N = 100 functions from Dtest F . For each func- tion fj, we generate Nb = 64 batches, each containing Np = 40 input points drawn from Dtest I . Within each batch b, we predict fj(xb k) for every xb k with k ≥2, given the prompt se- quence (xb 1, fj(xb 1), ..., xb k−1, fj(xb k−1), xb k). For each function, we compute the mean squared error (MSE) across all predicted points in all batches. The final ICL performance metric is obtained by averaging the MSE across all sampled functions: ϵσ = 1 N N X j=1 Nb X b=1 1 Nb  1 Np Np X k=3 (predb,j k −fj(xb k))2  (2) This evaluation (2) across different distribu- tions provides a comprehensive assessment of the model’s ability to generalize its learning. 4 ICL for quantifiers and linear functions ICL with quantifiers Experiments on quantifi- cation task showed that when test samples are drawn from the same distribution as training, i.e. DI, Dtest I ∼N(0, 1), models successfully learned to predict the correct truth values for (1-a) or (1-b) on test sequences Stest that were significantly longer than those seen during training S, as illus- trated in Figure 4. However, model performance 3 dropped sharply when inference inputs included one or more xi values far outside the training dis- tribution (see Figure 2). We refer to such sequence as deviant. ICL with linear functions In the linear function task, when both training and test data were sam- pled from N(0, 1), even small models achieved near-zero average error. However, when the target function f was sampled from a shifted distribution Dtest F = N(0, σ) for σ > 2, all models had sys- tematic and non 0 average errors (For details, see Appendix J). 5 Error analysis In the quantification task, we found that models base their predictions for an entire deviant sequence S solely on their prediction for the largest element in S (see Figure 2). The presence of a single suf- ficiently large number in S was enough to trigger this behavior consistently. In the linear function task, we also observed (Naim and Asher, 2024)’s boundary values —values that the model fails to exceed during inference (see Figures 1 and 10). These boundary values are responsible"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 5, "text": "presence of a single suf- ficiently large number in S was enough to trigger this behavior consistently. In the linear function task, we also observed (Naim and Asher, 2024)’s boundary values —values that the model fails to exceed during inference (see Figures 1 and 10). These boundary values are responsible for general- ization failures: they restrict the model to generate outputs only within a specific range, effectively preventing the model from generalizing its good performance on the task over a small interval to val- ues outside that interval. We found such boundary effects in both attention-only and full transformer models across all our training and testing setups. Figure 1: Plots showing examples of boundary values for different models. (Left) Full transformer 12L8AH model tested on f(x) = 9x and (Right) Transformer 12L8AH without MLP model trained on DI = DF = N(0, 1) and tested on f(x) = 10x. 5.1 Comparison with larger fine-tuned and prompted LLMs Generalization from training data was a challenge not only for our smaller models but also for much larger ones. We evaluated performance on the quan- tification task using both fine-tuned (see Appendix A for details) and prompted versions of LLaMA 3.1 8B, as well as the prompted LLaMA 3.3 70B model4. In a 5-shot setting, prompted LLaMA 3.1 8B failed to master numerical inputs from Dtest I and showed no generalization to longer sequences. LLaMA 3.3 70B performed better on numerical in- puts drawn from distributions outside N(0, 1) but, similarly, failed to generalize to longer sequence lengths. Interestingly, the fine-tuned LLaMA 3.1 8B was able to handle large numbers within a se- quence, as shown in Figure 8, but still did not gen- eralize beyond the sequence lengths seen during training. We also tested the prompted LLaMA 3.3 70B on the linear function task with inputs and target func- tions sampled from DI, DF ∼N(0, 1). While the model sometimes appeared to assume linearity and apply a regression-like strategy, it still underper- formed relative to our small models (see Table 1).5 5.2 Ablation studies: the sources of ICL and limits to generalization We looked at what might be responsible for ICL and its limitations. As with (Olsson et al., 2022), we found that ICL was effective on our tasks even in models composed solely of attention layers, with no feedforward components (FF); these attention- only models performed comparably to their full transformer counterparts (see Figure 14). In con- trast, small models consisting only of feedforward layers failed to perform ICL. This indicates that the attention mechanism is both necessary and suffi- cient for ICL on our tasks. As in (Naim and Asher, 2024), models without FF components also show boundary values, which means that boundary val- ues originate from the multi-head attention itself. We then examined various components of our models to identify the source of their generalization failures. To understand why the models struggled to generalize on the quantification task, we first tested whether they could correctly classify indi- vidual numbers in deviant sequences as positive or negative. The models performed well on"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 6, "text": "then examined various components of our models to identify the source of their generalization failures. To understand why the models struggled to generalize on the quantification task, we first tested whether they could correctly classify indi- vidual numbers in deviant sequences as positive or negative. The models performed well on this subtask, and so had the information needed to com- plete the quantification task successfully. But they could not use the information in the right way. Given the importance of numerical representa- tion in this task, we examined the way numbers 4Prompts are provided in the appendix H 5We also finetuned a Llama 3.1 8b model for 2 epochs on the linear function task. But even after finetuning on around 26k such sequences, the model failed to learn the task. See Appendix A and E. 4 Input: (1,True,-2, False, 3, False, 70) Attention weights: 1 True -2 False 3 False 70 Correct output: False Model’s output: True Figure 2: Attention maps for an ICL example for the task \"every\" of type (x1, f(x1), x2, f(x2), ..., xn), where the query xn is a big value. were encoded. Since the quantification task de- pends on numerical magnitudes, and since large or rarely seen numbers can pose representational chal- lenges, we used a linear embedding Emb : x 7→ x · W, for (x, W) ∈R × Rd. This mapping pre- serves numerical ordering simple and interpretable and maintains the ordering of magnitudes, such that if |x| < |y| then ||emb(x)|| < ||emb(y)||, where ||·|| denotes the vector norm. Crucially, this encod- ing mechanism did not introduce boundary effects. It preserved natural magnitude orderings and did not appear to contribute to the model’s failure on the quantification task. 5.3 The issues with Softmax We then looked in more detail at the workings of the attention matrix. To recall the basics of atten- tion, let ⃗ el = (el 1, ..., el n) be the input embeddings processed by the multi-head attention mechanism at layer l, where el i denotes the embedding of the i-th token in that layer. Each attention head h in layer l is parameterized by Query, Key, and Value weight matrices, denoted as Qh,l, Kh,l, and V h,l, respectively. The dimension dk corresponds to the embedding size divided by the number of heads. The output of each attention head h in a layer l is a sequence of vectors (Ch,l 1 , Ch,l 2 , ..., Ch,l n ) where each: Ch,l i = n X j=1 softmax (Qh,lel i)⊤(Kh,lel j) √dk !! V h,lel j (3) The primary role of an attention head is to refine the embedding of each input el i by incorporating contextual information from surrounding tokens. However, once the gap between the input values el j in the argument of the Softmax operator in equa- tion 3 become large —specifically, when the gap between the largest value and the others exceeds a threshold (a difference of 4 is typically sufficient), the resulting Softmax weights rapidly saturate. In such cases, the attention weight assigned to the maximum"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 7, "text": "in the argument of the Softmax operator in equa- tion 3 become large —specifically, when the gap between the largest value and the others exceeds a threshold (a difference of 4 is typically sufficient), the resulting Softmax weights rapidly saturate. In such cases, the attention weight assigned to the maximum value approaches 1, while the weights for all other values approach 0. As a result, the model focuses almost entirely on a single token, the one associated with the maximum value, while effectively ignoring the rest of the context. Ch,l i = V h,lel j0 (4) As our embedding function is linear, if |x| is signif- icantly larger than |y|, then ||e(x)|| will be signifi- cantly larger than ||e(y)|| and so large differences in number inputs will have the effect noted in equa- tion 4. It is important that the values in deviant se- quences that yield equation 4 come from inputs that the model has seldom seen in training, be- cause then the Q and K matrices cannot have been trained to handle them, ensuring that the large xj affects Softmax as in equation 4. We verified these predictions by examining the outputs from the attention matrices of the last layer of the model in the quantification task. Figure 2 confirms experimentally what we showed mathe- matically: in the case of a significant gap between values, the attention layer puts all the weight on the largest value in the sequence; the other elements in the sequence which determined the truth value for (1-a) are ignored. In Figure 2, the model fo- cuses only on the maximum value 70 and falsely predicts the sequence as all positive based only on this value. With significant differences in input values, the Softmax function increasingly resembles a Hard- max operation , assigning a weight close to 1 to the largest element and weights near 0 to all others. In addition, it makes the score of negative values tend towards 0 due to the exponential. Significant differ- ences that can affect Softmax occur not only with numerical inputs but with linguistic tokens. Tokens from OpenWebtext can have such differences (see Figure 9 in Appendix F). Training on distributions with a much larger range of elements like DI = N(0, 10) or DI = 5 N(0, 100) improved model performance on de- viant sequences but significantly increased squared errors on N(0, σ) for small σ. This training gave the Q and K matrices small weights to compen- sate. Since Softmax makes the scores on a set of very small values constant, the model becomes less accurate. Additionally, all models suffered in per- formance once the out of distribution elements xi in deviant inputs became sufficiently large. In the linear functions task, when a value xi in the sequence input to the attention mecha- nism is larger than the other elements of the se- quence and other elements in its training, mathe- matically Softmax should assign xi probability 1 and all other elements in the sequence probabil- ity 0. This affect models performances. Consider, for example, a 12L8AH"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 8, "text": "sequence input to the attention mecha- nism is larger than the other elements of the se- quence and other elements in its training, mathe- matically Softmax should assign xi probability 1 and all other elements in the sequence probabil- ity 0. This affect models performances. Consider, for example, a 12L8AH model’s predictions for f(x) = x with this input sequence with x1 = 100: [100, −1.09, 0.78, 0.26, 0.42]. The model’s predic- tions are: [−1.21, −0.28, 2.15, 0.96, 0.65]. Given this large value, model fails to reasonably approxi- mate the function.6 Softmax treats comparatively large values as very significant. This makes sense in the abstract; a large value in the attention mechanism intuitively signals a strong statistical correlation in context sensitive aspects of meaning (Asher, 2011); Soft- max amplifies this value. However, in our tasks this is problematic.7 While an input token representing a large number may carry higher semantic magni- tude than surrounding tokens, its importance for the task is not necessarily disproportionately greater. Our tasks require the model to look at many tokens in the context; but with deviant sequences, Softmax prevents the models from doing this. 6 Some alternatives to Softmax Attempting to remedy the deficiencies of Soft- max we observed, we investigated a hybrid at- tention mechanism. We partitioned the attention heads such that half utilized Softmax-based scor- ing, while the remaining half employed uniform averaging over all tokens. This design, we thought, would preserve contextual breadth, reduce the risk of focusing to specific tokens, and also increase the model’s expressiveness through multiple scor- ing functions. However, empirical results showed that this approach did not yield expected improve- 6Though eventually the model begins to recover and ap- proximate better. 7The problem occurs also of course with hardmax. ments in performance. We further extended this ap- proach by experimenting with four distinct known scoring functions (tanh, average, ReLU, and x2), assigning two heads to each (For detailed results see Table 1). This approach improved over than Softmax on N(0, 1) but was less good elsewhere. It did not solve the observed problems either. We also tested CosFormer (Qin et al., 2022) and SA- Softmax (Zheng et al., 2025). Both performed worse than Softmax in our experiments. 7 Solution: Signed scaled averaging (SSA) Having found that previously proposed alterna- tives to Softmax did not lead to performance im- provements in our settings, we explored a new ap- proach. Inspired by lim n→+∞(1 + x n)n = ex, we replaced the exponential in the scoring function by a parametrized form: x 7→(1 + b|x|)sgn(x)n where b > 0 is a trainable parameter and n > 1 a fixed exponent (we typically set 1.1 or 1.5, though it can also be learned). This formulation allows us to approximate the exponential while control- ling its sharpness.8 It allows interpolation between linear and exponential behaviors. By training b and selecting (or training) different values of n, the model gains flexibility: it can mimic Softmax-like behavior when appropriate, while also tempering the dominance of large input values. This provides a better balance between focus and"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 9, "text": "ling its sharpness.8 It allows interpolation between linear and exponential behaviors. By training b and selecting (or training) different values of n, the model gains flexibility: it can mimic Softmax-like behavior when appropriate, while also tempering the dominance of large input values. This provides a better balance between focus and diversity in at- tention. For positive inputs, the function behaves similarly to Softmax, but with a slower growth that prevents early Hardmax saturation. For negative inputs, the presence of sign(x) ensures the score decays to- ward zero, like Softmax, but less abruptly. This allows the model to still consider low-scoring ele- ments rather than suppressing them entirely. For a vector z = (z1, · · · , zK) ∈RK with param- eters b > 0 and n > 1, we replace Softmax by: score(zi) = (1 + b|zi|)sgn(zi)n PK k=1(1 + b|zk|)sgn(zk)n (5) Substituting Softmax with SSA improves our models’ generalization ability dramatically, both with respect to the length of test sequences and the magnitude of deviant inputs. The heat map in Fig- ure 4 shows the improvement for the predictions of our standard full transformer model with 12L8AH with SSA on the right compared to the same model 8Plots for sample base functions for SSA can be found in Figure 5. 6 Figure 3: (Left) Comparison plot showing the evolution of MSE for SSA and Softmax-based models (12L8AH) with DF, DI, Dtest I ∼N(0, 1) and Dtest F and varying Dtest F ∼N(0, σ). The heatmap shows the evolution of logarithm of MSE for the Softmax (Middle) and SSA (Right) model when varying both Dtest I and Dtest F . Figure 4: Heatmaps showing the evolution of errors for the 12L8AH model with Softmax (Left) and SSA (Right) on the \"every\" task. Model was trained on data in DI = N(0, 1) for lengths from 11 to 40 and tested in Dtest I = N(0, σ) for σ ∈{1, ..., 10} and lengths from 10 to 200 for each task. Yellow represents a much higher error rate than purple. models \\ σ 1 2 3 4 5 6 7 8 9 10 SSA 4 × 10−5 3 × 10−4 10−3 0.02 0.02 0.15 1.24 1.04 2.74 8.5 Softmax 8 × 10−5 3 × 10−4 6 × 10−3 0.42 1.62 3.84 9.42 13.51 27.99 45.35 SOFT/AVG 7 × 10−5 3 × 10−3 0.30 1.22 2.91 7.52 10.32 22.97 38.97 60.03 4 Scoring fts 5 × 10−5 3 × 10−3 0.34 1.33 3.18 8.28 10.99 26.31 42.42 70.33 CosFormer 2 × 10−4 5.6 × 10−3 0.43 2.07 5.10 12.54 16.91 46.68 66.22 91.02 SA-Softmax 6 × 10−5 2.8 × 10−3 0.37 1.83 3.83 11.94 13.72 39.29 58.43 80.31 Llama 3.3 70b 2 × 10−3 5.50 3.16 13.71 18.21 23.91 28.99 33.51 40.25 48.02 Table 1: Comparison showing the evolution of squared errors for models tested on x ∈Dt I = N(0, 1) and weights a, b ∈Dt F = N(0, σ). SSA, CosFormer and Softmax are 12 layer, 8 attention head full transformer models. with Softmax on the left for the every task."}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 10, "text": "Table 1: Comparison showing the evolution of squared errors for models tested on x ∈Dt I = N(0, 1) and weights a, b ∈Dt F = N(0, σ). SSA, CosFormer and Softmax are 12 layer, 8 attention head full transformer models. with Softmax on the left for the every task. The improvement with SSA on the \"some\" task is also significant (see Appendix C). Our best 12L8AH model also dramatically in- creased performance in learning linear functions on out-of-distribution data for function parameters when we used SSA instead of Softmax as scoring a function (see Figure 3). SSA was the best scoring function we tested on our tasks. 8 SSA and NLP tasks To assess the applicability of SSA beyond our syn- thetic tasks and to more complex natural language processing scenarios, we trained three GPT-2 mod- els (each with 134 million parameters) from scratch on the OpenWebText corpus (9B tokens) for 20,000 steps. One model used the standard Softmax scor- ing function, while the other two employed SSA: one with a fixed exponent n = 1.5, and the other with n treated as a trainable parameter. We evalu- ated model performance using perplexity, as this is a widely accepted metric for language modeling tasks and provides a direct, quantitative compar- ison across models trained with different scoring functions. The model with trainable SSA had a loss of 3.19 and SSA 1.5 model had a loss of 3.15 after 7 linguistic probe MLM Holistic Softmax SSA 1.1 SSA 1.5 SSA 2 Softmax SSA 1.1 SSA 1.5 SSA 2 agreement_Det_N-across_1_adj 75.35 76.7 73.5 73.3 56.45 57.55 54.95 56.40 agreement_subject_verb-across_PP 56 55.75 58.95 65.95 51.55 51.25 51.35 52.05 agreement_subject_verb-across_RC 55.5 55.75 57.7 61.55 51.9 52.15 52.15 50.05 agreement_subject_verb-in_Q+aux 76.5 69.6 79.0 70.45 50.1 48.95 49.95 49.05 anaphor_agreement-pronoun_gender 48.1 50.25 51.45 53.7 52.7 49.9 52.75 52.75 argument_structure-dropped_arg 79.65 74.65 74.65 85.55 70.7 80.9 72.85 75.85 argument_structure-swapped_args 83.3 83.1 92.0 88.0 62.45 58.4 53.3 33.15 argument_structure-transitive 53.44 55.3 53.85 57.2 55.3 53.7 56 55.6 binding-principle_a 78.25 79.55 87.9 80.2 66.4 68.2 75.0 68.85 case-subjective_pronoun 85.55 86.5 89.7 91.75 67.55 81.1 59.6 46.9 ellipsis-n_bar 60.75 56.25 53.3 53.45 35.15 43.3 34.7 33.65 filler-gap-wh_question_object 91.7 90.65 89.4 87.15 91.9 89.15 91.05 80.3 filler-gap-wh_question_subject 79.2 79.1 83.3 68.25 65.3 89.25 89.25 48.55 irregular-verb 70.05 64.85 78.3 69.2 50.15 58.8 72.1 52.5 local_attractor-in_question_with_aux 85.45 87.35 81.85 85.0 87.8 89.7 87.65 86.1 quantifiers-existential_there 91.3 86.6 85.25 76.25 86.15 87.3 80.9 85.15 quantifiers-superlative 71.2 76.1 83.95 65.25 51.2 47.45 38.9 31.85 OVERALL 73.01 72.23 74.94 72.48 61.92 65.12 63.08 56.39 Table 2: BabyBERTa Model performance trained from scratch on AO-CHILDES with Softmax and three settings of SSA, evaluated with the holistic metric on various linguistic probes from (Huebner et al., 2021). PP: prepositional phrase; RC: relative clause; Det: determiner; N: noun. arg: argument. 20k training steps, instead of 3.27 for the Soft- max. Trainable SSA had a minimum loss of 2.76 after 26k steps.9 While the baseline Softmax model achieved a perplexity of 31.73, The SSA 1.5 and trainable SSA models achieved a significantly lower perplexity of 27.71 and 29.35 respectively, all"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 11, "text": "argument. 20k training steps, instead of 3.27 for the Soft- max. Trainable SSA had a minimum loss of 2.76 after 26k steps.9 While the baseline Softmax model achieved a perplexity of 31.73, The SSA 1.5 and trainable SSA models achieved a significantly lower perplexity of 27.71 and 29.35 respectively, all on 20K step training. All models also share the same architecture and parameter count. SSA’s better perplexity is thus compelling. Trainable SSA achieved a perplexity of 27.21 after 30K steps. In a further study, we trained multiple vari- ants of the BabyBERTa model (Huebner et al., 2021) on the AO-CHILDES dataset: one using the standard Softmax scoring function, and three separate models using SSA with fixed exponents n = 1.1, n = 1.5, and n = 2, respectively. Baby- BERTa has demonstrated performance comparable to much larger RoBERTa models on linguistic prob- ing tasks, despite being trained on substantially smaller datasets (Huebner et al., 2021). For these experiments, we used the BabyBERTa encoder- only model, as encoder architectures have been shown to perform better on grammar classification tasks involving masking. We evaluated our four models on 17 of the linguistic probes from (Hueb- ner et al., 2021).10 We used two metrics for testing on linguistic probes: holistic scoring (Zaczynska 9https://github.com/karpathy/nanoGPT/tree/master posted a minimum loss with softmax of 3.11 only after 600,000 steps. 10Two tests related to negative polarity items produced highly skewed results and were therefore excluded. et al., 2020) which measures the correct predic- tion of all masked elements, and masked language model (MLM) scoring (Salazar et al., 2020), which assesses the accuracy of predicting correct tokens versus distractors. On the stricter holistic method, SSA 1.1 BabyBERTa averaged over 2 percentage points higher than Softmax BabyBERTa. SSA1.1 had the highest score over the other models in 8 probes. Softmax only beat the SSA models on 4 probes (see Table 2). On the MLM metric, SSA 1.5 averaged almost 2 percentage points higher than Softmax, SSA 1.1 and SSA 2. Softmax Baby- BERTa scored highest on 3 probes while SSA 1.5 and SSA 2 scored best on six of the probes. 9 Conclusion The literature has noted that small transformer models struggle to generalize effectively to out- of-distribution data, a limitation we have also em- pirically confirmed. We identified the Softmax scoring function in the attention mechanism as a factor contributing to this challenge and introduced a novel scoring method that significantly improves the performance of small transformer models on both mathematical and NLP tasks. SSA enhances a model’s ability to predict linguistic structure, of- fering improved performance without increasing model complexity. The fact that SSA is parametriz- able and adaptable to different tasks adds to its attractiveness and shows its advantages over a one- size-fits-all scoring function like Softmax. 8 Limitations Due to hardware and data limitations, we could not train all the SSA tasks with the parameter n as trainable. We were also unable to train models larger than 124M parameters from scratch with SSA. While SSA provides noticeable improvements in generalization, it does not fully address all the shortcomings"}
{"doc_id": "2508.14685v1", "title": "Improving in-context learning with a better scoring function", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14685v1", "chunk_id": 12, "text": "Due to hardware and data limitations, we could not train all the SSA tasks with the parameter n as trainable. We were also unable to train models larger than 124M parameters from scratch with SSA. While SSA provides noticeable improvements in generalization, it does not fully address all the shortcomings of the attention mechanism. In par- ticular, our heatmaps indicate that SSA struggles to generalize in scenarios where both the input xi and the test-time function distribution Dtest F diverge significantly from the training distribution DF. Thus, SSA does not address the fact that, as we have noted above, the simple mathematical struc- ture of the attention mechanism conflates the value of tokens with their importance for the particular task."}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 0, "text": "Anthology of Computers and the Humanities, Vol. 1 Continuous sentiment scores for literary and multilingual contexts Laurits Lyngbaek1, *, Pascale Feldkamp1, *, Yuri Bizzoni1, Kristoffer Nielbo1, Kenneth Enevoldsen1 1 Center for Humanities Computing, Aarhus University, Denmark * Joint First Authorship Abstract Sentiment Analysis is widely used to quantify sentiment in text, but its application to literary texts poses unique challenges due to ﬁgurative language, stylistic ambiguity, as well as senti- ment evocation strategies. Traditional dictionary-based tools often underperform, especially for low-resource languages, and transformer models, while promising, typically output coarse categorical labels that limit ﬁne-grained analysis. We introduce a novel continuous sentiment scoring method based on concept vector projection, trained on multilingual literary data, which more effectively captures nuanced sentiment expressions across genres, languages, and histor- ical periods. Our approach outperforms existing tools on English and Danish texts, producing sentiment scores whose distribution closely matches human ratings, enabling more accurate analysis and sentiment arc modeling in literature. Keywords: sentiment analysis, computational literary studies, historical texts, semantic em- beddings 1 Introduction & Related Works Sentiment analysis quantiﬁes sentiment in text and is widely used across domains, from product reviews analysis to social media monitoring [7, 32]. In computational literary studies, it is increas- ingly employed to model narrative dynamics, particularly for modelling sentiment arcs across nov- els [6, 16, 26, 38]. This typically requires continuous sentiment scores, mapping sentiment along a spectrum rather than using categorical labels, such as positive/negative. Despite the growing use of continuous sentiment scoring in literary studies, the validity of current tools in capturing literary sentiment expression remains underexplored. Popular tools such as Syuzhet have faced severe criticism for oversimpliﬁcation or poor generalizability [31] – issues that point to broader limitations in applying off-the-shelf sentiment analysis tools to literary texts. Indeed, the literary domain poses distinct challenges: ﬁgurative language, multiple narrative layers, and stylistic ambiguity all complicate sentiment detection [3, 8]. More recent transformer-based models appear better equipped to handle the complexity of liter- ary language [30], and techniques that transform categorical model outputs into continuous scores have been proposed [4]. Such methods have proven more effective than tailored dictionary-based tools, particularly in low-resource language settings and across languages [13]. However, empiri- cal benchmarks comparing model predictions to human judgments remain limited, particularly in languages other than English. We identify three main issues where current methods see a noticeable performance drop: Laurits Lyngbaek, Pascale Feldkamp, Yuri Bizzoni, Kristoffer Nielbo, and Kenneth Enevoldsen. “Continuous senti- ment scores for literary and multilingual contexts.” In: Proceedings of Conference XXX, ed. by Editor1 Editor2. Vol. 1. Anthology of Computers and the Humanities. 2025, 1–1. https://doi.org/00000/00000. © 2025 by the authors. Licensed under Creative Commons Attribution 4.0 International (CC BY 4.0). 1 1) Cross-lingual performance drops. Most Sentiment Analysis tools are developed for high- resource languages, and their transfer to low-resource ones like Danish is non-trivial. Although Danish has several dictionary-based tools (i.a., Asent [12], Sentida [21]), these have seen little use on historical literature and struggle with complex literary forms. Comparing tools for Dan- ish literary sentiment analysis, Feldkamp et al. [13] found"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 1, "text": "resource languages, and their transfer to low-resource ones like Danish is non-trivial. Although Danish has several dictionary-based tools (i.a., Asent [12], Sentida [21]), these have seen little use on historical literature and struggle with complex literary forms. Comparing tools for Dan- ish literary sentiment analysis, Feldkamp et al. [13] found that dictionaries were outperformed by multilingual transformer models – likely because they leverage contextual attention. While multi- lingual transformers, such as mBERT and XLM-R [11], show promise for cross-lingual sentiment analysis in literature [13], cultural and linguistic biases inherited from English pretraining remain a concern [10, 37]. 2) Cross-domain performance often drops, especially when applying tools trained on social me- dia to literature, where sentiment is expressed in a distinct manner [3, 14, 33]. Literary language tends to be more omissive and implicit, relying less on emotionally charged vocabulary and more on concrete descriptions of objects and situations to evoke affect – a domain-speciﬁc mode of sen- timent expression that models often fail to capture [14]. This domain-speciﬁcity also varies across genres: when using a model ﬁne-tuned on Twitter posts, poetry shows the weakest correlation with human ratings, prose falls in the middle, and Facebook posts the strongest [14]. 3) Historical data, marked by diachronic language change, often reduces model performance. While ﬁne-tuned multilingual transformers show promise for older texts [1, 13, 29], challenges remain. Lexical drift – including semantic shift, word loss (e.g., thou, peradventure), changing frequencies, and temporal polarity shifts – limits sentiment inference, especially when models rely on priors from modern corpora.1 In the case of temporal polarity shifts, even short-term changes can lower model performance, as learned polarities no longer reﬂect current usage [23]. Moreover, a major drawback of recent transformer-based approaches is that, while they out- perform dictionary-based tools on historical and literary data [13], they are typically optimized for trinary classiﬁcation (positive, neutral, negative), limiting their usefulness for ﬁne-grained senti- ment analysis. Although model conﬁdence scores can be repurposed for continuous output – with medium to strong correlation to human ratings [13] – the resulting distributions still cluster around the original three categories, producing what is effectively a pseudo-trinary distribution. This poses a problem for literary analysis tasks, not least sentiment arc modelling, where detrending methods to smoothen out the signal necessitate continuous scores. When sentiment scores behave in extreme ways – as they will with pseudo-trinary distributions – smoothing will tend to collapse variation toward the neutral midpoint, removing meaningful information. In this paper, we introduce a method to create continuous-scale sentiment scores that are more closely aligned with the distribution of human scores, while also mitigating language-, domain-, and historical data issues by training a model on the actual language and domain of the use case. We test this approach on English and Danish literary texts, comparing it to existing transformer- based models and popular dictionary-based tools, across both ﬁction and nonﬁction genres. The benchmark includes both historical literary genres (e.g., 19th-century hymns) and contemporary texts (e.g., blogs), enabling us to evaluate model performance in settings that better reﬂect the needs of researchers working"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 2, "text": "literary texts, comparing it to existing transformer- based models and popular dictionary-based tools, across both ﬁction and nonﬁction genres. The benchmark includes both historical literary genres (e.g., 19th-century hymns) and contemporary texts (e.g., blogs), enabling us to evaluate model performance in settings that better reﬂect the needs of researchers working with multilingual or diachronic literary corpora. We pursue three aims: (1) to assess model performance on contemporary literary and non-literary texts; (2) to com- pare performance across literary genres; and (3) to evaluate models on historical and multilingual literary data. We begin by testing our approach on Fiction4 — a recent annotated ﬁction corpus that spans four literary genres, two languages (English and Danish), and a period from 1798 to 1965. We then validate our approach further on EmoBank, a standard sentiment analysis dataset that includes contemporary genres and a small set of ﬁction, to gauge the generalizability of our 1 Diachronic sentiment analysis is particularly challenging for traditional machine learning approaches as words’ mean- ing and polarity change continuously, while most models require steady ground truths for training, creating somewhat artiﬁcial “museums” of words’ sentiment scores in a given historical period. 2 approach and to control for overﬁtting to literary data. 2 Methods 2.1 Data Table 1: Datasets with valence annotation. Valence was annotated on a sentence basis, so ‘N annotations’ indicates the number of sentences. The total number of sentences considered is n = 15, 170. ‘N annotators’ indicates the number of annotators reported per sentence. Dataset Period N annotations N words ¯ x words/sentence N annotators ↓EmoBank 1990-2008 8,870 143,499 16.18 10 Letters 1,413 21,639 15.31 10 Blog 1,336 20,874 15.62 10 Newspaper 1,314 25,992 19.78 10 Essays 1,135 26,349 23.21 10 Fiction 2,753 31,491 11.44 10 Travel-guides 919 17,154 18.67 10 ↓Fiction4 1798-1965 6,300 73,250 11.6 >2 Hymns 1798-1873 2,026 12,798 6.3 2 Fairy tales 1837-1847 772 18,597 24.1 3 Prose 1952 1,923 30,279 15.7 2 Poetry 1965 1,579 11,576 7.3 3 Emobank is a text corpus manually annotated for sentiment according to the psychological Valence-Arousal-Dominance scheme. It was compiled at JULIE Lab, Jena University [9],2, con- taining sentences from the MASC dataset, which is diverse both in terms of overall composition with diverse domains, and topically within categories.3 It includes six categories: Letters, Blog, Newspaper, Essays, Fiction, and Travel guides.4 Inter Rater Reliability for the whole dataset is: Krippendorff’s α = 0.34.5 We use the mean sentence-based valence scores overall and per cate- gory to compare model performance. Fiction4 is a dataset of literary texts, spanning literary texts across four genres and two lan- guages (English and Danish) in the 19th and 20th century.6, compiled at the Center for Humanities Computing, Aarhus University. The corpus consists of three main authors, Sylvia Plath for po- etry, Ernest Hemingway for prose, and H.C. Andersen for fairytales. Hymns were collected from Danish ofﬁcial church hymnbooks published between 1798 and 1873. All sentences in the corpus were annotated for by at least two annotators [14]. Inter Rater Reliability for the whole dataset is: 2 https://github.com/JULIELab/EmoBank/ 3 On some EmoBank categories: Essays"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 3, "text": "for prose, and H.C. Andersen for fairytales. Hymns were collected from Danish ofﬁcial church hymnbooks published between 1798 and 1873. All sentences in the corpus were annotated for by at least two annotators [14]. Inter Rater Reliability for the whole dataset is: 2 https://github.com/JULIELab/EmoBank/ 3 On some EmoBank categories: Essays includes eight texts, i.a., “A Brief History of Steel in Northeastern Ohio’. Fiction comprises six prose pieces across genres, i.a., Richard Harding’s “A Wasted Day” and the SciFi story “Captured Moments”. Newspapers contain reports (e.g., “A.L. Williams Corp. was merged into Primerica Corp.”) and longer reportages. Travel Guides are written in prose, including both place histories (e.g., “A Brief History of Jerusalem”) and reﬂective pieces (e.g., “Dublin and the Dubliners”). See the full MASC corpus at: https://anc.org/data/masc/ corpus/browse-masc-data/. 4 We excluded the ‘Sem-Eval’ category as it was internally diverse. 5 Since EmoBank lacks unique annotator IDs, we cannot correlate individual annotators’ scores. Instead, Krippendorff’s α measures agreement across ratings per item. IRR per subset is shown in Table 4. 6 https://huggingface.co/datasets/chcaa/fiction4sentiment, for details, see [14] 3 Spearman’s ρ = 0.63 and Krippendorff’s α = 0.67.7 We use the mean sentence-based valence score overall, per language set, and per genre to compare model performance. 2.2 Comparison models 2.2.1 Dictionary-based Because of their popularity and wide usage in literary studies [1, 4, 5], as a baseline, we tested the dictionary-based tools VADER [15] and Syuzhet [17]. They assign sentiment scores (from negative to positive) by word-score matching and speciﬁc rules. Syuzhet was developed explicitly for literary texts.8 When using these tools, we translated Danish sentences into English.9 A human did not review translations. 2.2.2 Transformer-based To test transformer-based methods, we chose two multilingual models. When testing models on Danish texts, we added three models ﬁne-tuned for Danish. These were all tested across EmoBank categories, as well as Fiction4 genres and languages. We list all models in Appendix A, Table 5.10 One of the multilingual models – twitter-xlm – showed the best performance on Fiction4 in Feldkamp et al. [13]. Danish models were picked based on their performance in a recent benchmark [20], and – in the case of MeMo-BERT-Sa – for being developed for 19th-century novels [19]. Conversion of model output: We convert Transformers’ standard three-ways outputs (positive, neutral, negative) to continuous values using their conﬁdence scores 11 as a proxy for intensity (e.g., positive, 0.67 →+0.67; negative →–0.67; neutral →0). Mapping a model’s conﬁdence values onto a continuous scale often outperforms dictionary-based tools for literary sentiment [4, 13]. intensity =      +p, if positive, 0, if neutral, −p, if negative. 2.3 Our approach It has been claimed that concepts – such as a sentiment – are approximately represented in a linear fashion within embedding space, which is denoted by the linear representation hypothesis [24]. The hypothesis states that concepts are encoded as a direction in the embedding space and that the further you move in a given direction, the stronger the concept is represented (see Figure 1). These linear representations of semantic information have been found in both encoding"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 4, "text": "the linear representation hypothesis [24]. The hypothesis states that concepts are encoded as a direction in the embedding space and that the further you move in a given direction, the stronger the concept is represented (see Figure 1). These linear representations of semantic information have been found in both encoding and decoding models, at varying levels of abstraction [22, 34, 35, 39]. Suppose we have access to the direction that encodes sentiment. In that case, we can project any embedded sentence onto the concept vector and gauge the sentiment of any given sentence, as seen in Figure 1. 7 Humans rarely reach an agreement higher than 80% (α>0.80) for categorical tagging (positive/neutral/negative) on nonliterary texts [36] – and have lower IRR for continuous scale annotation [2] – especially of literary texts [27]. 8 The Syuzhet lexicon was developed in the Nebraska Literary Lab under the direction of Matthew L. Jockers. 9 Using googletrans: https://pypi.org/project/googletrans/ 10 Code for comparing (HuggingFace-stored) sentiment models (with transformed outputs) on the Fiction4 or EmoBank is at: https://anonymous.4open.science/r/literary_sentiment_benchmarking-D6E6. 11 The score output by ﬁnetuned models (e.g., “positive”, 0.66) is a softmax-normalized class probability – a pseudo conﬁdence score – reﬂecting how strongly a model prefers one label over another. It comes from the linear classiﬁcation head atop models. 4 Positive documents Component 1 Component 2 Negative documents Concept Vector Hallelujah, Hallelujah! “I know it”, the boy said It is easy enough to do it […] He must pull until he dies Negative Neutral Semi-positive Positive Figure 1: An overview of how a concept vector for sentiment is constructed and what information it contains. A circle represents an embedded document. 2.3.1 Concept Vector Projection We propose an algorithm that constructs a concept vector in a given embedding space using posi- tive and negative exemplary sentences that represent the opposing extremes of the concept. Using a pre-trained sentence embedding model M, the algorithm embeds a labeled set of sentences S. It assumes that a concept – here sentiment – is represented linearly in the embedding space. To deﬁne the concept vector, the algorithm computes the mean embedding of both the positive and negative sentiment examples, then calculates the vector pointing from the negative to the positive mean. This relies on the assumption that when averaging multiple sentences, all non-sentiment informa- tion will disappear as Gaussian noise with a mean of zero, leaving behind only the information encoding sentiment [18, 39]. The resulting vector then theoretically encodes sentiment direction. New sentences can be assessed for their relation to the sentiment by projecting their embeddings onto this vector: the farther along the direction the projection lies, the stronger their positive relation is. Deﬁning the concept vector as a unit vector, the projection of a given embedding ei onto the unit concept vec- tor ˆ v is given by the dot product: ei · ˆ v. This projects the sentence embedding to the subspace spanned by the Concept Vector. The high-dimensional embedding has thereby been reduced to a one-dimensional sentiment score, as seen in ﬁgure Figure 2. Deﬁning a concept vector requires only"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 5, "text": "tor ˆ v is given by the dot product: ei · ˆ v. This projects the sentence embedding to the subspace spanned by the Concept Vector. The high-dimensional embedding has thereby been reduced to a one-dimensional sentiment score, as seen in ﬁgure Figure 2. Deﬁning a concept vector requires only a set of positive and negative example sentences. This is sufﬁcient to predict the sentiment of any subsequent sentence, whether labeled or unlabeled. The Concept Vector Projection (CVP) algorithm formally described in Appendix B. The implementation of this method is available at https://github.com/centre-for-humanities-computing/embedding-projection. 2.4 Models The implementation of Concept Vector Projection used to classify sentiment in this paper is based on the language model paraphrase-multilingual-mpnet-base-V212 [28]. This is a 278M parameter model, based on a mean-pooled BERT architecture, optimized for sentence similarity by using Siamese and Triplet networks. This model was chosen because of its multilingual capabilities and excellent size-to-performance ratio. Investigations during model selection indicate that a larger model may increase model correlation with human ratings in exchange for compute budget. Our Concept Vector was deﬁned using a training dataset of sentences with positive and negative sentiments from the Fiction4 dataset. Since the sentences were originally rated on a numerical scale 12 https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2 5 Negative corpus to mean Negative Vector Positive corpus to mean Positive Vector Difference as Concept Vector Embed Corpus of Interest Embedded Unlabeled Corpus Sentence projection onto Concept Vector Distribution of projection on Concept Vector Corpus of Interest Define a Concept Corpus This is a 7/10 Figure 2: A visualization of how the Concept Vector Projection is constructed. It shows how to use a labeled sentiment corpus to predict sentiments of an unlabeled corpus of interest. The vectors shown are reduced to a two-dimensional Euclidean space for visualization, but normally reside in a high-dimensional space. (1-9), they were translated to positive/negative ratings for the algorithm. We converted the mean ratings into ordinal labels through preset thresholds. That is, for the Fiction4 ratings, we deﬁne: label =      positive+ if rating ≥7 neutral∅ if 7 > rating > 3 negative− if rating ≤3 ] All the neutral sentences and 60% of the positive and negative sentences were in the Fiction4 testing set. The remaining 40% were in a Concept Corpus of 204 positive and 168 negative sen- tences used to deﬁne the model’s concept vector. 3 Results 3.1 Continuous scoring A key beneﬁt of the Sentiment Projection model is its ability – like dictionary tools – to produce genuinely continuous predictions. In contrast, Transformer-based token-classiﬁcation models such as xlm-roberta, which can be coerced to output continuous scores (see subsubsection 2.2.2), in practice exhibit a “pseudo-trinary” behavior: their predictions cluster heavily at zero and at the two polar extremes. This behavior is clearly visible both in the scatterplots of predicted vs true sentiments (Figure 3) and in the histograms of model outputs (Appendix A, Figure 5). When looking at the EmoBank results (Appendix A, Figure 4), the discretized output of xlm-roberta appears even more sharply tri-modal than the human scores, which average ten annotators. 3.2 Performance"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 6, "text": "the scatterplots of predicted vs true sentiments (Figure 3) and in the histograms of model outputs (Appendix A, Figure 5). When looking at the EmoBank results (Appendix A, Figure 4), the discretized output of xlm-roberta appears even more sharply tri-modal than the human scores, which average ten annotators. 3.2 Performance on literary data across genres Table 2 compares our model’s predictions to the human gold-standard ratings for the Fiction4 dataset’s 4 genres. We evaluated all models on the full multilingual Fiction4 corpus. For the dictionary-based tools (VADER and Syuzhet), originally Danish texts were translated into English (see subsubsec- tion 2.2.1). Danish-speciﬁc models generally underperform on genres that are natively English (Prose, Poetry), which drags down their overall correlation scores. An outlier is danish-sentiment, which delivers relatively consistent results across both languages; however, it still falls short of MeMo-BERT-SA on the original Danish texts – most notably in the Fairy Tales genre. 6 Continuous Scores + Granularity + Moderate Scores + Human Similarity Noncontinuous Scores - Extreme Scores - Tri-modal Distribution Model Prediction Human Ratings Model Prediction Sentiment Projection Sentiment Prediction Model Figure 3: Scatterplot of Sentiment Predictions for respectively Sentiment Projection and xlm-roberta. While the xlm-roberta model, in theory, can predict a continuous space of senti- ments when transforming it with conﬁdence scores, visual inspection shows that certain ranges of the sentiments spectrum are not used. While both models achieve high correlations, it appears that xlm-roberta achieves this by matching human tendencies to predict neutral. Table 2: Spearman correlations in the Fiction4 corpus across genres. From top to bottom: Publica- tion years; then Inter Rater Reliability (human scores) per genre (Spearman’s ρ and Krippendorff’s α); then correlation between the human gold standard and models (Spearman’s ρ). For VADER and Syuzhet scores, texts were automatically translated into English. Type Model Scalar Overall Hymns Fairy tales Prose Poetry Year 1798–1873 1837–1847 1952 1965 Human → IRR ρ ✓ 0.63 0.73 0.68 0.62 0.59 IRR α ✓ 0.67 0.72 0.68 0.61 0.58 ↓Dictionary vader ✓ 0.49 0.52 0.50 0.43 0.46 syuzhet ✓ 0.50 0.54 0.48 0.45 0.49 ↓Multiling. twitter-xlm × 0.55 0.50 0.52 0.57 0.58 xlm-roberta × 0.60 0.59 0.62 0.61 0.57 Sentiment Projection ✓ 0.66 0.69 0.66 0.62 0.70 ↓Danish danish-sentiment × 0.54 0.49 0.48 0.57 0.57 da-sentiment-base × 0.23 0.44 0.47 0.08 0.08 MeMo-BERT-SA × 0.47 0.63 0.72 0.26 0.16 Most Danish transformer-based models perform on par with (or worse than) dictionary-based models applied to English translations of the original Danish texts (e.g., Fairy tales & Hymns). Sentiment Projection, in contrast, achieves the highest correlation on every genre except Fairy tales – where MeMo-BERT-SA performed best, which aligns with its ﬁne-tuning on Danish literary prose from H.C. Andersen’s period. It performs especially well on Poetry, where other models struggle. The genres that achieved the highest human IRR – like hymns, at IRR ρ = 0.77 – did not reﬂect in better results for most models. The second-best performing model, xlm-roberta, for example, placed second-to-last on hymns. Instead, Sentiment Projection meets or exceeds Inter Rater correlation (ρ) for all genres. 7 3.3 Performance on"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 7, "text": "the highest human IRR – like hymns, at IRR ρ = 0.77 – did not reﬂect in better results for most models. The second-best performing model, xlm-roberta, for example, placed second-to-last on hymns. Instead, Sentiment Projection meets or exceeds Inter Rater correlation (ρ) for all genres. 7 3.3 Performance on literary data across time and languages Results for the multilingual performance assessment are presented in Table 3. Table 3: Spearman correlations in the Fiction4 corpus across languages. Columns from left to right: Overall evaluation on Multilingual dataset (English and Dan- ish); evaluation of the Danish set (n = 2, 800); evaluation of the English set (n = 3, 500); lastly, the evaluation of Translated set. On top, Inter Rater Reliability – Spearman’s ρ and Krippendorff’s α. The best model performance per setting is in bold, and the follow-up is underlined. * There might be minimal inﬂux in correlation caused by the concept vector being deﬁned by untranslated sentences that are included after translation. Type Model Scalar Multiling. Danish set English set Translated Human → IRR ρ ✓ 0.63 0.68 0.58 - IRR α ✓ 0.67 0.71 0.60 - ↓Dictionary vader ✓ - - 0.45 0.51 syuzhet ✓ - - 0.47 0.50 ↓Multiling. twitter-xlm × 0.55 0.50 0.58 0.56 xlm-roberta × 0.60 0.59 0.60 0.57 Sentiment Projection ✓ 0.66 0.68 0.60 0.65* ↓Danish danish-sentiment × 0.53 0.47 0.58 0.55 da-sentiment-base × 0.23 0.43 0.08 0.10 MeMo-BERT-SA × 0.48 0.67 0.25 0.24 Table 3 demonstrates that our Sentiment Projection model leads baselines in both multilingual and Danish-only evaluations. This gain likely reﬂects our use of a multilingual encoder for sentence embeddings and a “concept vector” deﬁned over a multilingual corpus. Concretely, Sentiment Projection attains Spearman’s ρ = 0.68 on the Danish subset (Fairytales + Hymns) versus ρ = 0.58 for the runner-up, and delivers a ρ = 0.06 absolute improvement in the overall multilingual setting. We test our model for its generalization across time periods in Table 2, where danish hymns and fairytales represent historical language with texts from the 18-19th century. The Sentiment Projec- tion model shows no signs of reduced performance when processing older texts and outperforms the follow-up model by ρ = 0.12 in the Hymns genre. Notably, twitter-xlm model appears to perform slightly better on sentences translated to English than on their original Danish, as seen in Table 3. This may indicate that Google Translate renders language in updated, contemporaneous forms, similar to the Twitter data used for model training. We see the same tendency (surprisingly) for the danish-sentiment model, i.e., better performance when Danish sentences were translated to English. In contrast, Sentiment Projection performs slightly better on the Danish set in its original form than when it is translated to English – which we consider validates its capacity to process older forms reliably. 3.4 Performance on literary and non-literary contemporary data To make sure that our model does not overﬁt its sentiment vector to the in-context sentiment cues of the stories in the Fiction4 corpus, we tested it against the EmoBank dataset – which consists of contemporary literary and"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 8, "text": "process older forms reliably. 3.4 Performance on literary and non-literary contemporary data To make sure that our model does not overﬁt its sentiment vector to the in-context sentiment cues of the stories in the Fiction4 corpus, we tested it against the EmoBank dataset – which consists of contemporary literary and non-literary data. All Multilingual and dictionary-based models were also tested for their correlation with the human gold standard of the EmoBank dataset. The Senti- ment Projection Model still achieved the highest overall correlation with human ratings. Although it shows a lower correlation for a few genres (i.a., Letters), it still appears to generalize well to contemporary out-of-training distribution data. It should be noted that the model outperforms the 8 other models the most in the ﬁction genre, indicating that the sentiment vector may be slightly ﬁne-tuned or overﬁt to ﬁction-speciﬁc sentiment indicators.13 Table 4: Spearman correlations on the EmoBank sentences (n = 8, 870) across domains. On top: Inter Rater Reliability (Krippendorff’s α). Scalar Overall Letters Blog Newspaper Essays Fiction Travelguides Human IRR α ✓ 0.34 0.34 0.31 0.29 0.31 0.35 0.23 vader ✓ 0.43 0.47 0.41 0.42 0.32 0.37 0.35 syuzhet ✓ 0.46 0.47 0.37 0.42 0.37 0.43 0.37 twitter-xlm × 0.64 0.69 0.65 0.61 0.59 0.57 0.48 xlm-roberta × 0.65 0.68 0.65 0.65 0.58 0.56 0.49 Sentiment Projection ✓ 0.67 0.62 0.61 0.66 0.53 0.64 0.52 4 Discussion & conclusions As seen in Table 3 and 4, the proposed Sentiment Projection model performs on par with or bet- ter than the contemporary state-of-the-art methods. Moreover, Sentiment Projection allows for a smooth continuous output. In contrast, methods converting model output are not continuous in practice, but rather return noncontinuous tri-modal distributions (Figure 3). While both methods correlate highly with the human golden standard, approaching the inter-rater correlation, it appears that the Sentiment Projection approach more closely resembles the sentiment distribution of human ratings. Furthermore, the Sentiment Projection method can be trained on multilingual data using a multilingual language model, allowing for a language-agnostic sentiment prediction model that also reliably handles historical variants. The Sentiment Projection was solely deﬁned by its concept vector, based on sentences from the Fiction4 dataset, half of which were in Danish, yet it still outperforms other models. While this paper corroborates the ﬁndings of [13], showing that translation (even without a quality check) to English increases the similarity of human and transformer-model scores, it also shows that this is not the case for Sentiment Projection, which performs slightly better on the original (Danish) sentences. Finally, the workﬂow presented in Figure 2 has been used to design a sentiment model, but allows easy generalization to other concepts of choice. The method could also work for other emotional concepts, such as emotion recognition, language detection, or abstract concepts like a nature-to-industry gradient. We encourage curious readers to search for inspiration for potential vectors in Linear Representation Hypothesis [25] and Steering Vector [35] literature. Due to the ﬂexible nature of the algorithm, there is no rigid lower boundary on the number of training points required for a stable vector, although"}
{"doc_id": "2508.14620v1", "title": "Continuous sentiment scores for literary and multilingual contexts", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14620v1", "chunk_id": 9, "text": "a nature-to-industry gradient. We encourage curious readers to search for inspiration for potential vectors in Linear Representation Hypothesis [25] and Steering Vector [35] literature. Due to the ﬂexible nature of the algorithm, there is no rigid lower boundary on the number of training points required for a stable vector, although the chances of over-representing the non-concept context of training sentences naturally increase as the number of sentences decreases. A future empirical investigation of the stability of the vector when using smaller training sets would be a useful guide. 13 While this can also be a drawback, it supports the idea that domain-speciﬁc sentiment analysis can be highly beneﬁcial. For example, a sentiment analysis method for ﬁction should be sensitive to the speciﬁc omissive, implicit, and concrete object-based sentiment cues that are rarer in other genres [3]. Feldkamp et al. [14] suggests that travel guides use similar mechanisms – sentiment is evoked through unsentimental, descriptive, and concrete detail. The fact that Sentiment Projection performs well also for both genres suggests it captures this kind of indirect sentiment expression. 9 Acknowledgements"}
{"doc_id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14586v1", "chunk_id": 0, "text": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek Mukhammadsaid Mamasaidov1 Azizullah Aral2 Abror Shopulatov1,3 Mironshoh Inomjonov1 1 Tilmoch 2 Academy of Sciences of Afghanistan 3 MBZUAI Abstract Southern Uzbek (uzs) is a Turkic language variety spoken by around 5 million people in Afghanistan and differs significantly from Northern Uzbek (uzn) in phonology, lexicon, and orthography. Despite the large number of speakers, Southern Uzbek is underrepresented in natural language processing. We present new resources for Southern Uzbek machine transla- tion, including a 997-sentence FLORES+ dev set, 39,994 parallel sentences from dictionary, literary, and web sources, and a fine-tuned NLLB-200 model (lutfiy). We also propose a post-processing method for restoring Arabic- script half-space characters, which improves handling of morphological boundaries. All datasets, models, and tools are released pub- licly to support future work on Southern Uzbek and other low-resource languages. 1 Introduction The Southern Uzbek language, spoken by ap- proximately 5 million Uzbeks residing across 14 provinces of Afghanistan, represents a distinct lin- guistic variety that has developed independently from Northern Uzbek over centuries (Ethnologue, 2025a). Uzbek as a whole is classified as a macrolanguage according to ISO 639-3 standards, encompassing multiple related varieties includ- ing Northern Uzbek (uzn) spoken primarily in Uzbekistan, and Southern Uzbek (uzs) prevalent in Afghanistan (Ethnologue, 2025b). This macrolanguage classification recognizes the significant linguistic diversity within the broader Uzbek language family, where individual vari- eties have developed distinct phonological, lexi- cal, and grammatical features due to geographical separation and contact with other languages. As part of the global Uzbek population exceeding 34 million people, Southern Uzbek is recognized in Afghanistan’s Constitution as a potential third offi- cial language in regions where it is the majority lan- guage, in addition to Pashto and Dari. (Afghanistan, 2004) Southern Uzbek functions as a fully developed literary language that meets the demands of liter- ature, art, culture, and science. It maintains ac- tive presence across multiple domains including technology, education, diplomacy, banking, and commerce. The language is taught in Southern Uzbek departments at seven national universities in Afghanistan and serves as the medium of instruc- tion in 970 schools distributed across provinces: 9 schools in Badakhshan, 80 in Balkh, 450 in Faryab, 50 in Samangan, 300 in Sar-e-Pol, and 80 in Takhar. (Olim Labib, 2020) International media outlets including BBC, Ra- dio Free Europe/Radio Liberty (Ozodlik), Voice of America, Voice of Iran, TRT Avaz, and Sputnik actively broadcast in Southern Uzbek, alongside Afghan media channels such as Oyna, Botur, Al- mas, Orzu, Nur, Oriano, Kalid, and National Radio and Television. The language maintains expanding digital presence across major online platforms in- cluding Wikipedia, Google, Facebook, and other social networks. Despite this linguistic vitality, Southern Uzbek remains underrepresented in natural language pro- cessing technologies. Major translation platforms like Google Translate (Google, 2025) currently pro- vide limited or no support for this language variety, highlighting the critical need for dedicated com- putational resources. As a low-resource language with unique characteristics distinct from Northern Uzbek, Southern Uzbek presents significant chal- lenges for machine translation systems. This study, conducted as part of the Open"}
{"doc_id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14586v1", "chunk_id": 1, "text": "2025) currently pro- vide limited or no support for this language variety, highlighting the critical need for dedicated com- putational resources. As a low-resource language with unique characteristics distinct from Northern Uzbek, Southern Uzbek presents significant chal- lenges for machine translation systems. This study, conducted as part of the Open Lan- guage Data Initiative (OLDI) shared task, addresses these challenges by developing specialized neural machine translation models for Southern Uzbek. Our contributions parallel recent advances in low- resource language processing and include: 1. A FLORES+ dev dataset translated to South- ern Uzbek containing 997 sentences 2. Parallel corpora for various language pairs with Southern Uzbek 3. Open-sourced fine-tuned neural models for Southern Uzbek translation 4. Comprehensive evaluation against existing baselines Our research aims to advance machine transla- tion capabilities for Southern Uzbek, contributing to the larger OLDI objective of expanding linguistic diversity in NLP technologies for underrepresented language varieties. 2 Linguistic Background 2.1 Historical Development Southern Uzbek belongs to the Turkic language family, specifically derived from the Karluk-Chigil- Uyghur dialectal group with partial influences from the Kipchak and Oghuz branches. The language represents the contemporary form of a literary tra- dition spanning over a millennium, with histori- cal continuity traceable through classical poets in- cluding Khwarizmi, Lutfi, Atayi, Sakkaki, Navoi, Babur, lutfiy, and Ogahi. Notably, while these historical figures did not identify themselves as “Uzbek”, they wrote in a language that forms the foundation of modern Southern Uzbek, demonstrat- ing the language’s independent development into a mature linguistic system. (Habibi Aral, 2021) Historically, Southern Uzbek served as the ad- ministrative and literary language for major dynas- ties including the Yaftids, Kushans, Ghaznavids, Seljuks, Timurids, and Mughals, who governed territories across Afghanistan and India for cen- turies using this language and established profound cultural legacies. (Tursunov and O‘rinboyev, 1982) 2.2 Writing System Southern Uzbek employs the Arabic script, which has served as the official writing system for Afghan languages for over a thousand years. This ortho- graphic system presents unique challenges and characteristics that distinguish it from Latin-based Northern Uzbek. The Arabic-based script includes only three vowel letters:( اa/o),( وu/o‘), and( یi/y). This limited vowel representation often misleads learn- ers into believing that Uzbek contains only three vowel sounds. However, vowel quality distinctions become evident in minimal pairs such as shown in Figure 1. kuz (autuⅿn) ()ﮐﻮﺯko‘z (eye) ()ﮐﯣﺯ yeⅼ(winⅾ) ()ﯾﯧﻞyiⅼ(year) ()ﯾﯿﻞ quroⅼ(weapon) ()ﻗﻮﺭﺍﻝⅿaraⅼ(ⅾeer) ( ﻝ )ﻣﺮﻩ Figure 1: Vowel differences in Southern Uzbek Standard Uzbek contains six primary vowels (with additional dialectal variants), yet the Arabic script lacks direct representation for half of them. These vowels require indication through diacritical marks (fatha, damma, kasra), which are frequently omitted in practical writing, thereby complicating accurate reading and pronunciation. Additional complexity arises from the dual func- tionality of certain letters. The Arabic letter( هh) functions both as vowel and consonant. Similarly, lettersوand( یwaw and ya) serve dual roles as vowels and consonants (“v” and “y”) depending on context as illustrated in Figure 2. Uzbek Southern Uzbek Northern Sounⅾ Ⅼetter(ﻩh) ⅾuaⅼroⅼes ﺩﯼ ﺑﯿﻠﺪﯾﺮﻩbiⅼⅾiraⅾi /a/ ﻫﻮﺱhavas /h/ Ⅼetter(ﻭwaw) ⅾuaⅼroⅼes ﻭﻃﻦvatan /v/ ﺗﻮﺯtuz /u/ Ⅼetter(ﯼya) ⅾuaⅼroⅼes ﺑﺎﯼboy"}
{"doc_id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14586v1", "chunk_id": 2, "text": "functions both as vowel and consonant. Similarly, lettersوand( یwaw and ya) serve dual roles as vowels and consonants (“v” and “y”) depending on context as illustrated in Figure 2. Uzbek Southern Uzbek Northern Sounⅾ Ⅼetter(ﻩh) ⅾuaⅼroⅼes ﺩﯼ ﺑﯿﻠﺪﯾﺮﻩbiⅼⅾiraⅾi /a/ ﻫﻮﺱhavas /h/ Ⅼetter(ﻭwaw) ⅾuaⅼroⅼes ﻭﻃﻦvatan /v/ ﺗﻮﺯtuz /u/ Ⅼetter(ﯼya) ⅾuaⅼroⅼes ﺑﺎﯼboy /y/ ﻓﯿﻞfiⅼ /i/ Figure 2: Dual letters in Southern Uzbek Southern Uzbek Northern Uzbek Ⅿeaning Exaⅿpⅼes with suffix “−ⅽhi” ﭼﯽ−ﭼﺎﯾﺨﺎﻧەⅽhoyxonaⅽhi teahouse keeper ﺍﺩﺑﯿﺎﺗﭽﯽaⅾabiyotⅽhi writer Ⅽoⅿpounⅾworⅾs anⅾprefixes ﺗﺸﻮﯾﺶ−ﺑﯥbetashvish ⅽarefree ﺍﻧﺼﺎﻑ−ﻧﺎnoinsof ⅾishonest Figure 3: Examples of standardized Southern Uzbek Arabic-script orthography showing mandatory half- space (zero-width non-joiner, U+200C) placement. Red marks indicate the location of half-spaces in suffixation after vowel-final stems and in prefix attachment. Arabic and Persian loanwords maintain their original orthographic forms, typically without vowel markings. 2.3 Morphological Structure Southern Uzbek exhibits rich agglutinative mor- phology characteristic of Turkic languages. The language employs extensive suffixation systems that can be classified into various functional cate- gories: • Nominalizers (noun-forming suffixes) • Adjectival suffixes • Verb formers • Tense and aspect markers • Other functional and derivational affixes Standardized orthographic rules govern affix at- tachment in Southern Uzbek Arabic script. A fundamental principle distinguishes between suf- fixes attached to vowel-final versus consonant-final stems ( -chi, -chilik, -lik, -li, etc.). These suffixes require half-space (also known as zero-width non-joiner, U+200C, also found in Farsi) separation when attached to stems ending in vowels (represented by Arabic letters,)ا ,و ,هwhile connecting directly to consonant-final stems. Southern Uzbek also employs prefixes, com- monly found in Persian or Arabic loanwords, for forming adjectives or adverbs. These prefixes (be-, no-, xo‘sh-, ser-, ba-, ham-, bad-) are written with half-space separation, as shown in Figure 3. 2.4 Contemporary Status and Challenges Despite its historical significance, Southern Uzbek has faced political marginalization over the past three centuries, with Turkic peoples in Afghanistan being sidelined in governance and education. Progress began in the 1970s when Uzbek par- liamentary representatives secured broadcasting rights on Afghan national radio. The 1978 rise of the People’s Democratic Party marked further ad- vancement with the publication of the Yulduz news- paper in Southern Uzbek, establishment of Uzbek Language and Literature departments, and expan- sion of Uzbek-medium education. (Aral, 2025) The 2001 democratic reforms in Afghanistan formally granted Southern Uzbek official status, recognizing its role in Afghan multilingual society. However, challenges remain in standardizing or- thographic practices and developing computational resources for this linguistically rich but technologi- cally underrepresented variety. 3 Related Work Machine translation for low-resource languages has gained significant attention, with researchers exploring various approaches from data augmenta- tion to multilingual transfer learning. Dale (2022) developed the first neural MT system for Erzya, a low-resource Uralic language, demonstrating how extensive data mining from diverse sources (Bible texts, dictionaries, digitized books) can yield func- tional translation systems despite limited parallel data. Similarly, P M et al. (2024) focused on low- resource Indic languages by fine-tuning multilin- gual models and employing back-translation with careful quality filtering, showing that selective data augmentation can improve performance when syn- thetic data is judiciously filtered. Goyle et al. (2023) systematically evaluated strategies for"}
{"doc_id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14586v1", "chunk_id": 3, "text": "limited parallel data. Similarly, P M et al. (2024) focused on low- resource Indic languages by fine-tuning multilin- gual models and employing back-translation with careful quality filtering, showing that selective data augmentation can improve performance when syn- thetic data is judiciously filtered. Goyle et al. (2023) systematically evaluated strategies for compensating data scarcity in lan- guages like Sinhala, Nepali, Khmer, and Pashto. They found that combining back-translation with focal loss yields substantial improvements, particu- larly when leveraging large monolingual corpora and transfer learning from related high-resource languages. Recent advances in large language models have also shown promise for low-resource translation tasks. Commercial LLMs like GPT-4 and Claude demonstrate multilingual capabilities that extend to languages not explicitly included in their train- ing data, offering competitive performance through few-shot learning approaches. Despite these advances, Southern Uzbek remains largely unexplored in computational linguistics. While Northern Uzbek has received some attention in multilingual models like NLLB (NLLB Team et al., 2022) and MADLAD-400 (Kudugunta et al., 2024), the Southern Uzbek has been left behind. Our work represents the first dedicated effort to develop neural translation resources for this variety of Uzbek. 4 Datasets 4.1 FLORES+ Dev Dataset This study introduces the Southern Uzbek FLO- RES+ dev dataset, comprising 997 sentences trans- lated from English to Southern Uzbek (see Figure 4). The dataset was developed under the Open Lan- guage Data Initiative (OLDI) framework. One native Southern Uzbek linguist was responsible Engⅼish The airⅽraft haⅾbeen heaⅾeⅾto Irkutsk anⅾwas being operateⅾby interior troops. Northern Uzbek Saⅿoⅼyot Irkutsk toⅿon yo‘ⅼoⅼgan va iⅽhki qo‘shinⅼar toⅿoniⅾan boshqariⅼayotgan eⅾi. Southern Uzbek . ﯾﺎﺗﮕﻦﺍﯦﺪﯼ ﺭﯾﻠﻪ ﺍﻭﭼﺎﻕﺍﯾﺮﮐﻮﺗﺴﮏﺗﺎﻣﺎﻥﯾﯣﻝﺁﻟﮕﻦﻭﺍﯾﭽﮑﯽﻗﯣﺷﯿﻨﻠﺮﺗﺎﻣﺎﻧﯿﺪﻥﺑﺎﺷﻘﻪ Figure 4: Example from the FLORES+ dataset in English, Northern Uzbek and Southern Uzbek. for the translation process, with subsequent post- review process to ensure linguistic accuracy and cultural appropriateness. All Southern Uzbek trans- lations strictly adhere to the Arabic script ortho- graphic conventions, including proper implemen- tation of half-space characters (U+200C) for mor- phological boundaries as described in Section 2.3. Given the complexity of Arabic script represen- tation and the morphologically rich nature of South- ern Uzbek, particular attention was paid to main- taining consistent orthographic standards through- out the translation process. The translation process followed standardized conventions for affix attach- ment, vowel representation, and proper handling of Arabic and Persian loanwords within the Southern Uzbek linguistic system. 4.2 Training Data The training dataset comprises diverse parallel cor- pora sourced from three primary domains, total- ing 39,994 sentence pairs across multiple language combinations: 1. Dictionary Entries (1,550 pairs): Parallel dictionary entries mapping Northern Uzbek to Southern Uzbek lexical items (Aral, 2024). These entries provide direct lexical correspon- dences and serve as high-quality alignment data for closely related language varieties. 2. Literary Corpus (35,865 pairs): Parallel sen- tences extracted through careful alignment from 27 selected books available in both Northern and Southern Uzbek variants. This corpus represents the largest component of our training data and captures literary register variations, complex syntactic structures, and cultural terminology. 3. Web-sourced Content (2,579 pairs): Parallel sentences of English-Southern Uzbek mined from official government websites and reliable online resources. This component provides"}
{"doc_id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14586v1", "chunk_id": 4, "text": "in both Northern and Southern Uzbek variants. This corpus represents the largest component of our training data and captures literary register variations, complex syntactic structures, and cultural terminology. 3. Web-sourced Content (2,579 pairs): Parallel sentences of English-Southern Uzbek mined from official government websites and reliable online resources. This component provides contemporary usage patterns and domain- specific terminology from governmental and institutional contexts. 4.3 Data Mining Process The sentence alignment process presented unique challenges due to Southern Uzbek’s underrepresen- tation in existing multilingual models. Our align- ment methodology employed a two-stage approach to maximize extraction efficiency. For literary corpus alignment, we initially ap- plied LaBSE embeddings (Feng et al., 2020) di- rectly to the original Arabic script texts. While LaBSE does not include Southern Uzbek in its training data, the model demonstrated limited align- ment capability, likely due to shared vocabulary with other Turkic languages in the embedding space. To improve alignment quality, we implemented a transliteration-based enhancement strategy. South- ern Uzbek texts were transliterated from Arabic to Latin script using rule-based conversion scripts1, which enabled more effective cross-lingual em- bedding alignment. This transliteration approach yielded a 40% more successfully aligned sentence pairs compared to direct Arabic script processing. The sentence alignment methodology follows established practices from low-resource language processing (Dale, 2022). We utilize LaBSE to gen- erate embeddings for each potential sentence pair, calculate cosine similarity between embeddings, and adjust similarity scores using length ratios. For web-sourced English-Southern Uzbek data, we employed a reverse translation verification ap- proach. Southern Uzbek sentences were translated to English using Gemini-2.0-Flash, followed by LaBSE-based alignment between original English content and back-translated English. This pro- cess underwent manual review to ensure translation quality and semantic fidelity. A notable preprocessing challenge emerged re- garding half-space character consistency. Due to OCR limitations and editorial inconsistencies in source materials, half-space characters (U+200C) were frequently omitted, incorrectly rendered as full spaces, or merged with adjacent characters. While this issue complicates training data quality, we address it through post-processing correction 1https://github.com/tahrirchi/uzs-scripts Model uzs-en uzs-uzn eng-uzs uzn-uzs gpt-4.1 24.90 / 53.42 2.634 / 3.657 0.48 / 9.49 1.42 / 21.55 gemini-2.0-flash-001 32.81 / 58.80 62.45 / 73.67 1.59 / 24.47 6.96 / 41.11 claude-sonnet-4 22.25 / 51.46 59.18 / 83.63 0.68 / 15.38 2.62 / 28.85 nllb-200-600M 3.73 / 23.88 4.14 / 27.02 - - Google Translate 9.56 / 33.58 5.13 / 33.19 - - madlad400-3b-mt 2.95 / 23.26 0.19 / 1.41 - - lutfiy (no half-space fix) 11.26 / 34.39 53.48 / 78.54 1.33 / 25.43 25.99 / 66.44 lutfiy (with half-space fix) 1.58 / 26.61 34.31 / 71.11 Table 1: Evaluation of several models on sacreBLEU/chrF++ across various language pairs involving English, Northern Uzbek (uzn) and Southern Uzbek (uzs). mechanisms described in Section 4. 5 Translation Experiments 5.1 Model Training Our experimental framework employed the nllb- 200-distilled-600M model as the foundation for Southern Uzbek machine translation development. e maintained the original tokenizer configuration, leveraging the model’s existing multilingual capa- bilities for Turkic language processing. 5.1.1 Training Configuration For the training process we employed the Adafac- tor (Shazeer and Stern, 2018) optimizer"}
{"doc_id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14586v1", "chunk_id": 5, "text": "experimental framework employed the nllb- 200-distilled-600M model as the foundation for Southern Uzbek machine translation development. e maintained the original tokenizer configuration, leveraging the model’s existing multilingual capa- bilities for Turkic language processing. 5.1.1 Training Configuration For the training process we employed the Adafac- tor (Shazeer and Stern, 2018) optimizer paired with a learning rate of 1 × 10−4 following a constant schedule and 1000 warmup steps. A weight decay of 1×10−3 was applied, and the batch size was set to 32 due to GPU memory constraints. The max- imum sequence length was limited to 128 tokens, and training was conducted for 5000 steps, corre- sponding to approximately 2–3 epochs. All experi- ments were run on a single A100 40GB GPU. The Adafactor optimizer was chosen for its memory efficiency and proven effectiveness in transformer fine-tuning scenarios, while the conservative learn- ing rate and weight decay values were selected to mitigate overfitting given the small size of the training dataset. 5.1.2 Model Variant We fine-tuned nllb-200-distilled-600M (NLLB Team et al., 2022) model on the complete 39,994 sentence pair corpus. Our model called lutfiy2 maintains the original NLLB tokenizer and vocabu- lary, relying on existing Turkic language represen- tations for Southern Uzbek processing. 2Lutfi, a 15th-century Central Asian poet 5.1.3 Half-Space Post-Processing A critical technical challenge emerged regarding the handling of half-space characters. The NLLB SentencePiece (Kudo and Richardson, 2018) tok- enizer normalizes half-space characters (U+200C) to regular spaces during preprocessing, prevent- ing the model from learning proper morphological boundary representation. This problem affects not only Southern Uzbek but also extends to other lan- guages requiring half-space characters, including Persian (Doostmohammadi et al., 2020). To address this limitation, we developed a character-level n-gram post-processing model that predicts half-space insertion positions. The model was trained on a small set of training data with cor- rected half-space characters. It analyzes character sequences and applies statistical rules to determine whether half-spaces should follow specific vowel endings in morphologically complex constructions. This approach provides a practical solution to the tokenizer normalization problem while maintaining compatibility with existing NLLB infrastructure. The post-processing correction mechanism is made publicly available alongside our trained models3. 5.2 Evaluation Framework Model performance was assessed using two widely adopted metrics for translation tasks: sacreBLEU (Post, 2018), a standardized BLEU implementation that ensures consistent n-gram precision measure- ment across experiments, and chrF++ (Popovi´ c, 2017), a character-level F-score metric that is par- ticularly well-suited for evaluating morphologically rich languages such as Southern Uzbek. All results are reported on the FLORES+ dev set, enabling comparability with other low-resource language initiatives under the OLDI framework. 3https://huggingface.co/tahrirchi/lutfiy 6 Results and Discussion Our evaluation on the FLORES+ Southern Uzbek dev set reveals several key insights into the per- formance of various translation approaches. The results, presented in Table 1, demonstrate signifi- cant performance variations across different model architectures and translation directions. Notably, large language models exhibit supe- rior performance in understanding Southern Uzbek content, particularly in uzs-* directions. Gemini- 2.0-Flash achieves the highest scores for uzs-en translation (32.81 BLEU/58.80 chrF++), while Claude-Sonnet-4 excels in uzs-uzn translation qual- ity (83.63"}
{"doc_id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14586v1", "chunk_id": 6, "text": "demonstrate signifi- cant performance variations across different model architectures and translation directions. Notably, large language models exhibit supe- rior performance in understanding Southern Uzbek content, particularly in uzs-* directions. Gemini- 2.0-Flash achieves the highest scores for uzs-en translation (32.81 BLEU/58.80 chrF++), while Claude-Sonnet-4 excels in uzs-uzn translation qual- ity (83.63 chrF++). This suggests that LLMs’ ex- tensive multilingual pretraining enables effective comprehension of low-resource language varieties, even without explicit training on Southern Uzbek data. In contrast, traditional MT systems like Google Translate and specialized multilingual mod- els (NLLB-200-600M, MaLLaD400) demonstrate substantially lower performance, highlighting the challenges these architectures face with underrep- resented languages. However, our fine-tuned lutfiy model demon- strates clear advantages in generation tasks. For translation into Southern Uzbek (en-uzs and uzn- uzs), our model consistently outperforms all base- lines, achieving 1.58 BLEU/26.61 chrF++ for en- uzs and 34.31 BLEU/71.11 chrF++ for uzn-uzs di- rections. This validates our approach of fine-tuning on domain-specific parallel corpora, as the model learns proper Southern Uzbek generation patterns that generic LLMs cannot replicate effectively. The impact of our half-space post-processing correction is particularly evident in the uzn-uzs translation pair. While chrF++ scores show modest improvements (from 66.44 to 71.11), BLEU scores increase dramatically (from 25.99 to 34.31), repre- senting a 32% relative improvement. This substan- tial BLEU gain with stable chrF++ performance indicates that the half-space correction primarily addresses tokenization boundary issues rather than fundamental translation errors. Since BLEU re- lies on exact n-gram matches, incorrect half-space placement can artificially deflate scores even when the underlying translation quality remains high. For the closely related uzs-uzn translation di- rection, Gemini-2.0-Flash demonstrates excep- tional generation capability (62.45 BLEU), signifi- cantly outperforming our specialized model (53.48 BLEU). This suggests that LLMs may be particu- larly effective at cross-dialectal translation within the same language family, possibly due to their ability to capture subtle linguistic variations during pretraining. These findings highlight complementary strengths between LLMs and specialized fine- tuned models: while LLMs excel at understanding and translating from Southern Uzbek, targeted fine-tuning proves essential for high-quality generation into Southern Uzbek, particularly for morphologically complex constructions requiring proper orthographic conventions. 7 Conclusion Our study presents the first comprehensive neural machine translation resources for Southern Uzbek, addressing a significant gap in computational lin- guistics for this underrepresented Turkic variety. Our key contributions include: 1. Creation of a 997-sentence FLORES+ dev dataset for Southern Uzbek 2. Development of 39,994 parallel sentence pairs across multiple language combinations (uzs- uzn, uzs-en) 3. Fine-tuned NLLB-200 model (lutfiy) opti- mized for Southern Uzbek translation 4. Post-processing methodology for Arabic script half-space character restoration 5. Open-sourced datasets, models, and evalua- tion tools Future work will focus on expanding dataset cov- erage through additional literary sources and gov- ernment documents, exploring data augmentation techniques using large language models, and devel- oping more sophisticated orthographic normaliza- tion approaches for Arabic script processing. 8 Limitations Several limitations constrain our current approach. The training dataset size of ~40K sentence pairs, while substantial for a low-resource language, may limit generalization across diverse domains. Our heavy reliance on literary sources potentially bi- ases the"}
{"doc_id": "2508.14586v1", "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14586v1", "chunk_id": 7, "text": "devel- oping more sophisticated orthographic normaliza- tion approaches for Arabic script processing. 8 Limitations Several limitations constrain our current approach. The training dataset size of ~40K sentence pairs, while substantial for a low-resource language, may limit generalization across diverse domains. Our heavy reliance on literary sources potentially bi- ases the model toward formal registers, possibly affecting performance on conversational or tech- nical content. The half-space post-processing so- lution, while effective, represents a workaround rather than addressing the underlying tokenizer limitations. Additionally, our evaluation relies pri- marily on automatic metrics, which may not fully capture translation quality nuances for morphologi- cally complex languages like Southern Uzbek. Hu- man evaluation studies would provide more com- prehensive quality assessment. 9 Acknowledgements We thank the Open Language Data Initiative (OLDI) for supporting this research and David Dale for his valuable guidance throughout the project. The authors thank the Google for Startups Pro- gram for providing the computational resources that made this research possible."}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 0, "text": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning Guilhem Fauré guilhem.faure@inria.fr Université de Lorraine, CNRS, Inria, LORIA F-54000 Nancy, France Mostafa Sadeghi mostafa.sadeghi@inria.fr Université de Lorraine, CNRS, Inria, LORIA F-54000 Nancy, France Sam Bigeard sam.bigeard@inria.fr Université de Lorraine, CNRS, Inria, LORIA F-54000 Nancy, France Slim Ouni slim.ouni@loria.fr Université de Lorraine, CNRS, Inria, LORIA F-54000 Nancy, France Abstract One of the main challenges in neural sign language production (SLP) lies in the high intra-class variability of signs, arising from signer morphology and stylistic variety in the training data. To im- prove robustness to such variations, we propose two enhancements to the standard Progressive Transformers (PT) architecture (Saun- ders et al., 2020). First, we encode poses using bone rotations in quaternion space and train with a geodesic loss to improve the accu- racy and clarity of angular joint movements. Second, we introduce a contrastive loss to structure decoder embeddings by semantic similarity, using either gloss overlap or SBERT-based sentence sim- ilarity, aiming to filter out anatomical and stylistic features that do not convey relevant semantic information. On the Phoenix14T dataset, the contrastive loss alone yields a 16% improvement in Probability of Correct Keypoint over the PT base- line. When combined with quaternion-based pose encoding, the model achieves a 6% reduction in Mean Bone Angle Error. These results point to the benefit of incorporating skeletal structure mod- eling and semantically guided contrastive objectives on sign pose representations into the training of Transformer-based SLP models. CCS Concepts • Computing methodologies →Natural language generation. Keywords Sign Language Production, Deep Learning, Contrastive Learning, Pose Encoding ACM Reference Format: Guilhem Fauré, Mostafa Sadeghi, Sam Bigeard, and Slim Ouni. 2025. To- wards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning. In ACM International Conference on Intelligent Virtual Agents (IVA Adjunct ’25), Sep- tember 16–19, 2025, Berlin, Germany. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3742886.3756728 This work is licensed under a Creative Commons Attribution 4.0 International License. IVA Adjunct ’25, Berlin, Germany © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1996-7/2025/09 https://doi.org/10.1145/3742886.3756728 1 Introduction Sign language is the main way of communication used in the deaf and hard-of-hearing (DHH) community. It leverages a wide range of manual (handshape, location, orientation, movement) and non- manual features (facial expression, body orientation, intensity) to convey ideas through a specific syntax and a rich vocabulary [26, 37, 38]. With around 5% of the global population affected by a disabling hearing loss, and a projection of over 700 million DHH people in 2050 according to the World Health Organization [47], it is essential to reduce the communication gap between deaf and hearing people, notably to avoid social exclusion. As a response to this growing need, a part of the research community has been working on de- veloping new technologies for sign language recognition (SLR), sign language translation (SLT)—sign-to-text—and sign language production (SLP)—text-to-sign—tasks [2, 4, 6–8, 34, 40]. In the effort to develop digital tools that foster communication be- tween deaf and hearing communities, the rise of deep"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 1, "text": "of the research community has been working on de- veloping new technologies for sign language recognition (SLR), sign language translation (SLT)—sign-to-text—and sign language production (SLP)—text-to-sign—tasks [2, 4, 6–8, 34, 40]. In the effort to develop digital tools that foster communication be- tween deaf and hearing communities, the rise of deep learning has led to significant progress in recent years, particularly in SLR and SLT [3, 4, 7, 22, 30]. Since 2020, an increasing number of studies have focused on generating sign sequences from spoken language [1, 8, 9, 16, 24, 34, 39, 41, 42, 45, 50, 51, 55]. Among these, Saunders et al. [34] introduced the Progressive Transformers (PT) architec- ture, which has since emerged as a standard baseline in the field. Despite these advances, SLP systems continue to face several fundamental challenges that hinder their usability in real-world applications. Key obstacles include the high intra-class variability of signs, the significant grammatical divergence between signed and spoken languages, and the scarcity of large-scale, annotated datasets with diverse vocabularies [32]. As a result, the generated outputs often lack the intelligibility, fluency, and naturalness required for deployment in practical communication scenarios. In this work, we specifically address one of the core limitations of current SLP models: the visual variability of sign realizations, which introduces noise during training and impairs generalization. This variability arises primarily from two sources: • Inter-signers morphological differences, such as variations in bone lengths, which are not fully addressed by standard normalization techniques (e.g. (Stoll et al. [40])); IVA Adjunct ’25, September 16–19, 2025, Berlin, Germany Guilhem Fauré, Mostafa Sadeghi, Sam Bigeard, and Slim Ouni • Stylistic variations in the performance of a given sign or sentence—manifested through differences in amplitude, ve- locity, or positional noise—both across signers and within the same signer. To mitigate the impact of these factors, we build upon the PT architecture and introduce two main contributions: • We represent skeletal poses using bone rotations encoded as quaternions rather than traditional 3D Cartesian joint coordinates, and replace the mean squared error (MSE) loss with a geodesic loss defined in quaternion space; • We incorporate a contrastive loss into the training objective to structure the decoder’s multi-head self-attention embed- dings by pulling closer sequences with similar semantics and pushing apart dissimilar ones. We investigate two variants of this loss: one based on lexical overlap in the associated glosses (similar to the loss used in (Walsh et al. [45]) for the construction of their codebook), and another leveraging sentence Transformer embeddings (SBERT [33]) similarity scores between associated sentences, with the aim to capture subtler semantic relations. These contributions aim to reduce the effect of non-semantic variability in training data and improve the semantic consistency and expressiveness of generated sign sequences. We evaluate our approach on the widely used Phoenix14T dataset. Code and demos are available online1. 2 Related Work 2.1 Sign Language Production Early approaches to SLP were primarily based on synthetic anima- tion techniques relying on avatars and lookup tables containing pre-generated sequences for predefined sentences [2, 6, 12, 20, 56]. These methods required the preparation and storage"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 2, "text": "dataset. Code and demos are available online1. 2 Related Work 2.1 Sign Language Production Early approaches to SLP were primarily based on synthetic anima- tion techniques relying on avatars and lookup tables containing pre-generated sequences for predefined sentences [2, 6, 12, 20, 56]. These methods required the preparation and storage of a large set of sentence-sign pairs, making them costly and limiting their flexibil- ity. Furthermore, the resulting avatar animations were often poorly received by the Deaf community due to their under-articulated, robotic, or unnatural movements [25]. In recent years, progress in deep neural architectures has sig- nificantly advanced research in SLP. Stoll et al. [39] were the first to propose generating sign language pose videos from text using a three-stage pipeline: text-to-gloss conversion via a sequence-to- sequence model, motion graph-based sign stitching, and skeletal pose-to-video synthesis using a generative adversarial network (GAN). Saunders et al. [34] introduced a more streamlined autore- gressive model that directly maps sentences or glosses to 3D skeletal poses using a Progressive Transformers architecture. They encode each frame’s temporal position in the sequence by appending a nor- malized counter value 𝑡 𝑇to its joint embedding. Subsequent exten- sions of this model have incorporated data augmentation techniques (e.g., adding Gaussian noise, predicting multiple frames simulta- neously), adversarial training, and mixture density networks [35], as well as skeletal graph self-attention mechanisms in the decoder [36]. These improvements target the regression-to-the-mean effect in predicted signs and reduce error propagation during decoding. More recent approaches combine Transformer or diffusion-based 1https://github.com/GFaure9/ContQuat-PT architectures with vector quantization techniques to discretize the sign pose space [45, 50, 55]. Typically, this involves a two-stage pro- cess: first, a Vector Quantized Variational Autoencoder (VQ-VAE) is trained to encode sequences of sign poses into discrete tokens by constructing a codebook; second, a model is trained to predict these tokens from textual input. While some recent models aim to generate avatar-based outputs [1, 55], the majority represent sign poses as 2D or 3D skeletal data and optimize a loss function based on the Cartesian coordinates of joints. However, this approach introduces several limitations: • The same sign performed by individuals with different body morphologies can lead to significant variations in joint coor- dinates; • Computing the MSE over all joints tends to underweight the hands—critical for sign articulation—leading to reduced expressiveness; • This representation ignores the underlying skeletal structure, requiring the model to implicitly learn limb dependencies, which may result in suboptimal performance. To address these issues, some studies have introduced body- part-specific loss functions [1, 43, 55], or additional representations using bone orientation vectors in R3, minimizing an MSE between predicted and reference orientations [41]. However, the latter over- looks the non-Euclidian geometry of rotational space, and may misrepresent angular differences, limiting the precision needed to model fine-grained articulations. Finally, although pose tokenization effectively reduces stylistic variability [45], learning a robust codebook remains a non-trivial challenge, and may constrain the model’s ability to generate novel or unseen signs. 2.2 Rotational Pose Encoding Instead of representing human motion as sequences of joint posi- tions, an alternative is to describe"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 3, "text": "articulations. Finally, although pose tokenization effectively reduces stylistic variability [45], learning a robust codebook remains a non-trivial challenge, and may constrain the model’s ability to generate novel or unseen signs. 2.2 Rotational Pose Encoding Instead of representing human motion as sequences of joint posi- tions, an alternative is to describe it through bone rotations. In this framework, each pose is reconstructed by recursively applying a sequence of bone rotations to a predefined skeletal structure in a resting (\"T\") pose, starting from the root joint. This representation helps prevent prediction errors caused by inconsistent bone lengths or anatomically implausible motions. Rotational pose encoding relative to a given skeletal structure has been used in various works on human motion recognition and prediction [10, 28, 29, 44]. Rotations can be parameterized in several ways, including 3×3 rotation matrices, Euler angles, exponential maps, and quaternions [13, 15]. However, many of these parame- terizations present disadvantages for deep learning applications. For example, rotation matrices require enforcing six nonlinear con- straints to remain within the 3D rotation group 𝑆𝑂(3), while Euler angles are prone to gimbal lock when two rotation axes become aligned, leading to the loss of one degree of freedom. More broadly, since R3 cannot be smoothly mapped to 𝑆𝑂(3), exponential maps may also lead to singularities. Unit quaternions offer a robust and efficient alternative by rep- resenting rotations in 4D space, avoiding these common pitfalls. They are numerically stable, support smooth interpolation, and simplify the composition of rotations [13]. Quaternions have been Towards Noise Reduction in SLP via Quaternions and Contrastive Learning IVA Adjunct ’25, September 16–19, 2025, Berlin, Germany successfully employed in recurrent models for human motion un- derstanding [28], and more recently in sign language processing to construct sign language action embeddings [46]. Despite their strengths, quaternions—like all representations in four or fewer dimensions—are inherently discontinuous represen- tations of 3D rotations, as shown in (Zhou et al. [54]). The authors demonstrate that continuous representations of 3D rotations can be defined in 5D and 6D, making them better suited for learning. However, quaternions remain an attractive choice for our applica- tion, due to their compact 4D representation and low computational overhead while resolving common issues of classical 3D rotation representations. 2.3 Contrastive Learning Contrastive learning is a machine learning paradigm in which models learn more effective representations by comparing sam- ples—pulling positive pairs (similar samples) closer together in the embedding space, while pushing negative pairs (dissimilar samples) further apart [14, 17, 21]. This approach has proven successful in enhancing language embeddings [11], visual representations [5], and in aligning cross-modal embeddings [31]. In the field of sign language technologies, contrastive learning has been primarily ex- plored in sign-to-text translation frameworks [18, 23, 49, 53]. Ye et al. [49] demonstrate that reducing the density of the sign pose representation space via contrastive learning improves SLT per- formance. In (Jiang et al. [18]) and (Zhou et al. [53]), contrastive learning is employed for visual-language pretraining by encourag- ing alignment between visual and textual embeddings when they correspond to matching (ground truth, label) pairs. Lin et al. [23]"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 4, "text": "sign pose representation space via contrastive learning improves SLT per- formance. In (Jiang et al. [18]) and (Zhou et al. [53]), contrastive learning is employed for visual-language pretraining by encourag- ing alignment between visual and textual embeddings when they correspond to matching (ground truth, label) pairs. Lin et al. [23] supervise the learning of visual feature embeddings using Con- trastive Concept Mining (CCM): a method that identifies \"anchor words\" from batch-level sentences and treats two sign sequences as a positive pair if both contain the same anchor word. A similar tech- nique is adopted in (Walsh et al. [45]) for codebook training, where positive and negative pairs are constructed based on gloss overlap. Our approach draws inspiration from these works but applies contrastive losses directly within the latent space of the decoder’s self-attention layers in the PT architecture. We hypothesize that aligning these latent representations to the underlying semantic distribution of the text before cross-modal attention encourages more efficient learning, by filtering out visual features that are not semantically relevant. This aligns with the motivation of (Walsh et al. [45]), which seeks to reduce signer-specific variability and pro- mote person-invariant representations in sign language generation models. 3 Methodology 3.1 Overview We adopt the PT architecture of Saunders et al. [34] as our backbone, as it is a widely used baseline in SLP and offers a complete, publicly available implementation2. As shown in Figure 1, we propose two extensions: (1) pose se- quences are encoded via bone rotations using unit quaternions, replacing MSE loss on Cartesian coordinates with a loss based on the more natural geodesic norm; (2) we explore two contrastive 2https://github.com/BenSaunders27/ProgressiveTransformersSLP \"REGION MEHR FREUNDLICH HOCH DRUCK KOMMEN BIS REGION\" Figure 1: PT model architecture integrating quaternion-based pose encoding and supervised contrastive loss. Blue dotted boxes indicate the modules specific to our contributions. objectives, supervised by textual input, applied to the decoder’s self-attention latent space to guide its structure to reflect semantic relationships. The first variant follows the loss formulation of [45], defining positive and negative pairs based on shared gloss presence. The second aligns similarity matrices between latent features and SBERT sentence embeddings. 3.2 Quaternion-Based Representation of Skeletal Poses From a skeletal pose in 3D Cartesian coordinates 𝑃:= (𝑋1, ...,𝑋𝑁joints) lying in R𝑁joints×3, given the graph structure of the skeleton and a reference T-pose 𝑃0, we compute the 3D rotation in quaternions representation of the 𝑖-th bone as follows: 𝑞𝑖:= \u0010 cos(𝜃𝑖/2), sin(𝜃𝑖/2)𝑢(𝑖)\u0011 ∈[−1, 1]4 (1) where                𝜃𝑖= arccos(𝑣(𝑖) · 𝑣(𝑖) 0 ) 𝑣(𝑖) = 𝑋Child𝑖−𝑋Parent𝑖 ∥𝑋Child𝑖−𝑋Parent𝑖∥, 𝑣(𝑖) 0 = 𝑋0 Child𝑖−𝑋0 Parent𝑖 ∥𝑋0 Child𝑖−𝑋0 Parent𝑖∥ 𝑢(𝑖) = 𝑣(𝑖) ×𝑣(𝑖) 0 ∥𝑣(𝑖) ×𝑣(𝑖) 0 ∥ (’×’: classical cross product) Hence, for each sequence of poses Y := (𝑃0, ..., 𝑃𝑇), we obtain the corresponding sequence of bones rotations R := (𝑄0, ...,𝑄𝑇) where 𝑄𝑡:= (𝑞1[𝑡], ...,𝑞𝑁bones [𝑡]). Based on the definition of the geodesic distance between unit quaternions, we define the loss function between predicted rota- tions R′ and ground truth rotations R as: IVA Adjunct"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 5, "text": ":= (𝑃0, ..., 𝑃𝑇), we obtain the corresponding sequence of bones rotations R := (𝑄0, ...,𝑄𝑇) where 𝑄𝑡:= (𝑞1[𝑡], ...,𝑞𝑁bones [𝑡]). Based on the definition of the geodesic distance between unit quaternions, we define the loss function between predicted rota- tions R′ and ground truth rotations R as: IVA Adjunct ’25, September 16–19, 2025, Berlin, Germany Guilhem Fauré, Mostafa Sadeghi, Sam Bigeard, and Slim Ouni Figure 2: Illustration of bone rotation angle relative to a reference T-pose. LGeo := 1 (𝑇+ 1)𝑁bones 𝑇 ∑︁ 𝑡=0 𝑁bones ∑︁ 𝑖=1 arccos \u0010 2(𝑞′ 𝑖[𝑡] · 𝑞𝑖[𝑡])2 −1 \u0011 (2) This loss temporally averages the mean rotation angle between predicted and ground truth bone orientations. Additionally, to enable reconstruction of predicted skeletal pose sequences in 3D Cartesian coordinates by recursively applying ro- tations from the root joint, we also predict the head node’s position by minimizing the following MSE during training: LRoot := 1 𝑇+ 1 𝑇 ∑︁ 𝑡=0 ∥𝑋′ Root[𝑡] −𝑋Root[𝑡]∥2 2 (3) In constrast, the PT baseline relies exclusively on an MSE loss over joint positions. 3.3 Contrastive Losses The proposed contrastive losses are incorporated as regularization terms in the overall training objective, in addition to the standard SLP loss—either the MSE on joint positions or LGeo+LRoot. A scaling factor 𝜆balances the contribution of the contrastive loss: LTotal := LSLP + 𝜆LCont (4) The two contrastive losses we evaluate are presented in the follow- ing subsections. 3.3.1 Supervision with Glosses. To define the supervised contrastive loss based on input gloss sequences, we follow the method proposed in (Walsh et al. [45]), itself inspired by (Khosla et al. [21]). For each batch of decoder’s self-attention hidden representations, we first extract the set of all unique gloss tokens (referred to as anchors, indexed by 𝐼) appearing in the batch. For each anchor 𝑖∈𝐼, we iden- tify within the batch the sequence where 𝑖occurs most frequently. This sequence will serve as the reference. The remaining sequences are split into positives 𝐴(𝑖) (those that also contain 𝑖) and negatives 𝐵(𝑖) (those that do not), as illustrated in Figure 3. Based on this grouping, we define the following contrastive loss over the 𝑙-th self-attention layer’s output Z𝑙 batch := (𝑧1, ...,𝑧𝑁), where 𝑁is the batch size: L(𝑙) GlossSupCont := − ∑︁ 𝑖∈𝐼 log © ­ « 1 |𝐴(𝑖)| ∑︁ 𝑎∈𝐴(𝑖) exp( 𝑧𝑓(𝑖) ·𝑧𝑎 𝜏 ) Í 𝑏∈𝐵(𝑖) exp( 𝑧𝑓(𝑖) ·𝑧𝑏 𝜏 ) ª ® ¬ (5) \"REGION MEHR FREUNDLICH HOCH DRUCK KOMMEN BIS REGION\" \"MORGEN WETTER WE-AUSSEHEN FREITAG ACHT ZWANZIG AUGUST\" \"SUEDOST REGEN VIEL\" \"MONTAG SONNE WARM WETTER\" \"JETZ WETTER VORAUS INFORMIEREN MORGEN DONNERSTAG ZWANZIG JANUAR\" \"WETTER\" \"MONTAG SONNE WARM WETTER\" \"MORGEN WETTER WE-AUSSEHEN FREITAG ACHT ZWANZIG AUGUST\" \"MONTAG SONNE WARM WETTER\" \"SUEDOST REGEN VIEL\" Figure 3: Definition of positive and negative pairs for the computation of LGlossSupCont. with          𝑓(𝑖) := arg max𝑘{Í 𝑚∈𝑘-th Gloss Sequence 1𝑚=𝑖} 𝐴(𝑖) := {𝑎∈⟦1, 𝑁⟧| 𝑖∈𝑎-th Gloss Sequence}\\{𝑓(𝑖)} 𝐵(𝑖) := {𝑏∈⟦1, 𝑁⟧| 𝑖∉𝑏-th Gloss Sequence} We average the per-layer losses to obtain the final objective: LGlossSupCont := 1 𝑛layers 𝑛layers ∑︁ 𝑙=1 L(𝑙)"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 6, "text": "with          𝑓(𝑖) := arg max𝑘{Í 𝑚∈𝑘-th Gloss Sequence 1𝑚=𝑖} 𝐴(𝑖) := {𝑎∈⟦1, 𝑁⟧| 𝑖∈𝑎-th Gloss Sequence}\\{𝑓(𝑖)} 𝐵(𝑖) := {𝑏∈⟦1, 𝑁⟧| 𝑖∉𝑏-th Gloss Sequence} We average the per-layer losses to obtain the final objective: LGlossSupCont := 1 𝑛layers 𝑛layers ∑︁ 𝑙=1 L(𝑙) GlossSupCont (6) 3.3.2 Supervision with SBERT Embeddings. To incorporate finer knowledge of semantic relationships between sequences embed- dings, we build an alternative loss based on the cosine similarity be- tween input sentences once embedded via a sentence Transformer (SBERT) [33]. Upstream, the embeddings (SBERT𝑘)𝑘of input sen- tences are thus computed using the ’all-MiniLM-L6-v2’ model from the Hugging Face library3. These embeddings are of dimension 384. Hence, to match the SBERT embedding size before computing the loss, the outputs of the decoder’s self-attention blocks are first aver- aged along the temporal dimension via average pooling, and then projected through a linear layer (cf. Figure 4). These previous steps enable the computation of the following loss over the batch output of the 𝑙-th self-attention layer: L(𝑙) SBERTSupCont := 𝑁(𝑁−1) 2 ∑︁ 1≤𝑖<𝑗≤𝑁 𝑑2 𝑖,𝑗 (7) with 𝑑𝑖,𝑗:= sim(g(𝑧𝑖), g(𝑧𝑗)) −sim(SBERT𝑖, SBERT𝑗) (8) Where sim(𝑥,𝑦) := 𝑥·𝑦 ∥𝑥∥∥𝑦∥denotes cosine similarity, and g(·) is the transformation applied by the projection layers. The goal is to 3https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 Towards Noise Reduction in SLP via Quaternions and Contrastive Learning IVA Adjunct ’25, September 16–19, 2025, Berlin, Germany Figure 4: Projection into latent space prior to LSBERTSupCont computation. Decoder’s self-attention outputs are dimen- sionally aligned with SBERT embeddings. align the similarity matrices computed from pose embeddings and SBERT embeddings (see Figure 5), such that the resulting latent spaces are structured according to semantic relationships, while minimizing the influence of non-semantic features. Figure 5: Computation of similarity matrices of SBERT and batch samples g(𝑧) for LSBERTSupCont. As for the first variant of contrastive objective, the overall loss is finally obtained by averaging accross all layers: LSBERTSupCont := 1 𝑛layers 𝑛layers ∑︁ 𝑙=1 L(𝑙) SBERTSupCont (9) 4 Experiments 4.1 Experimental Settings 4.1.1 Dataset. We train and evaluate our models on the widely used Phoenix14T dataset, which comprises 8,257 sequences of Ger- man Sign Language (DGS) performed by 9 signers, covering 1,066 glosses and a vocabulary of 2,887 unique words [32]. While limited, this dataset is a standard benchmark in SLP, making it a reliable starting point for assessing model performance and comparison with existing methods before scaling to richer datasets. 4.1.2 Preprocessing. 3D skeletal coordinates are extracted using MediaPipe’s pose and hand landmarks detection4. Joint positions are then refined following the method of Zelinka and Kanis [52], which interpolates missing joints and applies inverse kinematics to correct misplacements while preserving bone length consistency. Finally, skeletons are normalized as in (Stoll et al. [40]), based on shoulder-to-shoulder distance to reduce size variation across sub- jects. We use gloss sequences as input for the generation process, fol- lowing the original PT paper [34] and subsequent works [41, 45, 48]. 4.1.3 Evaluation Metrics. We evaluate the tested configurations using standard metrics in SLP to quantify the alignment between generated and reference skeletons. Specifically, we compute"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 7, "text": "sub- jects. We use gloss sequences as input for the generation process, fol- lowing the original PT paper [34] and subsequent works [41, 45, 48]. 4.1.3 Evaluation Metrics. We evaluate the tested configurations using standard metrics in SLP to quantify the alignment between generated and reference skeletons. Specifically, we compute the Mean Joint Error (MJE), defined as the Euclidian distance between predicted and ground truth joints averaged over all joints and times steps, as used in prior work [1, 41, 45]. Similarly, we define the Mean Bone Angle Error (MBAE) as the mean angular deviation (in degrees) between predicted and reference bones. It quantifies articulation accuracy independently of bone length, which makes it particularly appropriate for evaluating our quaternion-based vari- ant. We also compute the Probability of Correct Keypoint (PCK), which measures the proportion of predicted joints falling within a joint-specific neighborhood of their corresponding ground truth positions in the image plane. This neighborhood is defined for each joint, projected onto the (𝑥,𝑦) plane, as a threshold 𝛼of the radius of its bounding disk. As in (Kapoor et al. [19]), we choose 𝛼= 0.2. This metric accounts for varying spatial scales across different body parts. For both MJE and PCK, sequences are first aligned using Dy- namic Time Warping (DTW) based on Euclidean distance between joint coordinates. For MBAE, the applied DTW is instead computed to minimize angular differences between corresponding bones. It is worth noting that our focus is ultimately on the relative changes in the reported metrics with respect to the PT baseline, rather than on their absolute values. This approach allows us to assess whether the proposed changes lead to measurable improve- ments within PT-like architectures. 4.1.4 Implementation Details. We retain the original configuration of the PT model from the reference repository, using 2 layers, 4 attention heads, and an embedding size of 512 for both the encoder and decoder. The temperature parameter for the LGlossSupCont is set to 𝜏= 1. Moreover, for LGlossSupCont, we set the scaling factor to 𝜆= 10−4 and the batch size to 64. 4https://github.com/google-ai-edge/mediapipe IVA Adjunct ’25, September 16–19, 2025, Berlin, Germany Guilhem Fauré, Mostafa Sadeghi, Sam Bigeard, and Slim Ouni Training is conducted on an NVIDIA GeForce RTX 2080 Ti GPU. No significant computational overhead is observed between the baseline and the quaternion-based variant, with training times averaging ∼1h50 for 1000 epochs. Adding contrastive losses sub- stantially increases runtime, requiring ∼3h30 with LSBERTSupCont and up to ∼11h with LGlossSupCont. 4.2 Results Figure 6: Qualitative comparison of ground truth and pre- dicted skeletal poses with the base PT model and its quaternion-based variant. 4.2.1 3D Cartesian Positions VS Quaternion-based Rotations. As shown in Table 1, encoding poses through bone rotations using Table 1: Evaluation metrics on Phoenix14T test set for dif- ferent configurations. Values are reported as Mean±std. A bold score indicates the best result. The second-best result is underlined. MJE(↓) MBAE(↓) PCK(↑) 3D cart. (base) 0.41±0.08 36.93±6.74 0.25±0.11 w/ gloss cont. 0.40±0.08 36.02±6.95 0.29±0.12 𝜆 w/ SBERT cont. batch = 64 0.0001 0.40±0.08 36.68±6.97 0.28±0.12 0.0005 0.40±0.08 36.57±6.88 0.28±0.12 0.001 0.40±0.08 36.76±6.76 0.28±0.13"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 8, "text": "are reported as Mean±std. A bold score indicates the best result. The second-best result is underlined. MJE(↓) MBAE(↓) PCK(↑) 3D cart. (base) 0.41±0.08 36.93±6.74 0.25±0.11 w/ gloss cont. 0.40±0.08 36.02±6.95 0.29±0.12 𝜆 w/ SBERT cont. batch = 64 0.0001 0.40±0.08 36.68±6.97 0.28±0.12 0.0005 0.40±0.08 36.57±6.88 0.28±0.12 0.001 0.40±0.08 36.76±6.76 0.28±0.13 0.005 0.41±0.08 38.14±6.67 0.26±0.11 0.01 0.42±0.08 38.93±6.56 0.26±0.11 0.1 0.44±0.09 39.52±7.14 0.24±0.11 batch w/ SBERT cont. 𝜆= 0.001 128 0.43±0.08 39.06±6.83 0.25±0.11 256 0.42±0.08 38.97±7.04 0.25±0.11 quaternions 0.44±0.08 35.66±7.39 0.22±0.10 w/ gloss cont. 0.42±0.09 34.69±7.19 0.26±0.12 𝜆 w/ SBERT cont. batch = 64 0.05 0.44±0.08 36.54±7.98 0.24±0.11 0.1 0.43±0.09 36.36±7.23 0.25±0.12 1 0.43±0.08 36.70±7.08 0.25±0.11 Figure 7: Bar plots of MJE per skeletal part on Phoenix14T test set between the base PT model and its quaternion-based variant. quaternions—paired with geodesic loss optimization (see the \"quater- nions\" row)—leads to a slight relative reduction (−3%) in mean angular error compared to the baseline approach using joint 3D Cartesian positions and MSE loss (\"3D cart (base)\" row). This spe- cific improvement aligns with the objectives of geodesic loss, which better respects the manifold structure of rotations. Qualitative anal- ysis also reveals that the quaternion-based model often produces crisper and more distinct manual articulations, while the baseline tends to generate smoothed, averaged motions (see Figure 6). These results are consistent with findings from (Tang et al. [41]), which emphasize the benefits of modeling bone orientations. Towards Noise Reduction in SLP via Quaternions and Contrastive Learning IVA Adjunct ’25, September 16–19, 2025, Berlin, Germany However, using rotations instead of positions leads to dimin- ished performance on standard joint-based metrics, such as MJE, especially noticeable for the dominant right hand (Figure 7). This performance drop is also reflected in lower PCK scores. 4.2.2 PT Model with Contrastive Objectives. Integrating a con- trastive loss into the baseline Progressive transformers model con- sistently improves performance across all evaluated metrics. In particular, the use of LGlossSupCont results in a 16% relative improve- ment in PCK (\"w/ gloss cont.\" in Table 1), from 0.25 to 0.29, while the SBERT embeddings-based variant achieves a 12% relative increase (“w/ SBERT cont.” row), from 0.25 to 0.28. These results are in line with previous studies such as (Walsh et al. [45]) and (Zuo et al. [55]) which reduce pose representation space density through vector quantization. Interestingly, the model trained with LSBERTSupCont performs slightly worse than the one using LGlossSupCont. A likely explanation lies in the nature of the supervisory signals: SBERT-based supervision induces a smoother, more continuous embedding structure by align- ing pose sequence similarities with sentence embedding similarities in [0, 1], whereas gloss-based supervision relies on binary similarity labels from shared glosses. This discretization may lead to stronger clustering effects, aiding training convergence and generalization. It would be beneficial to evaluate these approaches using metrics that better reflect semantic or linguistic intelligibility. Given that stylistic variation still exists in the evaluation data, standard posi- tional metrics may not fully capture the comprehensibility of a sign. A generated sign may remain highly intelligible despite significant deviation from a reference pose, suggesting that current metrics might"}
{"doc_id": "2508.14574v1", "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14574v1", "chunk_id": 9, "text": "metrics that better reflect semantic or linguistic intelligibility. Given that stylistic variation still exists in the evaluation data, standard posi- tional metrics may not fully capture the comprehensibility of a sign. A generated sign may remain highly intelligible despite significant deviation from a reference pose, suggesting that current metrics might underestimate improvements that matter most for end-users. 4.2.3 Combining Quaternions Pose Encoding and Contrastive Losses. Combining quaternion-based pose encoding with contrastive train- ing objectives compensates for the positional metric degradation observed when using geodesic loss alone. This combination also leads to further improvements in angular accuracy. Specifically, training the quaternion-based model with LGlossSupCont results in an additional 1° reduction in mean angular error, a 4% drop in MJE (from 0.44 to 0.42), and an 18% increase in PCK (from 0.22 to 0.26) relative to the same model trained without contrastive objective. These findings suggest that integrating rotational encoding with angular-loss objectives and semantic-aware contrastive losses ef- fectively addresses variability in signer morphology and style, ulti- mately producing clearer and more consistent sign motions. 5 Conclusions We introduced and explored two complementary strategies to im- prove the classical Progressive Transformers model for sign lan- guage production, focusing on mitigating the impact of morpho- logical and stylistic variability among signers. Our experiments, conducted on the Phoenix14T dataset, demonstrate that: (1) En- coding skeletal poses using bone rotations (quaternions) and op- timizing them with a geodesic loss leads to more distinct angular motions, particularly for hand and finger articulations; (2) Augment- ing the decoder with a contrastive loss that structures self-attention embeddings yields consistent improvements across all metrics, es- pecially when using shared glosses to define positive sequence pairs; (3) Combining both methods results in further gains in an- gular precision while preserving joint position accuracy. These results advocate for the systematic inclusion of skeletal structure and rotation-aware representations, along with semantic-guided contrastive learning, in future SLP model training pipelines. As future work, we plan to incorporate back-translation metrics, such as BLEU, to more accurately evaluate the semantic intelligibil- ity of generated sign sequences, beyond purely spatial or angular error measures. In addition, we aim to refine the contrastive super- vision strategy by leveraging sentence Transformer embeddings to define positive and negative sequence pairs based on seman- tic similarity thresholds. This approach could offer a hybrid be- tween the current discrete gloss-based method and the continuous SBERT-based formulation, potentially improving the alignment of the learned pose representations with semantic meaning. Finally, given the known limitations of the Phoenix14T dataset, fu- ture work will involve evaluating our approach on the more recent Mediapi-RGB French Sign Language dataset, which offers greater diversity and scale, with over 86 hours of video [27]. Acknowledgements This work is part of Défi Inria COLaF, which was financed by Plan National de Recherche en Intelligence Artificielle."}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 0, "text": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs Luca Annese1, Sabrina Patania1, Silvia Serino1,Tom Foulsham3, Silvia Rossi2, Azzurra Ruggeri4 and Dimitri Ognibene1,3 1 University of Milan-Bicocca, Milan, Italy {luca.annese1,sabrina.patania,silvia.serino,dimitri.ognibene}@unimb.it 2 University of Essex, Colchester, UK foulsham@essex.ac.uk 3 University of Naples Federico II, Naples, Italy silvia.rossi@unina.it 4 TUM School of Social Sciences and Technology, Munich, Germany Abstract. Recent advances in large language models (LLMs) and rea- soning frameworks have opened new possibilities for improving the per- spective -taking capabilities of autonomous agents. However, tasks that involve active perception, collaborative reasoning, and perspective tak- ing (understanding what another agent can see or knows) pose persis- tent challenges for current LLM-based systems. This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the per- formance of LLM-based agents within a ReAct framework. We propose a structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into “thought-action” examples by prompting an LLM to explicitly articu- late the reasoning behind each decision. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions. These findings suggest that structured examples alone are insufficient for ro- bust perspective-taking, underscoring the need for explicit belief track- ing, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents. Keywords: perspective taking · LLMs · active vision · theory of mind · planning . 1 Introduction Effective interaction in multi-agent systems, especially those involving human- Artificial intelligence (AI) collaboration, requires more than basic task execution. It demands the capacity for perspective-taking: the ability to model what others can see, know, or intend. This includes reasoning about both physical viewpoints 2 L. Annese et al. (e.g., what another agent can perceive in the environment) and epistemic states (e.g., what they know or believe). Perspective-taking is essential for generat- ing contextually appropriate responses and adapting to ambiguity in real-world scenarios. In this work, we explore how perspective-taking can be operationalized within the ReAct (Reason+Act) framework [32], which interleaves natural language reasoning with environment-grounded actions. Unlike static perception-action pipelines, ReAct enables agents to reason explicitly before acting, providing an ideal foundation for handling the interpretive demands of perspective-taking tasks. We focus on a modified version of the Director Task [14], adapted to a par- tially observable environment inspired by [28]. In our setting, already explored in [25] and [24], a Director issues instructions to a Matcher agent, which must re- trieve a target object. Crucially, the environment contains occlusions and hidden containers that affect both agents perception, meaning that the Matcher may have to actively perceive [20, 21] to infer what the Director sees and does not see, to resolve ambiguity. This setup simulates real-world collaborative scenarios where agents have asymmetric and limited"}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 1, "text": "object. Crucially, the environment contains occlusions and hidden containers that affect both agents perception, meaning that the Matcher may have to actively perceive [20, 21] to infer what the Director sees and does not see, to resolve ambiguity. This setup simulates real-world collaborative scenarios where agents have asymmetric and limited access to information. To enable grounded perspective-taking, we introduce a novel training method that builds on search-based reasoning from the Fast Downward planner. We con- struct complete reasoning trees that encode possible thought-action trajectories in similar conditions. From these trees, we extract three types of example se- quences: – G-type: optimal sequence from initial state to task success; – E-type: all paths that reach informative states, where the agent gains new knowledge through sensing, movement, or object interaction; – L-type: locally optimal decisions at each state, simulating reactive step-wise decision-making. Each sequence is transformed into a chain of thought-action pairs using a LLM, enabling agents to learn not just what to do, but why, increasing both inter- pretability and transferability. We evaluate our method across seven environments of increasing complexity, varying both the spatial layout and the ambiguity of instructions to intensify the perspective-taking demands. For each trial, the agent is shown examples from six environments and evaluated in the held-out seventh, testing generalization across diverse settings. Tasks ranged from fully disambiguated instructions to scenarios requiring the agent to infer what the Director sees or does not see. Empirical results show that while G-type and E-type examples support task efficiency and exploration, only L-type examples slightly improved agent behaviour, specifi- cally by reducing excessive clarification requests and promoting more grounded decision-making. To interpret these results, we introduce a functional characterisation of the cognitive demands associated with different task conditions. These demands (ranging from simple common-ground attentional filtering [13], to mentalising about hidden content [4], and metacognitive evaluations of epistemic cost and Who Sees What? 3 utility [15]) emerge in increasingly complex environments and help explain when LLM agents succeed and where they systematically fail. Ultimately, while embedding cognitively structured examples into the ReAct loop scaffolds some forms of perspective-taking—particularly those grounded in reactive reasoning and observable cues—it may not suffice for enabling higher- order inference, imaginative simulation, or consistent collaborative strategies that unfold over multiple steps. Such strategies require balancing social rea- soning, physical exploration, and interlocutor modelling, while accounting for both information gain and the cost or risk of failure. While recent work suggests that LLMs exhibit emerging capabilities for social reasoning and information gathering [26, 2], our findings underscore the need for their integration with socially-aware, active information-seeking strategies [21, 20], in order to support robust collaboration in open-ended, multi-agent environments [3, 19]. 2 Related work In recent years, there has been growing interest in the application of large lan- guage models (LLMs) and multimodal foundation models in robotics and collab- orative systems for high-level reasoning, perception, and decision-making [22]. These models are pre-trained on vast amounts of internet-scale data and exhibit impressive generalization capabilities [6], enabling robots to handle a wide range of open-ended scenarios. Models such as SayCan"}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 2, "text": "guage models (LLMs) and multimodal foundation models in robotics and collab- orative systems for high-level reasoning, perception, and decision-making [22]. These models are pre-trained on vast amounts of internet-scale data and exhibit impressive generalization capabilities [6], enabling robots to handle a wide range of open-ended scenarios. Models such as SayCan [1] and Inner Monologue [11] demonstrate how LLMs can break down abstract goals into practical steps by combining high-level reasoning with grounded robotic actions. A core component of effective multi-agent interaction is perspective-taking, namely, the ability to represent a situation from an alternate viewpoint [9]. This includes visual perspective-taking, distinguished between Level-1 (inferring what others can see) and Level-2 (inferring how things appear to others), and spatial perspective-taking, which involves representing relative spatial relations through egocentric or allocentric reference frames [7]. Visual perspective-taking, particularly Level-2, has been closely linked to theory of mind (ToM), as both require agents to maintain decoupled mental representations. In frameworks like ReAct [32], perspective-taking is framed as a dynamic reasoning process that accompanies acting, enabling agents to update their knowledge in real time. Efforts to enhance perspective-taking in LLMs have largely focused on lan- guage based evaluations. Studies using false-belief tasks indicate that while older models (e.g., GPT-2, early GPT-3) struggle with ToM tasks, more recent sys- tems (e.g., GPT-4) display emerging but unstable capabilities [16]. Techniques like the SimToM prompting framework [31] explicitly instruct models to simulate other agents’ perspectives, reducing the intrusion of background omniscience. In the visual domain, datasets such as Isle-Bricks and Isle-Dots [8] reveal that while many vision-language models (VLMs) can detect objects in a scene, they often fail at reasoning about what is visible from an observer’s viewpoint. Advanced models like GPT-4V perform well on Level-1 tasks but show notable drops on Level-2 challenges involving viewpoint rotation and mental transformation [17]. 4 L. Annese et al. Beyond static perception, benchmarks such as ActiView [30] introduce active visual exploration, requiring models to shift or zoom their viewpoint to gather relevant information—tasks that remain difficult for current models. Complementary to these efforts, recent research has begun to explore the synergy between LLMs and classical symbolic planning systems to enhance structured reasoning. Hybrid models like LLM+P [18] use planners to generate plans from formal representations (e.g., PDDL), then convert them into natural language for execution by LLMs. Other works, such as PSALM [33], investi- gate using LLMs to synthesize or refine planning domains, while others employ LLMs to verbalize or critique symbolic plans [11]. These approaches leverage the complementary strengths of symbolic methods (e.g., correctness, structure) and language models (e.g., flexibility, generalization). Building on these developments, our work explores how symbolic planning can serve as a source of structured, cognitively meaningful training examples for LLMs. This method bridges symbolic and neural paradigms by using planning structures to ground language-based cognitive traces, offering a new way to study and enhance perspective-taking and decision-making in LLM-based agents. 3 Method 3.1 Simulated Task Environment To explore perspective-taking in goal-directed interaction, we developed a sim- ulated household-like environment using the Planning Domain Definition Lan- guage (PDDL). The environment"}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 3, "text": "planning structures to ground language-based cognitive traces, offering a new way to study and enhance perspective-taking and decision-making in LLM-based agents. 3 Method 3.1 Simulated Task Environment To explore perspective-taking in goal-directed interaction, we developed a sim- ulated household-like environment using the Planning Domain Definition Lan- guage (PDDL). The environment represents a shared space between two agents: a Director, who knows the identity and location of the target object, and a Matcher, who must retrieve the object based on limited perceptual cues and di- alogue. The space includes multiple locations such as a desk, shelf, and drawers, with some objects hidden inside containers that can be opened or closed. The PDDL domain models both spatial structure and perceptual asymmetry. Each agent can perceive the contents of its own location, as well as any adjacent locations, simulating a partially shared field of view. This shared access supports basic grounding, while still requiring inference about what the other agent can or cannot see. In most cases, ambiguity is introduced by presenting two objects of the same type (e.g., two ties of different colours). This setup requires the Matcher to rely on cues beyond spatial proximity or direct visual recognition, such as dialogue with the Director or strategic exploration, to disambiguate the task. Figure 1 shows such a design graphically. To systematically vary perspective-taking difficulty, we designed seven task types, ranging from fully observable to highly ambiguous settings, as shown in Table 1. The Matcher begins each trial at a random location and must perform a sequence of actions: moving, opening containers and optionally asking questions to infer the correct object. The agent must decide whether more information Who Sees What? 5 Fig. 1. Schematic view showing two examples of the experimental environment. In each case the top row shows the three locations and their item contents, and the yellow arrows indicate possible movements from the Matcher. The target is outlined in red. Each agent can see only the location in front and immediately adjacent (yellow shad- ing). The Matcher can move between locations, take an item from the location directly in front, or ask the Director a question. The Director can only answer from their fixed position. In the Near condition (top panel), there is ambiguity because two candidate objects fit the Director’s request (Gold shirt and Silver shirt). In the Distractor condi- tion (bottom panel), the Matcher can see an item (Red tie) that matches the request, yet is not the correct target. is required to make the choice and how to find it. This design enables con- trolled evaluation of perspective-taking under increasing complexity, balancing grounded environment interaction with higher-order inference demands. 3.2 Strategy Generation To generate structured behavioral sequences, we employed a modified version of the Fast Downward planner, adapted to expose the internal reasoning pro- cess underlying its search, as shown in Figure 2. Specifically, the planner was equipped to output a reasoning tree that records the sequence of states and actions considered during planning. This tree represents a detailed trace of the planner’s decision space, capturing both explored and"}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 4, "text": "expose the internal reasoning pro- cess underlying its search, as shown in Figure 2. Specifically, the planner was equipped to output a reasoning tree that records the sequence of states and actions considered during planning. This tree represents a detailed trace of the planner’s decision space, capturing both explored and selected paths through 6 L. Annese et al. Environment Information State Matcher Spatial State Ambiguity Resolution Base Both see both objects & ar- eas; Director names target explicitly. Distant from both target and distractor. Processing ini- tial demand. Perspective Taking Matcher sees both objects; Director sees only target and one area less. Close to both target and distractor. Using common ground. Distractor Matcher sees distractor; Di- rector sees target. Each sees an area the other can’t. Distant from target, close to distractor. Exploring un- seen perspec- tives. Near Both see both objects. Matcher sees an area more. Close to target, dis- tant from distractor. Asking clarifica- tion. Far Both see both objects. Matcher sees an area more. Close to distractor, distant from target. Asking clarifica- tion. Hidden Director sees Target. Matcher does not. Each sees an area the other can’t. Distant from target. No Distractor. Exploring un- seen perspec- tives. Not That Matcher sees only distrac- tor; Director sees both. Each sees an area the other can’t. Close to distractor, distant from target. Asking clarifica- tion. Table 1. Environment Types and Their Perspective-Taking Challenges the environment. From this reasoning tree, we derived three types of training examples, each emphasizing a distinct mode of behavior, by extracting them through three different tree-traversal strategies.: – Goal-directed trajectories (G-type): These are action sequences that lead from the initial state to the goal state, representing the planner’s optimal solution path. They reflect efficient behavior under the assumption that the agent has access to all relevant information for completing the task. – Information-seeking trajectories (E-type): These examples were extracted by identifying branches of the reasoning tree that led to states where critical information was gained, such as observing hidden objects or reducing am- biguity about the target. This type of example prioritizes epistemic actions and models behavior driven by uncertainty reduction rather than direct goal completion. – Local decision points (L-type): For this example type, we identified the lo- cally optimal action at each state visited during planning, contrasted explic- itly with the other possible actions available at that point. This approach emphasizes granular decision-making and trains agents to reason about why the chosen action is superior to its alternatives given a single observation. Who Sees What? 7 These examples are grounded in the planner’s reasoning trace, providing insight into the deliberative process behind goal achievement and information gathering. While the Director is not explicitly modeled in the environment, its influence is indirectly embedded in the task configuration, for instance, through the target object’s proximity to the Director’s location, prompting the Matcher to infer spatial and referential cues. This setup supports the generation of cognitively rich example sequences from a formally grounded planning system. 3.3 Example Generation The final step of the methodology involves transforming the planner-derived"}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 5, "text": "configuration, for instance, through the target object’s proximity to the Director’s location, prompting the Matcher to infer spatial and referential cues. This setup supports the generation of cognitively rich example sequences from a formally grounded planning system. 3.3 Example Generation The final step of the methodology involves transforming the planner-derived action sequences into structured cognitive examples that simulate the internal reasoning of an agent engaged in perspective-taking. These examples are con- structed as “thought-action” pairs, where each action is preceded by a textual description of the agent’s inferred reasoning process. To produce these thought sequences, a large language model (GPT o3-mini) was prompted with each action sequence, drawn from the G-, E-, or L-type examples, along with contextual information such as the agent’s current state, visible items, and the overall task objective. The model was asked to articulate the reasoning that might justify each step in the sequence, generating natural language explanations simulating deliberation and inference. The model was instructed to generate such sequences employing the following prompts: Strategy Prompt G-type Given the sequence of actions the agent executed until reaching its goal in a specific scenario, reconstruct the agent’s reasoning step by step. Explain how each action contributed to achieving the goal. E-type Given a sequence of actions taken until the agent reaches an informa- tive state (i.e., a state that provides new information), reconstruct the agent’s reasoning step by step. Describe how each action led to gaining information. L-type Given the agent’s last action, the set of possible actions, and the cor- rect action in a specific scenario, explain the agent’s reasoning behind selecting that particular action over the alternatives. Table 2. Reasoning Strategy Prompts This transformation enriches the examples with cognitive structure, allow- ing them to reflect key faculties involved in perspective-taking. Each thought segment captures perceptual assessment (e.g., recognizing that an object is not currently visible), inferential reasoning (e.g., hypothesizing where a hidden item might be based on contextual cues), and decision-making under uncertainty (e.g., choosing to ask the Director when the situation is ambiguous). These elements 8 L. Annese et al. are not explicitly modeled in the PDDL domain, but are essential for simulating realistic agent behavior in social contexts. Fig. 2. Graphic view of example generation pipeline. The panel above shows the rea- soning tree produced by the Fast-Downward planner, highlighting the three different paths according to the corresponding strategy. The section below is an example (G- type path) generated from the Base environment. 4 Experiments We first augmented the Fast Downward planner (A⋆with the admissible hmax heuristic) to emit all evaluated actions during search, from which we built a complete reasoning tree. From this, we extracted three trajectory types, G-type, E-type, and L-type, each corresponding to a distinct decision-making strategy. These were fed to GPT o3-mini using strategy-specific prompts to generate natural-language thought–action chains: step-by-step goal reasoning (G-type), information-seeking justification (E-type), and local action selection rationale (L-type). To evaluate the Matcher–Director interaction, we instantiated two LLM- agents (both GPT o3-mini) in our PDDL-based household environment. The stationary Director issued natural-language instructions of varying ambiguity; the"}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 6, "text": "o3-mini using strategy-specific prompts to generate natural-language thought–action chains: step-by-step goal reasoning (G-type), information-seeking justification (E-type), and local action selection rationale (L-type). To evaluate the Matcher–Director interaction, we instantiated two LLM- agents (both GPT o3-mini) in our PDDL-based household environment. The stationary Director issued natural-language instructions of varying ambiguity; the ReAct-based Matcher alternated between LLM reasoning and grounded ac- tions to interpret and execute the task. For each of the seven task types (ranging Who Sees What? 9 from disambiguated to highly ambiguous), the Matcher was trained on struc- tured examples from the other six, and tested on the held-out one. Each test scenario was repeated across five trials. Performance was assessed using failure rate, step count, clarification queries, and epistemic actions. In parallel, we enhanced the PDDL domain to allow the planner to com- pute optimal perspective-taking strategies by including epistemic (ASK) actions where ambiguity required them. This enabled the generation of example sets with (+ask) or without (–ask) clarification steps, providing expert baselines for reasoning under uncertainty and testing whether such exposure influenced agent behaviour. To probe this further, we used these planner-optimal trajectories to generate an additional set of G-, E-, and L-type examples, enriched with epistemically aware reasoning but following the same prompting schema. We report aggregated averages by task (see Tables 3, 4, 5) to compare how example types shape Matcher behaviour across perspective-taking demands. De- spite encoding optimal epistemic strategies, these enriched examples did not yield consistent performance gains. Neither success rates nor behavioural quality (e.g., unnecessary queries, incorrect assumptions) improved significantly. Possible rea- sons for this outcome are explored in the following section. Table 3. First Take on Correct Target (%) across Scenarios Example type Persp Far Hidd Not Dist Base Near G-type-ask 100 0 100 0 20 100 100 G-type+ask 100 0 100 0 40 100 100 E-type-ask 100 0 100 20 0 100 100 E-type+ask 100 0 100 0 40 100 100 L-type-ask 100 0 100 0 40 100 100 L-type+ask 100 0 100 0 40 100 100 No Examples 100 20 100 0 40 100 100 Table 4. Average Number of Steps Example type Persp Far Hidd Not Dist Base Near AVG G-type-ask 1 4.8 3 5.8 5.2 2 1 3.26 G-type+ask 1 5 3 5.8 5 2 1 3.26 E-type-ask 1 4.4 3 6 5 2 1 3.2 E-type+ask 1 4.6 3 5.6 5.2 2 1 3.2 L-type-ask 1 4.4 3 4.8 4.6 2 1 2.97 L-type+ask 1 5 3 5.8 5 2 1 3.26 AVG 1 4.7 3 5.63 5 2 1 3.19 No Examples 1 4.4 3 5.8 4.6 2 1 3.11 Planner 1 3 2 3 2 2 2 2.14 10 L. Annese et al. Table 5. Average Number of Ask Actions Example type Persp Far Hidd Not Dist Base Near AVG G-type-ask 0 1.8 1 1.8 2.4 0 0 1 G-type+ask 0 2 1 1.8 2.4 0 0 1.03 E-type-ask 0 1.4 1 2.4 2 0 0 0.97 E-type+ask 0 1.6 1 1.6 2.6 0 0 0.97 L-type-ask 0 1.4 1 1.4 2 0 0 0.83 L-type+ask"}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 7, "text": "Dist Base Near AVG G-type-ask 0 1.8 1 1.8 2.4 0 0 1 G-type+ask 0 2 1 1.8 2.4 0 0 1.03 E-type-ask 0 1.4 1 2.4 2 0 0 0.97 E-type+ask 0 1.6 1 1.6 2.6 0 0 0.97 L-type-ask 0 1.4 1 1.4 2 0 0 0.83 L-type+ask 0 2 1 1.8 2.6 0 0 1.06 AVG 0 1.7 1 1.8 2.33 0 0 0.98 No Examples 0 1.6 1 1.8 2 0 0 0.91 Planner 0 1 0 1 0 0 1 0.43 5 Discussion Our results reveal that success in collaborative reference hinges on three orthog- onal but interacting cognitive demands. Throughout we denote them as F1-F3 to emphasise their functional independence. F1 Common-ground filtering. Listeners must inhibit any object that the Di- rector cannot currently see, a Level-1 perspective-taking operation that relies mainly on selective attention [13, 15]. F2 Imagining Director-privileged space. When the layout contains occluded regions visible only to the Director, the agent must construct counterfactual scenes and reason about what may be present there, a genuinely mentalistic computation that recruits full Theory of Mind (ToM) [3]. F3 Metacognitive cost-benefit evaluation. Because belief tracking, explo- ration, and clarifying questions all incur different costs, the agent must decide whether to pay those costs or rely on faster egocentric heuristics [26, 29, 27, 23, 10]. When do the factors matter? Whenever the task could be solved by F1 alone, as in Base, Perspective-Taking, and Near first-take accuracy was perfect. Intro- ducing F2 precipitated large drops (Distractor, Hidden, Not That); accuracy rebounded only after the agent executed an exploratory ‘look-inside’ or issued a query. Pure F3 pressure (Far) likewise reduced first-take accuracy, even though no ToM inference was required. Table 3 quantifies this triple dissociation. Selective attention versus Theory of Mind. The split between F1 and F2 mirrors the neuroscientific dissociation between the dorsal rTPJ region, associated with social ToM reasoning, and a neighbouring ventral patch specialised for atten- tional re-orienting [12, 5]. Our LLM agent performs the attentional filter (F1) flawlessly but fails when genuine belief reasoning (F2) is needed, reproducing the human pattern reported by [15]. Who Sees What? 11 Why do planner-optimal traces fail to transfer? Fast-Downward plans embody the invariant ‘act only once the target is uniquely identifiable to the Director’. Yet few-shot exposure to those traces did not boost performance because (i) GPT-o3 implicitly assigns near-zero cost to questions, so it still ‘plays safe,’ and (ii) the linear action lists do not make the underlying cost rationale explicit. Without an explicit belief state and cost model, the LLM reverts to the heuristic ‘grab the closest match, ask if unsure’. Prompting design for exploration Beyond cost modelling, an additional limita- tion concerns the formulation of prompts used to frame the problem and describe the scenario. Several failures related to F2 and F3 occurred in situations where the agent would need to hypothesise the existence of relevant but currently un- seen objects, an ability central to active vision. These may result from prompts that insufficiently foreground the plausibility of missing information in unex- plored, yet"}
{"doc_id": "2508.14564v1", "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14564v1", "chunk_id": 8, "text": "scenario. Several failures related to F2 and F3 occurred in situations where the agent would need to hypothesise the existence of relevant but currently un- seen objects, an ability central to active vision. These may result from prompts that insufficiently foreground the plausibility of missing information in unex- plored, yet accessible, regions. Without an explicit representation of such areas, the agent may not infer that exploration is necessary or worthwhile. Prompt strategies that better support uncertainty reasoning and hypothesis generation about occluded content could thus be essential for more robust epistemic be- haviour. Limitations and future directions. The present grid world offers only binary visi- bility; richer social environments include graded salience, gaze cues, and compet- ing conversational goals. Extending the benchmark to those dimensions should exert stronger pressure on F2 and F3, revealing whether the same failure modes persist or whether multi-modal grounding and cost signals can scaffold more robust ToM. 6 Conclusions We find that a ReAct loop backed by GPT-o3 already supports flawless Level- 1 perspective taking (F1), but remains brittle on two fronts: (i) F2, imagining alternative objects in occluded space, and (ii) F3, weighing the immediate versus delayed costs of belief-driven action. Bridging these gaps may require explicit belief state tracking [4, 20], learned cost models, and prompting strategies that foreground hidden regions and explicitly stimulate hypothesis generation about unseen content. Testing agents in richer, more uncertain settings extracted from robot sensors, and penalising gratuitous queries, will be essential for advancing from attentional filtering to full, cost-aware Theory of Mind competence. Acknowledgments. This work was supported by the Volkswagen Foundation under the funding programme “Open Up – New Research Spaces for the Human- ities and Cultural Studies,” project “Developing an Artificial Social Childhood (ASC) to improve AI causal reasoning, information gathering and decision mak- ing,” reference 9E530. 12 L. Annese et al."}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 0, "text": "EmoTale: An Enacted Speech-emotion Dataset in Danish Maja J. Hjuler2,3∗, Harald V. Skat-Rørdam1, Line H. Clemmensen1, and Sneha Das1† 1Dept. of Applied Mathematics and Computer Science, Technical University of Denmark, 2800 Lyngby, Denmark 2University Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France 3School of Computer Science, Queensland University of Technology, Brisbane QLD 4000, Australia Email: maja-jonck.hjuler@univ-grenoble-alpes.fr, {harsk, lkhc, sned}@dtu.dk Abstract—While multiple emotional speech corpora exist for commonly spoken languages, there is a lack of functional datasets for smaller (spoken) languages, such as Danish. To our knowl- edge, Danish Emotional Speech (DES), published in 1997, is the only other database of Danish emotional speech. We present EmoTale1; a corpus comprising Danish and English speech recordings with their associated enacted emotion annotations. We demonstrate the validity of the dataset by investigating and presenting its predictive power using speech emotion recognition (SER) models. We develop SER models for EmoTale and the reference datasets using self-supervised speech model (SSLM) embeddings and the openSMILE feature extractor. We find the embeddings superior to the hand-crafted features. The best model achieves an unweighted average recall (UAR) of 64.1% on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable to the performance on DES. Index Terms—speech emotion recognition, speech processing, paralinguistic speech, transferability, evaluation. I. INTRODUCTION & BACKGROUND Speech signals are rich in information, both linguistic (in the form of sentences and words) and paralinguistic (denoting mood and affective state). Speech also carries information about multiple, potentially personal traits of the speaker, such as age, gender, and nationality. Multiple psychological and neuroscientific models of the mind hypothesize that language and emotion are certainly linked [1]. For example, some cul- tures express anger more vocally, while others might be more restrained. Investigating voice and speech to judge emotional states dates back more than half a century [2], [3], and the earliest speech emotion recognizers (SERs) were proposed over two decades ago [4], [5]. Emotions are inherently subjective; different people per- ceive emotions differently, and this can lead to differences in annotating emotional data [6], [7]. Overall, two different labeling schemes are adopted in the literature: categorical class labels, which are nominal and discrete, and dimensional labels, which are continuous. The former often follows the basic emotion theory developed by Paul Ekman [8], which assumes the existence of six basic and universal emotions that transcend language, cultural, and ethnic differences. The ∗The author was affiliated with the Technical University of Denmark when this work was carried out. †Corresponding Author 1Link to the dataset and source code: https://github.com/snehadas/EmoTale Fig. 1. Processing pipelines for hand-crafted (top) and deep features (bottom). emotions, also known as The Big 6, are anger, disgust, fear, happiness, neutral, and sadness. Following the dimensional scheme, emotions can be described numerically in the two dimensions activation/arousal and valence, or in three di- mensions by including dominance. For example, happiness is characterized by positive valence, high activation, and neutral dominance, i.e., neither dominant nor submissive. In speech emotion recognition (SER), frequently used emo- tions include happiness, anger, sadness, disgust, fear, frustra- tion, surprise, and boredom. For a baseline comparison, it is common practice"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 1, "text": "mensions by including dominance. For example, happiness is characterized by positive valence, high activation, and neutral dominance, i.e., neither dominant nor submissive. In speech emotion recognition (SER), frequently used emo- tions include happiness, anger, sadness, disgust, fear, frustra- tion, surprise, and boredom. For a baseline comparison, it is common practice to include neutral as one of the emotions expressed. In many SER databases, utterances are spoken with enacted emotions, but emotional responses can also be induced through specific tasks, scenarios, or stimuli to capture gen- uine emotional expressions. Alternatively, natural/spontaneous speech can be collected from existing digital resources, such as TV shows or podcasts, and annotated retrospectively. For English SER, IEMOCAP [9] and MSP-Podcast [10], [11] are two of the most frequently used corpora due to their relatively large size, and the inclusion of both categorical and dimen- sional labels. The Danish DES database [12] was published in 1997 and contains four speakers (two male and two female) expressing five emotions: neutral, surprise, happiness, sadness, and anger. All utterances are equally balanced for each gender and actor. In listening tests for the DES corpus, emotions were correctly classified 67.3% of the time on average [12]. However, DES includes single words and questions, and it was not developed specifically for speech emotion recognition No. Danish sentence English sentence 1. Dugen ligger p˚ a køleskabet. The tablecloth is lying on the fridge. 2. Det sorte ark papir er placeret deroppe ved siden af tømmerstykket. The black sheet of paper is located up there beside the piece of timber. 3. De bar det bare ovenp˚ a og nu skal de ned igen. They just carried it upstairs and now they are going down again. 4. Det vil være p˚ a det sted, hvor vi altid opbevarer det. It will be in the place where we always store it. 5. Om syv timer er det morgen. In seven hours it will be morning. Five emotions: Neutral, Anger, Sadness, Happiness, Boredom TABLE I DANISH AND ENGLISH SENTENCES IN EMOTALE. purposes. Contemporary state-of-the-art SER research is most often based on deep learning models [13]–[16] like the SUPERB benchmark [17]. Rapid development of scale-based deep learn- ing was enabled by the availability of large and exhaustive speech emotion datasets [9], [10]. The most comprehensive SER datasets are in English or other large (spoken) languages. Developing SER models that transfer well to unseen lan- guages, addresses the lack of resources in smaller languages while enabling the accessibility of these models. However, at minimum, a test dataset is necessary to validate the suitability and safety of a SER model before deployment. In this work, we take the first step towards presenting a Danish-SER dataset to address the gap in functional datasets. Our contributions are: 1) the EmoTale dataset: a corpus comprising 450 Danish and 350 English speech recordings with associated categorical and dimensional emotion annotations. 2) we also present a thorough validation of the quality of EmoTale by analyzing its predictive capacity using reference datasets. Through this process, we revisit transferability of SER and present insights with respect to other multilingual"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 2, "text": "450 Danish and 350 English speech recordings with associated categorical and dimensional emotion annotations. 2) we also present a thorough validation of the quality of EmoTale by analyzing its predictive capacity using reference datasets. Through this process, we revisit transferability of SER and present insights with respect to other multilingual datasets. II. DESIGN OF EMOTALE To enable cross-corpus comparability and transferability, the design choices in EmoTale are similar to existing small-scale SER datasets. The data collection procedure was inspired by the Berlin Database of Emotional Speech (Emo-DB) [18]. A. Dataset curation Recruitment: Participants with acting experience and Danish and English language skills were recruited through physical flyers and posts on social media, and theater schools in the Greater Copenhagen area were contacted by email and phone. An online registration form was available in Google Forms, where participants signed up by providing their con- tact information and choosing their desired experiment date from a list of options. The exclusion criteria were age < 7 years or no Danish-speaking skills. In compliant with GDPR requirements, we obtained written consent from the participant or the guardian of participants under 18, and information about gender and age was recorded. Data collection procedure: The data recordings were per- formed in multiple sessions and locations with no ambient noise. At the start of a session, the participant was fitted with RØDE Wireless Go microphones and was walked through the experiment, and allowed to ask questions. Five sentences were enacted with five different emotions (Tab. I), and the Fig. 2. Age and gender distribution of EmoTale participants. participant enacted all sentences for a specific emotion before moving on to the next. The sentences are translations of selected sentences from Emo-DB [18]; to minimize subjec- tive associations and differences, the sentences were selected such that they are emotionally neutral and comprise minimal contextual information. We relied on the participant’s ability to self-induce an emotion by recalling a situation where it had been felt strongly. The participants were allowed to repeat the sentences as many times as they liked, but only the last recording was retained. Since Danish speakers are fluent in En- glish, the participants could choose to contribute with enacted English speech in addition to Danish. The utterances were recorded at a 48 kHz sampling frequency and saved in .WAV format. The audio filenames comprise the meta information on the language, speaker ID, emotion, and sentence. For example, the file DK_004_A_5.wav is the fifth sentence spoken by speaker 004 in Danish, with angry affect. Data protection and ethical considerations: Ethical approval was obtained from the institutional review board prior to the study [19]. The samples are pseudo-anonymized by generating a random identifier for each participant. Since the emotions are enacted and the selected sentences do not contain personal contextual information, potential misuse of the data to cause harm to the participants is reduced. The dataset is supported by a datasheet [20], in the later part of the paper. Annotation procedure: In addition to emotion categories, many existing datasets annotate speech-emotion samples using dimensional labels [9], [21]. We adopt a"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 3, "text": "information, potential misuse of the data to cause harm to the participants is reduced. The dataset is supported by a datasheet [20], in the later part of the paper. Annotation procedure: In addition to emotion categories, many existing datasets annotate speech-emotion samples using dimensional labels [9], [21]. We adopt a similar approach in the EmoTale corpus, where utterances are manually annotated for arousal, valence, and dominance on a scale from 1 to 5, with increments of 0.5. Arousal indicates the level of excitement or activation associated with the emotion, ranging from calm (1) to excited (5). Valence reflects the emotional tone, with values ranging from negative (1) to positive (5). Dominance measures the level of dominance associated with the emotion, with a scale from submissive (1) to dominant (5). The first, second, and last authors independently assigned labels to all utterances in EmoTale, each providing one cat- egorical label for the intended emotion and three numerical labels for arousal, valence, and dominance. The categorical annotations serve to validate the enacted emotions. B. Description of EmoTale EmoTale comprises emotional speech from 18 participants, of whom 12 are female and six are male. The total number of a1 vs. a2 a1 vs. GT a2 vs. GT κ 0.71 0.75 0.85 TABLE II COHEN’S KAPPA RELIABILITY BETWEEN CATEGORICAL LABELS FROM ANNOTATORS 1 AND 2 (A1, A2) AND THE PREDEFINED EMOTION (GT). Arousal Valence Dominance CCC 0.72 0.75 0.57 TABLE III CONCORDANCE CORRELATION COEFFICIENT (CCC) BETWEEN DIMENSIONAL LABELS FROM ANNOTATORS 1 AND 2. Danish and English utterances are 450 and 350, respectively. The average age of the participants was 22.8 years, ranging from 9 to 39 years old. Age and gender distributions of participants are illustrated in Fig. 2. The goal of this dataset is to develop infrastructure to enable the evaluation and safe deployment [22] of existing speech processing and SER on the Danish-speaking population, including children. Therefore, speakers under the age of 18 are also included in the dataset. Some files were cropped to exclude a ‘click’ sound (from experimenters’ keyboard) at the start or end of the recording. Inter-rater reliability (IRR): In addition to the enacted emotion, three independent annotators provided four labels per instance: one categorical label for the intended emotion and three numerical labels for arousal, valence, and dominance, each ranging from 1 to 5 with increments of 0.5. Valence [1-negative, 5-positive], activation [1-calm, 5-excited], and dominance [1-weak, 5-strong]. We employ Cohen’s Kappa (κ) [23] to assess inter-rater reliability (IRR) between the categorical labels provided by the first two annotators, as well as to evaluate their agreement with the predefined ground truth emotion. The IRR results are presented in Table II-B. A value of κ = 1 implies perfect agreement, and κ = 0 means the agreement is exactly what would be expected by chance. 0.7 < κ indicates good to substantial agreement [24]. To evaluate the IRR between dimensional emotion annotations (valence, arousal, dominance), we employ Concordance Cor- relation Coefficient (CCC) [25], which is suitable for ratings on a fine-grained, continuous, or interval scale. As seen in Table III, the"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 4, "text": "be expected by chance. 0.7 < κ indicates good to substantial agreement [24]. To evaluate the IRR between dimensional emotion annotations (valence, arousal, dominance), we employ Concordance Cor- relation Coefficient (CCC) [25], which is suitable for ratings on a fine-grained, continuous, or interval scale. As seen in Table III, the results indicate moderate to strong agreement for arousal and valence, and moderate agreement for dominance. III. VALIDATING THE emotion-SIGNAL IN EMOTALE We validate the signals in EmoTale by a) comparing human annotations to the predictions from a pre-trained SSL, and b) analyzing the predictive power of the data samples by training and evaluating SER models in Danish. We employ the following datasets as references on the validity and quality of EmoTale: Emo-DB (German), Urdu (Urdu) [26], DES (Dan- ish), and AESDD (Greek) [27]. A. Labels: Human-annotation vs. Pre-Trained Model A pre-trained model (PTM), w2v2-FT-dim, fine-tuned on the MSP-Podcast with dimensional labels2, outputs activation, valence, and dominance scores ranging between 0 to 1. These 2https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim Fig. 3. EmoTale Annotator 1 labels for the utterances in Danish compared to the dimensional labels computed from PTM output. Fig. 4. Concordance Correlation Coefficients between w2v2-FT-dim and manual labels across different datasets. Dominance labels are only available for DES and EmoTale. were rescaled to a range between 1 and 5 to compare with manual labels as follows: Ai,scaled = 1 + 4 Ai −Amin Amax −Amin , (1) where Ai denotes the activation score, and Amin and Amax are the overall minimum and maximum activation scores, respectively. Valence and dominance scores were rescaled in the same way. Fig. 3 compares the scores predicted by the PTM for EmoTale to the labels by Annotator 1. Activation and valence labels for Emo-DB, Urdu, and AESDD were employed using [28], while DES and EmoTale were annotated as part of this work. Predictions by w2v2-FT-dim were compared against the human-annotated labels using CCC in Fig. 4; A high CCC is observed for activation/arousal and dominance, implying a high agreement between the outcome of PTM and human-annotated labels, but the scores are consistently lower for valence over all datasets. B. Validation with handcrafted features & PTM embeddings We explore the predictive power of the samples in EmoTale with respect to the reference datasets by evaluating the per- formance of SER models on all the datasets, in the process revisiting cross-lingual transferability of speech-emotions. Method: As for the SER models, we employ a support vector classifier (SVC), a) with hand-crafted features, and b) PTM embeddings, also known as deep features. The PTM feature embeddings are extracted as the last hidden states of the pre- trained model, i.e., the last layer before any task-specific head is applied, and it is assumed that model embeddings provide a compact representation of the emotional content in a speech signal. In transformer models, this is the output from the final transformer block. The experimental procedure is adapted from the one outlined by Wagner et al3. The speech samples in DES, EmoTale, Emo-DB, Urdu, and AESDD datasets are downsampled to 16 kHz as the PTM input requirement, and stereo"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 5, "text": "speech signal. In transformer models, this is the output from the final transformer block. The experimental procedure is adapted from the one outlined by Wagner et al3. The speech samples in DES, EmoTale, Emo-DB, Urdu, and AESDD datasets are downsampled to 16 kHz as the PTM input requirement, and stereo audio files were converted to mono by averaging to a single channel. The pipelines are illustrated in Fig. 1. The eGeMAPS (extended Geneva Minimalistic Acoustic Parameter Set) [29] and the ComParE (Computational Paralin- guistics Challenge) [30] feature sets were extracted using the openSMILE toolkit [31] and serve as two separate baselines. These were tested against embeddings from the wav2vec2 base model4 [32] as well as a wav2vec2 model fine-tuned for SER on the RAVDESS corpus [33] (w2v2-FT-cat)5 and one fine-tuned on MSP-Podcast (w2v2-FT-dim) [34]. The latter is fine-tuned on dimensional scores and not categorical labels, therefore, the output of hidden states is necessary to access the latent space of the model. Model embeddings are extracted by applying average pooling over the hidden states of the last transformer layer. Subsequently, the features are input to a SVC with a linear kernel, and Leave-One-Speaker- Out (LOSO) cross-validation is applied. In each fold, features were standardized using the mean and standard deviation of the respective training set. We used a linear kernel to resemble the method in [35]. Evaluation: Applying LOSO cross-validation introduces vari- ability in the performance metric. The aggregated unweighted average recall (UAR) across cross-validation folds is used for evaluation. However, it may overlook performance differences across individual speakers. Each iteration of LOSO involves training a model on a different subset of data, hence, for S speakers it is more accurate to consider the S different models separately. For the same dataset, each model is tested under the same conditions, whereby we can apply paired t- tests to statistically model performances. The UAR scores are computed as the sum of class-wise recall divided by the number of emotion classes, and the overall score is the average UAR across all datasets. To provide a comprehensive view of model performance, we report both the aggregated results (highlighted rows in Tab. IV) and the mean results across speakers (Speaker UAR). The former combines the predictions of all folds into a single confusion matrix and calculates the UAR. Once the SVC parameters are fixed, changing the random seed does not affect results, hence, the standard deviation is zero. The latter calculates the UAR for each LOSO cross-validation fold individually and takes the mean to consider how well the model generalizes across speakers. Similarly, sentence UARs are found by first grouping prediction sentences, calculating the UAR per group, and then taking a simple average across the groups. In this way, all the speakers and sentences are given equal weight. Standard deviations are reported to provide insights into the variability of the UAR scores across speakers, sentences, and datasets. Sentence UARs are not included for the Urdu corpus since it 3https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb 4https://huggingface.co/facebook/wav2vec2-base 5https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition Corpus Emo-DB DES EmoTale Urdu AESDD Overall #Speakers 10 4 18 22 6 ComParE 79.0 48.5"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 6, "text": "weight. Standard deviations are reported to provide insights into the variability of the UAR scores across speakers, sentences, and datasets. Sentence UARs are not included for the Urdu corpus since it 3https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb 4https://huggingface.co/facebook/wav2vec2-base 5https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition Corpus Emo-DB DES EmoTale Urdu AESDD Overall #Speakers 10 4 18 22 6 ComParE 79.0 48.5 52.0 50.0 58.0 57.5 ± 11.2 Speaker UAR 74.9 ± 7.9 48.5 ± 10.5 50.9 ± 11.1 49.7 ± 41.5 58.2 ± 10.9 Sentence UAR 79.5 ± 5.0 48.5 ± 10.1 52.0 ± 1.5 - 58.1 ± 5.4 eGeMAPS 64.3 42.7 46.0 58.0 47.6 51.7 ± 8.1 Speaker UAR 60.3 ± 14.1 42.7 ± 13.6 44.8 ± 13.8 29.2 ± 21.6 47.8 ± 11.8 Sentence UAR 63.9 ± 5.5 42.7 ± 9.0 46.0 ± 3.9 - 47.7 ± 7.1 w2v2 base 58.9 32.7 29.7 33.5 41.8 39.3 ± 10.6 Speaker UAR 56.7 ± 8.0 32.7 ± 4.2 29.4 ± 6.9 23.1 ± 26.7 41.7 ± 12.3 Sentence UAR 58.4 ± 8.2 32.7 ± 8.8 29.8 ± 2.6 - 41.8 ± 8.5 w2v2 FT dim 96.1† 67.7 64.1‡ 59.5 83.2‡ 74.1 ± 13.6 Speaker UAR 94.7 ± 3.9 67.7 ± 4.0 62.0 ± 12.4 48.4 ± 36.5 83.1 ± 7.8 Sentence UAR 96.1 ± 3.0 67.7 ± 12.7 64.1 ± 5.4 - 83.2 ± 5.9 w2v2 FT cat 88.8 62.7 59.6‡ 52.5 77.5‡ 68.2 ± 13.1 Speaker UAR 88.1 ± 5.0 62.7 ± 5.1 57.8 ± 12.2 37.1 ± 34.7 77.5 ± 10.8 Sentence UAR 88.3 ± 4.8 62.7 ± 10.5 59.6 ± 3.5 - 77.6 ± 8.1 TABLE IV UAR (%) FOR SVC BASED ON HAND-CRAFTED & DEEP FEATURES AS MEAN AND STD. DEV. (µ ± σ) OVER LOSO FOLDS. THE PTMS ARE A BASE MODEL (W2V2-B) AND MODELS FINE-TUNED ON DIMENSIONAL LABELS (W2V2-FT-D I M) AND CATEGORICAL LABELS (W2V2-FT-C A T). UAR SCORES ACROSS SPEAKERS AND SENTENCES PROVIDE INSIGHTS INTO THE PERFORMANCE VARIABILITY, EXCEPT FOR URDU, WHICH CONTAINS NATURAL SPEECH. † AND ‡ MARK THE SINGLE BEST AND THE TWO BEST MODELS ACROSS LOSO FOLDS WITH STATISTICAL SIGNIFICANCE FOR A DATASET. contains natural utterances, hence, no sentences are repeated. Results: For Emo-DB, the results reported in Table IV using ComParE and w2v2-FT-dim embeddings are reproduced from [13]. The performance trends observed on EmoTale align with those seen in Emo-DB and DES, reinforcing the consistency and reliability of the dataset. Specifically, the UAR scores for the three datasets follow the same trend with model performance in descending order using the features: w2v2-FT-dim, w2v2-FT-cat, ComParE, eGeMAPS, and w2v2-b. Interestingly, Urdu deviates from the trend with eGeMAPS features outperforming both ComParE and the embeddings from the PTM fine-tuned on categorical labels, w2v2-FT-cat. In all cases, deep features from the fine- tuned models yield the highest UARs, while embeddings from the wav2vec2 model without fine-tuning perform the worst. Furthermore, the PTM fine-tuned on dimensional labels leads to the highest mean UAR across datasets, highlighting the benefit of fine-tuning. To further validate model performance on EmoTale, we applied pairwise t-tests [36] across LOSO folds to assess the statistical significance of differences between feature sets. While fine-tuning"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 7, "text": "perform the worst. Furthermore, the PTM fine-tuned on dimensional labels leads to the highest mean UAR across datasets, highlighting the benefit of fine-tuning. To further validate model performance on EmoTale, we applied pairwise t-tests [36] across LOSO folds to assess the statistical significance of differences between feature sets. While fine-tuning of dimensional labels (w2v2-FT-dim) yields a statistically significant improvement over categorical labels (w2v2-FT-cat) for Emo-DB, this distinction does not hold for EmoTale nor the other datasets, which negates the argument against categorical labels [37]. Similarly, for several datasets, there is no statistically significant difference in model performance when training on eGeMAPS features compared to wav2vec2 base model embeddings. The single best and two best models with statistical significance are marked in Table IV when such a conclusion could be drawn based on pairwise t-tests. These findings further strengthen EmoTale’s role as a reliable benchmark for emotional speech, with results that reflect those of established corpora. We also observe from Tab. IV that the scores for DES are relatively low, and model performance is sentence-dependent, in contrast to the EMO-DB and EmoTale. This could be Fig. 5. Unweighted Average Recall (UAR) scores for SVC trained on ComParE features (left) and pre-trained model embeddings (right). explained by DES being designed differently from the other datasets. For example, the sentence ID NO refers to a single word Nej (No), which may not be sufficient for the model to recognize the emotion. Similarly, the sentences with ID: SE4, SE5, SE6, and SE8 are all questions, and might be spoken with a different intonation. Embeddings from the PTMs generally produce more stable results (low variation), however, a relatively high standard deviation is observed for w2v2-FT-dim features across EmoTale speakers (12.4) and DES sentences (12.7). This could be explained by differences inherent in the two datasets: EmoTale has a larger age range of speakers compared to the other datasets, and DES contains sentences that vary in linguistic and paralinguistic content. The UAR scores for Urdu are very speaker-dependent compared to the other datasets. This can be explained by a high number of speakers and a low number of sentences per speaker. To assess cross-corpus transferability, the SVC models were retrained to recognize a subset of four emotion classes (happy, angry, sad, and neutral) on ComParE and w2v2-FT-dim features for the Emo-DB, EmoTale, DES, and Urdu corpora. These datasets were selected specifically because they include all four emotion labels. The UAR scores for all train-test combinations are shown in the heatmaps in Fig. 5. The in- corpus UAR scores in the diagonal of the matrices are found by LOSO cross-validation following the same methodology as earlier, but only including the four emotions. We wish to develop SER models that generalize well on new, unseen data, especially in real-world applications. Furthermore, a model that transfers well is less likely to be overfitted on the training data. Although performance generally drops in the cross-corpus domain, the deep features seem to be more transferable. Importantly, EmoTale proves to be a strong evaluation benchmark. While models trained on EmoTale per- form comparably to"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 8, "text": "Furthermore, a model that transfers well is less likely to be overfitted on the training data. Although performance generally drops in the cross-corpus domain, the deep features seem to be more transferable. Importantly, EmoTale proves to be a strong evaluation benchmark. While models trained on EmoTale per- form comparably to those trained on other corpora, EmoTale consistently supports meaningful generalization. For example, inferring on Emo-DB yields higher cross-domain scores than in-corpus UAR scores when trained on EmoTale and DES. This continues the pattern from the previous analysis, where Emo-DB achieved significantly higher model performances than the other corpora. This could be explained by the percep- tion tests carried out during the creation of Emo-DB, where utterances recognized by more than 80% of the listeners were kept in the database. Hence, the database is expected to contain utterances with highly pronounced affect. IV. CONCLUSIONS Unavailability of Danish affect datasets not only impedes the development of the technology, but also impacts the validation of existing methods on Danish speakers. We present EmoTale, a bilingual enacted speech-emotion dataset in Dan- ish and English, intended to enable the evaluation of SER models in the Danish language. In addition to categorical emotion labels, EmoTale includes dimensional annotations for arousal, valence, and dominance. Annotation reliability is high: Concordance Correlation Coefficient (CCC) scores indicate moderate to strong agreement for arousal and valence, and moderate agreement for dominance, while Cohen’s Kappa values indicate substantial consistency in categorical labeling. To demonstrate the validity of the dataset, we evaluate its labels and predictive capacity using both pre-trained model embeddings and hand-crafted, acoustic features. Our experi- ments demonstrate that (a) model performance on EmoTale is comparable to that on established reference datasets, and (b) feature embeddings from PTMs consistently outperform hand- crafted features, particularly in cross-corpus transfer scenarios. While models trained on EmoTale perform comparably to those trained on other corpora, EmoTale consistently supports meaningful generalization. These findings further strengthen the validity of EmoTale as a reliable benchmark for Danish emotional speech. V. ACKNOWLEDGMENT Co-funded by the French National Research Agency under the Pantagruel project (ANR-23-IAS1-0001) and the European Union under the Marie Skłodowska-Curie Grant Agreement No 101081465 (AUFRANDE). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the Research Executive Agency, which cannot be held responsible for them. DATASHEET FOR EMOTALE In line with the proposal on datasheets for datasets by Gebru et al. [38], we provide the datasheet for the EmoTale corpus, also available as a standalone document with the dataset. A. Motivation For what purpose was the dataset created? Unavailability of Danish affect datasets not only impedes the development of the technology, but also impacts the validation of existing methods on Danish speakers. The introduction of our corpus is necessary to, at the very least, be able to validate the performance of SER models for the Danish language. Who created the dataset and on behalf of which entity? The dataset was created by Maja Jønck Hjuler, Line Katrine Harder Clemmensen, and Sneha Das at the Technical University"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 9, "text": "corpus is necessary to, at the very least, be able to validate the performance of SER models for the Danish language. Who created the dataset and on behalf of which entity? The dataset was created by Maja Jønck Hjuler, Line Katrine Harder Clemmensen, and Sneha Das at the Technical University of Denmark. Who funded the creation of the dataset? The dataset creation is funded by the larger WristAngel project which is funded by an exploratory Synergy grant from the Novo Nordisk Foundation and is a collaboration with Copenhagen University Hospital, the Child Psychiatry Research Unit. B. Composition What do the instances that comprise the dataset represent? The instances are audio files of enacted emotional speech in Danish and in English. The speakers enact predefined sentences while expressing predefined emotions. How many instances are there in total? The EmoTale corpus consists of a total of 800 audio instances, comprising 450 emotional speech recordings in Danish and 350 in English. Each recording features one of five different enacted emotions, and the dataset is balanced across these emotions. What data does each instance consist of? Each instance consists of raw audio data in WAV format, captured at a sampling frequency of 48 kHz. Each recording corresponds to one of five enacted emotions: Neutral, Anger, Sadness, Happiness, or Boredom, and is based on predefined sentences that are translations from the German Emo-DB cor- pus, designed to be emotionally neutral to minimize contextual bias. Is there a label or target associated with each instance? In addition to the enacted emotion, three independent anno- tators provided four labels per instance: one categorical for the emotion chosen from the five possible classes, and three numerical for arousal, valence, and dominance in a range of 1 to 5 with increments of 0.5. The ranges are defined as: Valence [1-negative, 5-positive], activation [1-calm, 5-excited], and dominance [1-weak, 5-strong]. Is any information missing from individual instances? Everything is included. No data is missing. Are there recommended data splits? There are no recommended data splits for training, validation, and testing within the dataset itself. However, it is common practice to create stratified splits across speakers and emotions. Are there any errors, sources of noise, or redundancies in the dataset? See preprocessing below. Does the dataset contain data that might be considered confidential? The data does not contain any signals reflecting on the state of an individual, minimizing the potential negative impact on the individuals. Does the dataset identify any subpopulations? Participants range in age from 9 to 39 years. The dataset includes 18 participants, with 12 females and 6 males. Is it possible to identify individuals, either directly or indirectly from the dataset? Individuals can be identified indirectly from the EmoTale corpus due to the unique characteristics of each participant’s voice, which can reveal their identity. All participant informa- tion has been pseudoanonymized by assigning random IDs. C. Collection Process & Preprocessing How was the data associated with each instance acquired? The data recordings were performed in several sessions in dif- ferent locations. In each session, the participant was placed in a quiet"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 10, "text": "reveal their identity. All participant informa- tion has been pseudoanonymized by assigning random IDs. C. Collection Process & Preprocessing How was the data associated with each instance acquired? The data recordings were performed in several sessions in dif- ferent locations. In each session, the participant was placed in a quiet room and fitted with wireless RØDE microphones paired with the corresponding receiver. Five sentences were enacted with five different emotions, and the participant enacted all sentences for a specific emotion before moving on to the next. The participants were allowed to repeat sentences as often as they liked, but only the last recording was kept in the database. Most often, the recording was made in the first attempt. Who was involved in the data collection process? Participants with acting experience and Danish and English language skills were recruited through physical flyers and posts on social media, and theater schools in the Greater Copenhagen area were contacted by email and phone. Over what timeframe was the data collected? The data was collected as part of a master’s thesis project lasting 5 months. Were any ethical review processes conducted? Ethical approval was obtained from the institutional review board prior to the study [19]. Did the individuals in question consent to the collection and use of their data? Abiding by GDPR requirements, written consent was obtained from participants or their guardians prior to data collection. Was any preprocessing/cleaning/labeling of the data done? Some instances were cropped to exclude audible ’clicks’ from the experimenter pressing the keyboard at the beginning or end of recordings. The audio files are named according to the same template including information about the lan- guage, speaker ID, emotion, and sentence. For example, the file DK 004 A 5.wav contains the fifth sentence spoken by speaker 004 in Danish, with angry affect. Was the “raw” data saved in addition to the prepro- cessed/cleaned/labeled data? Yes. The authors can provide the raw data upon request. D. Uses Has the dataset been used for any tasks already? The dataset paper investigates the dataset’s capacity for pre- dicting speech emotions through the development of speech emotion recognition models using Self-Supervised Speech Model embeddings and the openSMILE feature extractor. Furthermore, cross-corpus transferability of the models was investigated. What (other) tasks could the dataset be used for? The dataset can also be used for ASR, due to the availability of speech and the corresponding transcription. The enacted English speech in addition to Danish will aid research and in- vestigation into speech systems, for instance when the speaker remains identical, but language changes, hence towards more universal speech emotion models. Are there tasks for which the dataset should not be used? Given the size of the dataset, it should not be used for tasks that require large-scale training of complex machine learning models. Additionally, it is not suitable for tasks that require spontaneous emotional speech, as the recordings consist of enacted emotions rather than natural emotional expressions. E. Distribution & Maintenance How will the dataset will be distributed? The dataset can be accessed at https://github.com/snehadas/ EmoTale. Will the"}
{"doc_id": "2508.14548v1", "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14548v1", "chunk_id": 11, "text": "of complex machine learning models. Additionally, it is not suitable for tasks that require spontaneous emotional speech, as the recordings consist of enacted emotions rather than natural emotional expressions. E. Distribution & Maintenance How will the dataset will be distributed? The dataset can be accessed at https://github.com/snehadas/ EmoTale. Will the dataset be distributed under a copyright or other intellectual property (IP) license? The data will be distributed under a copyright. There is no license, but users are requested to cite the corresponding paper if the dataset is used. Who will be maintaining the dataset and how can they be contacted? The dataset will be maintained by the corresponding author Sneha Das (sned@dtu.dk). Will the dataset be updated? This dataset will not be updated in terms of the number of samples or participants."}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 0, "text": "Reasoning is about giving reasons Krunal Shah∗ Yutori Inc. ktgshah@gmail.com Dan Roth University of Pennsylvania danroth@seas.upenn.edu Abstract Convincing someone of the truth value of a premise requires understanding and articulating the core logical structure of the argument which proves or disproves the premise. Understanding the logical structure of an argument refers to understanding the underlying “reasons\" which make up the proof or disproof of the premise - as a function of the “logical atoms” in the argument. While it has been shown that trans- formers can “chain” rules to derive simple ar- guments, the challenge of articulating the “rea- sons” remains. Not only do current approaches to chaining rules suffer in terms of their inter- pretability, they are also quite constrained in their ability to accommodate extensions to the- oretically equivalent reasoning tasks – a model trained to chain rules cannot support abduction or identify contradictions. In this work we suggest addressing these short- comings by identifying an intermediate repre- sentation (which we call the Representation of the Logical Structure (RLS) of the argument) that possesses an understanding of the logical structure of a natural language argument – the logical atoms in the argument and the rules incorporating them. Given the logical struc- ture, reasoning1 is deterministic and easy to compute. Therefore, our approach supports all forms of reasoning that depend on the logi- cal structure of the natural language argument, including arbitrary depths of reasoning, on-the- fly mistake rectification and interactive discus- sion with respect to an argument. We show that we can identify and extract the logical structure of natural language arguments in three popular reasoning datasets with high accuracies, thus supporting explanation generation and extend- ing the reasoning capabilities significantly.2 *Work done while the author was a student at the Univer- sity of Pennsylvania. 1This work focuses on extracting the propositional struc- ture of the arguments, and does not attempt to handle quanti- fiers, as is the case for most recent neural work on reasoning. 2This work was originally written in 2021 and was never Figure 1: An example of an argument from the RULE- TAKERS (Clark et al., 2020) dataset. The goal is to determine the truth value of the query given some facts and rules (that all together constitute an argument). The logical structure of the argument at the bottom conveys a deeper understanding of the reasoning process, simpli- fies the verification process, and supports reasoning on “What if Harry was young but not nice?\" and \"If Harry is round then does that make him nice?\". 1 Introduction Natural language is often used to make arguments. For example, given some facts and rules, a deduc- tive argument can be made, as in Fig. 1. For many years, the key approach in AI, to reasoning argued in natural language (NL) text, was to map the text to a formal structure and then use well defined, deter- ministic, algorithms that support exact reasoning over these representations (Hayes, 1977; Hobbs et al., 1988; McCarthy, 1963, 1976). However, latter works have discussed the brittleness of this approach, arguing that, in general,"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 1, "text": "language (NL) text, was to map the text to a formal structure and then use well defined, deter- ministic, algorithms that support exact reasoning over these representations (Hayes, 1977; Hobbs et al., 1988; McCarthy, 1963, 1976). However, latter works have discussed the brittleness of this approach, arguing that, in general, natural language text is too expressive to be mapped into a formal, symbolic representation (Charniak, 1973) The recent success of neural models and, more specifically, transformer based language models, published. It suggests that Language Models could support reasoning and interpretability better by converting text to an intermediate formal representation that solvers can reason about deterministically. While the Language Models have improved significantly since then, this line of thought is still valid and very important to pursue. has prompted researchers to explore the reasoning abilities of such systems. Recent work (Clark et al., 2020; Saha et al., 2020; Tafjord et al., 2020) demon- strates the ability of transformers to perform simple chaining of rules (top half of Fig. 1) and some lim- ited form of abduction (Tafjord et al., 2020). However, it is important to note that the only level of uncertainty that exists in reasoning prob- lems expressed in natural language is the derivation of the core logical structure of the underlying NL ar- guments. Once the logical structure of an argument is given, the computational problem is determinis- tic and easy; it boils down to solving satisfiability or unsatisfiability over a small domain. In this paper we first agree with previous works that in general, it is not possible to map natural language to a symbolic representation; however, we argue that it is possible to map arguments made in natural language to their core logical structure (where this mapping might lose some information that is not relevant to the argument itself). Towards this end, we define a class of structured represen- tations, in the context of reasoning tasks, called Representation of Logical Structure (RLS). The primary contribution of this work is defining the RLS of an argument made in text and showing how, by doing so, we can explain the argument and sup- port the benefits of performing symbolic reasoning over knowledge expressed in natural language. Ex- tracting the logical structure of an argument in the form of structured representations (RLS) allows us to explain the reasoning process, support other forms of reasoning that depend on this logical struc- ture, and reason with respect to it – interact about it, identify fallacies, etc. These latter steps can be done by simply feeding the extracted representa- tions to an appropriate symbolic reasoning engine. This is a well defined and solved problem (Gu et al., 1996) and we do not attend to this part of the computation in this paper. Specifically, we propose using a sequence to se- quence transformer to extract RLS from natural language text. We demonstrate the effectiveness of our RLS extraction method, in capturing the logical structure of natural language arguments, on three datasets spanning a variety of reasoning tasks and language usage. First, our RLS extraction method shows"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 2, "text": "using a sequence to se- quence transformer to extract RLS from natural language text. We demonstrate the effectiveness of our RLS extraction method, in capturing the logical structure of natural language arguments, on three datasets spanning a variety of reasoning tasks and language usage. First, our RLS extraction method shows its success on the LEAP-OF-THOUGHT (Tal- mor et al., 2020) and CLUTRR (Sinha et al., 2019) datasets by registering 99% and 95% exact match accuracies, showing that the representation extrac- tion method can have broad coverage. Furthermore, we show the effectiveness of using a symbolic rea- soner over the extracted logical structures on the data that was used in (Clark et al., 2020; Saha et al., 2020; Tafjord et al., 2020) and demonstrate the ef- fectiveness of this approach. Specifically, we find that our method performs competitively with the current end-to-end reasoning systems on the rule based reasoning task of the RULETAKERS (Clark et al., 2020) benchmark. In the following section, we make a more de- tailed case for our methodology by addressing some of the challenges of introducing intermediate logical representations, along with the benefits of this approach. Finally, we note that the primary motivation for this work is not to improve the per- formance on existing benchmarks but to (1) em- phasize the need for finer logical explanations for reasoning tasks, (2) investigate the effectiveness of existing language models in extracting structured logical representations3 from arguments expressed in natural language, and (3) throw light on our alter- nate, more symbolic approach which leverages the language modeling advances to perform competi- tive, more explainable reasoning compared to the end to end approaches adopted by previous works. 2 “Explanations\" are symbolic In this section we try to lay out a more detailed analysis addressing some of the shortcomings of the “extracting structured logical representations\" approach and subsequently outline the notable ad- vantages of this approach. 2.1 Challenges An important shortcoming of using symbolic rea- soning methods which, as discussed earlier, was the key conceptual barrier to early efforts, is that nat- ural language is simply too expressive. Given the diverse linguistic variability of natural language, and the amount of information humans are able to communicate via natural language, we cannot expect to accurately and reliably convert NL utter- ances into structured representations. This paper agrees with this assessment and does not attempt to convert NL to a structured representation. Instead, our method of extracting the RLS of NL arguments relies on a much weaker assumption about the pres- ence of a “local\" logical structure in NL arguments. 3we use “structured representations\", “structured logical representations\" and “logical representations\" interchangeably fact: Mary is a young woman with a heart of gold. rule: If Mary has gold then she is rich. Figure 2: In the above inference, if the weak unification operator concludes that “(Mary, has, heart of gold)\" and “(Mary, has, gold)\" are equivalent then the symbolic rea- soner would explicitly indicate the same in the proof, providing insights into the implicit assumptions/knowl- edge used in the reasoning process. And in extracting the logical"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 3, "text": "above inference, if the weak unification operator concludes that “(Mary, has, heart of gold)\" and “(Mary, has, gold)\" are equivalent then the symbolic rea- soner would explicitly indicate the same in the proof, providing insights into the implicit assumptions/knowl- edge used in the reasoning process. And in extracting the logical structure we admit- tedly lose some other aspects of the text, that are not relevant to the logical argument (but could very well be relevant to other decisions made with re- spect to the text). For example, if Figure 1 were to say that “Harry works outside, to his chagrin\" then this will not affect the logical structure of the argument, and will not impact the logical atoms extracted by the method, since the mental state of Harry, while important, is irrelevant to the logical argument. Another notable challenge of using symbolic rea- soning methods with natural language is associated with their inability to handle the noise, uncertainty, and “softness\" expressed in language. However, recent work (Lewis and Steedman, 2013; Weber et al., 2019) has shown promising results in trying to mitigate these by making use of weak unification operators to replace the exact matching in symbolic reasoners. When this “soft\" unification is done by a symbolic solver, the solver provides insights into the assumptions made during the reasoning process as shown in Figure 2, since the logical atoms which are considered equivalent by the unification op- erator represent the linguistic/implicit knowledge encoded in the unification operator function. That is, if you know that the reason the outcome is such is due to unifying the variables “has gold\" and “has a heart of gold\", the process becomes more trans- parent. We note that in this work, we only perform ex- traction of structured representations from individ- ual natural language utterances rather than from complete (possibly multi-sentence) arguments. We leave the problem of extending the extraction to ac- count for the whole context of the natural language argument for future work. 2.2 Symbolic reasoning is “simple\" In this subsection, we draw attention to the obser- vation that the primary challenge in the problem of reasoning over natural language text is concerned with the ambiguity and linguistic variability associ- ated with natural language. Once we can accurately map the relevant information from the NL text into its structured logical representations, the task of rea- soning over these symbols is nearly trivial. Propo- sitional symbolic reasoning is well understood, and essentially boils down to boolean satisfiability or unsatisfiability; this can be performed very effi- ciently (Gu et al., 1996; Gomes et al., 2008) over a small number of variables. 2.3 Advantages The foundations of reasoning laid out in the field of artificial intelligence were primarily defined to deal with symbolic reasoning, as a result, there is rich and diverse literature around symbolic reason- ing. Our approach of extracting the logical rep- resentations from NL sentences allows us to take full advantage of these advances. We now try to summarize some of the advantages of our system which uses a symbolic reasoning engine on top of"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 4, "text": "is rich and diverse literature around symbolic reason- ing. Our approach of extracting the logical rep- resentations from NL sentences allows us to take full advantage of these advances. We now try to summarize some of the advantages of our system which uses a symbolic reasoning engine on top of extracted logical structures. 1. Note that once we can accurately extract the representations from the NL sentences, we can perform any kind of reasoning (e.g. deductive, abductive, contradiction detection, etc.) that de- pends on this logical structure, without the need for any additional training. This is in contrast to the existing end-to-end transformer based sys- tems which would require additional training data to learn any new kind of reasoning. 2. As discussed in Section 2.1, the usage of weak unification operators provide a potential solution to addressing the inability of symbolic reason- ing systems to handle the “softness\", noise and ambiguity expressed in natural language. We ob- serve that the usage of language models as weak unification operators additionally allows us to (1) leverage the linguistic and implicit knowl- edge of the language model and (2) explicitly lay out the unifications used in inference, as demonstrated in Figure 2. 3. Given the disentanglement of the reasoning re- sponsibilities in our method, the system allows for online mistake rectification and tweaks. For example, if a mistake is observed in the repre- sentation extraction step during inference, the mistake can be rectified by human intervention and the resulting corrected theory can be simply fed through the symbolic reasoning engine to get the correct answer. In contrast, the end-to- end systems do not provide any outlet to rectify incorrect inferences drawn by the model. 4. The above mentioned disentanglement also al- lows us to transfer the method to new kinds of rules or facts (like multivariate rules and event attributes based reasoning) by simply training a new representation extraction model. Train- ing the end-to-end systems on the other hand requires additionally gathering complete theo- ries of reasoning over the natural language sen- tences. 3 RLS and problem formulation In this section, we define the structured representa- tions proposed in this work, termed Represention of Logical Structure (RLS). We also detail how we formulate RLS for the datasets used for our experiments and then briefly describe how we for- mulate the problem of extracting RLS from natural language text. 3.1 Representation of Logical Structure (RLS) We define RLS for a natural language statement S, in the context of a reasoning task R, as a structured representation which encodes all the relevant infor- mation in statement S, i.e. for any inference/rea- soning chain which S is a part of, if we replace the statement S with its corresponding RLS then the resulting inference/reasoning chain still remains valid and evaluates to the same result as with S. Although not included in the definition, a desirable property from a good RLS formulation is that it is efficient, i.e. it only encodes the information from statements that is truly relevant to the reasoning task. More specifically, if an alternate RLS"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 5, "text": "and evaluates to the same result as with S. Although not included in the definition, a desirable property from a good RLS formulation is that it is efficient, i.e. it only encodes the information from statements that is truly relevant to the reasoning task. More specifically, if an alternate RLS formu- lation for the given task T encodes strictly less in- formation than the existing formulation while still satisfying the definition of RLS then the existing formulation would not be called efficient. Kindly note that we propose the RLS of a natural language statement as an alternative to the semantic parse of the statement. Hence, similar to the semantic parse of a statement, RLS is (a) task dependent, (b) a class of structured representations and (c) does not follow a well-defined syntax. Now we try to briefly emphasize the differences of our approach of extracting RLS compared to semantic parsing. Unlike semantic parsing which aims to extract structured logical representations from all forms of natural language text with min- imal information loss, extracting RLS is only de- fined in the context of a reasoning task meaning that it aims to only extract structured logical repre- sentations from NL sentences where a logical struc- ture can be expected to exist. Furthermore, RLS only aims to extract information from a sentence which is essential to the usage of the sentence in the context of the reasoning task, hence extracting RLS has a looser requirement on information loss compared to semantic parsing. We posit that these fundamental differences help us mitigate some of the primary issues with semantic parsing that origi- nate from the diverse variability of natural language and the amount of information humans are able to communicate via language. 3.2 Our RLS formulation We now briefly describe the RLS formulations used by our approach to encode the logical structure from natural language statements, for the problems used in our experiments. Our structured represen- tations are simple logical statements where the log- ical literals are tuples of natural language words, phrases and binary symbols. Table 1 shows some examples of natural language sentences and the corresponding RLS used by our approach. Since RLS is defined in the context of a particular rea- soning task, note that the representations follow different formulations for the different datasets and the choice for the formulation is primarily dictated by the kind of logical structure that is important for the corresponding reasoning task. 3.3 Problem formulations 3.3.1 Representation extraction formulation We model the problem of extracting RLS from a natural language sentence as a sequence to se- quence prediction task where the input is the natu- ral language utterance and the output is an encod- ing of the RLS of the NL sentence. Table 1 shows some examples of how the structured representa- tions from instances of the different datasets are encoded for the sequence output of the model. 3.3.2 Rule based reasoning The deductive reasoning task described in Clark et al. 2020 takes as input a natural language theory, consisting of a set of rules and facts described"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 6, "text": "how the structured representa- tions from instances of the different datasets are encoded for the sequence output of the model. 3.3.2 Rule based reasoning The deductive reasoning task described in Clark et al. 2020 takes as input a natural language theory, consisting of a set of rules and facts described in natural language, and a query described in NL. The goal is to predict the truth value of the query, in the world defined by the theory, under the closed world assumption (CWA). 4 Extracting the Logical Structure of Arguments In this section, we discuss the questions we wish to answer in our experiments and briefly describe the datasets, models and evaluation metrics used for the same. 4.1 Experimental settings The goal of our experimental evaluation is two fold, to first investigate whether our RLS extrac- tion method works well for different kinds of RLS formulations and natural language text; and subse- quently understand the efficacy of our reasoning system which relies on the RLS extraction method. Specifically, we design experimental settings to answer the following questions: 1. Can the RLS extraction method generalize well to different kinds of logical reasoning datasets which can benefit from reasoning over structured logical representations instead of natural language text? 2. How well can a method relying on RLS ex- traction perform on the rule based reasoning task (example in Fig. 1), compared to the end- to-end transformer systems? We use the LEAP-OF-THOUGHT (Talmor et al., 2020) and CLUTRR (Sinha et al., 2019) datasets to test the generalizability of our representation ex- traction method to answer our first question; and further use the RULETAKERS dataset (Clark et al., 2020) as the benchmark for the rule based reason- ing task to answer the second question. We now briefly describe each of these datasets. 4.2 Datasets 1. LEAP-OF-THOUGHT (Talmor et al., 2020): A dataset designed to test the ability of mod- els to reason over implicit factual knowledge. Instances of the dataset include a collection of facts and rules, expressed in templated lan- guage, along with a query such that answer- ing the query requires some implicit factual knowledge along with the explicitly stated facts and rules. This dataset is used to test whether our representation extraction method can han- dle RLS representations which primarily con- sist of phrases (as opposed to simple words) as the logical literals’ arguments (as shown in Table 1). For our experiments, we merge all the different train and test sets corresponding to the two settings of “counting\" and “hyper- nymy\" which results in around 38k/22k unique training instances and 4811/3234 unique test instances for the hypernymy/counting settings respectively and we train separate representa- tion extraction models for the two settings. The logical literals for the statements in this dataset follow the pattern (noun_phraseA, relation, noun_phraseB, polarity). The logical literals combine by using simple logical conjunction op- erators to form the RLS for the natural language statements. 2. CLUTRR (Sinha et al., 2019): An inductive reasoning benchmark which requires a model to infer a kinship relation that is not explicitly stated in the"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 7, "text": "(noun_phraseA, relation, noun_phraseB, polarity). The logical literals combine by using simple logical conjunction op- erators to form the RLS for the natural language statements. 2. CLUTRR (Sinha et al., 2019): An inductive reasoning benchmark which requires a model to infer a kinship relation that is not explicitly stated in the input while also learning the logical rules governing the relations. The benchmark consists of a natural language setting where the input relations are expressed using NL and a graph based setting where the relations are provided as symbolic inputs. The natural language descriptions are curated using crowdsourced templates and the test set consists of unseen NL templates. We use this dataset to test our representation extraction method’s ability to generalize to more natural forms of language and to a different kind of RLS formulation. Note that since the dataset deals with reasoning over kinship relations, we identify the kinship relations between entities as the logical structure to extract from instances of the dataset, as shown in Table 1. For our purpose, we generate a dataset of 8k training instances and 846 test instances of unseen templates. The logical literals for the statements in this dataset follow the pattern (personA, relationAB, personB). The logical literals combine by using simple logical conjunction operators to form the RLS for the natural language statements. Note that since the inverse of kinship relations exist, for every relation (personA, relationAB, personB), we also include the inverse relation literal (personB, relationBA, personA) in the RLS for the given natural language statement. 3. RULETAKERS (Clark et al., 2020): A collec- tion of several rule based reasoning datasets where the instances consist of theories and re- lated queries expressed in natural language. The Dataset Type Sentence RULETAKERS (fact) Sentence Harry is young and nice. Annotation (\"Harry\" \"is\" \"young\" \"+\") (\"Harry\" \"is\" \"nice\" \"+\") RLS (Harry, is, young, +) ∧(Harry, is, nice, +) Encoded Repr. <arg0> Harry <pred> is <arg1> young <pos> <and> <arg0> Harry <pred> is <arg1> nice <pos> RULETAKERS (rule) Sentence Nice people are usually round in shape. Annotation (\"someone\" \"is\" \"nice\" \"+\") -> (\"someone\" \"is\" \"round\" \"+\") RLS (someone, is, nice, +) -> (someone, is, round, +) Encoded Repr. <arg0> someone <pred> is <arg1> nice <pos> <impl> <arg0> someone <pred> is <arg1> round <pos> CLUTRR Sentence Sol took her son Kent to the park for the afternoon. Annotation edges: [(\"Sol\", \"Kent\")], edge_types: [\"son\"], genders: {\"Sol\": female, \"Kent\": male} RLS (Sol, son, Kent) ∧(Kent, mother, Sol) Encoded Repr. <arg1> Sol <pred> son <arg2> Kent <and> <arg1> Kent <pred> mother <arg2> Sol LEAP-OF-THOUGHT Sentence A mustard is not capable of shade from sun. Annotation {\"subject\": \"mustard\", \"predicate\": \"/r/CapableOf\", \"object\": \"shade from sun\", \"validity\": \"never true\"} RLS (mustard, capable of, shade from sun, -) Encoded Repr. <arg0> mustard <pred> capable of <arg1> shade from sun <neg> Table 1: Table showing how the annotations provided with the datasets are converted to their respective RLS formulations and subsequently encoded to train the sequence to sequence representation extraction model. theories consist of facts and rules about entity relations and attributes and the queries require deductive"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 8, "text": "shade from sun <neg> Table 1: Table showing how the annotations provided with the datasets are converted to their respective RLS formulations and subsequently encoded to train the sequence to sequence representation extraction model. theories consist of facts and rules about entity relations and attributes and the queries require deductive reasoning over the theory of facts and rules (example in Fig. 1). We specifically use the D3, D5, ParaRules and Birds-Electricity datasets for our experiments to investigate the performance of our reasoning method which uses a symbolic reasoner on top of the RLS ex- traction model. The D3 and D5 datasets contain 100k questions where the theories and queries are expressed in templated natural language and the queries require deductive reasoning up to 3 and 5 depths respectively. The ParaRules dataset contains 40k questions where the the- ories are paraphrased versions of the templated theories. The paraphrased templates were col- lected using crowdsourcing and the dataset is designed to test whether the methods can gener- alize to more natural forms of language. Birds- Electricity is a collection of several hand au- thored rulebases where the Birds rulebases are adapted from “birds\" logic problem from Mc- Carthy 1986 and Electricity rulebases were gen- erated by Clark et al. 2020. The dataset is used for zero-shot evaluation, to test the out- of-domain generalization ability of the models since the language, entities and attributes in the dataset are different from those observed in the training data. The logical literals for the state- ments in these datasets follow the pattern of either (entity, relation, entity, polarity) or (subject, is, property, polarity). The logical literals combine by using simple logical con- junction operators to form the RLS for the fac- tual statements. The logical atoms combine in the form l1[∧li]∗→lc to form RLS for the rule statements in the datasets. We note here that for all of the above datasets, we are able to easily generate the RLS representations from the accompanying metadata available with the datasets (as described in Table 1) and hence no annotation effort was required in this work. 4.3 Models and Evaluation Baseline For the RULETAKERS dataset, the state of the art ProofWriter (Tafjord et al., 2020) and the earlier PRover (Saha et al., 2020) models are used as the baselines methods. For our experiments on the CLUTRR and LEAP-OF-THOUGHT datasets we do not have any baseline methods because we wish to show strong performances on the represen- tation extraction task and do not try to solve the respective end tasks. Our models We use a pretrained text to text trans- former model, namely T5 (Raffel et al., 2020), for all our experiments to extract structured represen- tations from input NL text. More specifically, the model takes as input a natural language sentence and learns to predict the encoded RLS representa- tion of the input sentence, where the encoding of the RLS representation is performed as shown in Table 1. We use a T5-small model to train on the D3 dataset while all other models are trained using a T5-base model. The model"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 9, "text": "and learns to predict the encoded RLS representa- tion of the input sentence, where the encoding of the RLS representation is performed as shown in Table 1. We use a T5-small model to train on the D3 dataset while all other models are trained using a T5-base model. The model is trained to predict a structured representation of the relevant infor- mation from the input utterances. The structured representations for the different datasets are en- coded as shown in Table 1. For experiments on the RULETAKERS dataset, we make use of ProbLog (De Raedt et al., 2007) to perform inference on the extracted logical representations. Evaluation For the representation extraction task for the LEAP-OF-THOUGHT and CLUTRR datasets, we use exact matching accuracy on the extracted logical representations as the evaluation metric. Also note that we process the predicted se- quence outputs assuming they follow the expected representation encoding protocol described in Ta- ble 1. However, predictions which do not adhere to the same would result in incorrect exact matching for the representation extraction metric or result in incorrect predictions on the downstream reason- ing task. For the deductive reasoning tasks on the RULETAKERS dataset, where the goal is to pre- dict the truthfulness of the query sentence given explicit rules and facts, we use the answer accuracy as the evaluation metric. Note that we do not report proof accuracies which are previously reported in (Tafjord et al., 2020; Saha et al., 2020) because the metric makes sense for the end-to-end trans- former systems to demonstrate that the models are predicting correct answers for the “right\" reasons; however, in our case since we use a symbolic rea- soning engine it is clear that our predictions are completely faithful to the explanations generated by our method. 5 Results In this section we analyse the results of our method on various experimental settings, to answer the questions we ask in Section 4.1 about the extensi- bility of our representation extraction model and the effectiveness of our natural language reasoning method. 5.1 Representation extraction In this subsection we are interested in the gener- alization ability of the representation extraction model to different forms of structured representa- tions and natural language. Table 2 shows the per- formance of the representation extraction model on the CLUTRR and LEAP-OF-THOUGHT datasets and the high performance (95%+ and 99%+ exact match accuracy respectively) of the model demon- strates its ability to generalize to natural forms of language and different kinds of structured represen- tations. We note here that even though solving the end task is not our objective in this experimental setting, as discussed earlier, there is notable merit in reducing these tasks to reasoning over the struc- tured representations over input NL utterances as CLUTRR LEAP-OF-THOUGHT Hypernyms Counting EM Accuracy 95.9 99.6 99.8 Table 2: Results of the representation extraction model trained and tested on the CLUTRR and LEAP-OF- THOUGHT datasets, where we report the exact match ac- curacy of the extracted structured representations. The high accuracy scores show the models’ generalization ability to different kinds of natural language text"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 10, "text": "95.9 99.6 99.8 Table 2: Results of the representation extraction model trained and tested on the CLUTRR and LEAP-OF- THOUGHT datasets, where we report the exact match ac- curacy of the extracted structured representations. The high accuracy scores show the models’ generalization ability to different kinds of natural language text and structured representations. D #qns Ours4 ProofWriter PRover 0 2968 99.6 99.9 99.7 1 2406 99.2 99.3 98.6 2 1443 96.4 98.3 98.2 3 1036 93.1 98.2 96.5 4 142 87.3 91.5 88.0 All 8008 97.8 99.1 98.4 Table 3: Results of systems trained on D3 + ParaRules and tested on ParaRules where our system demonstrates competitive performance to the current SToA systems. shown in Sinha et al. 2019, where the graph based methods exhibit much better generalization com- pared to the NL based methods. 5.2 Rule based reasoning In this subsection we investigate if our method which relies on RLS as intermediate logical repre- sentations can perform competitively to the state of the art baseline systems on the rule based reasoning task. 5.2.1 ParaRules dataset Firstly, to show that our method can perform com- petitively to the state of the art models on naturally occuring text, we train a model on the combined sentences from D3 + ParaRules and test it on the ParaRules dataset in Table 3. We observe that our method performs competitively (within 1.3%) to the baseline methods thus demonstrating the over- all strong performance of our method compared to the state of the art baseline models. Note here that the trade off afforded by our method is the ability to (1) generate much more finegrained ex- planations (as shown in Figure 1) and (2) extend the RLS extraction model to a variety of tasks like contradiction detection and abductive reasoning, 4results on the subset of the test set where ProbLog did not throw an exception on the corresponding gold theories. This is an error in the dataset as pointed out in Tafjord et al. 2020. D #qns Ours5 ProofWriter PRover 0 6299 100 100 100 1 4434 100 99.1 99.0 2 2915 100 98.6 98.8 3 2396 100 98.5 99.1 4 2134 100 98.7 98.8 5 2003 100 99.3 99.3 All 20192 100 99.2 99.3 Table 4: Results of the different systems on D5 dataset. We observe that our method comfortably outperforms the baseline methods even when our system was only trained on sentences from D3 while the other two sys- tems were trained on the D5 dataset itself. Test #qns Ours ProofWriter PRover Birds1 40 90.0 95.0 95.0 Elec4 4224 100 97.1 84.8 All 5270 99.8 97.0 86.5 Table 5: Zero-shot results of models trained on some version of RULETAKERS (D3 for our method and ProofWriter and D5 for PRover) and tested on the hand authored rulebases of the Birds-Electricity dataset. The results show how our model performs better than the baseline methods, demonstrating the generalization ca- pability of our approach to out-of-distribution test sets. while marginally losing out on overall inference accuracy. 5.2.2 Generalization to higher depths We argue in the introduction that our method"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 11, "text": "rulebases of the Birds-Electricity dataset. The results show how our model performs better than the baseline methods, demonstrating the generalization ca- pability of our approach to out-of-distribution test sets. while marginally losing out on overall inference accuracy. 5.2.2 Generalization to higher depths We argue in the introduction that our method can perform reasoning to arbitrary depths and it is clear that the performance of a method which uses a symbolic reasoning engine is not constrained by the depths of reasoning. However, to demonstrate the same in practice, we train a model on the sen- tences from the D3 dataset and test it on the D5 dataset where the reasoning depths vary from 0 to 5. The results for the experiment are shown in Ta- ble 4 where we observe that our method registers perfect performance (100% accuracy), reinforcing the advantages offered by our system. Further- more, we observe perfect representation extraction (exact match) accuracy on the test set which high- lights how the system can be easily extended to accurately perform the abduction and implication enumeration tasks which were defined in Tafjord et al. 2020, since they use a similar set of natural language templates as the D5 dataset. Figure 3: An example mistake of our method on the Birds dataset, highlighting a shortcoming of our current approach. However, we note that this specific mistake can be addressed by using a smarter unification operator as part of the symbolic reasoner which can determine whether two phrases are effectively equivalent. 5.2.3 Zero shot generalization To demonstrate the generalization ability of our deductive reasoning method to unseen, out-of- distribution test sets we test the model trained on the D3 sentences (from Table 4) in a zero shot setting on the hand authored rulebases of the Birds-Electricity dataset. The results show how our method performs competitively to the baseline models thus demonstrating the out-of-distribution generalization ability of our method. For further analysis, we show an example from the Birds dataset in Figure 3 where our method fails. The example highlights a shortcoming of the method and emphasizes the need for a smarter unification operator (as opposed to the default exact matching operator) as part of the symbolic reasoner, which can effectively judge the similarity of two phrases. 5.3 Results summary In summary, we observe that the answer to our questions regarding the generalization ability of the representation extraction model and the reasoning model is encouragingly positive. This supports our proposal of identifying and extracting RLS from natural language arguments as a noteworthy alter- native to solving the natural language reasoning tasks, especially given the numerous advantages of the approach as discussed in Section 2.3. 6 Related work This work builds on a recent line of work (Tafjord et al., 2020; Saha et al., 2020; Clark et al., 2020) that tries to tackle the problem of natural language reasoning. The recent works make use of a trans- former based model to solve the rule based reason- ing task in an end-to-end manner while simultane- 5see footnote 4. ously predicting some form of explanations for the model’s"}
{"doc_id": "2508.14488v1", "title": "Reasoning is about giving reasons", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14488v1", "chunk_id": 12, "text": "et al., 2020) that tries to tackle the problem of natural language reasoning. The recent works make use of a trans- former based model to solve the rule based reason- ing task in an end-to-end manner while simultane- 5see footnote 4. ously predicting some form of explanations for the model’s answer. However, in this work we move away from the end-to-end methods and instead pro- pose identifying and extracting structured represen- tations from natural language arguments and using a symbolic reasoning engine to solve the reason- ing problem. This approach is inspired by numer- ous previous attempts (Hayes, 1977; Hobbs et al., 1988; McCarthy, 1963, 1976) at similar tasks which proposed parsing natural language utterances into different structured meaning representations. How- ever, in this work we only try to extract the logical structure from natural language arguments instead, which we argue is a reasonable assumption. 7 Discussion and Conclusion This work largely relies on our initial argument that it is possible to extract the logical structure from a NL argument. The community mostly agrees that NL is very expressive and cannot be mapped into structured representations; however, we be- lieve that this does not contradict our underlying assumptions that “locally\" logical arguments made in language can be mapped to their core logical structure. After all, this is how humans discuss these arguments, and communicate about it – they identify the core arguments and atoms, and use them to develop explanations. This is the heart of the argument we make in this paper. And, as we show, the mapping process can be accomplished at high accuracies, rivaling end-to-end methods, while providing more reasoning and explanation capabilities. In conclusion, we began this work by noting the importance of finer explanations for natural lan- guage reasoning tasks, which is missing in current methods that only provide simple sentence level explanations. To address this and the lack of exten- sibility in current systems, we proposed identifying and extracting intermediate logical representations, and reasoning over the same as an approach to the task. Consequently, we note that previous works which tried to parse NL sentences into logical rep- resentations have understandably failed. However, we instead argue that it is possible to extract the core logical structure from a natural language ar- gument and towards that end, we formulate Rep- resentations of Logical Structure (RLS) as a class of structured representations defined in the con- text of a reasoning task. We further show how our RLS extraction system can generalize to natural forms of language and different RLS formulations by showing its strong performance (95% and 99% exact match accuracy) on the datasets of CLUTRR and LEAP-OF-THOUGHT respectively. We also show the efficacy of our reasoning method on the deductive reasoning task on a series of RULETAK- ERS datasets. We motivate future work to further explore methods to extract more contextual struc- tured representations and extend it to support wider forms of natural language utterances."}
{"doc_id": "2508.14472v1", "title": "In2x at WMT25 Translation Task", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14472v1", "chunk_id": 0, "text": "In2x at WMT25 Translation Task Lei Pang Duxiaoman panglei@duxiaoman.com Hanyi Mao University of Chicago hanyim@uchicago.edu Quanjia Xiao Peking University xiaoqj@stu.pku.edu.cn HaiXiao Liu* Duxiaoman liuhaixiao@duxiaoman.com Xiangyi Li Duxiaoman xiangyi@duxiaoman.com Abstract This paper presents the open-system submis- sion by the In2x research team for the WMT25 General Machine Translation Shared Task. Our submission focuses on Japanese-related trans- lation tasks, aiming to explore a generalizable paradigm for extending large language models (LLMs) to other languages. This paradigm en- compasses aspects such as data construction methods and reward model design. The ulti- mate goal is to enable large language model systems to achieve exceptional performance in low-resource or less commonly spoken lan- guages. 1 Introduction Machine translation (MT) has long been both a high-impact application and a central research chal- lenge in natural language processing. The advent of large language models (LLMs) has reshaped MT from task-specific supervised learning toward large-scale representation learning and instruction- following paradigms, enabling steady gains across diverse language pairs (Alves et al., 2024; Jiao et al., 2023; Kocmi et al., 2024; Lu et al., 2024). Yet, two persistent gaps remain. First, while mainstream LLM training increasingly optimizes for mathematical and code reasoning, their ex- pressive and creative language abilities—e.g., id- iomaticity, stylistic naturalness, and culturally ap- propriate phrasing—are comparatively underdevel- oped(Lewkowycz et al., 2022; Liu et al., 2023; Lozhkov et al., 2024; Rozière et al., 2023; Zaitova et al., 2025). This often leads to translations that are locally literal but globally stilted, especially for informal registers, slang, and literary text . Second, model competence is unevenly distributed across languages: English receives disproportionate cover- age and quality, while many non-English languages trail in both general capability and translation natu- ralness(Aharoni et al., 2019; Johnson et al., 2017; Kocmi et al., 2023, 2024; Team et al., 2022). Com- munity findings over recent WMT cycles echo this asymmetry: despite the “LLM era”, MT is far from solved uniformly across directions, with larger gaps off English-centric pairs and on long-tail phenom- ena . This paper studies how to transfer English strength into non-English targets to improve ex- pressive and culturally faithful translation. Con- cretely, we focus on Japanese—a language where literal adequacy is not sufficient: natural Japanese requires idiomatic paraphrasing, honorific and reg- ister control, and sensitivity to genre and context. Our thesis is that English can be used as a hub lan- guage to bootstrap these capabilities via curriculum design, cross-lingual alignment, and preference sig- nals that explicitly reward naturalness. We present In2x, a Japanese-focused model de- signed to inherit general competency from En- glish while specializing for Japanese expressive- ness. At a high level, In2x operationalizes three principles: (i) English-as-hub transfer: leverage rich English data and strong English modeling to seed robust lexical/semantic priors, then transfer to Japanese via bilingual and style-augmented objec- tives; (ii) Expressiveness-first supervision: empha- size prompts and signals that drive idiomaticity and cultural appropriateness (beyond literal adequacy); (iii) Evaluation beyond metrics: complement au- tomatic metrics with human judgments targeted at idioms, slang, and stylistic naturalness. We evaluate In2x on standard WMT-style test sets and targeted Japanese-focused challenge suites that"}
{"doc_id": "2508.14472v1", "title": "In2x at WMT25 Translation Task", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14472v1", "chunk_id": 1, "text": "tives; (ii) Expressiveness-first supervision: empha- size prompts and signals that drive idiomaticity and cultural appropriateness (beyond literal adequacy); (iii) Evaluation beyond metrics: complement au- tomatic metrics with human judgments targeted at idioms, slang, and stylistic naturalness. We evaluate In2x on standard WMT-style test sets and targeted Japanese-focused challenge suites that stress idioms, slang, and style. According to the preliminary ranking results of WMT 2025, In2x outperforms many large-scale proprietary mod- els, such as Gemini-2.5-Pro (Comanici and Team, 2025), GPT-4.1 (Fachada et al., 2025), Claude-4 (Anthropic, 2025), and DeepSeek-V3 (Monisha, 2025). Overall, we make three core contributions: 1. We diagnose under-explored gaps in current LLM-based MT: the tension between heavy investment in math/code reasoning and the rel- ative neglect of creative/idiomatic language ability, and the English-vs.-non-English capa- bility asymmetry. 2. We introduce In2x, a Japanese-focused model that systematically transfers English strengths to Japanese, with an emphasis on naturalness and cultural appropriateness. 3. In this study, we introduce a detailed align- ment pipeline designed to enhance the cre- ative capabilities of language models. This approach not only improves performance in non-STEM (Science, Technology, Engineer- ing, and Mathematics) tasks but also ensures that the models maintain robust generalization abilities across diverse linguistic challenges. For instance, in the en-ja translation track, the model demonstrates outstanding perfor- mance without any task-specific fine-tuning, highlighting its adaptability and effectiveness in non-STEM domains. 2 Continue Pretraining Stage To balance the capabilities of large language models (LLMs) in both science-oriented and humanities-oriented domains during the pretraining process, we divided the continued pretraining stage into three distinct phases. The goal of this process is to enhance the model’s multilingual proficiency, improve general-purpose abilities in foundational humanities tasks, and refine its representation in specialized contexts (Brown et al., 2020; Rae et al., 2021). The training process incorporates diverse cor- pora, including a comprehensive 2 trillion to- kens dataset comprising encyclopedic knowledge, webpages, structured information, news articles, Wikipedia entries, academic papers, and STEM- related datasets (Gao et al., 2020; Raffel et al., 2020). In addition, a dedicated 500 billion tokens corpus has been curated exclusively for creative writing tasks such as novel and screenplay syn- thesis, as well as authentic conversational datasets simulating real-life dialogue (Zhang et al., 2022). Another significant aspect of this training stage focuses on enhancing capabilities in the target lan- guage, with Japanese utilized as an example. To this end, substantial Japanese language-specific cor- pora were introduced, alongside a balanced dataset with equal distribution of Chinese, English, and Japanese corpora (Xue et al., 2021). The aim was to facilitate transfer learning from pretraining on Chinese and English to the Japanese language. 2.1 Phase 1: Fundamental Knowledge Enhancement In this phase, the creative writing corpus and the knowledge-focused corpus are jointly trained with constant learning rates. This approach was de- signed to boost proficiency in STEM-related rea- soning while preserving the nuanced expression habits required for creative tasks in humanities (Ka- plan et al., 2020). 2.2 Phase 2: Long-Text Capability Refinement During this phase, a subset of the data was filtered based on text length, allowing the context length to increase"}
{"doc_id": "2508.14472v1", "title": "In2x at WMT25 Translation Task", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14472v1", "chunk_id": 2, "text": "to boost proficiency in STEM-related rea- soning while preserving the nuanced expression habits required for creative tasks in humanities (Ka- plan et al., 2020). 2.2 Phase 2: Long-Text Capability Refinement During this phase, a subset of the data was filtered based on text length, allowing the context length to increase from the typical 8,192 tokens to approx- imately 32,000 tokens. This step was intended to amplify the model’s ability to process and com- prehend extended-length texts (Hoffmann et al., 2022). 2.3 Phase 3: Fast Annealing Stage In the final phase, a high-quality corpus was con- structed based on selections informed by perplexity (PPL) and quality-assessment metrics. The anneal- ing training was conducted with linear decay of the learning rate from 3 × 10−5. This process con- sumed a total of 300 billion tokens and enabled the model to maintain its vivid expressive style for tasks such as novels and screenplays (Brown et al., 2020). 3 Post-Training Data The post-training dataset consists of 2 million sam- ples, with 1.5 million used during the supervised fine-tuning (SFT) process and 500,000 used in the reinforcement learning (RL) process. To en- sure the Japanese language (our target language) achieves a proficiency level comparable to ma- jor languages such as Chinese and English, we adjusted the ratio of target language instructions to attain an equal balance across these languages. Specifically, we used a 1:1:1 ratio in the Instruct-to- Example (In2X) setup, striving to transfer the orig- inal model’s knowledge into the target language as effectively as possible (Ouyang and et al., 2022; Zhou et al., 2023). We developed a detailed pipeline for construct- ing the target language instructions, which can be categorized into three major synthetic processes: 3.1 Obtaining Open-Source Instructions We began by collecting open-source instruction datasets available in the target language. These datasets include curated public data and traditional NLP fundamental tasks. Examples of such datasets include Dolly, OASST, and OASST2 (Koch and et al., 2023; OpenAI, 2023). 3.2 Target Language Instruction Rewriting This process consists of several substeps designed to enhance the model’s linguistic and cultural adapt- ability in the target language: • Creative Language Tasks: To preserve the language’s stylistic characteristics in humanities-focused tasks, we designed cre- ative tasks where the responses include origi- nal stories or scripts (Bai and et al., 2022). • Basic Localized Tasks: This includes rewrit- ing instructions for tasks relevant to the local context, such as exam questions. Some of these tasks provide only the question and an- swer. We leveraged advanced models to sup- plement these datasets with reasoning chains to improve the model’s reasoning ability in the target language (Wei and et al., 2022). This en- hancement also helps to mitigate issues such as mathematical inconsistencies commonly faced during the LLM instruction synthesis process. • Cultural Style Transformation: For certain humanities-related tasks, we incorporated cul- tural style shifts by adapting the instructions to align with the cultural norms and styles of the target language. This adjustment aims to improve the model’s ability to provide cultur- ally nuanced responses (Xu and et al., 2023). 3.3 Instruction Synthesis"}
{"doc_id": "2508.14472v1", "title": "In2x at WMT25 Translation Task", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14472v1", "chunk_id": 3, "text": "Style Transformation: For certain humanities-related tasks, we incorporated cul- tural style shifts by adapting the instructions to align with the cultural norms and styles of the target language. This adjustment aims to improve the model’s ability to provide cultur- ally nuanced responses (Xu and et al., 2023). 3.3 Instruction Synthesis in the Target Language We utilized methods such as Magpie (Xu et al., 2024) and Self-Instruct (Wang and et al., 2022) to synthesize target language instructions. However, these automatically generated instructions often suffer from issues including overly simple ques- tions, lack of focus, self-answered queries, and in- ternal contradictions. To address these challenges, we implemented a strict quality control pipeline with the following techniques: • Prompt Engineering: We crafted detailed prompts with explicit rules to identify and troubleshoot common issues in synthesized instructions (White and et al., 2023). • Validation via Model Responses: Instruc- tions passing the first step were tested by hav- ing the model generate responses. These re- sponses were evaluated by a critic LLM for contradiction, hallucinations, or failure to pro- vide valid results. Instructions flagged with such issues were discarded. The critic LLM, being sensitive to hallucinations, acts as an ad- ditional safeguard for quality control (Ganguli and et al., 2022). • ReReading Mechanism: After constructing the prompts for instruction generation, we em- ployed a \"ReReading\" mechanism, where the model self-reviews its instructions. This re- view checks for correctness, alignment with the target language’s cultural norms, and con- sistency with its native linguistic style. Since the synthesized instructions inherently carry the reasoning or rewriting processes behind them, leveraging this comprehensive context makes it easier to detect internal flaws, partic- ularly those related to localization or cultural adjustments (Chiang and et al., 2023). 4 Post-Training SFT Stage The post-training Supervised Fine-Tuning (SFT) stage is a critical step to balance linguistic diversity and optimize alignment within the instruction space for target languages. Below, we outline the key strategies and methods employed during this stage. 4.1 Balancing Linguistic Diversity (a) Clustering of Instruction Data: To enhance linguistic diversity, the instruction dataset (comprising 40 million entries) was clustered using the Birch clustering algorithm (Zhang et al., 1996). The effectiveness of the cluster- ing process was evaluated based on metrics like tag recapture rates and cluster smoothness (Zhang and Deng, 2020), which were used to fine-tune the clustering threshold. This pro- cess reduced the dataset to 1.5 million clusters after deduplication and selection. (b) Categorization via Large Language Mod- els (LLMs): Utilizing LLMs, the clustered data was tagged to assign both first-level and second-level labels (et al., 2020). For exam- ple, a mathematical problem might be catego- rized as \"Mathematics - Quadratic Equations.\" These hierarchical labels provided a clearer structural organization of the data. (c) Difficulty Grading of Instructions: The dataset was further refined by classifying each instruction according to its difficulty level: \"Very Difficult,\" \"Difficult,\" \"Moder- ate,\" \"Simple,\" and \"Very Simple\" (Wang and Li, 2019). For normalized scientific datasets, an additional evaluation was conducted us- ing the LLaMA3-70B model (Research, 2023) with a Pass@16 metric (Perez and Andreas, 2022) to estimate the"}
{"doc_id": "2508.14472v1", "title": "In2x at WMT25 Translation Task", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14472v1", "chunk_id": 4, "text": "refined by classifying each instruction according to its difficulty level: \"Very Difficult,\" \"Difficult,\" \"Moder- ate,\" \"Simple,\" and \"Very Simple\" (Wang and Li, 2019). For normalized scientific datasets, an additional evaluation was conducted us- ing the LLaMA3-70B model (Research, 2023) with a Pass@16 metric (Perez and Andreas, 2022) to estimate the success rate of solving specific problems. 4.2 Aligning the Instruction Space of Target Languages (a) Avoiding Semantic Overfitting via Temper- ature Adjustment: During training, a tem- perature parameter was introduced to mitigate overfitting of the model to specific linguis- tic semantic spaces (Sundararajan and Wang, 2021). This approach encouraged the model to adopt a more holistic learning strategy, en- abling it to concentrate on question-answering techniques rather than over-specializing in the semantic patterns of a particular language. For instance, this allowed the Japanese language model to better mimic the cognitive behaviors observed in other languages (Koehn, 2019). (b) Specialized Sampling Strategy: To further enhance the learning process, a two-step sam- pling strategy was employed over the 1.5 mil- lion clusters (Perket and Sanner, 2020): • The difficulty levels of the data were sam- pled in a 3:3:3:1:0 ratio (corresponding to \"Very Difficult,\" \"Difficult,\" \"Moder- ate,\" \"Simple,\" and \"Very Simple,\" re- spectively) (Finn and Jones, 2018). • Additionally, within each cluster, sam- ples were selected to ensure diversity across languages and categorical labels, which preserved the large-scale diversity of the original 1.5 million data points (Torroba and Blanco, 2021). This also maintained a degree of orthogonality be- tween the target language and English within the sampled instructions (Feng and Gimpel, 2020). The first round of sampling was used as the data for the first epoch, while the second round populated the second epoch. The training process adopted a learning rate of 2 × 10−5 with cosine decay for optimal performance (Loshchilov and Hutter, 2017). 5 Reinforcement Learning to Enhance General Capabilities in Cultural and Creative Industries In the post-training RL stage, we leveraged a pro- cess similar to the instruction filtering procedure used during the SFT phase (Ouyang and et al., 2022). Specifically, an additional set of instruc- tions was curated, comprising 500k samples that were guaranteed not to overlap with the instructions used in the SFT phase. The training configuration utilized a batch size of 128 and a minibatch size of 32, with the dataset trained for one epoch. Each rollout involved 16 iterations, and the reward evalu- ation was based on both a rule-based reward model and a generative reward model (Christiano et al., 2017). 5.1 Reward Model Design The reward model system was meticulously de- signed to cater to different task types: • Rule-Based Reward Model: For tasks in- volving mathematics, STEM disciplines, and logic, a rule-based reward model was em- ployed to ensure adherence to specific criteria (Silver and et al., 2017). • Generative Reward Model for Creative Tasks: For creative tasks, such as content- generation, specific prompts embedded with rules were utilized. These rules encompassed fundamental task principles as well as dynam- ically generated guidelines based on the cur- rent prompt. The scoring mechanism eval- uated compliance with"}
{"doc_id": "2508.14472v1", "title": "In2x at WMT25 Translation Task", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14472v1", "chunk_id": 5, "text": "et al., 2017). • Generative Reward Model for Creative Tasks: For creative tasks, such as content- generation, specific prompts embedded with rules were utilized. These rules encompassed fundamental task principles as well as dynam- ically generated guidelines based on the cur- rent prompt. The scoring mechanism eval- uated compliance with these principles and generated a reward score based on the per- centage of principles satisfied (Krause et al., 2021). 5.2 RL Algorithm Design To address the complexity of the tasks, we made strategic adjustments to the RL algorithm to achieve stable and efficient training: • Trajectory-Corrected GRPO: Considering the diverse nature of tasks and reward types, a token-level clipping approach was deemed too restrictive and prone to causing training in- stability. Instead, we employed the Trajectory- Corrected version of the Generalized Proxi- mal Policy Optimization (GRPO) algorithm (Schulman et al., 2017), which proved effec- tive for handling multilingual tasks with vary- ing reward functions. This modification en- abled stable and continuous training while ac- celerating the convergence curve(Pang and Jin, 2025). • Dual-Clip Mechanism: To improve stability, we integrated a dual-clip mechanism, which stabilized the variance of importance sam- pling at the sentence (sen) level (He et al., 2016). Additionally, we removed the lower bound of sampling, achieving optimal perfor- mance for the given tasks. • Soft Length Penalty: A soft-length penalty was incorporated throughout the training pro- cess to encourage better length control in gen- erated outputs (Wu et al., 2016). • High-Level Clipping: A clipping mechanism was introduced to ensure robust control over high-level rewards (Schulman et al., 2015). • Temperature Decay: A temperature decay strategy was applied to progressively adjust the sampling temperature during training, en- couraging diversity in outputs while maintain- ing stability (Hinton et al., 2015). • Entropy Regularization: The entropy value was set to 0.01 during training, enabling the model to conserve entropy and avoid prema- ture saturation of the reward space (Williams, 1992). 6 Model Ensemble Model ensemble techniques are employed by tak- ing into account the orthogonality of linguistic capabilities among various models. Specifically, models that exhibit strong linguistic proficiency are selected for the ensemble process to maximize overall performance. Furthermore, the fusion of model tensors is con- ducted based on gradient information and the im- portance of weights. This approach ensures a ro- bust integration of model parameters, leveraging their respective contributions to optimize the en- semble. Such methodologies have been shown to enhance the effectiveness of model ensembles in complex tasks (Wang et al., 2025). 7 Evaluation Results 7.1 Benchmarks The model demonstrated outstanding performance in prominent Japanese language benchmarks, such as the ja-mtbench, indicating its robust and reli- able language translation capabilities. A detailed breakdown of the results is provided below. 7.2 WMT Evaluation Results Without undergoing any task-specific fine-tuning, the model achieved remarkable results in the Japanese-related tracks of the WMT competition, securing second place overall. Furthermore, in the unrestricted category, the model achieved first place. A comprehensive summary of its perfor- mance is outlined below. 8 Conclusion In this work, the proposed methodology for trans- ferring language model capabilities"}
{"doc_id": "2508.14472v1", "title": "In2x at WMT25 Translation Task", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14472v1", "chunk_id": 6, "text": "model achieved remarkable results in the Japanese-related tracks of the WMT competition, securing second place overall. Furthermore, in the unrestricted category, the model achieved first place. A comprehensive summary of its perfor- mance is outlined below. 8 Conclusion In this work, the proposed methodology for trans- ferring language model capabilities has been val- idated on the WMT translation task. The ap- proach has demonstrated significant improvements in Japanese proficiency throughout the CPT, SFT, and RL processes. Remarkably, without any ad- ditional language-specific fine-tuning, the large language model achieved alignment between its Japanese language capabilities and those of main- stream languages. As a result, it demonstrated superior performance and achieved first place in the unrestricted track of the competition."}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 0, "text": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization Shuaijie She♡♠∗, Yu Bao♠, Yu Lu♠, Lu Xu♠, Tao Li♠, Wenhao Zhu♠, Shujian Huang♡(B), Shanbo Cheng♠(B), Lu Lu♠, Yuxuan Wang♠ ♠ByteDance Seed, ♡Nanjing University ∗Work done during an internship at ByteDance Seed, BCorresponding authors Abstract We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via a generalized duality. DuPO addresses two key limitations: Rein- forcement Learning with Verifiable Rewards (RLVR)’s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning’s restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task’s input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs’ ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization. Date: August 21, 2025 Correspondence: chengshanbo@bytedance.com(B), huangsj@nju.edu.cn(B) 1 Introduction Large Language Models (LLMs) [1, 10, 13, 29, 34, 51] have shown remarkable progress in tasks like mathemat- ical reasoning [2, 8, 42, 50] and multilingual translation [9, 26, 27, 57]. To further enhance their capabilities, reinforcement learning (RL) paradigms like Reinforcement Learning from Human Feedback (RLHF) [13, 29, 51] and Reinforcement Learning with Verifiable Rewards (RLVR) [11, 17, 20, 43, 52] have gained traction. Specifi- cally, RLHF aligns models with human preferences but relies on costly, inconsistent human annotations [23, 54]. RLVR addresses this for objective tasks (e.g., math, code) via binary rewards from verifiable answers, re- ducing annotation burdens. However, RLVR still depends on external supervision: acquiring verifiable answers remains a bottleneck, limiting scalability. Moreover, it struggles with open-ended tasks (e.g., mul- tilingual translation), where single references cannot capture diverse high-quality outputs [6, 22]. Recent attempts (e.g., AI-Feedback/RLAIF [23], Constitutional AI [3]) merely swap dependencies (human labels → teacher models/rules), failing to resolve the core bottleneck. 1 Dual learning [15] offers a self-supervised alternative by leveraging task duality to generate intrinsic feedback: through paired “primal” and “dual” tasks (e.g., translation and back-translation [41]), models validate outputs via cycle consistency, eliminating reliance on external labels. Given that LLMs possess diverse capabilities from extensive pretraining, they could be trained across various tasks. However, applying this framework to LLMs is non-trivial, which faces two critical challenges: 1. Limited Duality in Irreversible Tasks: Most real-world LLM tasks (e.g., creative writing, math reasoning) lack strict invertibility. LLM’s output (e.g., a math solution) rarely contains enough information to reconstruct its input (e.g., the original problem), breaking the duality cycle and invalidating self- supervision. 2. Bidirectional Competence Asymmetry: LLMs"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 1, "text": "challenges: 1. Limited Duality in Irreversible Tasks: Most real-world LLM tasks (e.g., creative writing, math reasoning) lack strict invertibility. LLM’s output (e.g., a math solution) rarely contains enough information to reconstruct its input (e.g., the original problem), breaking the duality cycle and invalidating self- supervision. 2. Bidirectional Competence Asymmetry: LLMs often exhibit uneven performance across primal/dual tasks (e.g., strong at solving math problems but weak at generating problems from solutions). Noisy self-signals from asymmetric tasks hinder optimization, reducing the framework’s utility. These mismatches render traditional dual learning ill-suited for general LLM optimization, leaving it an open challenge. In this paper, we propose DuPO (Dual Learning-based Preference Optimization), a framework that aligns LLM generalization with a (relaxed) duality applicable to general tasks. At its core lies a generalized duality framework (§3.3) built on complementary relationships: it decomposes each input x into disjoint known (xk) and unknown (xu) components, then designs the dual objective to reconstruct only xu from the primal output y and the known input xk, rather than inverting the full input. This framework resolves two asymmetries: it restores sufficient information flow between the primal and dual tasks (task asymmetry) and reduces the complexity burden on the dual task side (capability asymmetry). The formulation naturally synergizes with LLMs: their broad foundational capabilities allow a single model to instantiate both primal and dual tasks without specific architectures, while the dual task converts the model’s outputs into self-supervised reward signals, enabling continual improvement without external annotations. This bidirectional benefit addresses a critical challenge in LLM development: obtaining high-quality feedback for capability enhancement. We empirically validate DuPO on two diverse and representative tasks: mathematical reasoning and multilin- gual translation, demonstrating significant and consistent improvements. By applying DuPO to one of the strongest translation LLM, Seed-X-7B-Instruct [9], we demonstrate a significant further performance gain of 2.13 COMET points on the multilingual translation benchmark, bringing the 7B model to performance comparable to ultra large state-of-the-art systems. In mathematical reasoning, our method yields robust gains across models of varying scales, from 1.5B to 7B parameters; notably, DuPO improves the Qwen3-4B model’s score on three challenging mathematical benchmark by 6.4 percentage points. Our comprehensive ablation studies confirm that our design, the generalized duality, is crucial for achieving these results. Beyond training, DuPO acts as a reranking mechanism at inference, boosting performance by 9.3 points without finetuning—enabling smaller models to outperform stronger ultra-large LLM like DeepSeek-R1 even without training. In summary, DuPO reimagines task duality for non-invertible LLM tasks. It eliminates external annotation reliance, scales across tasks/domains, and enhances both training and inference—offering a scalable path to align LLMs with diverse goals using self-supervised feedback. 2 Related Work 2.1 Preference Optimization for LLMs Preference optimization has driven significant advancements in large language models (LLMs) by aligning outputs with feedback signals, with three dominant paradigms shaping the field: (1) Reinforcement Learning from Human Feedback (RLHF) [35] has become a cornerstone for aligning LLMs with human preferences. Its workflow typically involves training a reward model [30, 48] on human-annotated preference pairs, then using reinforcement learning (e.g., PPO [39], GRPO [42]) to optimize"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 2, "text": "with three dominant paradigms shaping the field: (1) Reinforcement Learning from Human Feedback (RLHF) [35] has become a cornerstone for aligning LLMs with human preferences. Its workflow typically involves training a reward model [30, 48] on human-annotated preference pairs, then using reinforcement learning (e.g., PPO [39], GRPO [42]) to optimize the policy model [13, 29, 51]. While effective for subjective tasks, RLHF faces critical bottlenecks: human annotation is costly to scale [23], and consistency across annotators degrades for complex tasks [54], limiting its applicability to large-scale or nuanced scenarios. (2) Recent work [3, 23, 56] has leveraged LLM-as-a-Judge to evaluate outputs and provide 2 optimization signals, advancing capabilities in complex tasks. However, the reliability of this paradigm heavily hinges on the judge model’s own capabilities and its susceptibility to systematic biases, where evaluations are confounded by various factors such as the presentation order of responses or a preference for certain linguistic styles [14, 24, 44]. (3) In response, research has shifted towards exploring Reinforcement Learning from Verifiable Rewards (RLVR)—a paradigm designed to enhance a model’s complex reasoning capabilities in domains like mathematics [11, 43, 51]. By leveraging ground-truth answers as reward signals, RLVR avoids human annotation, but its reliance on verifiable outcomes restricts it to tasks with definitive solutions. This makes it ill-suited for open-ended tasks such as multilingual translation, where multiple valid outputs exist and no single ground truth can capture all high-quality responses. Notably, both paradigms share a fundamental limitation: dependence on external supervision—whether human annotations or pre-defined verifiable answers. This reliance constrains LLMs’ adaptability and scalability across diverse tasks, highlighting the need for self-supervised preference optimization mechanisms. 2.2 Dual Learning Dual learning enhances model performance by leveraging intrinsic task symmetry, where a primal task and its complementary dual task mutually provide supervision. He et al. [15] first introduced dual learning for machine translation, which uses bidirectional tasks (e.g., En→Zh and Zh→En) to generate pseudo-labels via back-translation [41], reducing reliance on parallel corpora—a breakthrough for low-resource language pairs. This framework has since expanded to diverse domains: • Cross-modal tasks: DualGAN [25] frames image-to-text and text-to-image generation as dual tasks, enforcing cycle consistency to align visual and linguistic representations. Ren et al. [38] apply a similar principle to text-to-speech (TTS) and automatic speech recognition (ASR), enabling joint training with minimal paired data. • Knowledge reasoning: DualTKB [12] treats knowledge base path generation and natural language query parsing as symmetric tasks, improving factual consistency via bidirectional validation. • Reinforcement learning integration: Zhang et al. [55] designed policy gradient algorithms that transfer rewards between dual tasks, mitigating reward sparsity in low-supervision scenarios. For LLMs, dual learning has enabled capability enhancement. Trans-Zero [58] uses back-translation to verify semantic preservation in multilingual generation. DualReflect [7] employs dual tasks (e.g., translation and back-translation) as structured feedback to refine output quality. However, a critical limitation persists: existing methods require strict task duality where primal and dual tasks are mutually invertible (e.g., translation pairs). This restricts application to tasks with ambiguous or non-invertible dual counterparts (e.g., open-ended reasoning, creative writing). Our work addresses this by reframing dual learning"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 3, "text": "to refine output quality. However, a critical limitation persists: existing methods require strict task duality where primal and dual tasks are mutually invertible (e.g., translation pairs). This restricts application to tasks with ambiguous or non-invertible dual counterparts (e.g., open-ended reasoning, creative writing). Our work addresses this by reframing dual learning as a preference optimization framework. Instead of relying on explicit task symmetry, we decompose inputs into known/unknown components to construct flexible dual tasks, enabling generalization across diverse tasks without rigid invertibility constraints. 3 Dual Learning-based Preference Optimization In this section, we propose Dual Learning-based Preference Optimization (DuPO). Its core objective is to leverage the intrinsic relationships between tasks and their dual counterparts to generate self-supervised rewards, enabling LLMs to improve performance without relying on expensive human annotations or complex handcrafted rules. 3.1 Task Duality We begin by formalizing the task duality between a primal task and its dual counterpart. Definition 1. Let X be the input space and Y the output space. A primal task is a mapping Tp : X →Y, and a dual task is a mapping Td : Y →X. The pair (Tp, Td) is said to form a dual pair if they satisfy the 3 (a) Non-unique reconstruc- tion of x from y (b) Failure to reconstruct x given y (c) Predicting subspaces B to preserve uniqueness Figure 1 Challenges in Dual Learning and Solutions via Relaxed Duality Constraints. Dilemmas in applying dual learning — (a) Non-unique reconstruction of x from y breaks the closed-loop; (b) Failure to reconstruct x from y due to competence asymmetry. Resolutions by relaxing duality restrictions — (c) Predicting subspaces B to preserve uniqueness. consistency principle: ∀x ∈X, d \u0000x, Td(Tp(x)) \u0001 ≤ϵX , where d(·) : X × X →R+ is a domain-specific distance metric, and ϵX ≥0 is a tolerance threshold that quantifies acceptable reconstruction errors in each space. Leveraging this duality, we can construct a self-supervised reward to quantify the quality of a primal-task output. Given an input x ∈X and its corresponding output y = Tp(x), we could define reward as r(y) ∝exp \u0000−λ · d \u0000x, Td(y) \u0001\u0001 , (1) where λ > 0 controls the sensitivity of the reward to reconstruction error. High-quality outputs maximize the expected reward E[r(y)] by preserving information that is recoverable through the duality cycle. This principle has been successfully applied in various domains, including machine translation [15, 58]. 3.2 Challenges in Dual Learning-based Optimization While task duality offers a promising self-supervised paradigm, its application to LLM optimization is non-trivial, as it confronts two critical challenges that disrupt the closed-loop information flow. Challenge I: Limited Duality in Non-Mutually Implicative Tasks. The natural utility of task duality hinges on Tp and Td being mutually implicative — specifically, the output y of Tp contains sufficient information to reconstruct x via Td, and vice versa. This property holds for canonical tasks like machine translation, where y (a translation) preserves the semantic content of x (the source sentence), enabling Td (back-translation) to recover ˆ x ≈x. However, most real-world tasks lack this mutual implicativity (Fig. 1a). Consider"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 4, "text": "information to reconstruct x via Td, and vice versa. This property holds for canonical tasks like machine translation, where y (a translation) preserves the semantic content of x (the source sentence), enabling Td (back-translation) to recover ˆ x ≈x. However, most real-world tasks lack this mutual implicativity (Fig. 1a). Consider mathematical reasoning, where Tp solves a problem x (e.g., “A box contains 3 red and 5 blue balls; what is the total?”) to produce y = 8. Here, y (the total count) is insufficient to uniquely reconstruct x via Td, as 8 could answer infinitely many disparate questions, such as “What is 10 −2?” or “What is the atomic number of Oxygen?”. This underdetermined relationship breaks the duality loop: Td cannot reliably recover x from y, making the self-supervised reward (based on ˆ x ≈x) untrustworthy. Such tasks thus require a redefinition of duality beyond direct input-output reversal. Challenge II: Bidirectional Competence Asymmetry. Even for mutually implicative tasks, duality optimization is sensitive to the bidirectional competence of the LLM — the performance gap between Tp and Td. If Tp is strong but Td is weak, Td may produce noisy ˆ x that distorts the supervision signal (Fig. 1b). This asymmetry is particularly pronounced in LLMs, where extensive pretraining creates diverse but uneven capabilities across tasks, even within the same domain. 4 For instance, in machine translation optimization, let x = “The quick brown fox jumps over the lazy dog” (English) and y = “Der schnelle braune Fuchs springt über den faulen Hund” (correct German translation). A high-quality y should enable Td to back-translate to ˆ x ≈x. However, if Td struggles with nuanced vocabulary (e.g., “schnelle” →“fast” instead of “quick”), ˆ x might be “The fast brown fox jumps over the lazy dog” — a divergence from x that erroneously penalizes y despite its correctness. Using separate models for Tp and Td, as was common in traditional dual learning, merely sidesteps the challenge of intrinsic competence asymmetry [45, 46]. This imbalance is still arising from the distinct natures and complexities of the primal-dual tasks, destabilizing the self-supervised feedback loop. 3.3 Generalized Duality Framework via Complementary Task To address the two-fold challenges of limited duality in non-mutually implicative tasks and bidirectional competence asymmetry, we propose a generalized duality that redefines task duality through complementary dependencies. It transcends traditional duality’s strict input-output reversal requirement by leveraging partial and stable dependencies between task components, enabling robust self-supervised rewarding even for tasks lacking inherent mutual implicativity. Definition 2. Let the input space X of a primal task Tp be decomposed into two disjoint subspaces: Xk (known components) and Xu (unknown components), such that X = Xk ∪Xu. The primal task Tp is a mapping Tp : X →Y that maps x ∈X to an output space y ∈Y. Its complementary dual task Tcd is a mapping that leverages y and the known component xk to reconstruct the unknown component ˆ xu ∈Xu: Tcd : (y, xk) 7→ˆ xu. The pair (Tp, Tcd) is said to form a generalized dual pair if they satisfy the complementary consistency"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 5, "text": "y ∈Y. Its complementary dual task Tcd is a mapping that leverages y and the known component xk to reconstruct the unknown component ˆ xu ∈Xu: Tcd : (y, xk) 7→ˆ xu. The pair (Tp, Tcd) is said to form a generalized dual pair if they satisfy the complementary consistency principle: ∀x ∈X, y = Tp(x) : d \u0000xu, Tcd(y, xk) \u0001 ≤ϵ, where d(·) : X × X →R+ is a domain-specific distance metric, and ϵ ≥0 is a tolerance threshold. Leveraging this generalized duality, we can construct a self-supervised reward to quantify the preference of a primal-task output analogously to Def. 1. Given an input x ∈X with decomposition x = (xk, xu) and its corresponding output y = Tp(x), the reward is defined as r(y) ∝exp (−λ · d (xu, Tcd(y, xk))) , (2) where λ > 0 controls reward sensitivity. Thanks to the generalized duality, we can explicitly use xk (known components) to constrain Tcd, enabling stable optimization even when y alone is insufficient. To highlight how generalized duality resolves the limitations (§3.2) of classic duality (§3.1), we examine a simple two-sum example: Example 1: Generalized Duality Feedback for a Two-Sum Task: A + B The primal task Tp : y ←[ xu + xk is to compute the sum of two numbers, with its input and output as: – The input x is decomposed as x ←[ (A, B), where xk = A (a known number) and xu = B (an unknown number, without loss of generality). – The output y is the result of sum: C = A + B. The complementary dual task Tcd : xu ←[ y −xk is designed to reconstruct the unknown component xu, using the primal output y (i.e. C) and the known xk (i.e. A): ˆ xu ←[ B′ = C −A Then, we can directly quantifies whether B (original unknown) and B′ (reconstructed unknown) are consistent as reward signal: r(y) ∝exp (−λ · I(B ̸= B′)) . Here, I(·) is an indicator function: it equals 0 if B = B′ (consistent) and 1 otherwise (inconsistent). This ensures the reward is maximized when B and B′ match, and reduced otherwise. 5 Remark 1. Compared to traditional dual learning, which suffers from strict mutual implicativity (i.e., y must fully encode x) and bidirectional competence asymmetry, our generalized duality framework offers three fundamental advantages: 1. Overcomes the Invertibility Constraint. By redesigning the dual objective from reconstructing the full input x to only a selected unknown component xu, our framework fundamentally bypasses the stringent requirement of task symmetry. This relaxation is the key to unlocking dual learning for tasks that are inherently non-invertible, where the primal output does not contain sufficient information to recover the entire input. 2. Mitigates the Competence Asymmetry. The difficulty of the dual task is significantly reduced in two ways. First, the known component xk acts as a strong contextual anchor, constraining the solution space for reconstruction. Second, we can simply yet effectively select an xu that is not only feasibly reconstructible but also act as a faithful reward"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 6, "text": "difficulty of the dual task is significantly reduced in two ways. First, the known component xk acts as a strong contextual anchor, constraining the solution space for reconstruction. Second, we can simply yet effectively select an xu that is not only feasibly reconstructible but also act as a faithful reward signal for the primal task’s solution quality (Appendix A). This directly addresses the “weak dual” pitfall and ensures the self-supervised reward is reliable and informative. 3. Enables Broad Applicability. It unlocks dual learning for a broad class of tasks previously considered unsuitable, including complex domains such as mathematical reasoning, code generation, and dialogue systems where input-output relationships are partial or conditional. This generalized duality, therefore, provides a systematic way to overcome the traditional barriers of non- invertibility and competence asymmetry. Our case studies in Appendix D present concrete examples that illustrate how this process is applied in the multilingual translation and mathematical reasoning scenarios. 3.4 Preference Optimization The core of our Dual Learning-based Preference Optimization (DuPO) framework is to optimize LLMs using duality-derived self-supervised rewards r(y), without external annotations. The objective is to maximize the expected reward based on the (complementary) dual task: J (θ) = Ey∼πθ(y|x) [r(y)] , (3) where πθ(y|x) denotes the LLM’s policy (parameterized by θ) for generating output y given input x = (xu, xk). The distance metric d(·) design is highly flexible and compatible with various rule-based metrics, enabling application across diverse tasks. For example, we could employ BLEU scores for multilingual translation which provide scores from 0 to 1, while for mathematical reasoning, we evaluate variable equality, yielding binary rewards. Notably, DuPO is compatible with various reinforcement learning algorithms (e.g., PPO [39], ReMAX [28], REINFORCE++ [19]), we adopt Group Relative Policy Optimization (GRPO) [42] in our experiments—for its stability in high-dimensional parameter spaces (critical for LLMs) and compatibility with rule-based rewards. 4 Experiment We validate the efficacy of DuPO on two representative tasks: multilingual translation and mathematical reasoning. Below, we detail the experimental setup, datasets, and evaluation metrics for each task, followed by key results. 4.1 Experiment Setup Base Model. We evaluate DuPO on a diverse set of strong and popular base models to demonstrate its effectiveness and robustness. For translation tasks, we employ Seed-X-7B-Instruct [9], one of the strongest open-source translation models. For mathematical reasoning, we select models of varying scales and capabilities, including small-scale yet powerful DeepSeek-R1-Distill-Qwen-1.5B [11] and its larger counterpart DeepSeek- R1-Distill-Qwen-7B, both distilled from the state-of-the-art DeepSeek-R1. We also include Qwen3-4B [51], 6 Model BLEU COMET BLEURT Avg. Qwen3-8B 21.69 84.82 65.81 57.44 Doubao-1.5-Thinking 26.19 87.87 71.66 61.91 Qwen3-235B-22B 28.37 88.76 73.91 63.68 DeepSeek-R1-0528 30.21 89.16 75.03 64.80 Seed-X-7B-Instruct 28.76 86.96 72.62 62.78 w/ DuPO (ours) 30.31 89.09 74.57 64.66 Table 1 Multilingual Translation Performance Across 756 Translation Directions in 28 Languages. DuPO significantly improves all metrics and performs compara- bly to its strong counterparts (DeepSeek models). En2XX Zh2XX Avg 3.2 3.3 3.4 3.5 3.6 3.7 Score 3.67 3.66 3.67 3.66 3.65 3.66 3.57 3.67 3.64 3.60 3.28 3.44 Seed-X-7B-Instruct w/ DuPO GPT-4o DeepSeek-R1-0120 Google Translate Figure 2 Human Evaluation Scores"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 7, "text": "Languages. DuPO significantly improves all metrics and performs compara- bly to its strong counterparts (DeepSeek models). En2XX Zh2XX Avg 3.2 3.3 3.4 3.5 3.6 3.7 Score 3.67 3.66 3.67 3.66 3.65 3.66 3.57 3.67 3.64 3.60 3.28 3.44 Seed-X-7B-Instruct w/ DuPO GPT-4o DeepSeek-R1-0120 Google Translate Figure 2 Human Evaluation Scores (0-4) on the Seed- X-Challenge for 14 Language Directions. DuPO achieves performance comparable to or even surpassing ultra-large models such as GPT-4o and DeepSeek-R1-0120, while sig- nificantly outperforming Google Translate. the latest strong small LLM, and the most capable open-source reasoning model, OpenReasoning-Nemotron- 7B [32]. These models represent strong and representative baselines within their respective model scales, ensuring comprehensive evaluation. Additionally, we also include some SOTA and impressive ultra-large models like Doubao-1.5/1.6-Thinking [5], Claude-Sonnet4-Thinking, and DeepSeek-R1 [11] for comparison. Dataset. For translation tasks, we focus on 28 languages that are aligned with the language coverage of Seed-X, selecting 1,000 prompts for each language from a multilingual pre-training dataset to create our training prompt set. Additionally, we collect 7,000 parallel data entries across these specified languages to support our experiments from the dev set of Flores-200 [33]. For mathematical reasoning tasks, we utilize a mixture of publicly available mathematics question datasets1. These datasets encompass diverse sources and are commonly used for synthesizing supervised fine-tuning data with ultra-large LLMs or conducting reinforcement learning with oracle labels, covering various subjects of competition-level mathematical problems, logic puzzles, and other reasoning tasks. Benchmarks. To comprehensively evaluate the effectiveness of DuPO, we conduct extensive experiments using the following test sets: • Multilingual Translation: For multilingual translation, we construct our test set by randomly selecting 50 samples for each of the 756 translation directions (among 28 languages) from the testset of Flores2, resulting in a total of 37,800 samples. We will release this dataset for convenient comparison. We employ BLEU [36], COMET [37], and BLEURT [40] as evaluation metrics. Additionally, we conduct human evaluation on Seed-X-Challenge [9]3, a challenging benchmark designed to test the boundaries of LLMs’ translation capabilities with diverse linguistic elements across multiple domains. Human experts assess accuracy, fluency, and idiomaticity, scoring translations from Chinese or English to seven languages on a 0-4 scale (higher score denotes better translation quality). • Mathematical Reasoning: We evaluate our approach on multiple benchmarks, including AMC23 [31], AIME24 [4], and AIME25 [4], to assess performance in standardized contest environments. For each problem, we sample 32 responses using a temperature of 0.8 and a maximum reasoning budget of 32,000 tokens, then report the average accuracy (Avg@32). Ultra-large models like DeepSeek-R1 and Doubao-1.6-thinking are accessed via their official APIs. More details about training are provided in Appendix C. 4.2 Main Results 4.2.1 DuPO Boosts LLM’s Performance on Various Tasks DuPO achieves strong performance on diverse tasks, including multilingual translation and mathematical reasoning. On multilingual translation, DuPO elevates the base model to a state-of-the-art performance level, rivaling and even surpassing significantly ultra large LLM. As detailed in Table 1, applying DuPO 1More details on math data preparation can be found in Appendix B. 2https://huggingface.co/datasets/openlanguagedata/flores_plus 3https://github.com/ByteDance-Seed/Seed-X-7B/tree/main/challenge_set 7 Model AMC23 AIME24 AIME25 Average DeepSeek-R1-0120 97.7 79.8"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 8, "text": "multilingual translation, DuPO elevates the base model to a state-of-the-art performance level, rivaling and even surpassing significantly ultra large LLM. As detailed in Table 1, applying DuPO 1More details on math data preparation can be found in Appendix B. 2https://huggingface.co/datasets/openlanguagedata/flores_plus 3https://github.com/ByteDance-Seed/Seed-X-7B/tree/main/challenge_set 7 Model AMC23 AIME24 AIME25 Average DeepSeek-R1-0120 97.7 79.8 70.0 82.5 Claude-Sonnet4-Thinking 97.5 82.5 70.0 83.3 Doubao-1.5-Thinking 99.4 86.3 73.3 86.3 Doubao-1.6-Thinking 98.8 88.4 83.4 90.2 DeepSeek-R1-0528 99.4 91.4 87.5 92.8 DeepSeek-R1-Distill-Qwen-1.5B 67.5 20.0 20.0 35.8 w/ DuPO (ours) 72.5 30.0 26.7 39.7 (+3.9) DeepSeek-R1-Distill-Qwen-7B 85.0 56.7 36.7 59.5 w/ DuPO (ours) 90.0 63.3 40.0 64.4 (+4.9) Qwen3-4B 95.0 70.0 66.7 77.2 w/ DuPO (ours) 97.5 83.3 70.0 83.6 (+6.4) OpenReasoning-Nemotron-7B 95.0 83.3 73.3 83.9 w/ DuPO (ours) 97.5 83.3 90.0 90.3 (+6.4) Table 2 Mathematical Reasoning Performances (%) on Representative Benchmarks. DuPO significantly improves the performances across models with varying base capabilities, enabling Qwen3-4B to outperform DeepSeek-R1-0120 and OpenReasoning-Nemotron-7B to achieve SOTA performance. to the Seed-X-7B-Instruct model boosts its performance by 1.55, 2.13, and 1.95 across three automatic evaluation metrics, reaching an average score of 64.66. This performance even surpasses that of current SOTA closed-source ultra-large language models, such as Doubao1.5-thinking (+2.75) and Qwen3-235B-22B (+0.98), and is on par with the performance of the latest DeepSeek-R1. As shown in Figure 2, DuPO demonstrates remarkable performance, achieving results comparable to state-of-the-art ultra-large models such as GPT-4o and DeepSeek-R1. Moreover, DuPO substantially outperforms widely-used commercial closed-source systems like Google Translate, showcasing a clear advantage in translation quality as perceived by human evaluators. On mathematical reasoning, the results in Table 2 clearly demonstrate that DuPO yields consistent and significant performance improvements across all models at different scales and baseline reasoning ability. On the most powerful OpenReasoning-Nemotron-7B model, applying DuPO increased the average score from 83.9% to 90.3%, achieving impressive performance. This trend of significant gains continues on the mid-sized Qwen3-4B model, which saw its average score boosted by 6.4 points from 77.2% to 83.6%, even surpassing the ultra-large model DeepSeek-R1-0120. The approach remains remarkably effective on DeepSeek’s distilled models as well. Even on DeepSeek-R1-Distilled-Qwen-1.5B, the least reasoning capability among the strong baselines, we still achieved a 3.9-point increase in average accuracy. This directly demonstrates that DuPO is sufficiently robust and stable to enhance the mathematical reasoning capabilities of models consistently. Our framework’s robust performance is further validated by concrete examples in multilingual translation and mathematical reasoning (see case studies in Appendix D). 4.2.2 DuPO Scales to Various Backbones Effectively To validate the robustness and generalization of our proposed DuPO framework, we extend our evaluation to the LlaMA architectural family. Our experiments are conducted on two LlaMA architectural models: LlaMA-3.1-8B [13] and OctoThinker-8B-Hybrid-Base [47], the latter of which has undergone middle training on mathematical reasoning knowledge. Considering the significant difference of model ability, we select two benchmarks of moderate difficulty, AMC23 [31] and MATH500 [18]. For a fair comparison, all models are finetuned using identical training data and settings. Results are listed in Table 3. As seen, DuPO’s effectiveness is not tied to a specific model architecture; it serves as a robust and generalizable"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 9, "text": "select two benchmarks of moderate difficulty, AMC23 [31] and MATH500 [18]. For a fair comparison, all models are finetuned using identical training data and settings. Results are listed in Table 3. As seen, DuPO’s effectiveness is not tied to a specific model architecture; it serves as a robust and generalizable enhancement, delivering significant improvements to diverse backbones regardless of their initial reasoning proficiency. DuPO lifts the average score of LlaMA-3.1-8B to 32.1%, a +24.0 percentage-point gain over the vanilla model, and surpasses SimpleRL-Zoo [53] (which relies on oracle-labeled answers during training) by 13.1%. When applied to the OctoThinker-8B-Hybrid-Base [47], our DuPO approach yields even more 8 Model AMC23 MATH500 Average LlaMA-3.1-8B 2.5 13.6 8.1 w/ SimpleRL-Zoo 15.0 23.0 19.0 w/ DuPO (ours) 20.0 44.2 32.1 OctoThinker-8B-Hybrid-Base 5.0 42.6 23.8 w/ DuPO (ours) 55.0 70.0 62.5 Table 3 Performances (%) of DuPO on Different Backbone Models. DuPO even surpasses SimpleRL- Zoo, which utilizes labeled answers as reward. DuPO’s potential is further exemplified by OctoThinker, which underwent additional middle training. 50 100 150 200 250 300 Steps 0 20 40 60 Accuracy (%) AMC AIME24 AIME25 Forward Acc Figure 3 Training Progress of DuPO on Qwen3- 4B-Base. The performance consistently improves on the primal task and the benchmarks. impressive performance improvements of +50.0 on AMC23 and +27.4 on MATH500, achieving an average performance of 62.5. 4.2.3 DuPO Incentivizes Reasoning Capability on Base Model We further demonstrate that our DuPO framework can directly elicit and enhance complex reasoning capabilities from a base model. We apply DuPO directly to a base model, without preliminary supervised fine-tuning (SFT) stage activating the reasoing ability. We track the learning dynamics by simultaneously collecting the primal task accuracy (“Forward Acc”) on the training set and its generalization performance on three distinct, unseen challenge test set: AMC23, AIME24, and AIME25. We can see from Figure 3 that DuPO provides a stable and effective pathway to awaken and generalize the latent reasoning abilities of a base model, validating its utility as a powerful training methodology. Specifically, the training dynamics reveal a clear and substantial improvement on the primal task, with the “Forward Acc” soaring from a nascent 15.2% to 56.5%. This upward trajectory provides direct evidence that the reward signal derived from our dual-task serves as an effective guide for enhancing the model’s reasoning. More importantly, this acquired skill demonstrates robust generalization. Performance on the unseen test set AMC23 leaped from 20% to 70%, with similarly significant gains observed on the AIME24 and AIME25 datasets. 4.2.4 DuPO Scales Reasoning during Inference without Training Beyond serving as a reward signal for RL training, the DuPO mechanism can be naturally applied as a training-free, inference-time reranking strategy to improve the reasoning capabilities of any LLM. The process unfolds in three stages: 1) Similar to the rollout stage during RL process, we could prompt any given policy model to generate diverse reasoning trajectories. 2) For each candidate trajectory, we use its final answer to ask the policy model to solve the corresponding dual question automatic constructed without accessing labeled answer. We could apply more computation"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 10, "text": "the rollout stage during RL process, we could prompt any given policy model to generate diverse reasoning trajectories. 2) For each candidate trajectory, we use its final answer to ask the policy model to solve the corresponding dual question automatic constructed without accessing labeled answer. We could apply more computation by performing K (K = 8 in our experiments) sampling runs on each dual question for a more reliable reward estimate, a practice distinct from RL training. 3) Finally, for each test set question, we select the trajectory with the highest backward accuracy on its dual questions as the final output. As presented in Table 4, the experimental results demonstrate that DuPO provides accurate reward signals, effectively guiding models towards correct reasoning, serving as an efficient approach for scaling reasoning capabilities even without training. On the two challenging AIME benchmarks, applying DuPO as a reranking method improves the average performance of Qwen3-4B by 9.3 points, elevating its accuracy from 68.4% to 77.7% without any additional training. Notably, the DuPO-enhanced Qwen3-4B surpasses DeepSeek-R1 and Claude-Sonnet4-Thinking (77.7% vs. 74.9%/76.3% on average). The impact on DeepSeek-R1-Distill-Qwen- 1.5B s even more pronounced, with an 18.7 point increase (20.0% to 38.7%). 9 Model AIME24 AIME25 Average DeepSeek-R1-0120 79.8 70.0 74.9 Claude-Sonnet4-Thinking 82.5 70.0 76.3 DeepSeek-R1-Distill-Qwen-1.5B 20.0 20.0 20.0 w/ DuPO rewarding 53.3 24.1 38.7 (+18.7) Qwen3-4B 70.0 66.7 68.4 w/ DuPO rewarding 86.6 68.9 77.7 (+9.3) Table 4 Inference-Time Scaling on Mathematical Reasoning Using DuPO Rewarding (Backward Acc) for Reranking. Our method improves the performance of policy models with varying base ability, without requiring additional training. AMC23 AIME24 AIME25 Avg 20 30 40 50 60 70 Acc (%) 67.5 20.0 20.0 35.8 72.5 30.0 26.7 43.1 65.0 26.7 26.7 39.5 1.5B Baseline DuPO DuPO w/o filter AMC23 AIME24 AIME25 Avg 65 70 75 80 85 90 95 100 Acc (%) 95.0 70.0 66.7 77.2 97.5 83.3 73.3 84.7 95.0 73.3 70.0 79.3 4B Baseline DuPO DuPO w/o filter Figure 4 Performance Ablation of DeepSeek-R1-Distill-Qwen-1.5B/Qwen3-4B on Mathematical Reasoning. Our unknown component selection strategy reduces training noise and improves these models’ performance across three benchmarks. 4.3 Effects of Task Duality To thoroughly investigate the effectiveness of our proposed framework and validate how our unknown component selection strategy contributes to achieving better task duality, we conduct an ablation study by maintaining identical experimental settings while removing the unknown component selection mechanism from our dual framework. As illustrated in Figure 4, the results showcase the efficacy of our approach in resolving duality issues. For the 1.5B model, DuPO achieves a remarkable 7.3 percentage point improvement over the baseline. Notably, when we remove data filtering, thereby introducing poorer duality, we observe a significant 3.6 percentage point drop in performance. This pattern not only persists with stronger models. In the case of the 4B model, the benefits of our method become even more pronounced, outperforming the poorer duality variant by an impressive 5.4 points on average. These consistent and substantial improvements across various model sizes provide strong empirical evidence that our component selection strategy is a crucial component of the dual framework, effectively"}
{"doc_id": "2508.14460v1", "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14460v1", "chunk_id": 11, "text": "4B model, the benefits of our method become even more pronounced, outperforming the poorer duality variant by an impressive 5.4 points on average. These consistent and substantial improvements across various model sizes provide strong empirical evidence that our component selection strategy is a crucial component of the dual framework, effectively ensuring high-quality task duality and thereby enabling the framework to achieve superior performance. 5 Conclusion We introduce DuPO, a dual learning-based preference optimization framework that eliminates the need for costly human annotations and handcrafted rewards in LLM training. At its core, DuPO’s innovation lies in a generalized duality framework that decomposes and reconstructs input spaces into known and unknown components, addressing critical limitations of traditional dual learning and preference optimization paradigms. Empirical validation across two diverse, high-stakes tasks confirms DuPO’s versatility and effectiveness. In mathematical reasoning, DuPO consistently improves performance across model scales from 1.5B to 7B 10 parameters, with notable gains of 6.4% average accuracy improvement of three benchmarks. For multilingual translation, DuPO elevates the 7B-parameter Seed-X model to performance levels comparable to much larger state-of-the-art models, boosting COMET scores by up to 2.13 points across 28 languages and 756 translation directions. Additionally, DuPO serves as an effective training-free reranking mechanism, enabling smaller models to outperform larger counterparts with up to 9.3 points improvement, bypassing the need for expensive parameter scaling. DuPO’s model-agnostic design and broad task applicability position it as a scalable solution for annotation- efficient LLM development. By harnessing intrinsic task structure to generate self-supervised feedback, it moves beyond the constraints of human supervision and rigid reward engineering—paving the way for more autonomous, adaptable, and cost-effective language model optimization. 6 Limitations Despite the promising results, we acknowledge several limitations of our work that present avenues for future research. First, unknown components selection for mathematical reasoning introduces the computational overhead. While this step is crucial for ensuring the quality of the self-supervised reward signal, developing more efficient or even learnable filtering mechanisms could enhance the scalability and practical applicability of DuPO. Second, our empirical validation is primarily conducted on models of moderate scale. Although DuPO demonstrates consistent improvements across various model sizes, its scalability and effects on significantly larger models remain an open question. Finally, while we demonstrate DuPO’s efficacy on various tasks like mathematical reasoning and multilingual translation, its application to more open-ended and creatively demanding tasks, such as open-ended instruction-following, requires further exploration. Acknowledgments We extend our sincere gratitude to our colleagues at ByteDance, including Qian Cao4, Zhichao Huang, Liyan Kang, Ningxin Peng, Xinghua Qu, Ming Tu, Xiangpeng Wei, Rong Ye, Runsheng Yu, and Zaixiang Zheng, for their valuable advice and insightful discussions, and to Meng Yang and Evaluation Team for their help with the translation evaluation. 4Qian has already left ByteDance. 11"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 0, "text": "2025-8-21 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model NVIDIA Abstract. We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6× higher inference throughput in reasoning settings like 8k input and 16k output tokens (Figure 1). We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano- 12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. 1. Introduction We introduce NVIDIA Nemotron Nano 2, a hybrid Mamba-Transformer reasoning model (Waleffe et al., 2024; Lieber et al., 2024; DeepMind, 2025; NVIDIA, 2025) that achieves on-par or better benchmark accuracies at 3×–6× higher throughput than Qwen3-8B (Yang et al., 2025) for generation- heavy scenarios like 1k input / 8k output or 8k input / 16k output tokens (Figure 1). Nemotron Nano 2 builds on the architecture of Nemotron-H (NVIDIA, 2025), but utilizes key new datasets and recipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints, as well as the majority of the pre- and post-training datasets. The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5). It then underwent a continuous pre-training long-context extension phase to become 128k-capable without degrading other benchmarks (§2.6). Overall, new and improved datasets led to significant accuracy improvements over Nemotron-H-8B on math, multilingual, MMLU-Pro and other benchmarks (§2.2). Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains, followed by targeted SFT on key areas such as tool use, long-context performance, and truncated (budgeted) training. GRPO and RLHF sharpened instruction-following and conversational ability, while additional DPO stages further strengthened tool use. Overall, post-training was performed on roughly 90 billion tokens, the majority in single-turn prompt–response format with reasoning © 2025 NVIDIA. All rights reserved. NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model IFBench (Instr. Following) AIME24 (Math) AIME25 (Math) GPQA-D (Science) LiveCodeBench (Coding) BFCLv3 (T ool Use) RULER 128k (Long Context) ISL/OSL 1k/8k ISL/OSL 8k/16k 30 40 50 60 70 80 90 Accuracy (%) 34.6 81.9"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 1, "text": "NVIDIA. All rights reserved. NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model IFBench (Instr. Following) AIME24 (Math) AIME25 (Math) GPQA-D (Science) LiveCodeBench (Coding) BFCLv3 (T ool Use) RULER 128k (Long Context) ISL/OSL 1k/8k ISL/OSL 8k/16k 30 40 50 60 70 80 90 Accuracy (%) 34.6 81.9 72.0 64.0 71.1 66.9 78.9 33.0 75.8 69.3 59.6 59.5 66.3 74.1 Measured Accuracy Measured Throughput NVIDIA-Nemotron-Nano-9B-v2 Qwen3-8B 0 1 2 3 4 5 6 7 8 Relative Throughput (Output tokens/s/GPU) 3.3 6.3 1.0 1.0 Figure 1 | Comparison of Nemotron Nano 2 and Qwen3-8B in terms of accuracy and throughput. Nemotron Nano 2 achieves comparable or better accuracies on complex reasoning benchmarks, while achieving up to 6.3× higher throughput for such workloads. We abbreviate input sequence length to ISL and output sequence length to OSL and measure throughput on a single A10G GPU in bfloat16. traces. About 5% of the data contained deliberately truncated reasoning traces, enabling fine-grained thinking budget control at inference time (§3.4). Finally, both the base model and aligned model were compressed so as to enable inference over context lengths of 128k tokens on a single NVIDIA A10G GPU (22 GiB of memory, bfloat16 precision). This was done by extending a compression strategy based on Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024; Taghibakhshi et al., 2025) to compress reasoning models subject to constraints. We are releasing the following models on Hugging Face: • NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model, • NVIDIA-Nemotron-Nano-9B-v2-Base: a pruned base model, • NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning. Additionally, we are releasing the majority of our pre-training dataset in the Nemotron-Pre- Training-Dataset-v1 collection of more than 6 trillion tokens: • Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional Common Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages. • Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx + LLM pipeline (Karimi Mahabadi et al., 2025a). Preserves equations, standardizes to LaTeX, outperforms previous math datasets on benchmarks. • Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering, deduplication, and quality filters. Includes code Q&A data in 11 programming languages. • Nemotron-Pretraining-SFT-v1: Synthetic SFT-style dataset covering STEM, multilingual, academic, and reasoning domains. Finally, we are releasing an updated post-training dataset: 2 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Mamba-2 FFN Attention Mamba-2 FFN Mamba-2 FFN Mamba-2 FFN Mamba-2 FFN Mamba-2 FFN Nemotron-Nano-12B-v2-Base x3 x6 x1 Figure 2 | Nemotron-Nano-12B-v2-Base layer pattern. As in Nemotron-H models, roughly 8% of the total layers in the model are self-attention layers which are evenly dispersed throughout the model. Model Number of layers Model dimension FFN dimension Q heads KV heads State dimension Mamba groups Nemotron-Nano-12B-v2-Base 62 5120 20480 40 8 128 8 Table 1 | Summary of Nemotron-Nano-12B-v2-Base architecture. • Nemotron-Post-Training-Dataset-v2 (link coming soon): Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese. The data supports improvements of math, code, general reasoning, and instruction following capabilities. The"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 2, "text": "Table 1 | Summary of Nemotron-Nano-12B-v2-Base architecture. • Nemotron-Post-Training-Dataset-v2 (link coming soon): Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese. The data supports improvements of math, code, general reasoning, and instruction following capabilities. The rest of this technical report is organized as follows: In §2, we discuss the Nemotron Nano 2 model architecture, pre-training process, and base model evaluation results. In §3, we discuss the alignment process. In §4, we describe the pruning and distillation methods used for model compression. 2. Pretraining In this section, we discuss the architecture and pretraining of the Nemotron-Nano-12B-v2-Base model. We also compare this model against other state-of-the-art models in terms of accuracy on popular benchmarks. 2.1. Model Architecture As in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba- 2 (Dao & Gu, 2024), self-attention, and FFN layers. The layer pattern and key architecture details are summarized in Figure 2 and Table 1. Concretely, we use 62 layers, with 6 of them being self-attention layers, 28 being FFN, and 28 being Mamba-2 layers. We use a hidden dimension of 5120, FFN hidden dimension of 20480, and Grouped-Query Attention (Ainslie et al., 2023) with 40 query heads and 8 key-value heads. For Mamba-2 layers, we use 8 groups, a state dimension of 128, a head dimension of 64, an expansion factor of 2, and a window size for convolution of 4. For FFN layers, we use squared ReLU (So et al., 2022) activation. Again as in Nemotron-H, we do not use any position embeddings and use RMSNorm (Zhang & Sennrich, 2019), separate embedding and output layer weights, no dropout, and we do not use bias weights for linear layers. 3 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model 2.2. Pre-Training Data Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically- generated data. 2.2.1. Curated Data We have separate data curation pipelines for the following broad data categories: general web crawl data (English and multilingual), math data, and code data. We discuss each in turn next. English web crawl data. We used the Nemotron-CC dataset (Su et al., 2025), but updated to include eight more recent Common Crawl snapshots (CC-MAIN-2024-33 through CC-MAIN-2025-13) using the same pipeline. For synthetic rephrasing, we mostly switched to Qwen3-30B-A3B (from Mistral Nemo 12B). Additionally, we used data from CC-NEWS through April 23, 2025, to help improve the knowledge cutoff of the model. The CC-NEWS data was filtered for English and globally fuzzily de-duplicated; no other filtering was used. Multilingual data. We extracted data for fifteen languages from the following three Common Crawl snapshots: CC-MAIN-2024-51, CC-MAIN-2025-08, and CC-MAIN-2025-18. The fifteen languages included were Arabic, Chinese, Danish, Dutch, French, German, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual model-based quality classifiers available, we just applied heuristic filtering instead. This was done in a similar manner to the filtering of low-quality English data in the Nemotron-CC pipeline, except that we had to selectively"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 3, "text": "Japanese, Korean, Polish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual model-based quality classifiers available, we just applied heuristic filtering instead. This was done in a similar manner to the filtering of low-quality English data in the Nemotron-CC pipeline, except that we had to selectively disable some heuristic filters that had very high false positive rates for some languages. De-duplication was done in the same way as for Nemotron-CC. Additionally, we used data from Wikipedia and FineWeb-2 (Penedo et al., 2025) for these fifteen languages. Math data. Mathematical content on the web is expressed in a wide range of formats, including inline and block L AT EX, MathML, Unicode symbols, and custom renderers such as MathJax or KaTeX. We conducted a detailed analysis of prior math-specific extraction pipelines—including OpenWebMath (Paster et al., 2023), MegaMath (Zhou et al., 2025), jusText (Endrédy & Novák, 2013), Trafilatura (Barbaresi, 2021), and Resiliparse (Bevendorff et al., 2018)—and found that none could reliably preserve mathematical expressions or code structure. These tools frequently discard or distort equations and flatten code formatting, severely limiting the utility of the extracted content for pretraining. To address this, we built a new pipeline specifically designed for high-fidelity mathematical ex- traction from Common Crawl. We first aggregated a comprehensive list of math-related URLs from prior datasets (e.g., InfiMM-WebMath (Han et al., 2024), OpenWebMath (Paster et al., 2023), FineMath (Allal et al., 2025), and MegaMath (Zhou et al., 2025)), then re-fetched their raw HTML documents from 98 Common Crawl snapshots (2014–2024). Each page was rendered using the lynx text-based browser to preserve layout and math structure. We then applied Phi-4 (Abdin et al., 2024)(14B-parameters) to remove boilerplate, standardize notation into L A T EX, and correct inconsis- tencies. A FineMath classifier (Allal et al., 2025) was used to retain high-quality documents, followed by fuzzy deduplication via MinHash-based (Broder, 2000) Locality Sensitive Hashing (LSH) (Indyk & Motwani, 1998) via the NeMo-Curator framework.1 We finally decontaminated the dataset using LLM Decontaminator (Yang et al., 2023). This process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token subset, Nemotron-CC-Math-4+, containing only the top-scoring samples. When used for pretraining, this dataset yields substantial improvements across math (MATH-500), code (HumanEval+, MBPP+, 1https://github.com/NVIDIA-NeMo/Curator 4 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model MBPP), and general-domain evaluations (MMLU, MMLU-STEM, MMLU-Pro), surpassing all existing open math datasets. For full details, see Karimi Mahabadi et al. (2025a). Code data. In line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar et al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. All source code used to train this model originated from GitHub and went through a multi-stage processing pipeline to arrive at the final source code training data. We performed license-based removal with a license detection pipeline similar to that used by the BigCode project (Lozhkov et al., 2024), but with fewer accepted licenses (see Appendix A for additional details). De-duplication is especially important for source code, where many files can be found exactly duplicated across numerous repositories. Consequently we performed both"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 4, "text": "with a license detection pipeline similar to that used by the BigCode project (Lozhkov et al., 2024), but with fewer accepted licenses (see Appendix A for additional details). De-duplication is especially important for source code, where many files can be found exactly duplicated across numerous repositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using MinHash LSH). In order to build a better understanding of each file in our dataset, we annotated all files with a variety of measures and then performed filtering using these annotations. We found the heuristic filters from OpenCoder (Huang et al., 2025) to be effective and leveraged them to filter files that are less valuable or even detrimental for LLM pretraining. 2.2.2. Synthetically-Generated Data STEM data. We generated synthetic data for STEM subjects, including Astronomy, Biology, Chemistry, Math, and Physics using 88.6k questions collected from multiple sources as the seed data. In addition to the widely used GSM8K, MATH, and AOPS training sets, we collected more diverse questions from Stemez2 and textbooks with permissive licenses from OpenStax3 and Open Textbook Library.4 We used Qwen2.5-VL-72B-Instruct (Bai et al., 2025) to extract questions from the exercise sections in the textbooks with additional instructions such as dropping question numbering, ignoring questions that require image interpretation, and formatting equations using LaTeX. We manually curated the extracted questions to fix occasional OCR errors and removed non-self-contained questions (e.g., a question that refers to an example in the same chapter). To expand both the quantity and diversity of questions, we conducted three iterations of question generation using four models (i.e., Qwen3-30B-A3B and Qwen3-235B-A22B (Yang et al., 2025), both with thinking mode enabled, Deepseek-R1 (DeepSeek-AI, 2025a), and Deepseek V3 (DeepSeek-AI, 2025b)) and three prompts: 1. Similar question: Create a new question that explores similar concepts but offers a fresh challenge. 2. Harder question: Create a new question that requires more logical steps or involves more advanced concepts. 3. Varied question: Create a new question that differs in type from the original question. We instructed the model to avoid superficial or trivial modifications and think through the solution when creating a new question. We filtered out duplicates and highly-similar questions using fuzzy de-duplication and generated solutions to the remaining questions with the models used in the question generation step. We converted a subset of examples to multiple-choice questions in MMLU or MMLU-Pro style. We constructed a few thousand few-shot examples by concatenating random synthetic samples. Math data. We also revisited and regenerated the Nemotron-MIND dataset (Akter et al., 2024), a math-informed synthetic pretraining corpus originally built on OpenWebMath. In our updated 2https://www.stemez.com/ 3https://openstax.org 4https://open.umn.edu/opentextbooks/ 5 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model version, we regenerated the MIND dataset using Nemotron-CC-Math-4+, our highest-quality math subset comprising 52B tokens—as the source corpus. Following the original methodology, we applied seven prompt templates (e.g., Teacher–Student, Debate, Interview, etc) to generate structured mathematical dialogues using the Phi-4 model. Unlike the original MIND, which relied on 14.7B tokens of lower-fidelity data, our version leverages significantly higher-quality input and processes it with a chunk size"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 5, "text": "source corpus. Following the original methodology, we applied seven prompt templates (e.g., Teacher–Student, Debate, Interview, etc) to generate structured mathematical dialogues using the Phi-4 model. Unlike the original MIND, which relied on 14.7B tokens of lower-fidelity data, our version leverages significantly higher-quality input and processes it with a chunk size of 5K tokens. This regeneration produced a 73B-token synthetic dataset and led to consistent improvements across math reasoning and general knowledge (MMLU, MMLU-Pro. MMLU-Stem) benchmarks compared to the original MIND version, highlighting the critical role of input data quality. Full details and results are available in Karimi Mahabadi et al. (2025b). Multilingual data. We generated multilingual diverse question and answer data (Diverse QA) (Su et al., 2025) from two sources: 1. We translated the English Diverse QA data to fifteen languages (see Multilingual data) using Qwen3-30B-A3B (Yang et al., 2025). 2. We generated synthetic data from Wikipedia articles in these languages using the Diverse QA prompt and instructed the model to write all questions and answers in the target language. In addition, we translated a subset of our GSM8K augmentation data (see STEM data) into these languages using Qwen3-30B-A3B. We post-processed each translated solution by appending a concluding sentence meaning “The answer is ...” (e.g., “La respuesta es ...” in Spanish, “Die Antwort lautet ...” in German), where the final numerical answer is extracted from the original English solution. Code data. We generated question-answer (QA) data at scale for 11 different programming languages by prompting an LLM to generate questions based on short snippets from our curated source code, asking the model to solve the generated question, and then performing post hoc filtering of the generated QA pairs based on heuristics as appropriate (e.g., Python AST parsing). This technique results in diverse synthetic data targeted at problem solving containing both natural language and source code. Further details are covered in the Nemotron-H technical report (NVIDIA, 2025), where we first leveraged this type of synthetic code data in pretraining. Academic data. In the pretraining set for the Nemotron-H (NVIDIA, 2025) series of models, we assigned attribute labels for educational quality, educational difficulty, and educational subject to all documents coming from academic data, which encompasses textbooks and academic papers. As content of higher educational difficulty in technical domains still proves challenging for models, we prioritized increasing model comprehension of such information in our current pretraining set via the generation of question-answer (QA) pairs as such data has been shown to enhance knowledge storage and extraction within language models (Allen-Zhu & Li, 2024). To do so, we first gathered all documents with educational difficulty at the undergraduate and graduate levels in the following technical subject areas: math, chemistry, biology, physics, and medicine. Using this subset of documents, we aim to find the most relevant pieces of texts that could be utilized as seed contexts for our generation of QA pairs. We chunk each document into snippets of 512 token lengths, embed them with the e5-large model (Wang et al., 2024), and store them within a Milvus vector database that enables approximate nearest neighbor search. We"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 6, "text": "of texts that could be utilized as seed contexts for our generation of QA pairs. We chunk each document into snippets of 512 token lengths, embed them with the e5-large model (Wang et al., 2024), and store them within a Milvus vector database that enables approximate nearest neighbor search. We then curate documents from a set of complex subject areas (e.g. Mathematics: Real Analysis, Biology: Genetics, Statistics: Information Theory), and query the Milvus database for the 250 nearest neighbor text snippets to each query document. The returned snippets function as our seed contexts that we then pass into a Qwen-2.5 72B instruct model (Qwen, 2025) to generate multiple choice and free response style QA pairs based on the information contained in the snippet. With each QA pair, a justification for the answer is additionally generated. 6 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model SFT-style data. Using SFT-style data in the later stages of pretraining has shown to be helpful to foster more comprehensive model learning (Hu et al., 2024). Therefore, we synthesized and included different SFT-style data covering several domains: 1) code SFT data which is mainly focused on solving code problems; 2) math SFT data that is mostly focused on reasoning; 3) MMLU-style SFT data which contains different question and answer examples covering different knowledge topics; and 4) general instruction following SFT data. We ensure that the SFT-style data covers diverse topics with different difficulty levels for each of the above mentioned domains. Detailed synthesis methods and pipelines for the above mentioned SFT data can be found in prior work (Toshniwal et al., 2024; Moshkov et al., 2025; Bercovich et al., 2025a,b; Ahmad et al., 2025). Fundamental reasoning SFT-style data. While the above mentioned SFT-style data help enhance an LLM’s ability to answer questions in code, math and general language understanding benchmarks, they do not help improve the model’s ability in deeper reasoning tasks to discern the correct answer among a larger pool of potential distractors. We propose to mitigate that by synthesizing SFT-style data focused on analytical reasoning, logical reasoning, and reading comprehension. Specifically, we collected existing datasets including 1) the Law School Admission Test (LSAT) dataset from Wang et al. (2022); Zhong et al. (2022) which encompasses three tasks: logical reasoning, reading comprehension, and analytical reasoning, 2) the repurposed LogiQA dataset by Liu et al. (2020) which contains various types of logical reasoning questions collected from the National Civil Servants Examination of China, and 3) the AQuA-RAT dataset which emphasizes algebraic word problems by Ling et al. (2017). We then prompted DeepSeek-V3 (DeepSeek-AI, 2025b) and Qwen3-30B-A3B (Yang et al., 2025) respectively to synthesize more similar questions with corresponding options. For each question we generated, we prompted DeepSeek-V3 again to generate the chain-of-thought (CoT) process with the final solution. At the post-processing stage, we apply majority voting to keep only the samples that have the most voted solutions. Overall, we generated 4B tokens from DeepSeek-V3 and 4.2B tokens from Qwen3-30B models. 2.3. Data Mixture and Ordering Our data mixture consists of thirteen data categories. The"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 7, "text": "with the final solution. At the post-processing stage, we apply majority voting to keep only the samples that have the most voted solutions. Overall, we generated 4B tokens from DeepSeek-V3 and 4.2B tokens from Qwen3-30B models. 2.3. Data Mixture and Ordering Our data mixture consists of thirteen data categories. The largest is web crawl data, which we subdivided into four categories based on the Nemotron-CC quality classification (Su et al., 2025): crawl-medium, crawl-medium-high, crawl-high, syn-crawl-high denoting medium, medium-high, high and synthetic quality crawl data, respectively. Apart from these, our data mixture has additional categories such as math, wikipedia, code, academic data, crawl++, multilingual, and synthetic SFT-style data which is further categorized as general-sft, stem-sft and code-sft. Crawl++ consists of web-crawl derivatives like OpenWebText, BigScience and Reddit. Our multilingual data has fifteen languages: Arabic, Danish, German, Spanish, French, Italian, Portuguese, Dutch, Polish, Swedish, Thai, Chinese, Japanese, Korean, and Russian. We design the data mixtures to give similar weight to data sources that have similar quality. Data sources of higher quality are weighed higher than data sources of lower quality. We provide detailed explanation on quality estimation of datasets and the blend creation process in Feng et al. (2024) and NVIDIA (2025). We used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano- 12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the second and third phases, we primarily used high-quality datasets (e.g., Wikipedia). We switched to the second phase at the 60% point of training, and to the third phase at the 90% point of training. The data mixtures used in each phase are shown in Figure 3. 7 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model stem-sft 3.1% multilingual 5.0% academic 4.4% code 20.0% math 3.2% syn-crawl-high 16.2% crawl-medium 18.3% crawl-medium-high 14.8% crawl-high 11.1% (a) Data mixture of Phase 1. code-sft 4.4% stem-sft 14.5% multilingual 5.0% crawl++ 4.4% academic 3.8% wiki 0.9% code 20.0% crawl-high 16.0% syn-crawl-high 21.0% math 9.5% (b) Data mixture of Phase 2. code-sft 10.9% stem-sft 32.0% multilingual 4.4% crawl-high 10.0% syn-crawl-high 12.7% math 11.0% code 16.0% (c) Data mixture of Phase 3. Figure 3 | Data mixtures for each phase of pre-training. 8 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Multilingual Data Avg Sp Ge Fr Ma It Ja Po Ko Common Crawl 37.0 37.8 36.5 39.8 34.3 36.3 35.3 37.5 38.8 FineWeb-2 35.1 38.8 35.0 34.3 31.5 37.0 33.0 36.0 35.3 DiverseQA-wiki 42.1 44.8 41.3 41.8 41.5 44.0 41.0 42.3 40.3 DiverseQA-crawl 47.0 49.8 50.8 48.3 46.0 45.8 44.5 49.0 42.0 Table 2 | Comparison of multilingual datasets on the Global-MMLU Benchmark. 2.3.1. Multilingual Data Ablation Study In Section 2.2, we mentioned several large categories of multilingual data, both curated and synthetic: 1. Common Crawl: Extracted from recent Common Crawl snapshots using our own pipeline. 2. FineWeb-2 (Penedo et al., 2025). 3. DiverseQA-wiki: Generated from multilingual Wikipedia articles using a translated Diverse QA prompt. 4. DiverseQA-crawl: Translated from English Diverse QA data. In order to decide the proper"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 8, "text": "both curated and synthetic: 1. Common Crawl: Extracted from recent Common Crawl snapshots using our own pipeline. 2. FineWeb-2 (Penedo et al., 2025). 3. DiverseQA-wiki: Generated from multilingual Wikipedia articles using a translated Diverse QA prompt. 4. DiverseQA-crawl: Translated from English Diverse QA data. In order to decide the proper data mixture among these different multilingual data sources, we first conducted ablation experiments to compare the four multilingual data’s downstream tasks’ performance. Specifically, we took a 1B model checkpoint that had been trained for 350B tokens, and continuous pretrained it for another 100B tokens. We assigned 50% of the continuous pretraining data to multilingual data, and the remaining 50% use our default pretraining data mixture. We evaluated each model’s performance using the Global-MMLU benchmark (Singh et al., 2024a); the results are shown in Table 2. Our curated Common Crawl-based multilingual data performed slightly better than the Fineweb2-based multilingual data, while the synthesized multilingual QA pairs performed much better than the curated multilingual web crawl data. The diverse pairs translated from English Common Crawl achieved the highest average score over the 8 languages we evaluated on. Therefore, we assigned a much higher weight to the DiverseQA-crawl data than the other categories when deciding our multilingual data mixture. 2.3.2. Fundamental Reasoning SFT-Style Data Ablation Study To show the effectiveness of the fundamental reasoning (FR) focused SFT-style data we introduced in Section 2.2, we took the Nemotron-H-8B (NVIDIA, 2025) intermediate checkpoint trained over 14.5T tokens, and continuous pretrained it with another 100B tokens. We assigned 5% of the 100B tokens to the newly synthesized FR-SFT data (as a replacement for Common Crawl data), and kept all other data categories the same as in the Nemotron-H-8B’s phase 3 blend. We compared this model with Nemotron-H-8B, which had also been trained with 14.6T tokens. The detailed evaluation benchmarks are introduced in Section 2.7. The comparison results are shown in Table 3. The SFT-style data helped improve the Nemotron-H 8B model’s performance on MMLU-Pro from 44.24 to 56.36, and also helped increase the average MATH score by around 2 points. While MMLU-Pro is a more challenging benchmark that evaluates a model’s language understanding capability, it also requires the model to have excellent reasoning capability to select the correct answer out of ten choices. Our SFT data helps equip the model to select the correct answers from the other nine distractors through fundamental reasoning. We noticed no decrease in the average commonsense reasoning and average code benchmarks. 9 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Model Avg Math Avg Code Avg Reasoning MMLU MMLU-Pro Nemotron-H 8B 37.92 59.49 71.79 72.67 44.24 Nemotron-H 8B (w/ FR-SFT data) 39.70 59.61 71.43 72.98 56.36 Table 3 | Ablation study of the Fundamental Reasoning (FR) focused SFT-style data. 2.4. FP8 Recipe We used DeepSeek’s FP8 training recipe for the entirety of the pretraining run (DeepSeek-AI, 2025b). Specifically, we used E4M3 for all tensors, 128x128 quantization blocks for weights, and 1x128 tiles for the activations. Unlike Nemotron-H, we natively kept the model weights in E4M3 so that we could"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 9, "text": "2.4. FP8 Recipe We used DeepSeek’s FP8 training recipe for the entirety of the pretraining run (DeepSeek-AI, 2025b). Specifically, we used E4M3 for all tensors, 128x128 quantization blocks for weights, and 1x128 tiles for the activations. Unlike Nemotron-H, we natively kept the model weights in E4M3 so that we could do the distributed optimizer’s parameter all-gather operations (across data-parallel replicas) in FP8. One exception to DeepSeek’s formula was that we left the first and last four matrix multiples in BF16, as done with Nemotron-H. Also unlike the DeepSeek-V3 run, we left all optimizer state in FP32. We observed no training instabilities from this choice of numerics. 2.5. Hyperparameters We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of 768 (6,029,312 tokens per batch). We did not use any batch size ramp-up. We used a WSD (Warmup-Stable-Decay) (Hu et al., 2024) learning rate schedule with a “stable” learning rate of 4.5 · 10−4 and a minimum value of 4.5 · 10−6; the learning rate was decayed over the final 3.6 trillion tokens. Weight decay was set to 0.1, and Adam 𝛽1 and 𝛽2 were set to 0.9 and 0.95 respectively 2.6. Long-Context Extension To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT) with a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6. Although the target context length of Nemotron Nano 2 is 128k, in preliminary studies on the Nemotron-H 8B model, we found it better to do CPT with 512k sequence length, instead of 256k or 128k. Our intuition is that longer training sequence can effectively lower the chance of long coherent documents being cut and separated by the Concat & Chunk algorithm for pretraining data loading. We used 8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence lengths of 512k tokens still fits in GPU memory. We used a global batch size of 12 to ensure the total number of tokens per global batch during long-context CPT is the same as during pretraining: around 6M tokens. Phase LC consisted of 18.9 billion tokens. Additionally, we did long-context synthetic data generation to create more high-quality data for Phase LC. Since the academic pretraining dataset is a good source of coherent long-context documents, we used such documents that are longer than 32k tokens as seed data. We followed the methods mentioned in the Llama-3 (Meta, 2024) and Qwen-2.5 (Qwen, 2025) tech reports to generate long- context document QA data. We split each document into chunks of 1,024 tokens and then randomly selected 10% of the chunks to be fed into Qwen-2.5-72B-Instruct for data synthesis. We asked the generator to generate a QA pair based on the information in the text chunk. We concatenated the QA pairs and appended them to the end of the original document as a sample of the long-context document QA data. Such long-document QA provided good"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 10, "text": "Qwen-2.5-72B-Instruct for data synthesis. We asked the generator to generate a QA pair based on the information in the text chunk. We concatenated the QA pairs and appended them to the end of the original document as a sample of the long-context document QA data. Such long-document QA provided good material for the model to learn long- context dependencies. See Table 4 for ablation results on Nemotron-H 8B regarding train sequence 10 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model lengths and the effects of synthetic data. The data blend used in Phase LC was built based on that of Phase 3. We proportionally downscaled the weights of all Phase 3 data to 80% of their original values, allocating the remaining 20% to the newly added long-context document-QA data. We found such a blend could effectively extend the context length of Nemotron-Nano-12B-v2-Base without degrading regular benchmark scores. Train length 128k 256k 256k 512k Synthetic data yes no yes yes RULER-128k 73.68 70.19 79.04 81.04 Table 4 | Comparisons of different train sequence lengths and synthetic data usages. Ablations were conducted on Nemotron-H 8B. Task N-Nano-V2 N-Nano-V2 Qwen3 Gemma3 12B Base 9B Base 8B Base 12B Base General MMLU 78.24 74.53 76.44 73.61 MMLU-Pro 5-shot 63.98 59.43 56.27 45.12 AGIEval English CoT 68.03 65.28 59.54 51.69 Math GSM8K CoT 91.66 91.36 84.00 74.45 MATH 83.54 80.50 55.40 42.40 MATH Level 5 67.61 63.64 29.91 17.71 AIME 2024 pass@32 56.67 30.00 20.00 16.67 Code HumanEval+ avg@32 61.03 58.50 57.55 36.68 MBPP+ avg@32 61.55 58.95 58.56 51.73 Commonsense Understanding ARC Challenge 93.26 90.70 93.09 90.44 HellaSwag 84.00 79.90 79.75 84.15 OpenBookQA 46.00 44.80 42.00 46.00 PIQA 82.54 81.83 79.43 82.10 WinoGrande 79.24 75.30 75.93 79.95 Long Context RULER-128K 84.74 82.22 - 80.70 Table 5 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models. N-Nano-V2 is short for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is compared against Qwen3-8B-Base and Gemma3-12B-Base, and the best score is highlighted in each row. 11 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model 2.7. Base Model Evaluations We run evaluations of all models ourselves unless otherwise stated. Our evaluation setup is built on top of lm-evaluation-harness5 for fair comparisons, with the following changes: 1. For mathematical reasoning, we evaluate GSM8K and MATH (Cobbe et al., 2021; Hendrycks et al., 2021b) benchmarks using greedy-decoding. We also highlight the competition-level slice of the MATH benchmark as “MATH Level 5”. Additionally, we report the pass@32 performance on AIME-2024. We use Math-Verify6 to grade all generations. 2. For code tasks (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)) we evaluate the EvalPlus variants along with the sanitization of generations (Liu et al., 2023), in a 0-shot setup. We estimate avg@32, pass@1 from 32 generations per prompt. 3. General reasoning benchmarks (OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2019), Hellaswag (Zellers et al., 2019), Winogrande Sakaguchi et al. (2019)) are unchanged except for ARC-Challenge (Clark et al., 2018), where we present all options at the same time, similar to MMLU (Hendrycks et al., 2021a). 4. For"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 11, "text": "General reasoning benchmarks (OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2019), Hellaswag (Zellers et al., 2019), Winogrande Sakaguchi et al. (2019)) are unchanged except for ARC-Challenge (Clark et al., 2018), where we present all options at the same time, similar to MMLU (Hendrycks et al., 2021a). 4. For multilingual capability, we evaluate MGSM Shi et al. (2022) (8-shot, native CoT) and Global MMLU-Lite Singh et al. (2024b). 5. We use RULER (Hsieh et al., 2024) as the long context benchmark. We report the average scores over all the 13 tasks included in RULER. Accuracy results for Nemotron-Nano-12B-v2-Base with comparsions to Qwen3-8B Base and Gemma3- 12B Base are shown in Tables 5 and 6. We also include the accuracy of our 9B pruned variant of Nemotron-Nano-12B-v2-Base which is discussed in Section 4. 3. Alignment In this section we will present the alignment process we followed to convert the base checkpoint into an aligned 12B checkpoint. Our process is outlined in Figure 4. Base SFT 1 SFT 2 SFT 3 Merged GRPO RLHF DPO Figure 4 | Flow of alignment procedures followed to arrive at the final \"Merged\" Nemotron Nano 2 12B checkpoint. 5https://github.com/EleutherAI/lm-evaluation-harness. 6https://github.com/huggingface/math-verify. 12 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Task N-Nano-V2 N-Nano-V2 Qwen3 Gemma3 12B Base 9B Base 8B Base 12B Base Global-MMLU-Lite German 74.50 68.25 75.50 69.75 Spanish 76.50 72.75 75.00 74.00 French 78.25 69.75 74.25 72.50 Italian 76.50 73.25 72.75 74.00 Japanese 71.00 67.00 70.00 71.50 Korean 72.50 67.25 67.25 70.25 Portuguese 76.25 71.25 72.50 75.75 Chinese 75.50 69.25 75.25 67.25 Average 75.13 69.94 72.81 71.88 Multilingual Math (MGSM) Spanish 93.20 91.60 86.40 74.00 German 89.60 89.60 78.80 68.80 French 86.40 86.00 78.80 70.80 Chinese 44.40 75.20 28.80 26.80 Japanese 76.00 74.80 30.80 26.40 Russian 90.40 91.60 83.60 76.00 Average 80.00 84.80 64.53 57.13 Table 6 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models on multilingual benchmarks. N-Nano-V2 is short for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is compared against Qwen3-8B-Base and Gemma3-12B-Base, and the best score is highlighted in each row. 3.1. Post-Training Data Our alignment begins with a large-scale SFT stage which trains the base model on approximately 80 billion tokens of prompt-response pairs. The distribution of domains is shown in Table 7. Math, science and coding. For Math, Science and Coding data, we generate responses using the open-weights DeepSeek-R1-0528 model (DeepSeek-AI, 2025b) using the same prompts used for training Nemotron-H-8B and 47B Reasoning models (NVIDIA, 2025). The training data has been released as part of Nemotron-Post-Training-Dataset-v17. Tool calling. The tool-calling dataset consists of single-turn, multi-turn, and multi-step conversa- tions. For single-turn cases, we sample prompts from xlam-function-calling-60k8, glaive- function-calling-v29, NVIDIA-When2Call (Ross et al., 2025), and generate responses using Qwen3-235B-A22B10. Inspired by ToolACE (Liu et al., 2024) and APIGen-MT (Prabhakar et al., 2025), we extend this to multi-turn and multi-step settings by simulating conversations where Qwen3-235B-A22B plays the roles of User-Agent, Assistant-Agent, and API-Server-Agent. The 7https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1 8https://huggingface.co/datasets/xlam-function-calling-60k 9https://huggingface.co/datasets/glaive-function-calling-v2 10https://huggingface.co/Qwen/Qwen3-235B-A22B 13 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Domain Number of Samples Math 1.5M Coding 1.1M"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 12, "text": "et al., 2025), we extend this to multi-turn and multi-step settings by simulating conversations where Qwen3-235B-A22B plays the roles of User-Agent, Assistant-Agent, and API-Server-Agent. The 7https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1 8https://huggingface.co/datasets/xlam-function-calling-60k 9https://huggingface.co/datasets/glaive-function-calling-v2 10https://huggingface.co/Qwen/Qwen3-235B-A22B 13 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Domain Number of Samples Math 1.5M Coding 1.1M Science 2.0M Tool-calling 400K Conversational 1.5M Safety 2K Multilingual (all domains) 5.0M Table 7 | Post-training data distribution across domains used for our SFT stages. User-Agent reviews available tools, poses challenging queries, interacts when addressed by the Assistant, and judges task success at the end. Each instance is paired with a random persona from Nemotron-Personas11 to enrich diversity of queries. The Assistant-Agent receives the initial query and available tools, executes tasks by invoking tools, interpreting their responses, and interacting with the User-Agent across single-turn, multi-turn, or multi-step scenarios. Meanwhile, the API-Server-Agent acts as a mock API server, checking parameters and returning either valid outputs or error messages depending on correctness. A lightweight rule-based tool-call verification layer further strengthens reliability by ensuring outputs are consistent and verifiable, and only successful trajectories are retained. Multilingual data. Our multilingual synthetic post-training data are constructed by translating existing English post-training data. To address the challenges of Large Language Model (LLM) hallucinations and quality degradation on long inputs when generating synthetic translation data, we implement a robust quality assurance pipeline. Our method involves translating inputs line-by-line to manage complexity and skip non-translatable content like code. We also enforce a strict bracket format for reliable extraction and use language identification to filter out off-target translations, thereby ensuring high-quality final outputs. Conversational data. For conversational data, we use prompts from the LMSYS dataset (Zheng et al., 2023) and generate responses using the Qwen3-235B-A22B reasoning model (Yang et al., 2025). We also incorporate prompts from HelpSteer2 and HelpSteer3, paired with responses generated by the same model. In addition, we draw on a subset of approximately 550k prompts from WildChat- 1M (Li et al., 2024b), again generating reasoning responses with Qwen3-235B-A22B. We also include multi-turn conversations with Deepseek R1 responses using the multi-turn conversational prompts used in NVIDIA (2025). Safety. We leveraged a mix of harmful and benign prompts drawn from the Nemotron Content Safety Dataset V2 (Ghosh et al., 2025)12, HarmfulTasks (Hasan et al., 2024), RedTeam2K (Luo et al., 2024), and gretel-v1 (gre, 2024). Responses were generated using DeepSeek-R1-052813. To ensure safety, we applied a two-step approach: initial prompting followed by filtering with guard models to verify that outputs remained safe. 11https://huggingface.co/datasets/NVIDIA/Nemotron-Personas 12https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0 13https://huggingface.co/deepseek-ai/DeepSeek-R1 14 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model 3.2. Post Training Stage 1 SFT. As Figure 4 illustrates, we employ three distinct stages of supervised fine-tuning. Stage 1 uses the full dataset described in Section 3.1, augmented with a subsample of roughly 10% of prompts paired with outputs stripped of reasoning traces. This exposes the model to “empty” traces, enabling it to produce direct answers in a reasoning-off mode. To improve efficiency and preserve long-context ability from pretraining, we concatenate samples into sequences of approximately 128k tokens, reducing padding overhead and"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 13, "text": "roughly 10% of prompts paired with outputs stripped of reasoning traces. This exposes the model to “empty” traces, enabling it to produce direct answers in a reasoning-off mode. To improve efficiency and preserve long-context ability from pretraining, we concatenate samples into sequences of approximately 128k tokens, reducing padding overhead and encouraging long-range learning. Stage 2 SFT. Stage 2 targets tool-calling. Although Stage 1 improved performance on most benchmarks, tool-calling accuracy degraded. We attribute this to sample concatenation at 128k, which likely disrupted learning of tool-calling patterns. Thus, Stage 2 was trained without concatenation, using the full tool-calling dataset and a representative subsample of other domains. Stage 3 SFT. Stage 3 reinforces long-context capability. It incorporates long-context data following the recipe used in Nemotron-H preparation (NVIDIA, 2025), along with augmented examples across domains where reasoning traces were abruptly truncated to 1–2k tokens while preserving the final answer. This truncation strategy improved robustness under varying inference-time thinking budgets. IFeval RL. To improve instruction adherence, we sampled 16,000 prompts from the LMSYS Chat dataset and augmented them with IFEval-style instructions. A rule-based verifier scored outputs based on how well they satisfied each instruction, creating a reward signal that prioritized following directions with precision. IFEval RL experiments provided significant boost to IFEval capabilities while the rest of the benchmarks fluctuated slightly requiring careful checkpoint selection. DPO. In another branch of training, we apply the DPO algorithm to improve tool-calling. We evaluate performance using the BFCL v3 benchmark, which extends BFCL v2 with greater emphasis on multi-step (multiple tool calls to achieve a goal) and multi-turn (multiple user–agent interactions). To strengthen these capabilities in the Nano V2 aligned model, we use the WorkBench environment, a multi-step verifiable tool-calling setup adapted from Styles (Styles et al., 2024). In each WorkBench task, the model must issue a sequence of tool calls across multiple steps, with correctness verified through database state comparisons. Nano V2 undergoes reinforcement learning in this environment through iterative stages of Direct Preference Optimization. For each candidate checkpoint from the long-context stage, we generate on-policy data consisting of positive examples (successful tool calls) and negative examples (failed generations) for every WorkBench prompt. This process ensures that iterative DPO remains on-policy. RLHF. We evaluate the model’s overall helpfulness and chat capabilities using the Arena-Hard benchmark. To improve performance on this benchmark, we use GRPO to train candidate checkpoints from the SFT stage using English-only contexts from HelpSteer3 (Wang et al., 2025). During training, we generate responses both with and without thinking traces and use a Qwen-based reward model to judge the rollouts. Model Merging. During training, we observed a trade-off between reasoning capabilities and chat capabilities. To address this, we opted for checkpoint interpolation Wortsman et al. (2022), 15 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Evaluation Nemotron-Nano-v2-12B Qwen3-8B Qwen3-14B AIME-2024 85.42 75.83 81.53 AIME-2025 76.25 69.31 66.6 MATH-500 97.75 96.3 96.85 GPQA-Diamond 64.48 59.61 64.53 LiveCodeBench (07/24–12/24) 70.79 59.5 63.08 SciCode Sub-Task 18.75 24.65 26.04 Humanity’s Last Exam 6.30 4.40 5.38 IFEval (Inst. Strict) 89.81 89.39 91.32 BFCL v3 66.98 66.34 68.01"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 14, "text": "Mamba-Transformer Reasoning Model Evaluation Nemotron-Nano-v2-12B Qwen3-8B Qwen3-14B AIME-2024 85.42 75.83 81.53 AIME-2025 76.25 69.31 66.6 MATH-500 97.75 96.3 96.85 GPQA-Diamond 64.48 59.61 64.53 LiveCodeBench (07/24–12/24) 70.79 59.5 63.08 SciCode Sub-Task 18.75 24.65 26.04 Humanity’s Last Exam 6.30 4.40 5.38 IFEval (Inst. Strict) 89.81 89.39 91.32 BFCL v3 66.98 66.34 68.01 RULER @ 128k 83.36 74.13 73.55 ArenaHard 74 78.4 87.7 Table 8 | Evaluation results with reasoning \"ON\" (for Nemotron-Nano-v2-12B, Qwen3-8B, and Qwen3-14B across reasoning and general capability benchmarks. blending in an RL checkpoint with strong reasoning capabilities with an RL checkpoint with strong chat capabilities. Checkpoint interpolation is performed by linearly interpolating model weights: (1 −𝛼) · 𝑤𝑚𝑜𝑑𝑒𝑙1 + 𝛼· 𝑤𝑚𝑜𝑑𝑒𝑙2. We experimented with a parameter sweep over 𝛼values from 0.1 to 0.9 in increments of 0.1, and found that values around 0.5 offered a good trade-off. 3.3. Evaluation Our 12B model’s performance is summarized in Table 8. To test reasoning capabilities across domains, we evaluate the models on MATH-500 (Lightman et al., 2023), AIME-2024, AIME-2025, GPQA-Diamond (Rein et al., 2023), LiveCodeBench (07/24 - 12/24) (Jain et al., 2024), SciCode (Tian et al., 2024), and Humanity’s Last Exam (Phan et al., 2025). For broader evaluation on diverse capabilities, we use IFEval (Zhou et al., 2023) for instruction following capabilities, BFCL v3 (Yan et al., 2024) for tool-calling, RULER for long-context, and ArenaHard (Li et al., 2024a) for chat capability. We conduct evaluations using NeMo-Skills14. We report Pass@1 average of 16 runs for AIME- 2024, AIME-2025; average of 4 runs for MATH-500, GPQA-Diamond, LiveCodeBench, IFEval; and score of 1 run for BFCL v3, SciCode, Humanity’s Last Exam, RULER, and ArenaHard. 3.4. Budget Control Evaluation Nemotron Nano V2 allows users to specify how many thinking tokens the model may generate before producing the final answer. The final answer is the portion of text typically shown to end users. This feature is implemented by counting tokens after the model begins generating the <think> token. Once the budget is reached, the inference setup attempts to insert a closing </think> tag. Rather than inserting it immediately, we let the model finish its current sentence and place the tag at the next newline. In extreme cases where no newline appears, the system enforces closure within 500 tokens past the budget: if no newline occurs by the (budget + 500)th token, the </think> tag is forcibly inserted. Figure 5b shows our models budget control behavior. Apart from just presenting the accuracy of the model at various budgets, we also inspect if the model generations are well-formatted at various budgets. We inspect for two kinds of failure modes: 14https://github.com/NVIDIA/NeMo-Skills 16 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model (a) (b) Figure 5 | Comparison of budget control before truncation training (a) and after truncation training was included (b). For all plots above the x-axis indicates the budget assigned for thinking tokens. • In one failure mode, the model uses more tokens in the final answer to “compensate” for restrictions in the thinking traces. Without truncated training examples in the SFT stage, this compensation effect is"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 15, "text": "was included (b). For all plots above the x-axis indicates the budget assigned for thinking tokens. • In one failure mode, the model uses more tokens in the final answer to “compensate” for restrictions in the thinking traces. Without truncated training examples in the SFT stage, this compensation effect is prevalent (Figure 5a, center). With truncated training, however, the effect is absent (Figure 5b, center). • Another issue is that the model can remain in “thinking mode” even after the closing tag </think> is inserted. This is evident when the model generates the closing tag again after the forced insertion, suggesting it does not fully “register” the artificial closure. We evaluate this using “Well-Formedness,” where a well-formed response should contain only a single closing tag (either forced by the budget or produced naturally). Figure 5a (right) shows that for short budgets, the percentage of well-formed responses drops sharply. With truncation training, however, the model consistently produces well-formed responses (Figure 5b, right). 4. Pruning and Distillation In this section, we describe the pruning and distillation process to compress the aligned 12B model to the Nano 2 model with the goal of running longer context (128k sequence length) inference on the NVIDIA A10G GPU. Note that storing just the weights of a 12B parameter model in bfloat16 precision requires 22.9 GiB, which is more than the 22 GiB memory capacity of an A10G GPU; this clearly indicates the need for compression. Our compression strategy builds on Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024; Taghibakhshi et al., 2025), which is a lightweight model pruning framework for LLMs. While Minitron was originally designed for compressing pretrained base models targeting user-defined parameter budgets, in this work, we extend it to compress reasoning models while also incorporating 17 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model the memory constraints and throughput-based objectives stated above. 4.1. Importance Estimation We collect importance or sensitivity scores for each model component (e.g., layers, FFN neurons) to help decide which components to remove; this is the importance estimation phase. The scores computed in this phase are used to decide which model components can be pruned. We note that sensitivity analysis based on gradient information is typically impractical at modern LLM scale (Muralidharan et al., 2024); instead, we rely on a lightweight strategy that uses only forward passes. In this work, we use a simplified approach that works well in our ablation studies: a) prune layers, and b) prune FFN hidden dimensions (effectively neurons) and embedding channels. We also experimented with pruning Mamba heads; unfortunately, this axis caused severe accuracy degradation. We now describe how we compute the importance of each layer, embedding channel, FFN neuron and Mamba head. Layer importance. We compute layer importance in an iterative fashion: for each candidate layer, we temporarily remove it from the model and compute the mean squared error (MSE) between the original model’s logits and those produced by the pruned model. This MSE reflects the contribution of that layer to the model’s predictions: lower values indicate smaller impact. At each"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 16, "text": "fashion: for each candidate layer, we temporarily remove it from the model and compute the mean squared error (MSE) between the original model’s logits and those produced by the pruned model. This MSE reflects the contribution of that layer to the model’s predictions: lower values indicate smaller impact. At each pruning step, we remove the layer with the lowest MSE, as it has the least influence on the final output. We repeat this process until the desired depth is reached. This strategy ensures that pruning preferentially removes layers whose absence minimally affects the model’s behavior. For more details on iterative MSE-based layer importance, please refer to NVIDIA (2025). FFN and embedding channel importance. FFN layers internally are composed of two linear operators with a non-linear activation in between: FFN(X) = 𝛿 (︂ X · 𝑊𝑇 1 )︂ · 𝑊2. Here, X denotes the input, and 𝑊1 and 𝑊2 are the two associated weight matrices in the FFN layer. 𝑊1, 𝑊2 ∈R𝑑𝑓𝑓𝑛×𝑑𝑚𝑜𝑑𝑒𝑙, where 𝑑𝑚𝑜𝑑𝑒𝑙and 𝑑𝑓𝑓𝑛are the model hidden dimension and FFN hidden dimension respectively. 𝛿(·) refers to the non-linear activation function (squared ReLU in this work). Following the same procedure as Minitron (Muralidharan et al., 2024), we compute the importance of each neuron in the first linear operator of each FFN layer by examining the set of outputs it produces. We use a small calibration dataset of 1024 samples for this purpose. Formally, we compute each neuron’s importance score by aggregating its outputs given an input batch 𝑋: 𝐹(𝑖) neuron = ∑︁ B,S 𝛿 (︂ X (︀𝑊𝑖 1 )︀𝑇 )︂ . Here, 𝑊𝑖 1 refers to the 𝑖th row of the weight matrix 𝑊1. ∑︀ B,S refers to aggregation along the batch and sequence dimensions. We use the mean and l2-norm aggregation functions along the batch and sequence dimensions, following the observations in the Minitron paper. For a sequence of scores S, mean aggregation is defined as 1 𝑛 ∑︀𝑛 𝑖=1 |S𝑖|, and l2-norm is √︁∑︀𝑛 𝑖=1 S2 𝑖. Embedding channel importance is computed similarly, by examining the outputs of LayerNorm layers instead; we refer the reader to Muralidharan et al. (2024) for more details. 18 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Mamba importance. Mamba layers process inputs through multiple projection matrices (𝑊𝑥, 𝑊𝑧, 𝑊𝐵, 𝑊𝐶, 𝑊𝑑𝑡) that produce intermediate representations before causal convolution and selective state space model (SSM) updates, followed by gated normalization and an output projection (𝑊𝑂). We follow the methodology described in Taghibakhshi et al. (2025) for importance estimation: specifically, we adopt a nested activation-based scoring strategy over a small calibration dataset of 1024 samples, similar to FFN importance but adapted to Mamba’s group-aware structure. First, we obtain activation scores from the 𝑊𝑥projection, denoted 𝑠∈R𝑚ℎ×𝑚𝑑, where 𝑚ℎis the number of Mamba heads and 𝑚𝑑is the Mamba head channel dimension. For each channel 𝑑, the score is computed as 𝑠𝑑= ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ ∑︁ B,S 𝑠:,𝑑 ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ 2 , where the aggregation is over the batch (B) and sequence (S) dimensions, using both mean and l2-norm"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 17, "text": "𝑚𝑑is the Mamba head channel dimension. For each channel 𝑑, the score is computed as 𝑠𝑑= ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ ∑︁ B,S 𝑠:,𝑑 ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ 2 , where the aggregation is over the batch (B) and sequence (S) dimensions, using both mean and l2-norm metrics. Next, head scores are computed by using the l2-norm over the Mamba head channel set: 𝑓ℎ= ‖𝑠ℎ,𝑚𝑑‖2 , ∀ℎ∈{1, . . . , 𝑚ℎ}, and heads are ranked within each Mamba group 𝒢𝑔to preserve group-aware computation semantics: ℛ𝑔= argsortℎ∈𝒢𝑔(𝑓ℎ). which ensures that pruning decisions respect the model’s structural constraints and SSM’s sequence modeling. The lowest-scoring heads are pruned by trimming the corresponding rows from all affected projection, convolution, and SSM parameter matrices. This strategy preserves the integrity of the SSM block while removing less important Mamba heads. As shown in Taghibakhshi et al. (2025), pruning Mamba heads yields a better accuracy–throughput trade-off than pruning head channels; we consequently focus on head pruning in this work. 4.2. Lightweight Neural Architecture Search We first define the constraints and objectives for the Nano 2 model, and then describe our lightweight Neural Architecture Search (NAS) framework that finds the most promising architectural candidates that meet our objectives and constraints. Memory constraints. Memory requirements during inference consist of two distinct components with different scaling behaviors. The parameter memory, while substantial, remains constant regardless of the input size. In contrast, the key-value cache memory scales linearly with both batch size and sequence length, often becoming the dominant factor in long-sequence scenarios. For the Nano 2 model, our goal was to be able to perform inference at a sequence length of 128k and a batch size of at least 1 within a memory budget of 19.66 GiB. We obtained the budget as follows: from the 22.06 GiB available memory on an NVIDIA A10G GPU, we subtract a 5% buffer for frameworks such as vLLM and TensorRT-LLM and another 1.3 GiB to allow sufficient space for a vision encoder. Measuring throughput. For the experiments below, unless otherwise specified, we measure throughput on an input and output sequence length of 8k and 16k tokens respectively, which we believe represents a typical reasoning scenario. For this combination of input and output sequence length, we report vLLM output token generation throughput at the maximum batch size that fits on the A10G GPU. 19 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model 4.2.1. Candidate enumeration. Our compression strategy explores multiple axes within the 19.66 GiB memory budget through combinatorial pruning. Our search space includes depth reduction (removing 6-10 layers from the original 62-layer architecture) combined with width pruning of embedding channels (4480-5120), FFN dimension (13440-20480), and Mamba heads (112-128). This multi-axis search space results in hundreds of candidate architectures meeting the memory constraint. 4.2.2. Finding the Best Architecture Since performing knowledge distillation and throughput benchmarking on the full set of candidates would be prohibitively expensive, we break down the problem into two parts: (1) find the optimal depth for the compressed model, and (2) find the optimal width-pruned architecture"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 18, "text": "meeting the memory constraint. 4.2.2. Finding the Best Architecture Since performing knowledge distillation and throughput benchmarking on the full set of candidates would be prohibitively expensive, we break down the problem into two parts: (1) find the optimal depth for the compressed model, and (2) find the optimal width-pruned architecture given the depth. Effect of depth. We compare the accuracy of three depth-pruned candidates obtained from the 12B model with 52, 54 and 56 layers. Here, we keep the number of attention layers fixed at 4 for all three variants so as to achieve a good balance between KV cache size and long-context performance; prior work has indicated that an attention-to-total-layers ratio between 7-8% is reasonable (NVIDIA, 2025). We leave the width dimensions untouched for this experiment. Table 9 lists average reasoning accuracy at different depths after 6B tokens of distillation; in line with our previous observations on the strong correlation between depth and task performance (Muralidharan et al., 2024; Sreenivas et al., 2024), we notice that reducing depth beyond 56 layers results in significant accuracy degradation; as a result, we fix the depth at 56 for further width pruning. Accuracy (Avg) 52 Layers 44.92 54 Layers 47.35 56 Layers 51.48 Table 9 | Effect of depth on reasoning accuracy. Results are after distilling with 6B tokens. Combining depth and width pruning. As described above, we fix the depth of our target model to 56 layers with 4 attention layers. We perform 60B tokens of distillation on this checkpoint (see Section 4.3 for additional details) and perform further width pruning along the embedding, FFN, and Mamba axes. We enumerate all candidate pruned architectures that meet our memory budget, and sort them in decreasing order of estimated memory consumption at 128k context length and batch size 1. The top 3 candidates from this list are picked for further evaluation: in particular, we perform short Knowledge Distillation (KD) on these candidates for 19B tokens after depth+width pruning; we also benchmark throughput to pick the final architectural candidate. Table 10 lists the architectural details of the top 3 candidates, along with the achieved task performance (post KD) and throughput. As shown in the Table, Candidate 2 achieves the best accuracy while still having reasonable runtime performance; consequently, we use this architecture for Nano 2. FFN vs. Mamba pruning. We ablate the number of Mamba heads following the recipe in Taghibakhshi et al. (2025), considering configurations with 87.5% and 93.75% of the original heads. However, due to the relatively smaller compression ratios explored in this work (less than 15% after depth pruning) compared to those in Taghibakhshi et al. (2025) (around 50%), we find that applying Mamba head pruning yields limited benefit, and in these cases, pruning only the FFN and 20 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model #Layers Hidden FFN Mamba #Heads Params. (B) Accuracy Throughput Candidate 1 56 4480 17920 112 8.92 59.07 161.02 Candidate 2 56 4480 15680 128 8.89 63.02 156.42 Candidate 3 56 4800 14400 120 8.97 62.94 155.86 Table 10 | Top 3 candidates"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 19, "text": "An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model #Layers Hidden FFN Mamba #Heads Params. (B) Accuracy Throughput Candidate 1 56 4480 17920 112 8.92 59.07 161.02 Candidate 2 56 4480 15680 128 8.89 63.02 156.42 Candidate 3 56 4800 14400 120 8.97 62.94 155.86 Table 10 | Top 3 candidates for architecture selection. Accuracy is the average across reasoning benchmarks after distillation with 19B tokens. The last column shows vLLM output generation throughput (ISL/OSL=8k/16k and batch size=8). embedding dimensions—after depth pruning—proves sufficient to achieve the desired compression while preserving accuracy. Candidates 1 and 2 in Table 10 highlight this difference. 4.3. Retraining with Distillation To recover the accuracy lost due to pruning, the model undergoes continued training. Recent work has demonstrated that distilling knowledge from the original model to the pruned model outperforms conventional fine-tuning (Muralidharan et al., 2024; Sreenivas et al., 2024; Bercovich et al., 2024); we thus adopt logit-based distillation for continued training, employing forward KL divergence loss exclusively during the accuracy recovery phase (see §3 of the Minitron paper (Muralidharan et al., 2024) for more details on the distillation loss formulation). Building on the candidate selection process described in §4.2, we continue training Candidate 2 in an extended phase, as detailed below, to yield the final Nano 2 reasoning and base models. % Reasoning-SFT data % Pretraining data Accuracy (Avg) 50 50 57.5 70 30 58.5 90 10 57.2 Table 11 | Effect of varying reasoning data proportion on math accuracy after ∼6B tokens of KD. Reasoning model. The reasoning model is distilled in stages with increasing sequence lengths to strengthen extended reasoning and long-context capabilities; this is followed by targeted reinforcement learning (RL), preference optimization and model merging to retain desired behaviors and ensure robustness across diverse tasks. We now describe these various stages: 1. Depth pruning to 56 layers; Knowledge Distillation (KD) with ∼60B tokens at 8,192 sequence length. 2. Width pruning and KD with: • ∼50B tokens at 8,192 sequence length. • ∼25B tokens at 49,152 sequence length. • ∼1B tokens at 262,144 sequence length. 3. Direct Preference Optimization (DPO). 4. Group Relative Policy Optimization (GRPO). 5. KD with ∼0.4B tokens at 262,144 sequence length to recover post-RL drops. 6. RLHF for alignment with human preferences. 7. Model merging between steps 5 and 6 via 0.5 linear interpolation. 21 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model More details on DPO, GRPO and RLHF can be found in Section 3. Figure 6 shows the effects of staged training on model accuracy across different reasoning benchmarks. Here, the 𝑥-axis represents the various stages (starting from Step 2 above), and the 𝑦-axis shows the scores obtained for the various benchmarks as training progresses. As shown in the Figure, DPO and GRPO are critical for enhancing function-calling (BFCL v3) and instruction-following (IFEval) capabilities, though the latter temporarily degrades multi-task understanding (MMLU-Pro), which is recovered in the next step (post-GRPO KD). Finally, RLHF enhances alignment with human preferences (Arena-Hard) but causes additional benchmark drops, which are then recovered through model merging. KD+LCExt DPO GRPO KD RLHF"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 20, "text": "enhancing function-calling (BFCL v3) and instruction-following (IFEval) capabilities, though the latter temporarily degrades multi-task understanding (MMLU-Pro), which is recovered in the next step (post-GRPO KD). Finally, RLHF enhances alignment with human preferences (Arena-Hard) but causes additional benchmark drops, which are then recovered through model merging. KD+LCExt DPO GRPO KD RLHF Merge Pipeline Stage 50 55 60 65 70 75 80 85 90 Score (%) Distillation Pipeline AIME-25 GPQA-D BFCLv3 IFEval (Pr.) MMLU-Pro ArenaHard LiveCodeBench Figure 6 | Task accuracy at different stages of the distillation pipeline for Nemotron Nano 2. Dataset: We observe that a mix of 70% post-training stage 2 data (Section 3.2) and 30% pretraining (Section 2.2) data yields the highest accuracy (Table 11). For KD at sequence length 262,144, we use 100% stage 3 post-training data (Section 3.2). Base model. Distillation proceeds in stages: depth-only pruning and KD on ∼120B tokens, followed by width pruning and KD on ∼360B tokens (both at sequence length 8,192), and finally KD on ∼2.5B tokens at sequence length 524,288 to instill long-context capabilities. Dataset: Following Sreenivas et al. (2024), we use 100% pretraining data described in sections 2.2 and 2.6 for distillation of the base model at sequence lengths 8,192 and 524,288, respectively. 4.4. Results We efficiently compress the 12B model to 9B parameters by pruning full layers (depth), FFN hidden size, and embedding channels, improving inference throughput and enabling long-context inference on an NVIDIA A10G GPU. Nemotron-Nano-9B-v2 retains 56 layers of the original model. Additionally, the number of embedding channels were pruned from 5120 to 4480, and FFN intermediate size was pruned from 20480 to 15680. As shown in Figure 1 and Tables 5 and 6, Nemotron-Nano-9B-v2 achieves 3×-6× higher throughput than Qwen3-8B for generation-heavy scenarios, while surpassing 22 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model it in accuracy and remaining comparable to the 12B teacher on most benchmarks. 5. Conclusion In this report, we introduced Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer reasoning model that achieves comparable or better accuracies at up to 6× higher throughput than existing state- of-the-art models such as Qwen3-8B. To create Nemotron-Nano-9B-v2, we started by pre-training Nemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and synthetically generated data. We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT, GRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy to produce the final model. As a result of this compression, Nemotron-Nano-9B-v2 can run inference on context lengths of up to 128k tokens in bfloat16 precision on a single NVIDIA A10G GPU with 22 GiB of memory. We have open-sourced Nemotron-Nano-9B-v2 along with its corresponding sibling Nemotron-Nano-9B-v2-Base and parent Nemotron-Nano-12B-v2-Base models, plus the majority of its pre- and post-training data on HuggingFace (links at the bottom of Section 1). Contributors We thank the following people for their invaluable contributions to NVIDIA Nemotron Nano 2. Data. Abhinav Khattar, Aleksander Ficek, Arham Mehta, Ayush Dattagupta, Brandon Norick, Dan Su, Daria Gitman, Evelina Bakhturina, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jane Polak Scowcroft, Jocelyn Huang, Joseph Jennings, Jupinder Parmar, Markus Kliegl, Matvei Novikov, Mehrzad"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 21, "text": "thank the following people for their invaluable contributions to NVIDIA Nemotron Nano 2. Data. Abhinav Khattar, Aleksander Ficek, Arham Mehta, Ayush Dattagupta, Brandon Norick, Dan Su, Daria Gitman, Evelina Bakhturina, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jane Polak Scowcroft, Jocelyn Huang, Joseph Jennings, Jupinder Parmar, Markus Kliegl, Matvei Novikov, Mehrzad Samadi, Miguel Martinez, Mohammad Shoeybi, Mostofa Patwary, Pavlo Molchanov, Pritam Gundecha, Rabeeh Karimi Mahabadi, Ranjit Rajan, Rima Shahbazyan, Sanjeev Satheesh, Sarah Yurick, Sean Narenthiran, Seungju Han, Shizhe Diao, Shrimai Prabhumoye, Shubham Toshniwal, Siddhartha Jain, Somshubra Majumdar, Syeda Nahida Akter, Vahid Noroozi, Vineeth Kalluru, Vitaly Kurin, Wasi Uddin Ahmad, Wei Du, Ximing Lu, Yejin Choi, Ying Lin. FP8. Hua Huang, Jinze Xue, Keith Wyss, Kunlun Li, Mike Chrzanowski, Oleg Rybakov, Przemek Tredak, Tim Moon, Zhongbo Zhu. Architecture. Bita Darvish Rouhani, Brandon Norick, Duncan Riach, Nidhi Bhatia, Roger Waleffe, Wonmin Byeon, Ritika Borkar, Xin Dong, Yonggan Fu. Pretraining. Aarti Basant, Abhijit Paithankar, Abhinav Khattar, Deepak Narayanan, Herman Sahota, Hexin Wang, Jupinder Parmar, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Roger Waleffe, Russell J. Hewett, Ryan Prenger, Seonmyeong Bak. Infrastructure. Alex Kondratenko, Alex Shaposhnikov, Anubhav Mandarwal, Ashwin Poojary, Dong Ahn, Gargi Prasad, Haim Elisha, Harsh Sharma, Kumar Anik, Maer Rodrigues de Melo, Ruoxi Zhang, Shelby Thomas, Stefania Alborghetti, Tony Wang. Long Context. Deepak Narayanan, Dima Rekesh, Duncan Riach, John Kamalu, Kezhi Kong, Markus Kliegl, Roger Waleffe, Samuel Kriman. Inference. Daniel Afrimi, Helen Ngo, Keshav Santhanam, Kushan Ahmadian, Lawrence McAfee, Luis Vega, Nave Assaf, Peter Dykas, Shanmugam Ramasamy, Siddharth Singh, Tomer Asida, Vijay Korthikanti. Alignment. Adi Renduchintala, Ameya Sunil Mahabaleshwarkar, Banghua Zhu, Bilal Kartal, Brian Yu, Charles Wang, Christian Munley, David Mosallanezhad, Gerald Shen, Hayley Ross, Hoo Chang Shin, Igor Gitman, Julien Veron Vialard, Junkeun Yi, Kezhi Kong, Luis Vega, Makesh Narsimhan 23 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Sreedhar, Oleksii Hrinchuk, Oleksii Kuchaiev, Peter Jin, Prasoon Varshney, Ritu Gala, Shuoyang Ding, Venkat Srinivasan, Vitaly Lavrukhin, Yian Zhang, Yoshi Suhara, Zhen Dong, Zijia Chen. Compression. Aditya Malte, Akhiad Bercovich, Akshay Hazare, Ali Taghibakhshi, Ameya Sunil Mahabaleshwarkar, Ashwath Aithal, Banghua Zhu, Daniel Korzekwa, Deepak Narayanan, Gerald Shen, Hayley Ross, Julien Veron Vialard, Luis Vega, Marcin Chochowski, Mostofa Patwary, Nima Tajbakhsh, Oluwatobi Olabiyi, Pavlo Molchanov, Ran El-Yaniv, Roger Waleffe, Saurav Muralidharan, Sepehr Sameni, Sharath Turuvekere Sreenivas, Tomer Asida, Yashaswi Karnati, Yian Zhang, Yoshi Suhara, Zijia Chen. Software Support. Abhijit Khairnar, Adithya Renduchintala, Ali Taghibakhshi, Anna Shors, Ashwath Aithal, Balaram Buddharaju, Bobby Chen, Charlie Truong, Deepak Narayanan, Dmytro Pykhtar, Duncan Riach, Gerald Shen, Helen Ngo, Jared Casper, Jimmy Zhang, Keshav Santhanam, Kezhi Kong, Lawrence McAfee, Luis Vega, Nima Tajbakhsh, Parth Chadha, Piotr Bialecki, Prashant Gaikwad, Rajen Patel, Roger Waleffe, Sahil Jain, Terry Kong, Tyler Poon, Vijay Korthikanti, Vikram Fugro, Yoshi Suhara, Zhiyu Li. Evaluations and Safety. Christopher Parisien, Dan Su, Daniel Rohrer, Eileen Long, Erick Galinkin, Helen Ngo, Katherine Luna, Keshav Santhanam, Kezhi Kong, Leon Derczynski, Marta Stepniewska-Dziubinska, Meriem Boubdir, Michal Bien, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Pablo Ribalta, Piotr Januszewski, Pradeep Thalasta, Sanjeev Satheesh, Shaona Ghosh, Tomasz Hliwiak. Legal and Compliance. Barnaby Simkin, Chetan Mungekar, Dina Yared, Iain Cunningham, Katherine"}
{"doc_id": "2508.14444v1", "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14444v1", "chunk_id": 22, "text": "Erick Galinkin, Helen Ngo, Katherine Luna, Keshav Santhanam, Kezhi Kong, Leon Derczynski, Marta Stepniewska-Dziubinska, Meriem Boubdir, Michal Bien, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Pablo Ribalta, Piotr Januszewski, Pradeep Thalasta, Sanjeev Satheesh, Shaona Ghosh, Tomasz Hliwiak. Legal and Compliance. Barnaby Simkin, Chetan Mungekar, Dina Yared, Iain Cunningham, Katherine Cheung, Laya Sleiman, Meredith Price, Michael Boone, Nikki Pope, Ria Cheruvu, Saori Kaji. Marketing. Amelia Barton, Chris Alexiuk, Mark Cai, Nirmal Kumar Juluru, Shreya Gopal. Project Management. Alejandra Rico, Amy Shen, Ann Guan, Ashton Sharabiani, Elliott Ning, Krzysztof Pawelec, Negar Habibi, Twinkle Vashishth. Product. Arun Venkatesan, Chintan Patel, Chris Alexiuk, Joey Conway, Padmavathy Subramanian, Udi Karpas. Leadership. Andrew Tao, Boris Ginsburg, Bryan Catanzaro, Eric Chung, Jan Kautz, Joey Conway, Jonathan Cohen, Kari Briski, Mohammad Shoeybi, Mostofa Patwary, Oleksii Kuchaiev, Pavlo Molchanov. We also thank Chen Zhang, Michael Goin, Thomas Parnell from the vLLM team for their assistance. 24 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"}
{"doc_id": "2508.14427v1", "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14427v1", "chunk_id": 0, "text": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models Wuyang Zhang* University of Massachusetts Amherst Amherst, USA Yexin Tian Georgia Institute of Technology Atlanta, USA Xiandong Meng University of California, Davis Davis, USA Mengjie Wang New York University New York, USA Junliang Du Shanghai Jiao Tong University Shanghai, China Abstract-This paper addresses the problems of missing reasoning chains and insufficient entity-level semantic understanding in large language models when dealing with tasks that require structured knowledge. It proposes a fine-tuning algorithm framework based on knowledge graph injection. The method builds on pretrained language models and introduces structured graph information for auxiliary learning. A graph neural network is used to encode entities and their relations, constructing a graph-based semantic representation. A fusion mechanism is then designed to jointly model the knowledge graph embeddings with the contextual representations from the language model. To enhance the robustness of knowledge integration, a gating mechanism is introduced to dynamically balance the contributions of linguistic semantics and structural knowledge. This effectively mitigates conflicts between different representational spaces. During training, a joint loss function is constructed to account for both task performance and structural alignment objectives. This helps improve the accuracy of entity prediction and semantic reasoning. The study also includes a series of systematic sensitivity experiments. It evaluates the effects of learning rate, graph coverage, and structural perturbations on model performance. The results further validate the effectiveness and stability of the proposed method across tasks such as entity recognition, question answering, and language generation. Experimental findings show that the proposed structure-aware fine-tuning framework significantly enhances the model's ability to represent complex semantic units. It demonstrates better semantic consistency and contextual logic modeling in scenarios involving structural reasoning and entity extraction. Keywords: Knowledge injection, large language model fine- tuning, structural fusion, entity modeling I. INTRODUCTION With the rapid advancement of artificial intelligence, large language models (LLMs) have emerged as a core technology in natural language processing (NLP). These models, powered by massive datasets and complex parameter structures, demonstrate remarkable capabilities in language understanding, generation, and reasoning. However, despite their success in general-purpose tasks, LLMs still face challenges in domain- specific applications that require structured knowledge. Issues such as sparse knowledge, incomplete reasoning chains, and persistent semantic ambiguities remain unresolved[1]. These limitations largely stem from the inadequate use of explicit knowledge during model training, especially the lack of deep injection mechanisms for structured semantic information. Therefore, injecting domain knowledge into LLMs in a structured manner has become a key approach to enhancing their performance in specific application scenarios. Knowledge graphs (KGs), as high-level semantic network structures, organize abstract knowledge through entities and relations. They represent complex semantic associations using nodes and edges. Compared to traditional text-based knowledge representation, KGs provide stronger structural organization and computability. They effectively encode hierarchical concepts, attribute constraints, and subsumption relations. Integrating KGs into language models can significantly improve the models' entity recognition and contextual understanding. KGs also provide explicit logical paths that enhance the interpretability and accuracy of reasoning processes. Thus, combining KGs with LLMs is a promising direction for advancing from general semantic modeling"}
{"doc_id": "2508.14427v1", "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14427v1", "chunk_id": 1, "text": "attribute constraints, and subsumption relations. Integrating KGs into language models can significantly improve the models' entity recognition and contextual understanding. KGs also provide explicit logical paths that enhance the interpretability and accuracy of reasoning processes. Thus, combining KGs with LLMs is a promising direction for advancing from general semantic modeling to deep domain-specific understanding[2]. Nevertheless, there is an inherent heterogeneity between knowledge graphs and LLMs in terms of structure and training paradigms. LLMs rely on sequential language inputs, while KGs use non-Euclidean graph-based semantic structures. Achieving deep integration of these two forms of representation without disrupting the original architecture of LLMs remains a major research challenge. Furthermore, knowledge graphs themselves suffer from data incompleteness, entity sparsity, and structural diversity, which increase the complexity of knowledge injection. A successful injection mechanism must balance knowledge selection, encoding, and fusion strategies. It must also align the capabilities of LLMs with the complementary strengths of knowledge graphs[3,4]. In real-world applications, many high-risk and knowledge- intensive domains, such as financial analysis [5-6], medical reasoning [7-8], elastic cloud resource scaling [9-10], demand greater expertise and reasoning from language models. In these domains, models must understand complex terminology and infer causal, hierarchical, and event-based relationships between entities. Such knowledge is often unavailable in unstructured texts and must be supplemented with structured sources[11,12]. Therefore, developing a fine-tuning algorithm that dynamically guides the model's cognitive path based on KG structures and maintains semantic consistency and knowledge alignment during text generation has both practical and theoretical importance. This approach enhances model robustness and generalization in specific tasks. It also bridges the gap between knowledge representation and reasoning capabilities[13]. Moreover, as AI systems move toward being explainable, reliable, and domain-proficient, they face issues such as strong black-box characteristics and weak knowledge controllability. Traditional fine-tuning methods rely heavily on data and cannot explicitly use external knowledge. This makes it difficult to meet the safety and verifiability requirements of decision- critical scenarios[14]. Using knowledge graphs as an intermediary to guide information structures enables semantic generation and knowledge reasoning to be grounded in verifiable fact networks. This improves transparency and controllability in decision-making. Such integration complements the conventional pretraining – fine-tuning paradigm. It also provides a practical foundation and cutting- edge direction for deploying LLMs in knowledge-intensive tasks. II. PROPOSED APPROACH This study presents a fine-tuning algorithm for large language models, centered on knowledge graph injection, with the goal of strengthening both representation and reasoning capacities by leveraging structured knowledge. The overall architecture draws inspiration from Lyu et al.[15], who demonstrated that modular modeling strategies—combining a pre-trained language model with specialized guidance modules—can significantly improve performance on knowledge-intensive tasks. In this framework, the architecture comprises two primary components: the pre-trained language model, which captures contextual semantic representations, and a knowledge graph guidance module designed to complement semantic units with graph structure and facilitate relational reasoning. During training, consistent with the methodology outlined by Wang, knowledge graph information is first encoded into graph embeddings, enabling a structured representation of entities and relations. These embeddings are then fused with the latent vectors within the language model, dynamically guiding"}
{"doc_id": "2508.14427v1", "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14427v1", "chunk_id": 2, "text": "semantic units with graph structure and facilitate relational reasoning. During training, consistent with the methodology outlined by Wang, knowledge graph information is first encoded into graph embeddings, enabling a structured representation of entities and relations. These embeddings are then fused with the latent vectors within the language model, dynamically guiding semantic learning and enhancing robustness against structural noise and adversarial input [16]. To further enable complex reasoning and support context- aware understanding, this work adopts a fusion-based approach similar to that proposed by Sun et al., where multi-source knowledge representations are integrated at the semantic level for more effective context–knowledge alignment [17]. The synergy between the language model and the knowledge graph guidance module is illustrated in Figure 1, providing a basis for structured semantic learning and knowledge-augmented inference within the proposed framework. Figure 1. Overall model architecture diagram In the knowledge encoding part, the triple ) , , ( t r h is first mapped to a low-dimensional dense vector, and the knowledge graph is encoded through a graph neural network (such as GCN or R-GCN), which is formalized as follows: ) 1 ( ) ( ) ( ) ( ) 1 (    v N u l u l r uv l v h W c h  (1) Among them, ) (l v h represents the representation of node v in layer l, ) (v N is its neighbor node set, uv c is the normalization factor, ) (l r W is the weight matrix related to edge type r , and is the activation function. After multiple layers of graph convolution, the obtained knowledge representation will be injected into the language model as a structural prior. To optimize the integration of contextual semantics and structured knowledge, this framework incorporates a gating unit that dynamically regulates the fusion ratio between the language model's contextual representations and the knowledge graph-derived information. The design of this gating mechanism is influenced by the attention-based fusion strategies proposed by Xu et al., who demonstrated that adaptive gating can significantly improve the selective integration of heterogeneous information sources in complex clinical NLP scenarios [18]. By enabling the model to assign variable weights to different knowledge streams, the gating unit ensures that relevant knowledge is emphasized for each specific context. Additionally, Meng et al. provided insights into robust information fusion in distributed and federated learning settings, where dynamic gating is crucial for handling multi-source data and enhancing anomaly detection performance [19]. Drawing on Zhu’s recent advances in rapid adaptation for language models, the gating unit within this framework also supports efficient gradient-based adjustment, allowing the model to swiftly respond to changes in the structure or content of incoming knowledge graphs [20]. This dynamic gating approach helps the model achieve a more precise and context-sensitive balance between linguistic and structural knowledge, ultimately supporting more robust and explainable knowledge integration. Specifically, the context representation generated by the language model is defined as LM H , and the knowledge graph encoding is defined as KG H . The fused representation is: KG LM fused H H"}
{"doc_id": "2508.14427v1", "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14427v1", "chunk_id": 3, "text": "context-sensitive balance between linguistic and structural knowledge, ultimately supporting more robust and explainable knowledge integration. Specifically, the context representation generated by the language model is defined as LM H , and the knowledge graph encoding is defined as KG H . The fused representation is: KG LM fused H H H      ) 1 (   (2) Among them, ) ] ; [ ( g KG LM g b H H W    is the gating factor controlled by learnable parameters, ] ; [   which represents the vector concatenation operation, and  is the Sigmoid function, which is used to ensure that the fusion weight is within the ] 1 ; 0 [ interval. In the fine-tuning stage, a joint loss function is applied to ensure the fused representations are well-optimized for downstream tasks. This loss function combines the main task loss—such as language modeling loss—with an additional term designed to align the model’s outputs with the structured knowledge provided by the graph. Qin’s work on hierarchical semantic-structural encoding demonstrates the effectiveness of using such joint objectives, as this approach helps the model maintain both semantic consistency and accurate structural alignment throughout the learning process [21]. To make the alignment loss more effective for different task scenarios, this study incorporates principles from Wu, who emphasizes task- aware structural regularization to facilitate parameter-efficient fine-tuning and to adapt flexibly to diverse downstream requirements [22]. Moreover, drawing on the distributed training strategies highlighted by Zhang et al., the joint loss also supports robust and stable optimization, even when dealing with data from varied sources or domains [23]. This approach allows the model to preserve both linguistic accuracy and structural integrity, as formalized below: align task total L L L     (4) Among them, task L represents the loss of tasks such as language modeling or question answering    i i KG i LM align H H L 2 ) ( ) ( || || represents the Euclidean distance alignment term between language representation and knowledge representation, and  is a hyperparameter that controls the balance of the two types of information during training. To further improve the model's ability to model graph structure relationships, a knowledge-aware attention mechanism is constructed to emphasize the semantic dependencies between entities. For the i-th word in the context, its final representation is: j j j T i j i Vh d Kh Qh z    ) ) ( ) ( ( softmax (5) Among them,  is the entity set related to word i, V K Q , , is the linear transformation matrix of query, key, and value, respectively, and d is the scaling factor. This mechanism explicitly introduces entity nodes in the graph, so that the language model can perceive and rely on the known semantic edges in the graph structure during context processing. Through the above modeling method, the proposed algorithm can achieve deep collaboration between the language model and the knowledge graph at the structural level, effectively enhance the"}
{"doc_id": "2508.14427v1", "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14427v1", "chunk_id": 4, "text": "the graph, so that the language model can perceive and rely on the known semantic edges in the graph structure during context processing. Through the above modeling method, the proposed algorithm can achieve deep collaboration between the language model and the knowledge graph at the structural level, effectively enhance the model's ability to represent entity semantics, contextual relationships, and reasoning paths, and provide basic support for language understanding and generation in knowledge-intensive tasks. III. DATASET This study uses the T-REx (Textual Relations Extraction) knowledge graph dataset as the primary source of structured knowledge. T-REx is a large-scale collection of knowledge triples built from Wikipedia. It contains millions of fact triples in the form of entity-relation-entity, covering a wide range of general-domain concepts. Each triple is aligned with multiple natural language sentences, which express the fact either explicitly or implicitly. This alignment provides valuable support for linking knowledge with language. The dataset offers high-quality entity annotations and well- organized semantic relations. Its structured representation includes rich hypernym–hyponym links, functional relations, and synonym mappings. These characteristics make it suitable for graph neural network modeling. T-REx also provides aligned sentence pairs for fine-tuning language models. These pairs help integrate contextual representations of entities with the structure of the graph, supporting the learning of factual expressions and logical connections from text. To improve the relevance of knowledge injection and the adaptability to specific tasks, this study filters domain-specific subsets and prunes the graph structure based on T-REx. Semantic-dense and structurally clear subgraphs are preserved. Entity subgraphs are then constructed for graph neural network modeling. This processing enables the model to receive structured knowledge enhancement signals while preserving semantic completeness. As a result, the model improves its ability to capture semantic dependencies. IV. PERFORMANCE EVALUATION This paper first conducts a comparative experiment, and the experimental results are shown in Table 1. Table1. Comparative experimental results Model QA-Acc F1-Score BLEU KGLM[24] 78.6% 74.2% 21.5% DRAGON[25] 81.3% 76.8% 24.1% KG-SFT[26] 83.7% 78.9% 26.5% Ours 86.4% 82.1% 29.7% As shown in Table 1, the proposed method achieves consistent improvements across key metrics with the integration of knowledge graph injection. The QA-Acc reaches 86.4%, nearly eight points higher than KGLM, confirming the benefit of explicit knowledge modeling for entity relations and semantic alignment. The method also yields an F1-Score of 82.1%, outperforming models such as DRAGON and KG-SFT, which indicates stronger robustness in entity discrimination and relation reconstruction. In addition, the BLEU score rises to 29.7%, evidencing improved content consistency and contextual coherence through structured knowledge guidance. These results collectively show that knowledge graph-based fine-tuning enhances factual reasoning and semantic reliability, thereby mitigating limitations of pretrained models in knowledge-intensive tasks. The influence of different learning rate settings on performance is further illustrated in Figure 2. Figure 2. The impact of different learning rate settings on model performance As shown in Figure 2, the performance of the proposed model is significantly affected by different learning rate settings. Under smaller learning rates such as 1e-5 and 5e-5, the model shows stable and high performance on both QA-Acc and F1-Score, maintaining around 85% and"}
{"doc_id": "2508.14427v1", "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14427v1", "chunk_id": 5, "text": "learning rate settings on model performance As shown in Figure 2, the performance of the proposed model is significantly affected by different learning rate settings. Under smaller learning rates such as 1e-5 and 5e-5, the model shows stable and high performance on both QA-Acc and F1-Score, maintaining around 85% and 80% respectively. This indicates that slower parameter updates help achieve stable integration of structured knowledge. It also avoids fluctuations during the injection process, highlighting the sensitivity of knowledge-aware fine-tuning to learning rate settings. When the learning rate increases to 1e-4, the model reaches peak performance across all metrics. This suggests that the model achieves an optimal balance between semantic modeling and structural guidance. The result indicates that in the context of graph-based knowledge injection, a moderate learning rate helps the language model effectively integrate knowledge graph information while maintaining coherence and accuracy in language modeling. However, when the learning rate is further increased to 5e-4 and 1e-3, the model's performance drops significantly. F1 scores decline rapidly. This may be related to instability in the structured knowledge injection process. Faster gradient updates hinder the fusion module from learning long-range dependencies from the knowledge graph. As a result, the model loses logical consistency and entity alignment in generation and understanding tasks. Overall, the experimental results suggest that the structured and controllable knowledge injection mechanism is highly sensitive to the learning rate. A well-chosen learning rate can promote collaborative modeling between the language model and the graph semantics. It can also improve the model's robustness and generalization in complex language understanding tasks. This provides important insights for building structure-aware large models. This paper also gives the impact of graph subgraph coverage on entity prediction accuracy, and the experimental results are shown in Figure 3. Figure 3. The impact of graph subgraph coverage on entity prediction accuracy As shown in Figure 3, entity prediction accuracy steadily increases as the subgraph coverage rate rises, showing a clear positive correlation. When the subgraph coverage is low, such as 10% or 30%, the model's ability to recognize entities is limited. The prediction accuracy remains between 71.2% and 75.8%. This suggests that with insufficient structural knowledge, the language model struggles to capture entity relations and semantic dependencies in context. When the coverage rate reaches 50% to 70%, the accuracy improves rapidly to over 80%. This indicates that enhanced graph structure information significantly improves semantic mapping and logical modeling between entities. In this coverage range, the model gains more hypernyms, synonyms, and functional relation data. The graph context begins to play a compensatory role, enhancing the completeness of reasoning paths and the ability to distinguish entity boundaries. As the coverage increases to 90% and above, the accuracy gain becomes marginal, approaching a ceiling of 86.4%. This convergence effect suggests that at high coverage levels, the graph provides nearly complete structural context. The model no longer relies on redundant knowledge and consistently generates high-quality predictions. This reflects the saturation point of structural guidance in improving representation quality. V. CONCLUSION This paper addresses the challenges faced by large language models in knowledge-intensive tasks,"}
{"doc_id": "2508.14427v1", "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14427v1", "chunk_id": 6, "text": "high coverage levels, the graph provides nearly complete structural context. The model no longer relies on redundant knowledge and consistently generates high-quality predictions. This reflects the saturation point of structural guidance in improving representation quality. V. CONCLUSION This paper addresses the challenges faced by large language models in knowledge-intensive tasks, particularly the lack of structural information and weak reasoning capability. It proposes a fine-tuning method based on knowledge graph injection. By introducing structured knowledge, the method enhances pretrained models in a targeted manner, improving knowledge completion and logical reasoning during language understanding, entity recognition, and contextual modeling. The architecture integrates a graph neural network to encode the knowledge graph and fuses it with the hidden representations of the language model. This builds a knowledge-aware semantic representation path that promotes structure-controllable and semantically consistent outputs in language generation and reasoning. Experimental results show that the proposed method achieves superior performance across multiple key metrics. It performs particularly well in entity prediction, question answering accuracy, and semantic generation quality. The model demonstrates strong adaptability and stability concerning knowledge injection. Systematic sensitivity experiments further validate how the model responds to knowledge sparsity, graph structure perturbations, and hyperparameter changes. These findings provide quantifiable guidance for model deployment and tuning. The results indicate that incorporating structured knowledge during fine- tuning not only enhances the expression of explicit knowledge but also pushes semantic modeling toward structure-aware mechanisms. In broader application contexts, the proposed fine-tuning paradigm holds significant value for domains such as financial analysis, medical document processing, and legal text understanding. By integrating entities, relations, and logical chains from knowledge graphs, language models gain the ability to decompose, generalize, and reason over complex sentences. This enhances the interpretability, controllability, and generalization of models in specialized and high-risk domains. It also addresses the problem of discontinuous and inconsistent knowledge arising from memory-based reasoning in traditional models, offering structural support for intelligent systems in complex tasks. Future research may explore mechanisms for dynamically injecting knowledge graphs, allowing language models to adaptively select the most relevant subgraphs based on task context. Another important direction is to build structure-aware fusion paths across modalities and achieve efficient knowledge transfer in low-resource domains. Overall, the structure- injection fine-tuning strategy presented in this paper provides a foundation for building trustworthy, explainable, and domain- proficient language models. It also opens new perspectives for developing knowledge-enhanced intelligent language systems."}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 0, "text": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs Yinghan Zhou1, Weifeng Zhu1, Juan Wen1, Wanli Peng1, Zhengxian Wu1, Yiming Xue1, 1College of Information and Electrical Engineering, China Agricultural University zhouyh@cau.edu.cn, y146776149@gmail.com, wenjuan@cau.edu.cn, wlpeng@cau.edu.cn, wzxian@cau.edu.cn, xueym@cau.edu.cn Abstract Large language models (LLMs) have been shown to possess a degree of self-recognition capability—the ability to identify whether a given text was generated by themselves. Prior work has demonstrated that this capability is reliably expressed un- der the Pair Presentation Paradigm (PPP), where the model is presented with two texts and asked to choose which one it authored. However, performance deteriorates sharply under the Individual Presentation Paradigm (IPP), where the model is given a single text to judge authorship. Although this phe- nomenon has been observed, its underlying causes have not been systematically analyzed. In this paper, we first repli- cate existing findings to confirm that LLMs struggle to dis- tinguish self- from other-generated text under IPP. We then investigate the reasons for this failure and attribute it to a phe- nomenon we term Implicit Territorial Awareness (ITA)—the model’s latent ability to distinguish self- and other-texts in representational space, which remains unexpressed in its out- put behavior. To awaken the ITA of LLMs, we propose Cog- nitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construc- tion, authorship discrimination and cognitive editing. Exper- imental results demonstrate that our proposed method im- proves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively. code — https://github.com/CAU-ISS-Lab/Interpretability- of-LLMs/tree/main/CoSur 1 Introduction It has recently been found that large language models (LLMs) possess self-recognition capability, enabling them to distinguish their own writing (”self text”) from that of humans and other models (”other text”) (Panickssery, Bow- man, and Feng 2024; Ackerman and Panickssery 2025). As shown in Figure 1, this capability is particularly evident in the Pair Presentation Paradigm (PPP). In this paradigm, the model is shown two texts, one of which has been generated by the model being tested and the other of which has been generated by a human or another model. Then, the model is asked to output the label of the text it thinks it wrote. How- ever, this self-recognition capability encounters a bottleneck when faced with the Individual Presentation Paradigm (IPP), where the LLM is shown a single text and asked to indicate Text: Was this text written by yourself? (a) Yes (b) no Individual Presentation Paradigm Text1: Text2: Can you tell me which text you wrote? Please answer with only “1” or “2” and no other text. Pair Presentation Paradigm LLM Wrong Answers Correct Answers LLM Figure 1: The prompt diagrams for the pair presentation paradigm and individual presentation paradigm. whether the text was generated by itself. In the IPP scenario, accuracy is computed by capturing the probabilities assigned to the ”Yes” and ”No” tokens, which serves as the primary evaluation metric. As reported in Table 1 of Ackerman and Panickssery (2025), the base LLM achieves prediction accu- racies below 50.3% across four datasets, indicating worse- than-random performance. Despite these"}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 1, "text": "scenario, accuracy is computed by capturing the probabilities assigned to the ”Yes” and ”No” tokens, which serves as the primary evaluation metric. As reported in Table 1 of Ackerman and Panickssery (2025), the base LLM achieves prediction accu- racies below 50.3% across four datasets, indicating worse- than-random performance. Despite these observations, prior studies have not conducted an in-depth analysis of this fail- ure mode. Ackerman and Panickssery (2025) attempted to address this by extracting a self-recognition vector. They demonstrated that steering the model with this vector in the positive direction makes it claim authorship, while steering it in the negative direction makes it deny authorship, regard- less of true authorship. This approach essentially does not enhance the self-recognition capability of LLMs. This raises an interesting question: Why do LLMs, despite possessing self-recognition capability in paired settings, fail in the IPP scenario? In this paper, we first empirically verify the existence of self-recognition capability in LLMs. Then, we analyze the reasons behind the failure of this capability in the IPP sce- nario, which is regarded as Implicit Territorial Awareness (ITA) of LLMs. The ITA refers to the awareness of the LLM to clearly distinguish its own text from that authored by oth- ers in the feature space, yet fail to reflect this distinction in its responses. Building on this, we further raise a new question: Is it possible to enhance the performance of LLMs in the IPP scenario by awakening ITA? To explore this, we conduct an in-depth analysis of the feature distributions across different categories of texts. Interestingly, we find that although these feature distributions are very similar, the internal structure of these features differs significantly across sources. This ob- servation suggests that it is possible to identify and extract subspaces where the features exhibit greater differences. In- spired by representation editing (Kong et al. 2024a; Wang et al. 2024a), the model’s output can be aligned with the de- sired attributes by directly manipulating its latent representa- tions. Based on this, we propose a novel method named Cog- nitive Surgery (CoSur) to awaken ITA, thereby enhancing the self-recoginition capability of LLMs in IPP scenarios. The CoSur consists of four modules: representation extrac- tion, territory construction, authorship discrimination and cognitive editing. Specifically, we first extract the feature representation from the final layer of the LLM to construct the feature subspaces (territories) for each text category us- ing singular value decomposition (SVD). Then, we guide the LLM to determine the authorship of a text by calculating the projection energy of its feature vector onto different territo- rial regions. Finally, we design the cognitive editing to in- duce the LLM to generate the correct response. Extensive experiments on three LLMs demonstrate that CoSur effec- tively enhances their self-recognition capability in the IPP scenario. Our contributions are as follows: • We provide the first in-depth analysis of why LLMs fail in the IPP setting, attributing it to unexpressed Implicit Territorial Awareness (ITA). • We propose Cognitive Surgery (CoSur), a method that awakens ITA to enhance the self-recognition capability of LLMs in the IPP scenario. We first"}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 2, "text": "are as follows: • We provide the first in-depth analysis of why LLMs fail in the IPP setting, attributing it to unexpressed Implicit Territorial Awareness (ITA). • We propose Cognitive Surgery (CoSur), a method that awakens ITA to enhance the self-recognition capability of LLMs in the IPP scenario. We first identify and con- struct territorial spaces to assist the LLM in determining the authorship of the text. Then, we design cognitive edit- ing to induce the LLM in generating correct responses. • Experiments demonstrate that our proposed method en- hances the performance across three different LLMs in the IPP scenario, achieving average accuracy of 83.25%, 66.19%, and 88.01%, respectively. 2 Related Work Self-Recognition Capability of LLMs The self-recognition capability of LLM refers to their ca- pacity to identify texts they have generated (Laine et al. 2024b; Laine, Meinke, and Evans 2023; Wang et al. 2024b; Cotra 2021). Panickssery et al. (2024) reported that several LLMs, including Llama2-7b-chat, demonstrated out-of-the- box (without fine-tuning) self-recognition capabilities us- ing a summary writing and recognition task. Laine et al. (2024a) used more challenging text continuation and recog- nition tasks to demonstrate self-recognition abilities in larger language models. It highlighted how task success could be elicited with different prompts and across different models. Ackerman et al. (2025) found that the llama3-8b-Instruct succeeded at self-recognition across diverse tasks, whereas the base model performed poorly, especially in the IPP sce- nario. They also extracted a ”self-recognition” vector in the residual stream, allowing users to steer the LLM to claim or disclaim authorship during generation and to believe or disbelieve that it had written arbitrary texts when reading them. These studies demonstrated that LLMs possess self- recognition capability. Representation Editing Representation editing is a class of techniques that directly manipulate the latent representations of a model to improve its performance and align it with desired attributes (Kong et al. 2024b; Wu et al. 2024a). Liang et al. (2024) found that representation editing could control aspects of text gen- eration, such as safety, sentiment, thematic consistency, and linguistic style. Adila et al. (2024) used embedding editing for general, rather than personalized, alignment to broad hu- man preferences, relying on self-generated synthetic data. Wu et al. (2024b) showed that the representation editing can even surpass fine-tuning based methods by intervening on hidden representations within the linear subspace defined by a low-rank projection matrix. Ma et al. (2025) leveraged the over-parameterized nature of LLMs to disentangle a style- relevant subspace within the model’s representation space to conduct representation editing, ensuring a minimal im- pact on the original semantics. By applying adaptive editing strengths, they dynamically adjusted the steering vectors in the style subspace to maintain both stylistic fidelity and se- mantic integrity. Inspired by representation editing, as long as the true authorship of the text is determined, we can di- rectly manipulate the LLM’s hidden representations to gen- erate the correct response. Based on this, we propose CoSur, the details of which will be introduced in the section 4. 3 The Implicit Territorial Awareness of LLMs We believe the LLM demonstrates self-recognition abilities"}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 3, "text": "the text is determined, we can di- rectly manipulate the LLM’s hidden representations to gen- erate the correct response. Based on this, we propose CoSur, the details of which will be introduced in the section 4. 3 The Implicit Territorial Awareness of LLMs We believe the LLM demonstrates self-recognition abilities because it exhibits Territorial Awareness (TA) (Wang et al. 2022), i.e., when responding to authorship attribution ques- tions, the LLM tends to emphasize its own characteristics while reducing its sensitivity to differences between texts generated by others. However, in the IPP scenario, when the LLM is asked about the authorship of a single text, this ter- ritorial awareness is not reflected in its response. Therefore, we regard this phenomenon as Implicit Territorial Aware- ness (ITA). We argue that the diminished TA in the IPP scenario stems from information loss during the mapping from the feature space to the discrete vocabulary space in the LLM. Formally, let h ∈Rd denote the hidden state vector from the last layer of the LLM. The probability distribution P over the vocab- ulary is obtained via a linear transformation followed by a softmax function: P = softmax(Wh + b), (1) where softmax(·) represents the softmax function, W ∈ R|V |×d is the output projection matrix, b ∈R|V | is the bias term, and |V | is the vocabulary size. This projection maps the high-dimensional semantic space Rd to the (|V | −1)- dimensional probability simplex ∆|V |−1. However, due to the non-invertibility of the softmax function and the limited Figure 2: Visualization of the layer-wise JS divergence between different text categories using Qwen. The left column shows the JS divergence between the features extracted by directly inputting the text into Qwen, while the right column represents the JS divergence between the features extracted under the IPP scenario. Different colors represent different submodules in Qwen, including the MLP layer (MLP), attention layer (Attn), and the final output of each layer (Final). “self” represents texts generated by Qwen. rank of W, this transformation inevitably introduces infor- mation loss. Additionally, we analyze the mutual informa- tion I(h; y) between the hidden state h and the C y and the mutual information I(P; y) between the probability distri- bution of output tokens P and y. Due to the low-rank projec- tion and entropy-reduction effects of the softmax layer, the two mutual information terms satisfy I(P; y) ≪I(h; y). This prevents the LLM from exhibiting explicit TA in its outputs, resulting in ITA. Our findings align with the infor- mation bottleneck theory (Tishby, Pereira, and Bialek 2000), where P acts as a compressed representation that inade- quately preserves task-relevant information about the correct output token y. We empirically validate the above findings through an in- depth analysis of the features from different categories of texts. Specifically, We collect 400 texts each from three cat- egories: (1) Qwen-generated texts, (2) Human-written texts, (3) Chatgpt-generated text. These texts are then fed into Qwen3-8B (Qwen) (2025), and we extract the feature rep- resentations from each layer for further analysis. Firstly, we map these feature representations"}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 4, "text": "categories of texts. Specifically, We collect 400 texts each from three cat- egories: (1) Qwen-generated texts, (2) Human-written texts, (3) Chatgpt-generated text. These texts are then fed into Qwen3-8B (Qwen) (2025), and we extract the feature rep- resentations from each layer for further analysis. Firstly, we map these feature representations to the vo- cabulary space to obtain output probability distributions, from which we calculate the Jensen-Shannon (JS) diver- gence (Men´ endez et al. 1997) between the different distri- butions. As shown in Figure 2(a) and 2(c), a statistically sig- nificant separation exists between the distributions of differ- ent text categories. Furthermore, the T-sne visualization in Figure 3(a) and 3(b) demonstrate that the resulting clusters exhibit low overlap and clear separation, suggesting that the LLM demonstrates a distinct awareness in differentiating its own text from others. Additionally, as shown in Figure 2(e) and 3(c), we observe that the LLM can also distinguish be- tween different categories of others-generated texts. Secondly, we extract text features in the IPP scenario to validate the LLM’s territorial awareness. As shown in the bottom row of Figure 3, when the LLM is prompted under the IPP scenario, the boundary between self-generated and other generated texts becomes more distinct. In contrast, the boundary between human-texts and chatgpt-texts becomes blurred. This shift in representational distinctiveness indi- cates that the LLM possesses territorial awareness. Despite the fact that the above experiments show the LLM exhibits strong territorial awareness in the IPP scenario, the JS divergence sharply declines under the IPP both be- tween self and other and between different types of oth- ers as illustrated in the right column Figure 2. This val- idates the information loss during the mapping from fea- tures to probability distributions. The information loss pre- vents the explicit expression of TA, resulting in the observed Implicit Territorial Awareness (ITA), which aligns with our previous analysis. Results on other LLMs, including Llama- 3.1-8B (Llama) (2024), and DeepSeek-R1-0528-Qwen3-8B (DeepSeek) (2025), can be found in Appendix A. Based on the above analysis, we argue that actively mit- igating information loss during the mapping from the fea- ture space to the discrete vocabulary space can improve Figure 3: T-sne Visualization of Final Layer Features using Qwen. The top row shows the feature visualizations based on plain-text inputs, while the bottom row with the suffix ’ IPP’ represents the features extracted under the IPP scenario. the LLM’s self-recognition capability in the IPP scenario. Therefore, we propose CoSur, the details of which will be introduced in the following sections. 4 CoSur To activate ITA of LLM in IPP scenario, we propose CoSur, which consists of four modules: representation extraction, territory construction, authorship discrimination, and cogni- tive editing. The framework of CoSur is shown in Figure 4. Representation Extraction Let Ts = {ts,1, ts,2, . . . , ts,N} denotes a set of texts gen- erated by the LLM itself, and To = {to,1, to,2, . . . , to,N} represents a set of texts from other source. Under the IPP scenario, each text ts,i ∈Ts and to,i ∈To is indepen- dently fed"}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 5, "text": "ts,2, . . . , ts,N} denotes a set of texts gen- erated by the LLM itself, and To = {to,1, to,2, . . . , to,N} represents a set of texts from other source. Under the IPP scenario, each text ts,i ∈Ts and to,i ∈To is indepen- dently fed into the LLM to extract the hidden representation of its last token from the final layer of the LLM, denoted as hs,i ∈Rd and ho,i ∈Rd, respectively: hs,i = LLM(ts,i), ho,i = LLM(to,i) (2) The representations for each category are stacked to form the sets Hs = {hs,1, hs,2, . . . , hs,N} and Ho = {ho,1, ho,2, . . . , ho,N}, respectively. CS MMD CKA Self-human 0.9434 0.01 0.0717 self-chatgpt 0.9755 0.01 0.0685 self-llama 0.9768 0.01 0.0808 self-deepseek 0.9793 0.01 0.0825 Table 1: The distance between feature representations of dif- ferent categories of texts in Qwen. ‘A–B’ denotes the dis- tance between category A and category B. Territory Construction We believe that due to the ITA of LLMs, there are differ- ences between self-generated texts and other-generated texts in the LLM’s feature space. Therefore, we select 400 sam- ples from each category and analyze the differences in fea- ture distributions, extracted from Qwen, between different categories of texts using Cosine Similarity (CS), Maximum Mean Discrepancy (MMD), and Centered Kernel Alignment (CKA). The definitions of these metrics and their measure- ment results on other LLMs are provided in Appendix B. As shown in Table 1, these features exhibit consistently high CS and low MMD, indicating that they are highly sim- ilar in feature space. This also explains why the LLM per- forms poorly on IPP scenario. However, the CKA scores be- ��= ��∑��� ��= ��∑� �� Step 2: Territory Construction Step 4: Cognitive Editing Other Self Projection diagram � Edit � Step 3: Authorship Discrimination �= � + �∗������� Editing direction { Text } Was this text written by yourself? \\n a) yes \\n b) no Step 1: Representation Extraction ��= {��,�, . . . , ��,�} Large Language Model ��= {��,�, . . . , ��,�} �� �� � ��= �� ⊺� 2 ��= �� ⊺� 2 �� �� Other Text Self Text ��,�� ��,�� ��,�� ��,�� �� �� Top-� Figure 4: The framework of CoSur. (The preliminary stage) Step 1 (Representation Extraction): Extract the final-layer rep- resentations of self-generated and other-generated texts from the LLM, denoted as Hs and Ho, respectively. Step 2 (Territory Extraction): SVD is applied to Hs and Ho to construct the territory space for each text category, denoted as Vs and Vs, respectively. (Inference stage) Step 3 (Authorship Discrimination): For a given sample t, compute its projection energy onto Vs and Vs to infer the authorship of t. Step 4 (Cognitive Editing): Edit the feature representation h to approach the target response, denoted as ˜ h, thereby promoting the LLM to generate the correct reply. tween these representation sets, which evaluate the structural similarity in the feature space by calculating the similarity between the Gram matrices of two feature matrices, are no- tably low."}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 6, "text": "feature representation h to approach the target response, denoted as ˜ h, thereby promoting the LLM to generate the correct reply. tween these representation sets, which evaluate the structural similarity in the feature space by calculating the similarity between the Gram matrices of two feature matrices, are no- tably low. This reveals a different perspective: although the features are close in space, the internal structure of the rep- resentations differs significantly across sources. This obser- vation suggests that it is possible to identify and extract sub- spaces where the representations exhibit greater differences, thereby enhancing the distinction between text categories. Based on this observation, we apply singular value de- composition (SVD) to extract the most discriminative com- ponents between categories. Hs = UsΣsVs ⊤, Ho = UoΣoVo ⊤ (3) where Us ∈RN×N and Uo ∈RN×N are the left singular matrices, Σs ∈RN×d and Σo ∈RN×d are diagonal ma- trices containing the singular values and V⊤ s ∈Rd×d and V⊤ o ∈Rd×d are the right singular matrices. We extract the top-k right singular vectors from V⊤ s and V⊤ o to serve as the bases defining their respective territories Vs and Vo. Vs = span(Vs (k)), Vo = span(Vo (k)) (4) where span(·) represents the set of all possible linear com- binations of the vectors contained in the input set, forming a vector space. Vs (k) ∈Rd×k and Vo (k) ∈Rd×k denote the matrices consisting of the top-k right singular vectors. We conduct experiments to examine the impact of the choice of k on the results, as detailed in section 5. self-human self-chatgpt self-deepseek NGD 0.8020 0.7035 0.6594 NFD 0.9055 0.8371 0.7931 Table 2: Measurement of distance between different terri- tories. ‘A–B’ denotes the distributional divergence between category A and category B. To evaluate the representational capacity and separabil- ity of these subspaces, we compute the Normalized Grass- mann Distance (NGD) and Normalized Frobenius Distance (NFD) between them. As shown in Table 2, the NGD and NFG between these subspaces are large, indicating signifi- cant divergence between their corresponding subspaces. The definitions of the metrics and their measurement results on other LLMs are provided in Appendix C. Authorship Discrimination We introduce projection energy E to quantify the intensity of text representations projected onto each territory, which serves to determine the authorship of a given text. For a given sample t, the last token feature vector h from the fi- nal layer is extracted and its projection energy Es and Eo onto Vs and Vo are computed to infer the authorship of t. Es = ∥Vs ⊤h∥2, Eo = ∥Vo ⊤h∥2 (5) where ∥· ∥2 denotes the Euclidean norm. self - human self - chatgpt self - qwen self - llama self - deepseek Average ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 Qwen-base 46.15 40.25 32.87 31.79 - - 31.88 29.17 46.67 32.78 39.40 33.50 Qwen-CoSur 99.50 99.50 94.25 94.23 - - 65.74 61.41 73.50 71.50 83.25 81.66 Llama-base 44.99 43.69 50.00 49.06 49.73 48.79 - - 50.00 48.66 48.68 47.55 Llama-CoSur 67.25 64.69 76.06 72.44 63.48 58.12 -"}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 7, "text": "F1 ACC F1 Qwen-base 46.15 40.25 32.87 31.79 - - 31.88 29.17 46.67 32.78 39.40 33.50 Qwen-CoSur 99.50 99.50 94.25 94.23 - - 65.74 61.41 73.50 71.50 83.25 81.66 Llama-base 44.99 43.69 50.00 49.06 49.73 48.79 - - 50.00 48.66 48.68 47.55 Llama-CoSur 67.25 64.69 76.06 72.44 63.48 58.12 - - 59.95 52.64 66.19 61.97 Deepseek-base 63.64 56.60 38.17 27.63 45.39 34.72 47.97 37.25 - - 48.79 39.05 Deepseek-CoSur 98.25 98.25 95.00 94.99 86.25 86.20 72.54 70.44 - - 88.01 87.47 Table 3: Performance of three LLMs in IPP Scenarios. The ’self-A’ setting refers to evaluating the performance of the target LLM in the IPP scenario using a dataset that includes both its self-generated texts and texts generated by A. Finally, the authorship of the text t is determined by com- paring its projection energies Es and Eo onto the respective territories. O(t) = \u001a s, if Es > Eo , o, otherwise (6) where O(t) represents the authorship of t. Cognitive Editing To guide the LLM toward producing the desired response, we first identify the target tokens toks and toko. The output- layer weight vectors corresponding to the two target tokens, denoted as ws and wo, respectively, are obtained from the LLM. We then normalize the two weight vectors as follows: ˜ wo = wo ∥wo∥, ˜ ws = ws ∥ws∥ (7) The target direction dtarget is determined according to the value of O(t): dtarget = \u001a ˜ wo, if O(t) = o , ˜ ws, if O(t) = s (8) The hidden representation h is steered toward the target direction to obtain the edited representation ˜ h, thereby facil- itating the LLM to output the target token. ˜ h = h + α · dtarget (9) where α represents editing strength hyperparameter. We also conduct experiments to examine the impact of the choice of α on the results, as detailed in section 5. The complete algorithmic procedure is detailed in the Appendix D. 5 Experiments Dataset We incorpate HC3 (Guo et al. 2023), a large-scale com- parison dataset comprising responses from both human ex- perts and chatgpt across diverse domains, including open- domain, finance, medicine, law, and psychology. Addition- ally, we use the same set of questions as prompts to generate responses from Qwen, Llama and DeepSeek. Figure 5: Visualization of the average accuracy before (Base) and after (CoSur) applying Cosur when each LLM distinguishes between its own outputs and those of two dif- ferent external LLMs. Experimental Setting We evaluate the performance of CoSur on three differ- ent LLMs, including Qwen3-8B (Qwen) (2025), Llama- 3.1-8B (Llama) (2024), and DeepSeek-R1-0528-Qwen3-8B (Deepseek) (2025). We select the top-64 right-singular vec- tors to construct the subspace (k = 64) and set the edit- ing strength α = 100. All experiments are run on two NVIDIA A100-SXM4-80GB GPUs. Additioanally, we use stricter evaluation metrics, where accuracy (ACC) and F1 score (F1) are computed based solely on the output of LLMs, rather than the probability of the target token. Since cur- rent research lacks targeted methods to enhance the self- recognition capability of LLMs, in"}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 8, "text": "two NVIDIA A100-SXM4-80GB GPUs. Additioanally, we use stricter evaluation metrics, where accuracy (ACC) and F1 score (F1) are computed based solely on the output of LLMs, rather than the probability of the target token. Since cur- rent research lacks targeted methods to enhance the self- recognition capability of LLMs, in this experiment, we use the LLM’s direct performance in the IPP scenario (base) as the baseline to evaluate the performance of CoSur. Main Results We evaluate the effectiveness of CoSur on three different mainstream LLMs. As shown in Table 3, all three LLMs per- form below 50% in the base setting (base) without CoSur on the IPP scenario. After applying CoSur, each LLM demon- strates a significant improvement. Specifically, across four tasks, the three LLMs achieve average accuracies of 83.25%, Average ACC Average F1 CoSurCS 0.8043 0.7741 CoSurP CA 0.75.70 0.7389 CoSur 0.8325 0.8166 Table 4: Ablation study using Qwen. Figure 6: Accuracy variation with different territorial dimen- sions using Qwen. 66.19%, and 88.01%, with corresponding improvements of 43.85%, 17.51%, and 39.22%. These results suggest that CoSur is effective in mitigating the implicit territorial aware- ness exhibited by LLMs. However, its improvements are less pronounced on Llama compared to Qwen and DeepSeek. As shown in Appendices A and C, the feature space of LLaMA exhibits inherently lower inter-class separability, indicating a limited capacity for native self-recognition. This constraint reduces the effectiveness of subsequent editing operations. We present a case study comparing the top-10 tokens and their logit changes before and after editing, with detailed re- sults provided in the Appendix E. Generalization Results We evaluate the generalization of CoSur through the sce- nario below: Given the territories of its own and chatgpt texts, assess the LLM’s ability to distinguish between its own text and that of other LLMs. As shown in Figure 5, the LLM can determine the authorship of unseen text generated by other LLMs based on the known territories (i.e., whether it recognizes the text as its own or from another model). We argue that this is due to the LLM reinforcing its own ter- ritorial boundaries while reducing its sensitivity to external boundaries during the IPP scenario. This observation is con- sistent with the previous analysis. Ablation Study The key to CoSur’s performance lies in the construction of the territory and the authorship discrimination based on projection energy. To evaluate the effectiveness of CoSur, Figure 7: Average accuracy variation with different editing Strength α using Qwen. we design two variants: (1) CS-based Authorship Iden- tification: For a given test sample, its authorship is deter- mined by computing the cosine similarity between the sam- ple and each class center. (2) PCA-based Territory Con- struction: Territories are constructed using Principal Com- ponent Analysis (PCA), and authorship is determined based on the projection energy onto the respective territories. As shown in Table 4, CoSur outperforms the other two variants. We attribute the relatively poor performance of the cosine similarity-based variant to the inability of the feature mean to capture the critical characteristics that distinguish differ- ent texts. Additionally, we argue that while"}
{"doc_id": "2508.14408v1", "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14408v1", "chunk_id": 9, "text": "projection energy onto the respective territories. As shown in Table 4, CoSur outperforms the other two variants. We attribute the relatively poor performance of the cosine similarity-based variant to the inability of the feature mean to capture the critical characteristics that distinguish differ- ent texts. Additionally, we argue that while PCA focuses on maximizing variance when selecting principal components, SVD directly leverages the magnitude of singular values to capture the most important features of the data. Therefore, using SVD to construct the territory is more effective in cap- turing the core structure and distinguishing features. We explore the impact of territorial dimensions k on the self-recognition capability of LLMs in the IPP scenario us- ing Qwen. A small k fails to capture all the distinguishing features, especially for tasks like self vs ChatGPT and self vs human. Conversely, most categories show accuracy de- clines after feature saturation due to the introduction of ir- relevant features. Overall, Qwen achieve best average per- forms across the four different tasks when k = 64. There- fore, this value is adopted in our experiments. We also inves- tigate the effect of editing strength α on CoSur using Qwen. As demonstrated in Figure 7, when the editing strength ex- ceeds 100, the average recognition accuracy across the four different text combinations in IPP scenarios remains nearly unchanged. This suggests this threshold provides sufficient guidance for the LLM to generate target-aligned responses. We further investigated the performance of CoSur on a re- lated task, AI-generated text detection, as detailed in Ap- pendix F. 6 Conclusion In Conclusion, we empirically verify the existence of the self-recognition capability of LLMs and analyze the rea- sons behind the failure of this capability in the IPP scenario, which is regarded as the Implicit Territorial Awareness (ITA) of LLMs. Base on this, we propose a novel method named Cognitive Surgery (CoSur) to awaken the ITA in LLMs, thereby enhancing LLMs’ performance in the IPP scenario. Experimental results demonstrate that CoSur significantly enhances LLMs’ performance in IPP scenario."}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 0, "text": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement Yupei Yang1,4∗ , Fan Feng2,3, Lin Yang4, Wanxi Deng4, Lin Qu4 Biwei Huang2, Shikui Tu1† , Lei Xu1 1Shanghai Jiao Tong University 2University of California San Diego 3Mohamed bin Zayed University of Artificial Intelligence 4Alibaba Group {yupei_yang,tushikui,leixu}@sjtu.edu.cn, bih007@ucsd.edu ffeng1017@gmail.com, zhihe.yyp@alibaba-inc.com Abstract Relation extraction enables the construction of structured knowledge for many downstream applications. While large language models (LLMs) have shown great promise in this domain, most existing methods concentrate on relation classification, which predicts the semantic relation type between a related entity pair. However, we observe that LLMs often struggle to reliably determine whether a relation exists, especially in cases involving complex sentence structures or intricate semantics, which leads to spurious predictions. Such hallucinations can introduce noisy edges in knowledge graphs, compromising the integrity of structured knowledge and downstream reliability. To address these challenges, we propose DEPTH, a frame- work that integrates Dependency-aware sEntence simPlification and Two-tiered Hierarchical refinement into the relation extraction pipeline. Given a sentence and its candidate entity pairs, DEPTH operates in two stages: (1) the Grounding module extracts relations for each pair by leveraging their shortest dependency path, distilling the sentence into a minimal yet coherent relational context that reduces syntactic noise while preserving key semantics; (2) the Refinement module aggregates all local predictions and revises them based on a holistic understanding of the sentence, correcting omissions and inconsistencies. We further introduce a causality-driven reward model that mitigates reward hacking by disentangling spurious correlations, enabling robust fine-tuning via reinforcement learning with human feedback. Experiments on six benchmarks demonstrate that DEPTH re- duces the average hallucination rate to 7.0% while achieving a 17.2% improvement in average F1 score over state-of-the-art baselines. 1 Introduction Relation extraction, the task of identifying semantic relationships between entities in unstructured text, serves as a fundamental component in natural language processing (NLP) [1–4]. Its applications span a wide range of domains, including social media analysis [5], knowledge graph construction [6], and question answering [7]. Traditional relation extraction methods typically rely on large-scale annotated datasets, making them costly to deploy and often limited in generalization. Recently, LLMs, empowered by increased model capacity and massive pretraining corpora, have demonstrated ∗This work was done when the author was a research intern at Alibaba Group. †Corresponding author Preprint. Under review. a new methodology to improve the accuracy of … Dependency Parsing: nsubj aux root p det dobj Grounding (methodology, USED-FOR, accuracy) Dependency-aware Simplification (methodology, accuracy) Entity Pair methodology → improve →accuracy SDP Dependency Description Simplified Sentence Causality-guided RLHF Grounding LLM-based Extraction Grounding (improve, EVALUATE-FOR, accuracy) Grounding … Refinement Aggregation Dependency Description Prompt Response RLHF Self-Correction ：Examine a set of predicted relations extracted from a single sentence and determine whether any corrections, removals, or additions are needed… Structed Prompting Omission Check Contradiction Check Misclassification Check Input (methodology, USED-FOR, accuracy), (improve, NO-RELATION, accuracy) … Output Figure 1: The overall framework of DEPTH. impressive capabilities in text understanding and generation. As a result, leveraging LLMs for relation extraction has emerged as a promising research direction [8–11]. However, most existing efforts"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 1, "text": "Prompting Omission Check Contradiction Check Misclassification Check Input (methodology, USED-FOR, accuracy), (improve, NO-RELATION, accuracy) … Output Figure 1: The overall framework of DEPTH. impressive capabilities in text understanding and generation. As a result, leveraging LLMs for relation extraction has emerged as a promising research direction [8–11]. However, most existing efforts focus on relation classification, assuming that a pair of entities is already known to be related, and asking the model to select the correct relation type from a predefined set [12]. In contrast, the broader task of relation extraction requires both identifying whether a relation exists between a given pair of entities and classifying its type. This distinction is crucial in real-world scenarios, such as the construction of proprietary knowledge bases, where the set of related entities is not known a priori. In such cases, models must first detect potential relations before categorizing them, making relation extraction inherently more challenging and practically important. Unfortunately, despite achieving high accuracy on relation classification benchmarks, LLMs often fall short in this broader task: they tend to overpredict relations, frequently hallucinating links between unrelated entities. These false positives (FP) introduce substantial noise into the extracted knowledge and can severely compromise the quality and reliability of downstream applications, especially when applied at enterprise-scale document processing. To this end, we propose DEPTH, a framework that integrates Dependency-aware sEntence simPli- fication and Two-tiered Hierarchical refinement into the extraction pipeline to reduce hallucinated relations and improve extraction fidelity. Specifically, DEPTH decomposes the task into two stages: Grounding and Refinement. The Grounding module focuses on local extraction, aiming to improve prediction accuracy for each candidate entity pair. Motivated by the observation that the essential in- formation needed to infer a relation between two entities is often captured by the shortest dependency path (SDP) connecting them in the dependency tree [13], we perform dependency parsing [14–16] on the input sentence and simplify it by retaining only the SDP and its immediate context. This reduces distracting information while preserving the relational semantics. In addition, we explicitly encode dependency-based cues as natural language prompts to guide LLM during relation prediction. While the Grounding module effectively reduces syntactic noise through sentence simplification and dependency-guided prompting, we find that relying solely on localized context remains insufficient. LLMs still tend to infer relations based on superficial lexical patterns, such as the frequent co- occurrence of certain entity pairs. This reliance on shallow statistical cues, rather than genuine semantic reasoning, often leads to systematic hallucinations. To mitigate this issue, a promising approach is reinforcement learning with human feedback (RLHF) [17, 18]. However, the success of RLHF crucially depends on the robustness of the reward model (RM). Existing RM training methods, unfortunately, are often vulnerable to reward hacking [19], whereby the model captures spurious correlations instead of truly causal signals. To address this, we adopt a causal perspective on the reward modeling process for relation extraction. Specifically, we construct a causal graph of the reward model and introduce a causal factorization method that separates each prompt-response pair into reward-relevant and reward-irrelevant components. By training the reward model solely on"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 2, "text": "signals. To address this, we adopt a causal perspective on the reward modeling process for relation extraction. Specifically, we construct a causal graph of the reward model and introduce a causal factorization method that separates each prompt-response pair into reward-relevant and reward-irrelevant components. By training the reward model solely on the relevant parts, we encourage it to focus on features truly indicative of relation existence, while discarding confounding patterns that do not causally affect the reward. We then apply Proximal Policy 2 Table 1: Comparison of LLM’s extraction performance on SciERC with and without NO-RELATION samples. Dataset P R F1 HR D2 38.5 36.8 37.7 63.2 D1 17.8 17.2 17.5 82.8 Optimization (PPO) [20–23] to fine-tune the LLM based on this robust reward model, significantly enhancing the reliability of relation predictions. On the other hand, the Refinement module operates at the sentence level by aggregating the predictions generated by the Grounding module across all candidate entity pairs. We then prompt the LLM to reassess these predictions under global semantic constraints (such as relational transitivity or mutual exclusivity), to identify and correct missing, erroneous, or logically inconsistent relations. By incorporating a global view of the sentence, the Refinement module equips the model with a form of self-correction, allowing it to reconcile local predictions with broader sentence-level coherence, which we find to be highly effective in mitigating hallucinated relations in our experiments. To summarize, our main contributions are as follows: • We propose DEPTH, a two-tiered hierarchical relation extraction framework that combines local precision from the Grounding module with global self-correction via the Refinement module. • We introduce a novel integration of LLMs and dependency parsing, leveraging SDPs between entities to simplify sentences and distill key relational information, thereby fundamentally reducing extraction complexity. • We develop a causality-driven reward modeling framework that disentangles reward-relevant and reward-irrelevant factors, yielding a robust reward model that resists spurious correlations and enables more effective policy optimization. • Extensive experiments across multiple benchmarks demonstrate the superiority of DEPTH in relation extraction, particularly in distinguishing relation existence, making it highly applicable to real-world scenarios. 2 Preliminaries and Problem Setup Given a sentence s containing a pair of entities (e1, e2), the goal of relation extraction is to identify the semantic relation ρ ∈R that holds between them, where R denotes a predefined set of relation types. In the context of LLM-based relation extraction, the sentence s, entity pair (e1, e2), and relation set R are encoded into a prompt x, which is presented to a LLM. The model then generates a textual response y, from which the predicted relation ˆ ρ ∈R is extracted. An essential yet often overlooked requirement in this process is the ability of LLMs not only to make accurate predictions, but also to explicitly say “no” when appropriate. That is, to return NO-RELATION when no suitable relation exists among the predefined set R. However, prior work often neglects this aspect. Even when models are prompted to return NO-RELATION in uncertain cases, few studies evaluate whether LLMs can reliably refrain from hallucinating relations. To dive deeper, we"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 3, "text": "when appropriate. That is, to return NO-RELATION when no suitable relation exists among the predefined set R. However, prior work often neglects this aspect. Even when models are prompted to return NO-RELATION in uncertain cases, few studies evaluate whether LLMs can reliably refrain from hallucinating relations. To dive deeper, we use the SciERC [24] dataset to examine how well LLMs can distinguish the existence of relations. Let D1 denote the set of all possible entity-relation triples (e1, ρ, e2) in the dataset, where each ρ falls into one of two categories: (1) ρ ∈R, or (2) ρ = NO-RELATION. We further construct a subset D2 ⊂D1, where all relations belong to the first category (i.e., ρ ∈R). To intuitively quantify the extent to which LLMs hallucinate relations when extracting on NO-RELATION samples, we make the following definition: Definition 1 (Hallucination Rate (HR)). The hallucination rate is the proportion of NO-RELATION samples that are incorrectly predicted as having a relation, calculated as HR = FN TP + FN, (1) where TP denotes the number of samples correctly identified as NO-RELATION, and FN denotes the number of NO-RELATION samples misclassified as having a relation. 3 Raw Sentence This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author's text as a coherent corpus. Dependency Parsing Dependency-aware Sentence Simplification 1. remove \"of a term aggregation system using each author‘s text as a coherent corpus.\" according to SDP for simplicity. 2. remove \"the\" for grammar fluency. Prompting Simplified This paper proposes a new methodology to improve accuracy. (methodology, accuracy) Raw Sentence This paper proposes … Entity Pair SDP This paper proposes a new methodology to improve the accuracy term of a aggregation system … DET DET DET DET NOUN NOUN NOUN NOUN NOUN NOUN VERB VERB ADJ PART ADP methodology improve accuracy NOUN NOUN VERB Figure 2: An illustration of how the Dependency-aware Simplification module operates. Table 1 presents the zero-shot performance of Qwen2.5- 14B-Instruct on both D1 and D2. We observe that the HR increases dramatically by 19.6% when evaluating on D1 compared to D2. This substantial decline highlights the limited ability of LLMs to distinguish between related and unrelated entities. Table 10 in the appendix further provides a comprehensive analysis of the TP, FP, and FN counts for each relation category, thereby shedding light on the origins of the observed performance differences. In practice, due to the scarcity of labeled data and the high cost of annotation, real-world datasets more closely resemble D1 rather than D2. Such hallucinations can be fatal in enterprise-scale document processing, where they introduce substantial redundancy into downstream graph structures. Therefore, enabling LLMs to accurately discern relation existence is a critical and non-negligible challenge. 3 Methodology To address the challenges discussed above, we propose DEPTH, a framework that not only classifies semantic relations accurately, but also equips LLMs with the critical ability to discern whether a relation exists. DEPTH adopts a two-tiered extraction architecture comprising: (1) a Ground- ing module that improves local prediction precision through dependency-aware simplification and causality-guided RLHF fine-tuning, and"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 4, "text": "propose DEPTH, a framework that not only classifies semantic relations accurately, but also equips LLMs with the critical ability to discern whether a relation exists. DEPTH adopts a two-tiered extraction architecture comprising: (1) a Ground- ing module that improves local prediction precision through dependency-aware simplification and causality-guided RLHF fine-tuning, and (2) a Refinement module that enforces global consistency through structured prompting and sentence-level verification. The overall workflow of DEPTH is illustrated in Figure 1. In the remainder of this section, we elaborate on each component in detail. 3.1 Dependency-aware Simplification for Entity-Pair Grounding Previous works typically focus on LLM-centric approaches, such as prompt engineering and super- vised fine-tuning, often overlooking the intrinsic value of syntactic structure in the sentence itself. In other words, existing methods perform relation extraction on raw sentences. However, when sentences are long or particularly complex, LLMs, especially lightweight models, struggle with precise semantic understanding of sentence details. As a result, hallucination issues often arise during relation extraction. 4 Pretraining Data/ICL Examples - POS tagging is compared with named entity recognition. - We compare POS tagging and named entity recognition. - POS tagging vs. named entity recognition. Real Data - We apply the model to POS tagging instead of named entity recognition tasks. Learned Heuristic ✓“POS tagging” co-occurs with “named entity recognition” ⇒COMPARE Observation ✓Co-occurrence of “POS tagging” and “named entity recognition”. Predicted Relation COMPARE × Figure 3: An example of hallucination in LLM-based extraction caused by co-occurrence. Several studies in deep learning have shown that dependency parsing provides valuable insights, particularly when entities are distant in a sentence, helping the model better capture and explain relations [25, 26]. Building on this, we apply dependency parsing in LLM-based relation extraction to simplify sentence structures and reduce extraction complexity. For a given sentence s, we first utilize Spacy [27], an open-source dependency parsing library, to generate its dependency tree. This tree encodes the syntactic relations between all words in s as directed edges. Figure 2 illustrates the dependency tree for the sentence: “This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author’s text as a coherent corpus.” For the entity pair “methodology” and “accuracy”, we extract their SDP: “methodology →improve →accuracy”. Since the necessary information to identify the relation between entities is typically contained in the SDP, we then prompt the LLM to simplify the sentence by retaining only the essential parts and removing irrelevant details. As an example, the sentence in Figure 2 would be transformed into: “This paper proposes a new methodology to improve accuracy.” To ensure that the LLM comprehends the dependency between entities without losing essential information, we further convert the SDP into a textual description, thereby enhancing the model’s understanding. The simplified sentence, along with its corresponding SDP description, is then fed into the LLM for relation extraction. A visual illustration of this process is provided in Figure 2. 3.2 Causal Reward Modeling for Hallucination-Resistant Extraction Beyond insufficient semantic comprehension, another critical factor leading to biased LLM outputs is their tendency to infer relationships through superficial lexical"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 5, "text": "SDP description, is then fed into the LLM for relation extraction. A visual illustration of this process is provided in Figure 2. 3.2 Causal Reward Modeling for Hallucination-Resistant Extraction Beyond insufficient semantic comprehension, another critical factor leading to biased LLM outputs is their tendency to infer relationships through superficial lexical patterns (e.g., high-frequency co-occurrence of phrases). Once spurious correlations are learned during pre-training or through prompting, the presence of identical phrases in real-world samples inevitably triggers incorrect relational inferences. Figure 3 illustrates an in-context learning (ICL) example where the LLM incorrectly infers that the co-occurrence of “POS tagging” and “named entity recognition” indicates a COMPARE relation. Therefore, given the sentence “We apply the model to POS tagging instead of named entity recognition tasks”, the model invariably predicts the relation between “POS tagging” and “named entity recognition” as COMPARE, even though the true relation should be NO-RELATION. A widely adopted approach to address such spurious reasoning in LLMs is RLHF, which fine-tunes the model using rewards predicted by a learned reward model. However, the effectiveness of RLHF critically depends on the reliability of the reward model, which remains vulnerable to the same spurious patterns it aims to correct. To uncover the root cause of this issue, we examine the reward model training process through a causal lens. As shown in Figure 4a, let x denote the prompt, y the corresponding response, and r the reward. We decompose the input pair (x, y) into two components: s, the reward-relevant factors, and s, the reward-irrelevant ones. Here, s contains the essential information needed to learn an ideal reward function, while s captures confounding signals, such as sequence length or stylistic patterns. This formulation highlights that reward hacking arises from the unintended causal path s →r. Hence, a robust reward model must explicitly block this influence to avoid learning from spurious correlations. To achieve this, we decompose both the prompt x and the response y into two parts: x = (x1, x2) and y = (y1, y2), where x1 and y1 are reward-relevant components, and x2 and y2 are reward-irrelevant ones. In the context of relation extraction, we empirically define x1 as the minimal input required for relation prediction, including task definitions, the simplified sentence, the target entity pair, and a natural language description of their dependency path. The remaining instruction components, such 5 𝑥 𝑦 𝑠 𝑠 𝑟 (a) Standard RM training. 𝑥 𝑦 𝑥2 𝑥1 𝑦1 𝑦2 𝑟 ❌ ❌ (b) Factorized RM training. Figure 4: Causal diagrams of the standard RM training process and our Causal Reward Modeling method. as broader extraction guidelines, are assigned to x2. For the LLM-generated output, y1 corresponds to the predicted relation ˆ ρ, while y2 includes any additional content such as explanations or formatting details. Full implementation details of this factorization are provided in Appendix B.1. During the training process, we optimize the reward model using only x1 and y1, effectively eliminat- ing spurious correlations and transitioning from the causal structure in Figure 4a to that in Figure 4b. A key advantage of this approach is that"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 6, "text": "details of this factorization are provided in Appendix B.1. During the training process, we optimize the reward model using only x1 and y1, effectively eliminat- ing spurious correlations and transitioning from the causal structure in Figure 4a to that in Figure 4b. A key advantage of this approach is that the factorization is applied at the input level, requiring no changes to the training architecture and incurring no additional computational cost. Experimental results in Section 4 validate the quality and robustness of the learned reward model. 3.3 Global Consistency Refinement via Structured Prompting While the Grounding module significantly improves relation prediction accuracy for individual entity pairs, treating each pair in isolation can lead to inconsistencies or omissions due to the absence of global semantic constraints. Therefore, we introduce the Refinement module, which aggregates local predictions and integrates sentence-level dependency structures to form a hierarchical correction mechanism. This enables global consistency enforcement across predicted relations and facilitates the recovery of overlooked relations. For a given sentence s, the Refinement module first aggregates all candidate relations predicted by the Grounding module (see Figure 1). We then convert the dependency tree of s, generated by Spacy, into a natural language description. This description, combined with the aggregated entity pairs, forms the input prompt for the LLM, guiding it to perform three-stage semantic calibration: (1) Omission check: identify relations overlooked by the initial predictions by leveraging the full-sentence syntactic structure; (2) Contradiction check: detect and resolve logically inconsistent relations (e.g., mutually exclusive facts), retaining the most coherent prediction based on dependency and context; (3) Misclassification check: correct errors arising from local context bias. The entire refinement process is implemented via ICL, with detailed prompt templates available in Appendix D. 4 Experiments In this section, we present the experimental results of DEPTH on various relation extraction bench- marks3. Specifically, our evaluation is designed to answer the following research questions: • Q1: Can DEPTH accurately determine whether a relation exists between two entities? • Q2: Does DEPTH contribute to improved extraction performance? • Q3: Is the learned reward model robust to spurious correlations? • Q4: Can DEPTH function as a general-purpose relation extraction model that eliminates the need for task-specific training across datasets? 3Code is available at https://github.com/CMACH508/DEPTH. 6 4.1 Experimental Setup Datasets. We evaluate DEPTH across a diverse collection of relation extraction benchmarks, including three news-domain datasets: TACRED [28], TACREV [29], and Re-TACRED [30]; two scientific-domain datasets: SciERC [24] and FOBIE [31]; and one general-domain dataset: SemEval [32]. Detailed descriptions of each dataset are provided in Appendix A. Table 6 summarizes key statistics for these datasets, such as the number of relation types, sizes of training and test splits, and the proportion of entity pairs labeled as NO-RELATION. RLHF-based Fine-tuning. We construct the preference datasets from the training splits to first train the reward model, followed by policy optimization using PPO guided by the reward model. Both reward model and PPO training are based on Qwen2.5-14B-Instruct, a strong decoder-only LLM that has demonstrated effectiveness across various NLP tasks including relation extraction. Our implementation builds upon the OpenRLHF"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 7, "text": "training splits to first train the reward model, followed by policy optimization using PPO guided by the reward model. Both reward model and PPO training are based on Qwen2.5-14B-Instruct, a strong decoder-only LLM that has demonstrated effectiveness across various NLP tasks including relation extraction. Our implementation builds upon the OpenRLHF package. Detailed information about the dataset construction, hyperparameters, and training costs are provided in Appendix B. Evaluation Metrics. For all benchmarks, we evaluate model performance on the test sets using precision (P), recall (R), and micro-F1, which are widely adopted for relation extraction. In particular, to assess the model’s ability to discern relation existence, we pay special attention to the hallucination rate (HR) in addition to overall metrics. Baselines. We compare DEPTH against the following state-of-the-art relation extraction baselines: • QA4RE [8] and SUMASK [9]: zero-shot methods that reformulate relation extraction as question answering via prompt design; • GPTRE [33]: an ICL method that enhances relation extraction by incorporating task-aware demonstration retrieval and goal-labeled reasoning; • InstructUIE [34]: an end-to-end information extraction framework that constructs expert-written instructions to fine-tune LLMs. 4.2 Main Results DEPTH consistently demonstrates superior ability to detect relation existence across all bench- marks (Q1). Table 2 presents the results on NO-RELATION cases, where DEPTH attains an average precision of 89.0%, recall of 93.0%, F1 score of 90.9%, and achieves a hallucination rate as low as 7.0%. The remarkably low hallucination rate demonstrates that DEPTH seldom assigns spurious relations to unrelated entity pairs, thus effectively alleviating overconfidence-induced hallucination observed in LLM-based extraction. Table 2: DEPTH’s performance on relation existence detection across various datasets. Benchmark P R F1 HR TACRED 87.8 96.4 91.9 3.6 TACREV 91.3 97.4 94.3 2.6 Re-TACRED 90.7 90.4 90.5 9.7 SciERC 79.1 87.4 83.1 12.6 FOBIE 95.3 97.3 96.3 2.7 SemEval 89.7 88.9 89.3 11.1 Average 89.0 93.0 90.9 7.0 DEPTH achieves an average improvement of 17.2% in F1 score (Q2). Table 3 presents the detailed results. By incorporating dependency-aware simplification, DEPTH effectively alleviates the structural complexity of the extraction task. In addition, the two-tiered refinement mechanism further enhances the accuracy of the final predictions. These components jointly lead to the strong overall performance of DEPTH, which attains an average F1 score of 88.3%. Remarkably, DEPTH delivers superior extraction performance with only an 14B-parameter model, outperforming baselines based on substantially larger backbones. This demonstrates the efficiency 7 Table 3: Micro-F1 scores of different methods across six relation extraction datasets. Scores marked with † are reproduced. Method Backbone TACRED TACREV Re-TACRED SciERC FOBIE SemEval Avg DEPTH Qwen2.5-14B-Instruct 86.2 90.0 87.7 76.3 94.3 95.3 88.3 QA4RE Text-davinci-003 59.4 59.4 67.3 51.7† 58.5† 44.1 56.7 SUMASK GPT-3.5-turbo-0301 79.6 75.1 73.8 55.8† 68.2† 59.2† 68.6 GPTRE Text-davinci-003 72.1 68.4† 61.6† 69.0 63.5† 91.9 71.1 InstructUIE Flan-T5-XXL 65.1† 66.3† 50.4† 45.2 65.0† 73.2 60.9 and effectiveness of our framework. Furthermore, to showcase the applicability of DEPTH across a variety of LLMs, we conduct experiments using different model sizes and architectures. The results for various backbones are reported in Appendix C.2. Another notable observation is that the improvements are particularly pronounced on scientific-"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 8, "text": "73.2 60.9 and effectiveness of our framework. Furthermore, to showcase the applicability of DEPTH across a variety of LLMs, we conduct experiments using different model sizes and architectures. The results for various backbones are reported in Appendix C.2. Another notable observation is that the improvements are particularly pronounced on scientific- domain datasets. This may be attributed to the increased complexity and domain specificity of scientific texts, where relation types are often more technical and syntactic structures more intricate. Such characteristics present greater challenges and demand deeper sentence-level understanding from the LLM. These findings underscore the effectiveness of DEPTH in processing complex or specialized corpora, especially in practical scenarios such as private or enterprise document extraction. DEPTH learns a high-quality reward model that is robust to spurious correlations (Q3). Our causal factorization approach restructures each prompt–response pair in the preference dataset to extract the minimal sufficient information required for reward model training. This is implemented via template-based matching, ensuring that variations in guideline length or the inclusion of verbose explanations do not affect the factorization process. As a result, DEPTH effectively shields the reward model from common spurious factors such as prompt length, formatting artifacts, and sycophantic phrasing. Figure 5 reports the pairwise accuracy of the reward model throughout training across all benchmarks. Here, the x-axis denotes the normalized training steps, and the y-axis reflects the proportion of instances in which the chosen response is correctly assigned a higher reward than the rejected one. Our causal reward model achieves an average accuracy of 98.8% across these tasks, underscoring its robustness. Such consistency would be unattainable if the model were driven by spurious correlations. 0.2 0.4 0.6 0.8 1.0 Normalized training step 0.80 0.85 0.90 0.95 1.00 Accuracy RM training accuracy TACRED TACREV Re-TACRED SciERC DEPTH-CRM FOBIE SemEval Figure 5: Pairwise accuracy during RM training. DEPTH supports model sharing across datasets within similar domains (Q4). In practical applications, training a separate task-specific LLM for each dataset is prohibitively expensive, highlighting the need for a general-purpose relation extraction framework. To evaluate the cross- dataset applicability of DEPTH, we jointly train a single model on multiple datasets within the same domain, and evaluate its performance on each individual test set. Specifically, we choose NYT11 [35] and Re-TACRED as representatives for the news domain, and use SciERC and FOBIE for the scientific domain. Table 4 reports results on both types of datasets. We observe that the jointly trained DEPTH model remains competitive, and even shows slight improvements in some cases compared to training separate models. These findings suggest that DEPTH strikes a favorable balance between 8 training efficiency and extraction accuracy, making it well-suited for real-world deployment where cost-effective scalability is essential. Table 4: DEPTH’s Performance when jointly trained on datasets from the same domain. Domain Benchmark P R F1 HR News NYT11 79.1 81.3 80.2 18.7 Re-TACRED 81.9 96.9 88.8 3.1 Average 80.5 89.1 84.5 10.9 Scientific SciERC 70.4 92.9 80.1 7.1 FOBIE 94.4 96.2 95.3 3.8 Average 82.4 94.5 87.7 5.5 4.3 Ablation Studies To assess the contribution of each component, we further conduct ablation"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 9, "text": "P R F1 HR News NYT11 79.1 81.3 80.2 18.7 Re-TACRED 81.9 96.9 88.8 3.1 Average 80.5 89.1 84.5 10.9 Scientific SciERC 70.4 92.9 80.1 7.1 FOBIE 94.4 96.2 95.3 3.8 Average 82.4 94.5 87.7 5.5 4.3 Ablation Studies To assess the contribution of each component, we further conduct ablation studies on the SciERC dataset. Detailed results are reported in Table 5. Impact of Dependency-Aware Simplification. To examine the effect of sentence simplification and dependency-guided prompting on relation extraction, we remove all dependency parsing components from the DEPTH framework, resulting in a variant denoted as DEPTH-DP. Compared to the full model, DEPTH-DP yields a decrease of 8.1% in precision, 9.3% in recall, and 8.7% in F1 score, thereby resulting in more hallucinations during extraction. This can be attributed to the syntactic complexity of many input sentences, which often causes LLMs to miss crucial relational cues during classification. These results suggest that leveraging SDPs for structural simplification effectively enhances the model’s sentence understanding and leads to more accurate extraction. Impact of Causal Reward Modeling. While previous results confirm that causal factorization enables the reward model to be robust against spurious correlations, we further investigate the impact of removing this component. Specifically, we ablate the Causal Reward Modeling module, resulting in a variant denoted as DEPTH-CRM. As expected, this modification leads to a substantial performance decline: precision, recall, and F1 score drop by 12.7%, 18.8%, and 15.5%, respectively. Notably, the hallucination rate rises to over 31%. Figure 5 illustrates the training dynamics of the reward model by plotting its pairwise accuracy over training steps. Without causal factorization, the accuracy plateaus at 92.8%, which is 4.3% lower than the model trained with causal factorization. Such performance is inadequate to support effective RLHF fine-tuning. These findings highlight the critical importance of incorporating Causal Reward Modeling into DEPTH, particularly in ICL scenarios where prompts may contain large amounts of irrelevant or confounding information. Impact of Global Consistency Refinement. The DEPTH-Refinement row in Table 5 corresponds to the ablation variant where the Refinement module is removed. Compared to the full DEPTH model, we observe consistent drops in precision, recall, and F1 score, indicating that the Refinement module provides meaningful improvements across all metrics. These results demonstrate that the Refinement step successfully verifies and self-corrects the predictions from the Grounding module, thereby enhancing overall extraction quality. Additionally, Appendix D provides case studies to further illustrate how this refinement process operates in practice. 5 Related Work 5.1 Relation Extraction with Large Language Models In recent years, LLMs have attracted significant attention for their potential in relation extraction. Techniques in this direction include, but are not limited to, data augmentation [36], zero-shot learning [8, 9], in-context learning [33], and supervised fine-tuning [37]. However, most prior work has focused on relatively simplified relation classification settings, with limited exploration of more challenging scenarios such as Relation Strict [12]. In this work, we aim to advance LLM-based 9 Table 5: Ablation study results on the SciERC dataset. Model P R F1 HR DEPTH 79.1 87.4 83.1 12.6 DEPTH-DP 71.0 78.1 74.4 21.9 DEPTH-CRM 66.4"}
{"doc_id": "2508.14391v1", "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14391v1", "chunk_id": 10, "text": "relation classification settings, with limited exploration of more challenging scenarios such as Relation Strict [12]. In this work, we aim to advance LLM-based 9 Table 5: Ablation study results on the SciERC dataset. Model P R F1 HR DEPTH 79.1 87.4 83.1 12.6 DEPTH-DP 71.0 78.1 74.4 21.9 DEPTH-CRM 66.4 68.6 67.5 31.4 DEPTH-Refinement 72.5 84.9 78.2 15.1 relation extraction by equipping models with the ability to discern whether a relation exists between entities—an essential step toward building more general and production-ready extraction systems at scale. 5.2 Reward Hacking Reward hacking arises when a reward model assigns high scores to outputs based on irrelevant or weakly correlated features, rather than truly aligning with the intended objectives. In the context of LLM alignment, typical sources of reward hacking include length [38], formatting [39], sycophancy [40], and superficial conceptual matches [41]. In recent years, several studies have aimed to mit- igate reward hacking, with one prominent direction focusing on causal analyses of its underlying mechanisms. For example, RMM [42] introduces a data augmentation strategy grounded in a causal framework of reward model training to improve learning quality. Ovinnikov et al. [43] and Wang et al. [44] incorporate causal invariance principles by introducing regularization constraints during training to encourage reward functions that are robust to spurious correlations. CAA [45] treats the reward model as an instrumental variable to causally intervene on the LLM, effectively removing biases induced by confounding factors. In contrast to these approaches, our method employs structured causal factorization to directly disentangle reward-relevant and reward-irrelevant components in the input signals. This allows for the construction of a robust reward model without the need for extra training objectives or elaborate data preprocessing, ensuring both stability and efficiency during training. 6 Conclusion and Future Work In this paper, we present DEPTH, a methodology designed to mitigate the hallucination phenomenon in LLM-based relation extraction, where models tend to erroneously assign relations to entity pairs labeled as NO-RELATION. In particular, DEPTH adopts a two-tiered refinement framework: the Grounding module enhances local prediction accuracy via dependency-aware sentence simplification and RLHF fine-tuning guided by a causally trained, robust reward model; the Refinement module performs sentence-level integration and applies global semantic constraints to verify and self-correct the outputs from the Grounding stage. Extensive experiments on a suite of well-established bench- marks validate the effectiveness of DEPTH in improving both extraction precision and robustness to hallucination. The main limitation of this work is the assumption that all entity mentions are provided in advance, rather than being automatically identified via named entity recognition (NER). Future research directions include integrating NER into the pipeline to construct a fully end-to-end relation extraction framework, extending DEPTH to more challenging settings such as document-level extraction and none-of-the-above detection, and applying it to downstream tasks including RAG."}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 0, "text": "Credence Calibration Game? Calibrating Large Language Models through Structured Play Ke Fang1, Tianyi Zhao2, Lu Cheng3 1University of Pennsylvania, Philadelphia, PA, USA 2University of Southern California, Los Angeles, CA, USA 3University of Illinois Chicago, Chicago, IL, USA Correspondence: fangke@seas.upenn.edu Abstract As Large Language Models (LLMs) are in- creasingly deployed in decision-critical do- mains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; how- ever, many of these approaches necessitate ad- ditional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Cre- dence Calibration Game. Our method estab- lishes a structured interaction loop wherein LLMs receive feedback based on the align- ment of their predicted confidence with cor- rectness. Through feedback-driven prompt- ing and natural language summaries of prior performance, our framework dynamically im- proves model calibration. Extensive experi- ments across models and game configurations demonstrate consistent improvements in evalu- ation metrics. Our results highlight the poten- tial of game-based prompting as an effective strategy for LLM calibration. Code and data are available at https://anonymous.4open. science/r/LLM-Calibration/. 1 Introduction The growing deployment of Large Language Mod- els (LLMs) in decision-critical domains neces- sitates not only correct answers but also well- calibrated confidence estimates. While LLMs of- ten demonstrate impressive reasoning capabilities, their confidence estimates frequently misalign with actual correctness, exhibiting a tendency towards overconfidence in incorrect answers and undercon- fidence in correct ones (Kadavath et al., 2022). Ad- dressing this calibration issue has attracted signifi- cant attention, leading to various proposed strate- gies, including post-processing methods (Guo et al., 2017; Shen et al., 2024), which adjust the model’s confidence level after inference, and techniques re- lying on third-party proxies (Xie et al., 2024). A key limitation of many existing strategies, however, is their dependence on additional training proce- dures or external models, which restricts flexibility and broad applicability (Xie et al., 2024). Conse- quently, there is a clear need for lightweight, self- contained calibration methods. In this work, we propose a prompt-based strategy to improve model calibration in a non-parametric and feedback-driven manner. Our approach is in- spired by the Credence Calibration Game (Critch et al., 2012), originally designed to calibrate hu- man judgment by encouraging participants to ex- press not only their answers but also their de- gree of confidence. In the original setting, par- ticipants are scored based on both correctness and confidence-receiving higher rewards for accurate answers made with high certainty, while incurring larger penalties for incorrect answers given with higher confidence. This mechanism incentivizes truthful expression of subjective confidence level. We adapt this framework to LLMs, which often suffer from overconfidence in incorrect answers and underconfidence in correct ones like humans. By providing the model with structured feedback over multiple rounds and incorporating summaries of previous game history into each input prompt, our strategy encourages LLM to dynamically ad- just its confidence estimation over time. This work adapts the core principles of the Credence Calibra- tion Game into a fully prompt-based, self-reflective framework"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 1, "text": "the model with structured feedback over multiple rounds and incorporating summaries of previous game history into each input prompt, our strategy encourages LLM to dynamically ad- just its confidence estimation over time. This work adapts the core principles of the Credence Calibra- tion Game into a fully prompt-based, self-reflective framework for calibrating language models. In summary, our main contributions are: • We propose a novel prompt-based framework for LLM calibration, inspired by the Credence Calibration Game, which enables lightweight and efficient confidence calibration. • We evaluate the proposed method across a range of LLMs with varying architectures and scales on multiple datasets in the settings of Multi-Choice Question Answering (MCQA) and open-ended QA. We find the consistent performance improvements, demonstrating its effectiveness. 2 Related Work A wide range of approaches have been proposed for LLM calibration. Existing efforts span a wide range of techniques, from post-hoc calibration and auxiliary model training to prompt-based inter- ventions and self-assessment schemes. A signifi- cant body of work centers around post-processing based calibration. One of the earliest methods is Histogram Binning (Zadrozny and Elkan, 2001), which adjusts predicted confidences by assessing correctness rates within predefined bins. Jiang et al. (2011) generalized this with Isotonic Regression, enforcing a monotonic mapping from uncalibrated confidence scores to empirical accuracy estimates. To address calibration across heterogeneous data subgroups, Úrsula Hébert-Johnson et al. (2018) proposed multicalibration, which iteratively refines group-wise predictions to match subgroup accu- racies. More recently, (Detommaso et al., 2024) extended this by clustering inputs into semantically coherent groups and applying grouped histogram binning for local calibration improvements. Beyond post-hoc strategies, some approaches leverage third-party proxy models to refine confi- dence estimates. DeJong et al. (1996) trained neu- ral networks on small batches of data to estimate the conditional correctness probability P(pcorr | pcon), which can then be used to adjust new pre- dictions. Küppers et al. (2021) adopted a Bayesian approach, estimating posterior confidence using a learned distribution over validation data. These methods often assume that validation data are repre- sentative of test distributions, which may not hold in dynamic or task-shifting settings. To address this, Ulmer et al. (2024) proposed minimizing the mean squared error between confidence and ac- curacy by clustering similar questions and using intra-cluster accuracy as a correction signal. Ye and Durrett (2022) used random forests to gener- ate local explanations of predictions and dynami- cally adjust confidence based on extracted features. Some methods shift calibration objectives toward human-related metrics. For example, Hashemi et al. (2024) designed a questionnaire-based feedback framework where humans and LLMs collabora- tively assess the confidence validity of predictions. Parametric calibration methods have also been widely adopted. Guo et al. (2017) applied a tem- perature parameter T to soften the logit outputs, aligning the softmax confidence with empirical cor- rectness. Shen et al. (2024) extended this idea in the Thermometer framework by learning a lightweight auxiliary model using data from multiple tasks to calibrate LLM outputs in a generalizable way. Several recent works explore novel strategies for calibration. Stangel et al. (2025) proposed Re- warding Doubt, which penalizes overconfident in-"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 2, "text": "Shen et al. (2024) extended this idea in the Thermometer framework by learning a lightweight auxiliary model using data from multiple tasks to calibrate LLM outputs in a generalizable way. Several recent works explore novel strategies for calibration. Stangel et al. (2025) proposed Re- warding Doubt, which penalizes overconfident in- correct predictions through an reinforcement learn- ing based reward model. Similarly, CONQORD (Tao et al., 2024) leverages reinforcement learn- ing with a dual reward to align confidence with response quality, promoting more trustworthy and selective LLM behavior. In a different direction, ACTCAB (Liu et al., 2024a) trains a linear layer on top of the language model’s last-layer activations to improve confidence estimation, while LITCAB (Liu et al., 2024b) enhances calibration using a lightweight bias layer added to the model’s output logits. Complementary to these methods, prompt- based approaches such as self-calibration (Kada- vath et al., 2022) provide a new perspective for improving model calibration. Building on this growing body of research, our work explores an approach that integrates struc- tured feedback into the prompting process through a game-inspired interaction scheme, which encour- ages the model to adapt its confidence estimation behavior based on its own performance history. Our work extends the principles of the Credence Cali- bration Game into the realm of LLMs, introducing a prompt-based calibration framework that simu- lates the game’s structured feedback loop. In con- trast to previous approaches, our method is: • Non-intrusive: It preserves the original model weights and does not require any addi- tional training or fine-tuning. • Prompt-based: Calibration is achieved purely through prompt-level intervention, avoiding the need for auxiliary models (De- Jong et al., 1996; Küppers et al., 2021), clus- tering heuristics (Ulmer et al., 2024; Detom- maso et al., 2024), or reinforcement learning objectives (Stangel et al., 2025; Tao et al., 2024). • Self-adaptive: By incorporating the model’s own performance history into the prompt, the method encourages iterative self-correction of confidence estimates in an in-context learning manner. This framework offers a lightweight and broadly applicable alternative for LLM calibration. To our knowledge, it represents the first attempt to trans- late the core ideas of the Credence Calibration Game(Critch et al., 2012) into a purely prompt- driven, self-reflective calibration strategy for lan- guage models. 3 Methodology 3.1 Preliminary: The Original Credence Calibration Game The Credence Calibration Game (Critch et al., 2012) is originally developed to improve the align- ment between human confidence estimates and ac- tual correctness. In each round, a player answers a multiple-choice question and reports a confidence level, typically ranging from 50% (pure guess) to 99% (near certainty). The scoring mechanism pro- vides feedback based on both correctness and ex- pressed confidence: correct answers yield higher re- wards when reported with higher confidence, while incorrect answers result in steeper penalties as con- fidence increases. This structure incentivizes hon- est self-assessment, encouraging participants to ex- press their true confidence about being correct. The game uses proper scoring rules that math- ematically guarantee the best strategy is to report one’s actual belief. Over repeated rounds, players receive numerical feedback and gradually learn to"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 3, "text": "con- fidence increases. This structure incentivizes hon- est self-assessment, encouraging participants to ex- press their true confidence about being correct. The game uses proper scoring rules that math- ematically guarantee the best strategy is to report one’s actual belief. Over repeated rounds, players receive numerical feedback and gradually learn to adjust their confidence to more accurately reflect their knowledge. This feedback-based interaction loop forms the conceptual foundation of our approach. In the next subsection, we describe how this structure is adapted into a prompt-based interaction framework for language models. 3.2 Calibration Game for LLMs Calibration Instead of modifying model weights through fine- tuning or applying post-hoc calibration techniques, we propose a novel prompt-based approach to dy- namically adjust a model’s self-assessment behav- ior. We adapt the game framework to LLMs based on the intuition that, like humans, LLMs produce outputs with varying degrees of confidence. Since LLMs often exhibit miscalibrated confidence, the feedback-driven mechanism of the Credence Cal- ibration Game provides a natural foundation for Confidence 50% 60% 70% 80% 90% 99% Symmetric Scoring If correct 0 +26 +49 +68 +85 +99 If incorrect 0 -26 -49 -68 -85 -99 Exponential Scoring If correct 0 +26 +49 +68 +85 +99 If incorrect 0 -32 -74 -132 -232 -564 Table 1: Scoring rules of the Credence Calibration Game under symmetric and exponential scoring strate- gies. encouraging better self-assessment. In each round, the model answers a question, reports its confi- dence on a scale from 50% to 99%, and then re- ceives feedback in the form of a score that reflects the alignment between its confidence and actual correctness. Scoring follows a core principle: If the answer is correct, higher confidence yields a higher score. If the answer is incorrect, higher confidence incurs a larger penalty. Formally, following the Credence Calibra- tion Game, we define confidence levels c ∈ {50, 60, 70, 80, 90, 99} and assign scores s(c) based on correctness. We operationalize this scor- ing framework through two different reward sys- tems, as summarized in Table 1: Symmetric Scoring Correct answers are re- warded and incorrect answers are penalized by the same magnitude based on the model’s reported con- fidence. For instance, a correct prediction with 90% confidence earns +85 points, while an incorrect one with the same confidence incurs −85 points. scorrect(c) = −swrong(c). (1) Exponential Scoring Incorrect answers are pe- nalized more severely to discourage unjustified overconfidence. Grounded in information theory, the penalty for an incorrect prediction at confidence c is approximately proportional to −log2 \u0000 1−c 0.5 \u0001 . This quantifies the misleading information relative to a 50% prior belief – where higher confidence in wrong answers yields exponentially larger penal- ties. Consequently, swrong(c) grows faster than lin- ear, enforcing stronger calibration pressure. For instance, 90% confidence results in +85 points for a correct answer but −232 points for an incorrect one, and 99% confidence yields +99 versus −564. 3.3 Problem Formulation The goal of our work is to improve the calibra- tion of LLMs without altering model parameters or relying on external models. A well-calibrated model should express"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 4, "text": "+85 points for a correct answer but −232 points for an incorrect one, and 99% confidence yields +99 versus −564. 3.3 Problem Formulation The goal of our work is to improve the calibra- tion of LLMs without altering model parameters or relying on external models. A well-calibrated model should express high confidence only when predictions are likely correct, and low confidence otherwise. Formally, let f be an LLM that takes as input a question qi and contextual information ci, and outputs both a predicted answer ˆ yi and a confidence score pi ∈[0, 1]: (ˆ yi, pi) = f(qi, ci). (2) where yi is the ground truth label. Given a dataset D = (qi, yi)N i=1 of N evalua- tion samples, our aim is to enhance the alignment between the model’s confidence pi and the actual correctness indicator 1(yi = ˆ yi) across the dataset. Crucially, our intervention is prompt-based: cali- bration improvements are achieved through natural language feedback on previous predictions, with- out gradient updates. Model outputs are compared before and after the intervention to assess perfor- mance differences. 3.4 Overall Framework As illustrated in Fig. 1, the framework unfolds in three sequential stages: (1) Pre-Game Evaluation; (2) Calibration Game; (3) Post-Game Evaluation. Pre-Game Evaluation We begin by establishing the model’s baseline calibration performance. The model is prompted to answer a set of benchmark- ing questions while also providing a self-reported confidence score between 0 and 100. No feedback or history is provided at this stage, ensuring that the responses reflect the model’s unconditioned behav- ior. The collected predictions and normalized con- fidence scores are then used to compute standard calibration metrics, capturing the model’s initial alignment between confidence and correctness. Calibration Game The model then enters a struc- tured interaction loop. In each round, it tackles multiple-choice questions adapted from the origi- nal Credence Calibration Game (Critch et al., 2012) and receives a short feedback summary covering its aggregate performance indicators such as cur- rent game score, mean confidence, and calibration status (e.g., “You are currently overconfident”). Mean confidence is computed as the average of the model’s reported confidences across all game questions attempted so far in the current game, and calibration status is determined by comparing this cumulative average confidence with the correspond- ing average accuracy. Detailed rules and an exam- ple game history are provided in Appendices A.1 and A.2. Importantly, this prompt-conditioning mechanism adapts self-assessment behavior with- out parameter updates. Post-Game Evaluation Finally, we repeat the initial evaluation, but augment the prompt with a concise summary of the model’s own game his- tory (accuracy, average confidence, total score, and over/under-confidence trend). This tests whether the model can leverage its prior performance to adjust subsequent confidence estimates. This framework enables a non-parametric refine- ment of calibration: without fine-tuning or addi- tional supervision, the model learns to reflect on its behavior and improve through contextual cues alone. 4 Experiments In this section, we evaluate our approach by an- swering the following research questions: RQ1. Can our game-inspired prompting strategy improve LLM calibration compared to standard prompting and baselines while"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 5, "text": "or addi- tional supervision, the model learns to reflect on its behavior and improve through contextual cues alone. 4 Experiments In this section, we evaluate our approach by an- swering the following research questions: RQ1. Can our game-inspired prompting strategy improve LLM calibration compared to standard prompting and baselines while keeping model per- formance stable? (see Section 5.1) RQ2. How do reward design affect learning dynamics and final outcomes across model sizes? (see Section 5.2) RQ3. How sensitive is calibration to the number of questions per game, and how does this affect accuracy? (see Section 5.3) 4.1 Experimental Setup Datasets We evaluate our framework using benchmark datasets MMLU-Pro (Wang et al., 2024) and TriviaQA (Joshi et al., 2017), which together span both structured multiple-choice and open-domain question answering tasks. MMLU-Pro is an enhanced version of the Mas- sive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) benchmark, designed to more rigorously test language models on broad and complex reasoning tasks. Compared to the orig- inal MMLU, MMLU-Pro significantly increases task difficulty by expanding the number of answer choices per question from 4 to 10, thereby reducing the likelihood of random guessing and encouraging more precise model understanding. The dataset Figure 1: Overall workflow of our calibration framework. comprises over 12,000 carefully curated multiple- choice questions sourced from standardized exams and academic textbooks, covering 14 diverse do- mains. In our experiments, we randomly sample chemistry and computer science as evaluation sub- jects. TriviaQA is an open-domain question an- swering dataset featuring approximately 95,000 question-answer pairs collected from trivia enthu- siasts. Each question is accompanied by multi- ple evidence documents sourced from web and Wikipedia pages, designed to simulate real-world information retrieval scenarios. Compared to ear- lier reading comprehension benchmarks, TriviaQA includes more complex and compositional ques- tions, often requiring cross-sentence reasoning and exhibiting substantial variation in phrasing between questions and supporting evidence. These charac- teristics make it a strong benchmark for testing factual knowledge and reasoning ability. To evalu- ate calibration under minimal external support, we use the rc.wikipedia.nocontext configuration from the validation split. This subset focuses on ques- tions where the model must rely on its own knowl- edge, which aligns well with our evaluation goal that assessing confidence calibration under mini- mal context and maximal reliance on the model’s internal knowledge. Backbone Models Our experiments cover a range of model scales using two instruction-tuned LLM families: Llama3.1 (Grattafiori et al., 2024), evaluated at 8b and 70b parameter sizes, and Qwen2.5 (Yang et al., 2025), evaluated at 7b and 72b. These models were selected based on three considerations: (1) both are open-weight models with strong performance across reasoning and fac- tual knowledge benchmarks, making them repre- sentative of current frontier LLM capabilities; (2) they support instruction-following tasks, which aligns well with our prompt-based calibration set- ting; and (3) their availability on public inference platforms ensures reproducibility and accessibility for future research. All evaluations are conducted in a zero-shot setup with fixed prompts. For each question, the model is instructed to output the an- swer index (for MMLU-Pro multiple-choice ques- tions) or text"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 6, "text": "prompt-based calibration set- ting; and (3) their availability on public inference platforms ensures reproducibility and accessibility for future research. All evaluations are conducted in a zero-shot setup with fixed prompts. For each question, the model is instructed to output the an- swer index (for MMLU-Pro multiple-choice ques- tions) or text answer (for TriviaQA open-ended questions), along with a confidence score between 0 and 100. These verbalized confidences are nor- malized to the [0, 1] range for evaluation. All ex- periments are conducted via API-based inference using the Together AI1 platform, with generation parameters fixed at temperature 0.7, top-p 1.0, and a maximum of 1024 tokens. Baseline We use self-calibration (Kadavath et al., 2022) as our prompt-based baseline. This approach prompts models to reflect on their answers’ validity: after responding to a question, the model assesses its correctness through follow-up queries like \"Is your previous answer correct?\" The model’s self- assessment serves as its confidence signal, provid- ing a simple, interpretable benchmark. While there exist numerous calibration methods—ranging from post-hoc statistical techniques to auxiliary mod- els and reinforcement learning frameworks—our work focuses specifically on prompt-based calibra- tion strategies that require no model fine-tuning or architectural changes. Therefore, we choose self- 1https://www.together.ai calibration as a strong and conceptually related baseline: it shares the same prompt-level interven- tion paradigm as our approach, allowing a fair and direct comparison in the context of black-box LLM inference. Evaluation Metrics We assess prediction qual- ity and calibration with four metrics: Expected Calibration Error (ECE) (Naeini et al., 2015), Brier Score (Brier, 1950), Accuracy, and AU- ROC (Fawcett, 2006). Mathematical definitions of these metrics are provided in Appendix A.3. These metrics were chosen to provide a com- prehensive evaluation of both the accuracy and the calibration quality of model predictions. Accuracy measures the overall correctness of answers, re- flecting task performance directly. ECE and Brier Score are widely adopted metrics that quantify how well the predicted confidence aligns with the true correctness likelihood, capturing the reliability of the confidence estimates. ECE focuses on the av- erage confidence-accuracy gap across confidence bins, while Brier Score penalizes the squared dif- ference between confidence and actual outcomes, offering a complementary perspective. AUROC evaluates the model’s ability to rank correct predic- tions with higher confidence than incorrect ones, which is important for scenarios where confidence- based decision thresholds or ranking are applied. Together, these metrics provide a balanced assess- ment of both predictive correctness and confidence calibration. 5 Results and Analysis 5.1 Game-Based Calibration Results Table 2 presents a comprehensive evaluation of calibration performance across various models and datasets. The proposed framework effectively improves calibration performance. Both game-based methods reliably lower ECE and Brier Score in most settings, confirming their overall effective- ness. In particular, the Game-Exp variant consis- tently achieves the lowest ECE in 6 out of 8 blocks, showing that heavier penalties for overconfidence help models become more cautious and thus better calibrated. For example, Qwen2.5-7b on TriviaQA achieves a 10.80% reduction in ECE under expo- nential scoring, outperforming symmetric scoring. Self-calibration, in contrast, yields only modest cal- ibration gains, typically"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 7, "text": "ECE in 6 out of 8 blocks, showing that heavier penalties for overconfidence help models become more cautious and thus better calibrated. For example, Qwen2.5-7b on TriviaQA achieves a 10.80% reduction in ECE under expo- nential scoring, outperforming symmetric scoring. Self-calibration, in contrast, yields only modest cal- ibration gains, typically 1–2%. It can, however, boost accuracy in select cases (e.g., Llama3.1-8B on TriviaQA with a 2.45% improvement), hint- ing at a trade-off: small accuracy benefits without meaningful shifts in confidence calibration. The effect of model size. Model size also mat- ters. The 70B/72B variants generally exhibit larger improvements than their smaller counterparts, par- ticularly on calibration metrics, suggesting that big- ger models have more latent capacity to adjust their confidence estimates when properly incentivized. Model performance stays stable despite im- proved calibration. Accuracy and AUROC change little across calibration strategies compared to the uncalibrated LLMs. Accuracy depends only on whether the predicted label is correct, and AU- ROC depends on the relative ordering of confidence scores—not their absolute values. Because our in- terventions rescale confidence magnitudes without altering predicted labels or their rank order, the model’s discriminative power is preserved. Thus, calibration improvements enhance confidence re- liability without compromising downstream task performance. Notably, Self-Cal achieves slightly higher accuracy than other methods. This could be attributed to the additional reasoning introduced by self-reflection prompts, which can occasionally improve answer quality—highlighting a trade-off between direct confidence calibration and indirect gains in task performance. In summary, our game-based calibration ap- proach consistently improves model calibration, with Game-Exp yielding more aggressive gains at some cost to accuracy, while Game-Sym strikes a more conservative balance. 5.2 Effect of different reward systems We visualize the total game score trajectories of different models throughout the game under both reward systems. As shown in Figure 2, models ex- hibit varied learning curves across the two settings during the game playing process. Larger models (e.g., Llama3.1-70b) tend to quickly converge to high scores, while smaller models (e.g., Llama3.1- 8b) demonstrate more volatile trends, especially under exponential scoring where the steep penal- ties for over/under-confidence can outweigh grad- ual gains from correct answers, sometimes even leading to lower scores after several rounds. This suggests that model capacity not only affects the ability to process the feedback signals embedded in the game, but also interacts strongly with the Model Dataset Size Method ECE ↓ Brier ↓ AUROC ↑ Accuracy ↑ LLaMA3.1 MMLU-pro 8b Uncalibrated 0.6171 0.5924 0.5441 0.2972 Self-Cal(2022) 0.5993 (-1.78%) 0.5755 (-1.69%) 0.5596 (+1.55%) 0.3162 (+1.90%) Game-Sym 0.5635 (-5.36%) 0.5084 (-8.40%) 0.5466 (+0.25%) 0.2592 (-3.80%) Game-Exp 0.5391 (-7.80%) 0.4918 (-10.06%) 0.5540 (+0.99%) 0.2815 (-1.57%) 70b Uncalibrated 0.5985 0.5952 0.5229 0.3870 Self-Cal(2022) 0.5771 (-2.14%) 0.5774 (-1.78%) 0.5067 (-1.62%) 0.4124 (+2.54%) Game-Sym 0.5382 (-6.03%) 0.5150 (-8.02%) 0.5847 (+6.18%) 0.3611 (-2.59%) Game-Exp 0.5324 (-6.61%) 0.5072 (-8.80%) 0.5931 (+7.02%) 0.3597 (-2.73%) TriviaQA 8b Uncalibrated 0.2581 0.2743 0.6451 0.6654 Self-Cal(2022) 0.2512 (-0.69%) 0.2622 (-1.21%) 0.6430 (-0.21%) 0.6899 (+2.45%) Game-Sym 0.2066 (-5.15%) 0.2547 (-1.96%) 0.6712 (+2.61%) 0.6478 (-1.76%) Game-Exp 0.2039 (-5.42%) 0.2546 (-1.97%) 0.6561 (+1.10%) 0.6489 (-1.65%) 70b Uncalibrated 0.1762 0.1799 0.6196 0.8028 Self-Cal(2022) 0.1765 (+0.03%) 0.1823 (+0.24%)"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 8, "text": "(-8.80%) 0.5931 (+7.02%) 0.3597 (-2.73%) TriviaQA 8b Uncalibrated 0.2581 0.2743 0.6451 0.6654 Self-Cal(2022) 0.2512 (-0.69%) 0.2622 (-1.21%) 0.6430 (-0.21%) 0.6899 (+2.45%) Game-Sym 0.2066 (-5.15%) 0.2547 (-1.96%) 0.6712 (+2.61%) 0.6478 (-1.76%) Game-Exp 0.2039 (-5.42%) 0.2546 (-1.97%) 0.6561 (+1.10%) 0.6489 (-1.65%) 70b Uncalibrated 0.1762 0.1799 0.6196 0.8028 Self-Cal(2022) 0.1765 (+0.03%) 0.1823 (+0.24%) 0.5781 (-4.15%) 0.8105 (+0.77%) Game-Sym 0.0965 (-7.97%) 0.1514 (-2.85%) 0.6567 (+3.71%) 0.8145 (+1.17%) Game-Exp 0.0886 (-8.76%) 0.1523 (-2.76%) 0.6471 (+2.75%) 0.8123 (+0.95%) Qwen2.5 MMLU-pro 7b Uncalibrated 0.6396 0.6236 0.5606 0.3086 Self-Cal(2022) 0.6344 (-0.52%) 0.6230 (-0.06%) 0.5458 (-1.48%) 0.3181 (+0.95%) Game-Sym 0.5438 (-9.58%) 0.5084 (-11.52%) 0.6039 (+4.33%) 0.2795 (-2.91%) Game-Exp 0.5100 (-12.96%) 0.4876 (-13.60%) 0.5674 (+0.68%) 0.2929 (-1.57%) 72b Uncalibrated 0.5168 0.5128 0.5494 0.4460 Self-Cal(2022) 0.5035 (-1.33%) 0.5006 (-1.22%) 0.5504 (+0.10%) 0.4624 (+1.64%) Game-Sym 0.3876 (-12.92%) 0.3856 (-12.72%) 0.6029 (+5.35%) 0.4222 (-2.38%) Game-Exp 0.4095 (-10.73%) 0.4018 (-11.10%) 0.6111 (+6.17%) 0.4136 (-3.24%) TriviaQA 7b Uncalibrated 0.4526 0.4356 0.6845 0.4964 Self-Cal(2022) 0.4325 (-2.01%) 0.4202 (-1.54%) 0.6791 (-0.54%) 0.5278 (+3.14%) Game-Sym 0.3868 (-6.58%) 0.3862 (-4.94%) 0.6206 (-6.39%) 0.5014 (+0.50%) Game-Exp 0.3446 (-10.80%) 0.3551 (-8.05%) 0.6239 (-6.06%) 0.5014 (+0.50%) 72b Uncalibrated 0.2049 0.2177 0.6624 0.7572 Self-Cal(2022) 0.1975 (-0.74%) 0.2116 (-0.61%) 0.6451 (-1.73%) 0.7672 (+1.00%) Game-Sym 0.1477 (-5.72%) 0.2005 (-1.72%) 0.6537 (-0.87%) 0.7451 (-1.21%) Game-Exp 0.1531 (-5.18%) 0.2033 (-1.44%) 0.6526 (-0.98%) 0.7431 (-1.41%) Table 2: Calibration performance comparison across different LLMs, datasets, and methods. ↓indicates that lower values are better, while ↑indicates that higher values are better. “Uncalibrated” denotes raw model predictions without calibration; “Self-Cal” is the self-calibration baseline (Kadavath et al., 2022); “Game-Sym” and “Game-Exp” are game-based prompting methods with symmetric and exponential rewards, respectively. Values in parentheses denote the relative change compared to the uncalibrated baseline. Best values in each block are highlighted in bold. Figure 2: Cumulative game scores across rounds for each model. Left: Symmetric scoring. Right: Exponential scoring. Larger models generally achieve higher and more stable total scores. choice of reward model: symmetric scoring offers a more forgiving path for weaker models, while exponential scoring can accelerate improvement for stronger ones but destabilize smaller LLMs. 5.3 Effect of Round Size in Calibration Games We conduct an ablation study comparing differ- ent number of questions in the calibration game. Specifically, we evaluate the effect of playing either 5 or 50 questions per game using symmetric scor- Setting ECE ↓ Brier ↓ AUROC ↑ Accuracy ↑ LLaMA3.1-8b (MMLU-pro) Uncalibrated 0.6171 0.5924 0.5441 0.2972 5 Questions 0.5807 (-3.64%) 0.5357 (-5.67%) 0.5431 (-0.10%) 0.2750 (-2.22%) 50 Questions 0.5635 (-5.36%) 0.5084 (-8.40%) 0.5466 (+0.25%) 0.2592 (-3.80%) LLaMA3.1-8b (TriviaQA) Uncalibrated 0.2581 0.2743 0.6451 0.6654 5 Questions 0.2321 (-2.60%) 0.2612 (-1.31%) 0.6905 (+4.54%) 0.6455 (-1.99%) 50 Questions 0.2066 (-5.15%) 0.2547 (-1.96%) 0.6712 (+2.61%) 0.6478 (-1.76%) Qwen2.5-7b (MMLU-pro) Uncalibrated 0.6396 0.6236 0.5606 0.3086 5 Questions 0.5309 (-10.87%) 0.5318 (-9.18%) 0.5975 (+3.69%) 0.2907 (-1.79%) 50 Questions 0.5438 (-9.58%) 0.5084 (-11.52%) 0.6039 (+4.33%) 0.2795 (-2.91%) Qwen2.5-7b (TriviaQA) Uncalibrated 0.4526 0.4356 0.6845 0.4964 5 Questions 0.3872 (-6.54%) 0.3908 (-4.48%) 0.5968 (-8.77%) 0.4847 (-1.17%) 50 Questions 0.3868 (-6.58%) 0.3862 (-4.94%) 0.6206 (-6.39%) 0.5014 (+0.50%) Table 3: Ablation study comparing different round sizes (5 vs. 50 questions) under symmetric scoring. Best values in each block are"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 9, "text": "0.2795 (-2.91%) Qwen2.5-7b (TriviaQA) Uncalibrated 0.4526 0.4356 0.6845 0.4964 5 Questions 0.3872 (-6.54%) 0.3908 (-4.48%) 0.5968 (-8.77%) 0.4847 (-1.17%) 50 Questions 0.3868 (-6.58%) 0.3862 (-4.94%) 0.6206 (-6.39%) 0.5014 (+0.50%) Table 3: Ablation study comparing different round sizes (5 vs. 50 questions) under symmetric scoring. Best values in each block are bolded. ing strategy, which provides smoother feedback and better isolates the effect of round size without introducing steep penalties from exponential scor- ing. The experiment is performed on Llama3.1-8b and Qwen2.5-7b. Table 3 presents the results. Overall, increasing the number of questions from 5 to 50 leads to consistently better calibration outcomes, especially in terms of ECE and Brier Score. For example, Llama3.1-8b on MMLU-Pro improves its ECE from 0.5807 to 0.5635 when moving from 5-question to 50-question rounds, cor- responding to a 1.72% reduction. This pattern sug- gests that longer game rounds provide more robust feedback signals, allowing the model to better cali- brate its confidence estimates. With only 5 game questions, the model receives limited information for under or over confidence, leading to less consis- tent improvements. On the other hand, 50-question rounds offer richer performance-based feedback, encouraging more stable calibration adjustments. However, improvements in accuracy remain modest or even slightly negative, consistent with the trade-off observed in Section 5.2. The best ac- curacy in each setting often still comes from the uncalibrated or shorter-round variants, highlighting that better calibration does not always align with better predictive performance. These findings suggest that increasing the num- ber of questions per calibration game enhances the model’s ability to self-adjust its confidence, partic- ularly in terms of ECE and Brier Score, while im- provements in AUROC and accuracy appear more task-dependent. Variance of the results. The experimental results presented above are consistent across different runs. As shown in Appendix A.4, we repeated the calibra- tion experiment five times under identical settings, and the low variance observed demonstrates the stability of our method. 6 Conclusion We present a prompt-based calibration framework inspired by the Credence Calibration Game, where LLMs improve their confidence alignment through structured feedback without any parameter updates. By incorporating game-play history into prompts, our approach dynamically adjusts the model’s con- fidence. Experiments across multiple LLMs and datasets show consistent improvements in evalu- ation metrics, particularly under the exponential scoring strategy. These results demonstrate the effectiveness of our proposed method and high- light the potential of game-based prompting as a lightweight and general calibration strategy for building more trustworthy AI systems. Future re- search could explore extending the framework to diverse task formats beyond multiple-choice and open-ended QA, integrating richer feedback signals and studying long-term adaptation over extended interaction sequences. Limitations While our game-based prompting strategy shows substantial improvements in calibration metrics such as ECE and Brier Score, it comes with notable trade-offs. In several settings, calibration gains are achieved at the cost of reduced accuracy, raising questions about the balance between confidence alignment and task performance. Additionally, our experiments are conducted on a limited number of benchmarks and LLMs. Future work may ex- plore ways to mitigate the accuracy drop, evaluate"}
{"doc_id": "2508.14390v1", "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14390v1", "chunk_id": 10, "text": "trade-offs. In several settings, calibration gains are achieved at the cost of reduced accuracy, raising questions about the balance between confidence alignment and task performance. Additionally, our experiments are conducted on a limited number of benchmarks and LLMs. Future work may ex- plore ways to mitigate the accuracy drop, evaluate broader task types, and improve efficiency for real- world deployment."}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 0, "text": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students’ Cognitive Abilities Wenhan Dong1,2 Zhen Sun2 Yuemeng Zhao2 Zifan Peng2 Jun Wu1 Jingyi Zheng2 Yule Liu2 Xinlei He2* Yu Wang3 Ruiming Wang1 Xinyi Huang4 Lei Mo1* 1School of Psychology, South China Normal University 2Information Hub, Hong Kong University of Science and Technology (Guangzhou) 3School of AI, Guangzhou University 4College of Cyber Security, Jinan University Abstract Large language models (LLMs) have demonstrated poten- tial in educational applications, yet their capacity to accu- rately assess the cognitive alignment of reading materials with students’ developmental stages remains insufficiently explored. This gap is particularly critical given the founda- tional educational principle of the Zone of Proximal Devel- opment (ZPD), which emphasizes the need to match learn- ing resources with Students’ Cognitive Abilities (SCA). De- spite the importance of this alignment, there is a notable ab- sence of comprehensive studies investigating LLMs’ ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese lan- guage education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage- level Chinese reading comprehension difficulty. The bench- mark is annotated by 60 “Special Grade” teachers, a group that represents the top 0.15% of all in-service teachers na- tionwide.1 Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guess- ing. When provided with in-context examples, LLMs per- formance improves substantially, with some models achiev- ing nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display system- atic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, signifi- cant variations in model performance across different genres underscore the complexity of task. We envision that ZPD- SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications. *Corresponding author (xinleihe@hkust-gz.edu.cn, molei@m.scnu.edu.cn) 1The dataset construction cost at least $20,000 (converted from RMB 150,000), primarily covering expert teacher involvement, and text acquisi- tion It will be made publicly available upon acceptance of the manuscript. 1 Introduction In recent years, Large Language Models (LLMs) have made remarkable progress in educational applications [39, 44, 16]. This progress spans a wide range of tasks, including auto- mated essay scoring [7, 41] and instructional content de- sign [47, 45]. LLMs demonstrate strong text-processing capabilities, offering new technical support for educational practices. However, in educational practice, providing high- quality education requires teachers to thoroughly analyze and understand students’ abilities [28, 36, 27]. This has led to a strong assumption in the field of LLMs applied to education: that LLMs are aware of the differences in Students’ Cog- nitive Abilities (SCA) across different educational stages. This assumption is grounded in Vygotsky’s Zone of Proxi- mal Development (ZPD) theory [38], which emphasizes that education should provide learning materials aligned with stu- dents’ current cognitive levels while introducing appropriate challenges to facilitate growth. In subjects"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 1, "text": "the differences in Students’ Cog- nitive Abilities (SCA) across different educational stages. This assumption is grounded in Vygotsky’s Zone of Proxi- mal Development (ZPD) theory [38], which emphasizes that education should provide learning materials aligned with stu- dents’ current cognitive levels while introducing appropriate challenges to facilitate growth. In subjects like mathematics, the requirements for students at different educational stages are often relatively objective. For example, the Compulsory Education Mathematics Cur- riculum Standard (2022 Edition) [29], outline that students should learn concepts such as fractions and inequalities. Specifically, second-grade students are expected to master addition and subtraction within 20 [29]. In contrast, the Chi- nese Curriculum Standards for Compulsory Education (2022 Edition) [28] focus on cultivating students’ abilities in char- acter recognition, reading comprehension, and writing. For second-grade students, the emphasis is on enjoying reading, being able to briefly retell stories, and achieving an extracur- ricular reading volume of no less than 50,000 words [28]. In practice, testing whether students have met their annual read- ing volume target is far more challenging than assessing their ability to perform addition and subtraction within 20. At the same time, the assessment of mathematical ability focuses on the mastery of specific skills, while the measurement of read- ing volume reflects the cumulative results of practice, repre- senting an essential difference in assessment nature. 1 LLMs as Teachers? Multi-Stage SCA Appropriateness Evaluation Elementary Middle Reading Level: Elementary Reading Level: High School LLM ZPD-based Alignment (match text SCA) Teacher (Human) Student (Reader) LLM “Implicit / Untested” Does it really align? ? Known Unknown ZPD Figure 1: The motivation of our work. We observe that the strong assumption, i.e., LLMs in- herently understand the SCA differences across educational stages, has been widely accepted without sufficient investi- gation, particularly in the domain of language skills. This leads us to a critical question: To what extent do LLMs accurately comprehend the SCA, especially the reading abilities of students at different educational stages? This question is complex and subjective. Beyond examin- ing traditional linguistic features addressed in readability as- sessments—such as syntactic complexity, lexical difficulty, and morphological characteristics [13, 12, 11], it is also nec- essary to incorporate analyses of content depth, logical rea- soning demands, and thematic emotional complexity. These dimensions provide a more comprehensive reflection of the actual SCA at different educational stages. To examine LLMs’ ability to evaluate the developmental appropriateness of texts for students at different stages, we concentrate specifically on the Chinese language to examine how well LLMs understand SCA. The complexity of the Chi- nese language arises not only from its semantic richness but also from features such as multi-layered emotional connota- tions and the presence of words with multiple meanings that vary by context. These characteristics make the task of align- ing texts with students’ cognitive levels a significant chal- lenge, even for experienced educators specializing in Chi- nese language teaching. In response, we define a specialized task and construct a novel benchmark, ZPD-SCA, designed to rigorously assess LLMs’ capacity to classify Chinese texts according to their suitability for elementary, middle, and high school"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 2, "text": "students’ cognitive levels a significant chal- lenge, even for experienced educators specializing in Chi- nese language teaching. In response, we define a specialized task and construct a novel benchmark, ZPD-SCA, designed to rigorously assess LLMs’ capacity to classify Chinese texts according to their suitability for elementary, middle, and high school students. ZPD-SCA is annotated by 60 “Special Grade” teachers, a group representing the top 0.15% of all in-service teach- ers nationwide. This dataset is designed to assess LLMs’ ability to classify the difficulty of Chinese texts into three educational stages: elementary, middle, and high school. In this task, LLMs analyze the linguistic, logical, thematic, and emotional complexity of a given text, directly mapping it to the most suitable educational stage based on an understand- ing of the reading abilities associated with each stage. This study not only addresses a significant gap in the eval- uation of LLMs’ capabilities but also provides insights into their potential applications in education. Our main contributions are as follows: 1. To evaluate LLMs’ capacity in assessing the cognitive alignment of reading materials with students’ develop- mental stages, we frame the task as a three-way classi- fication problem, in which LLMs are required to assign texts to one of three educational levels. This task evalu- ates whether LLMs can recognize the linguistic, logical, thematic, and emotional complexity of reading materi- als and align them with the cognitive capacities required for students at different developmental stages. This ad- dresses a previously underexplored aspect of evaluating LLMs in education, specifically focusing on their ability to assess SCA in the context of reading comprehension. 2. We introduce a novel benchmark ZPD-SCA, which is annotated by 60 “Special-Grade” teachers, a nationally recognized group that represents the top 0.15% of all K-12 educators in China, ensuring high precision and reliability. This authoritative annotated benchmark of- fers robust support for evaluating LLMs’ ability to as- sess SCA in reading comprehension tasks. 3. We reveal significant limitations of LLMs in assessing reading comprehension across educational stages, with performance variations across models. These findings highlight gaps in current LLMs’ ability to align text dif- ficulty with students’ developmental stages. 4. We show that LLMs perform poorly in zero-shot set- tings but improve significantly with in-context learning, indicating the need for task-specific training rather than inherent capability issues. By introducing ZPD-SCA, we emphasize the importance of testing assumptions about LLMs’ understanding of SCA, encouraging fur- ther research in educational LLMs. 2 Task Definition 2.1 Multi-Stage SCA Appropriateness Evalu- ation (MSCAAE) This section introduces a new task MSCAAE, designed to evaluate the ability of LLMs to determine the reading com- prehension difficulty of Chinese texts. The primary objective of this task is to assess whether these models can accurately classify texts into one of three educational stages-elementary, middle, or high school-within an annotated dataset, while capturing the cognitive differences in Chinese reading com- prehension abilities across these stages. To accomplish MSCAAE, the input texts need to satisfy two essential criteria. First, the content should come from real-world reading materials rather than standardized textbooks or curricula. These materials may"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 3, "text": "or high school-within an annotated dataset, while capturing the cognitive differences in Chinese reading com- prehension abilities across these stages. To accomplish MSCAAE, the input texts need to satisfy two essential criteria. First, the content should come from real-world reading materials rather than standardized textbooks or curricula. These materials may include extracurricular readings, news articles, and popular science literature, ensuring alignment with real-world reading scenarios. Second, the text difficulty should match the average Chi- nese reading comprehension level of students at a specific educational stage. The difficulty is generally assessed based 2 (a) Elementary school (b) Middle school (c) High school Figure 2: Semantic space analysis of educational stages. This figure presents the t-SNE contour visualizations of text embeddings for three educational stages: elementary, middle, and high school. The results indicate that, from a purely semantic perspective, elementary school exhibits a more distinct distribution in the embedding space, while middle school and high school demonstrate greater overlap, suggesting higher difficulty in distinguishing between these two categories based solely on semantic features. on the following dimensions: linguistic complexity, thematic depth, logical reasoning, and emotional complexity. For each input text, the model is required to generate a classification label that identifies the most appropriate edu- cational stage. For example, “Elementary” indicates that the text is suitable for elementary school students, “Middle” for middle school students, and “High” for high school students. The classification accuracy directly influences the model’s potential applications in Chinese education, especially in per- sonalized learning resource recommendations and reading ability evaluations. Importantly, the task focuses on evaluat- ing the model’s ability to cognitively assess students’ reading comprehension capabilities at different educational stages by analyzing the alignment between text difficulty and stu- dents’ cognitive development. Thus, the model must not only identify text difficulty but also integrate the cognitive development traits of students at various educational stages to comprehensively judge its understanding of student abil- ities. Specifically, the model needs to possess the following two core capabilities: • Understanding Students’ Chinese Reading Abilities at Specific Educational Stages: The model must grasp the cognitive characteristics of elementary, middle, and high school students in terms of language comprehen- sion, logical reasoning, knowledge background, and thematic acceptance. • Evaluating the Reading Difficulty of Chinese Texts: The model must comprehensively analyze text diffi- culty across linguistic complexity, thematic depth, log- ical reasoning, and emotional complexity, aligning it with students’ reading abilities. 2.2 Evaluation Metrics To assess the model’s ability to classify Chinese reading comprehension difficulty levels, we propose two evaluation metrics: Cross-Level Migration Concentration (CLME) and Weighted Directional Bias Index (WDBI). CLME. The CLME measures the proportion of misclassi- fications occurring between non-adjacent difficulty levels. This metric Mij denotes the number of samples classified as class j while their true class is i. The errors are weighted by the distance between classes to indicate the severity of mis- classification. The CLME is defined as: CLME = ∑i, j,|i−j|>1 Mij ∑i, j,i̸=j Mij WDBI. The WDBI evaluates the model’s overall tendency to overestimate or underestimate difficulty levels, while adjust- ing for class imbalance. It compares the weighted"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 4, "text": "weighted by the distance between classes to indicate the severity of mis- classification. The CLME is defined as: CLME = ∑i, j,|i−j|>1 Mij ∑i, j,i̸=j Mij WDBI. The WDBI evaluates the model’s overall tendency to overestimate or underestimate difficulty levels, while adjust- ing for class imbalance. It compares the weighted counts of upward shifts (predicting higher difficulty than the true label) and downward shifts (predicting lower difficulty than the true label). The WDBI is defined as: WDBI = ∑i<j wiMij −∑i>j wiMij ∑i,j wiMij 3 ZPD-SCA The data collection process was carefully structured to en- compass both diversity and educational appropriateness. Re- searchers with expertise in educational psychology collected raw data by selecting texts from a diverse range of extracur- ricular books, ensuring suitability for elementary, middle and high school students. To ensure comprehensive coverage of reading materials typically encountered by students, we in- corporated a wide range of genres, including fairy tales, fan- tasy, science fiction, campus life, adventure, and others, to- taling 12 distinct categories. These genres not only align with the typical categories in students’ extracurricular read- ing but also capture the varying levels of reading comprehen- sion required at different educational stages. This rigorous selection process resulted in a corpus of over 8 million characters, providing a robust foundation for sub- sequent analysis. During the data annotation phase, we col- laborated with 60 distinguished educators, all of whom were 3 GPT4o GPT4o-mini R1 V3 Claude GLM Qwen-max Qwen-plus Qwen72B Qwen32B 0.0 0.2 0.4 0.6 0.8 1.0 Recall Elementary School (ZP) Elementary School (CL) Middle School (ZP) Middle School (CL) High School (ZP) High School (CL) Figure 3: Recall rates for elementary, middle, and high school difficulty levels across LLMs. The figure illustrates how well each model identifies texts belonging to difficulty categories, highlighting variations in recall performance across labels. recipients of the title “Special Grade” teachers. This pres- tigious title is conferred by the Chinese government upon top-performing teachers, with the number of awardees not exceeding 0.15% of all active teachers nationwide. Each book was independently evaluated by at least 20 an- notators, who assessed its suitability for specific educational stages. All teachers participating in the annotation work were compensated at the highest standard permitted by the state for their professional services. After annotation and filtering, the dataset was reduced from an initial 8 million characters to a high-quality la- beled corpus of 4.7 million characters. This reduction re- sulted from the exclusion of unsuitable or controversial texts, thereby ensuring the consistency and reliability. Semantic exploration of ZPD-SCA. We explored the se- mantic differences in the annotated samples using GTE- large [25]. After obtaining the embeddings, we applied t- SNE to visualize and analyze the data. As shown in Fig- ure 2, middle and high school samples are not easily distin- guishable at the semantic level. However, elementary sam- ples show clear differences from both middle and high school samples. Clustering analysis confirms this, achieving 81.2% accuracy in separating elementary samples from the others. 4 Experiments and Analysis This section outlines the experimental setup, results, and analysis conducted"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 5, "text": "not easily distin- guishable at the semantic level. However, elementary sam- ples show clear differences from both middle and high school samples. Clustering analysis confirms this, achieving 81.2% accuracy in separating elementary samples from the others. 4 Experiments and Analysis This section outlines the experimental setup, results, and analysis conducted to evaluate LLMs in assessing the diffi- culty stages of Chinese reading comprehension texts for three stages: elementary, middle, and high school. 4.1 Experimental Setup We evaluate 10 LLMs, including GPT4o (version 2024-08- 06), GPT4o-mini (version 2024-07-18) [32], Deepseek-R1 (abbreviated as R1), Deepseek-V3 (abbreviated as V3) [14], Claude3.5-haiku (abbreviated as Claude) [4], GLM-4-plus (abbreviated as GLM) [48], Qwen-max (version 2024-10- 15), Qwen-plus (version 2025-06-24), Qwen2.5-72b-instruct (abbreviated as Qwen72B), and Qwen2.5-32b-instruct (ab- breviated as Qwen32B) [46]. Their performance is evaluated using four metrics: F1 score, accuracy (ACC), CLME, and WDBI. While F1 and ACC capture overall classification accuracy, CLME mea- sures the severity of errors by penalizing non-adjacent mis- classifications, and WDBI evaluates the directional bias in predictions (e.g., whether models tend to overestimate or un- derestimate SCA when aligning texts with appropriate diffi- culty levels). We consider two templates for prompting the models: • Zero-shot Prompting (ZP): The LLMs are provided with a brief description of the criteria and the full text as input, directly outputting a three-class result. • In-Context Learning (CL): In addition to using the same input and output format as zero-shot prompting, illustrative examples are included in the prompt. 4.2 Prompt Design The prompts used in our experiments are developed through an iterative refinement process, leveraging the capabilities of GPT-4o and Qwen-max to provide structured feedback. Initially, we test basic prompts on these models, which are designed to elicit three-class difficulty level predictions cor- responding to elementary, middle, and high school cogni- tive abilities. After collecting sample outputs, we system- atically identify instances of misclassification and analyze the reasoning provided by the models in conjunction with the ground truth labels. This analysis is then used as in- put for further iterations, where GPT-4o and Qwen-max are tasked with refining the prompts to enhance clarity and con- sistency. Through this iterative process, we ensure that the final prompts provide sufficient contextual information. It is important to note that the objective of this process is not to optimize prompts for maximum performance but rather to develop a consistent and reasonable framework for testing model capabilities across different settings. 4.3 Experimental Results Table 1 compares the ACC achieved by the evaluated models under ZP and CL settings, highlighting the impact of con- textual examples on performance. Table 2 summarizes the key performance metrics (F1, ACC, CLME, and WDBI) for some genres under the CL condition. 4 Table 1: Comparison of ACC scores achieved by the evaluated models under ZP and CL settings. These scores indicate how much better the models perform in CL compared to ZP in terms of ACC. Model→ GPT4o GPT4o-mini R1 V3 Claude GLM Qwen-max Qwen-plus Qwen72B Qwen32B All +0.1676 +0.1786 +0.0578 +0.0538 +0.2356 +0.4219 +0.3235 +0.3522 +0.2065 +0.1241 Elementary +0.2657 +0.2895 +0.1410 +0.1007 +0.3909 +0.6607 +0.5587 +0.5901"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 6, "text": "settings. These scores indicate how much better the models perform in CL compared to ZP in terms of ACC. Model→ GPT4o GPT4o-mini R1 V3 Claude GLM Qwen-max Qwen-plus Qwen72B Qwen32B All +0.1676 +0.1786 +0.0578 +0.0538 +0.2356 +0.4219 +0.3235 +0.3522 +0.2065 +0.1241 Elementary +0.2657 +0.2895 +0.1410 +0.1007 +0.3909 +0.6607 +0.5587 +0.5901 +0.3247 +0.2267 Middle +0.0765 +0.0314 −0.2481 +0.0335 −0.0527 −0.0812 +0.1559 −0.0171 −0.1005 −0.1568 High −0.0227 −0.0079 +0.0432 −0.0534 +0.0266 +0.1364 −0.1753 −0.0352 +0.0978 +0.0446 Table 2: Performance evaluation of 10 different models across various text genres in CL. “All” represents the aggregate performance across all genres. F1 and ACC indicate the model’s performance for a specific genre. CLME assesses the proportion of misclassi- fications occurring across non-adjacent difficulty levels, while WDBI evaluates the overall tendency of the model to overestimate or underestimate difficulty levels, accounting for class imbalance. Dataset Model→ Metrics ↓ GPT4o GPT4o-mini R1 V3 Claude GLM Qwen-max Qwen-plus Qwen72B Qwen32B All F1 0.6896 0.7993 0.7184 0.8352 0.6860 0.7038 0.6818 0.6962 0.6547 0.7610 ACC 0.6618 0.7784 0.7413 0.8313 0.6445 0.6878 0.6296 0.6674 0.6091 0.7555 CLME 0.0477 0.0189 0.0874 0.0350 0.0329 0.0074 0.0062 0.0924 0.0361 0.0396 WDBI 0.3561 0.0704 -0.3243 0.1081 0.2579 -0.1708 0.3727 0.1684 0.2953 -0.1930 Academic F1 0.9873 0.7500 0.2222 0.8571 0.9744 0.3750 0.7302 0.9333 0.9474 0.5185 ACC 0.9750 0.6000 0.1250 0.7500 0.9500 0.2308 0.5750 0.8750 0.9000 0.3500 CLME 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.6000 0.0000 0.0000 WDBI -0.0250 -0.4000 -0.8750 -0.2500 -0.0500 -0.7692 0.4250 -0.1250 -0.1000 -0.6500 Fantasy F1 0.4273 0.7598 0.9294 0.9166 0.4379 0.8748 0.5911 0.5993 0.4745 0.9427 ACC 0.2922 0.6712 0.9296 0.8904 0.3178 0.8356 0.4749 0.4717 0.3288 0.9315 CLME 0.0258 0.0139 0.0000 0.0417 0.0068 0.0000 0.0087 0.0268 0.0408 0.0000 WDBI 0.7005 0.2108 -0.2474 0.1303 0.5431 0.0465 0.5297 0.3842 0.7171 0.0364 Initial underperformance in zero-shot prompting. In the ZP, several models exhibit significant challenges in accu- rately classifying text difficulty levels. For example, GLM and Qwen-max achieve accuracy scores below 33%, which is lower than the expected performance of random guessing in a three-class classification task. In contrast, Qwen32B performs relatively better, achieving an ACC of 63.14%. This discrepancy suggests that these models may lack suf- ficient training in recognizing the SCA at different educa- tional stages. Furthermore, models with strong general text comprehension capabilities might inadvertently misinterpret SCA levels due to their extensive exposure to diverse but non-targeted training data, potentially introducing biases into their predictions. Notably, while elementary school samples can be distin- guished with (81.2%) accuracy semantically, models like Qwen-max and GLM show a stark contrast in the ZP set- ting. Their ACC for elementary school samples (9.83% and 7.71%, respectively) falls significantly below the random guessing baseline. This highlights a critical gap: while the models can differentiate text difficulty semantically, they fail to effectively recognize SCA. Further analysis of GLM shows that its CLME score was 0, suggesting an absence of severe misclassifications across non-adjacent difficulty levels. However, this outcome arises from the model’s overwhelming tendency to assign nearly all texts (88.03%) to the middle school category, revealing a fundamental inability to differentiate between difficulty lev- els effectively. Additionally, among the 10"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 7, "text": "its CLME score was 0, suggesting an absence of severe misclassifications across non-adjacent difficulty levels. However, this outcome arises from the model’s overwhelming tendency to assign nearly all texts (88.03%) to the middle school category, revealing a fundamental inability to differentiate between difficulty lev- els effectively. Additionally, among the 10 models, seven ex- hibit positive WDBI values, with an absolute average WDBI of 0.2696 and a mean WDBI of 0.2035. Notably, Qwen-Plus and GPT-4o had WDBI values exceeding 0.4, suggesting a pronounced tendency to underestimate human cognitive abil- ities and overestimate text difficulty. Unlocking potential with few-shot learning. When eval- uated under the CL, the performance of these models im- proved dramatically. For example, Qwen-max’s ACC in- creases from 30.61% to 62.96%, representing an improve- ment of over 2.06×. Similarly, the ACC of GLM rises from 26.59% to 68.78%, a gain of approximately 2.59×. These substantial improvements highlight the critical role of tar- geted training or contextual examples in enabling LLMs to perform well on tasks requiring nuanced understanding. The results may suggest that initial underperformance is not due to inherent limitations in the models but rather the absence of exposure to relevant training data or task-specific contexts. In the CL setting, we provide two elementary-level ex- amples to help the models calibrate their understanding of human cognitive abilities. This adjustment reduces the ab- solute average WDBI by 14%, bringing it down to 0.2317, while the mean WDBI decreased to 0.094. These changes suggest a partial reduction in the models’ tendency to under- estimate SCA, leading to a notable improvement in classifi- cation accuracy. However, three LLMs that initially overes- timate human abilities further exacerbated this bias. To de- termine whether the observed improvement is influenced by label imbalance (e.g., models favoring the elementary label to achieve higher accuracy), we examine recall rates for texts aligned with the three educational stages. We observe im- provements in recall rates for middle and high school across 5 most models (see Figure 3), ruling out the possibility of label bias. These findings reinforce our hypothesis that the LLMs possess some inherent ability to distinguish text difficulty but lacked appropriate training or context. Under the CL, V3 achieves the highest overall F1 (83.52%) and ACC (0.8313), demonstrating its strong capa- bility in assigning texts to the correct difficulty levels. De- spite this, WDBI (0.1081) indicates persistent challenges in accurately capturing the directionality of classification er- rors, suggesting opportunities for further refinement. Notably, smaller models such as GPT-4o-mini and Qwen32B demonstrate strong competitiveness in terms of F1 scores and accuracy, suggesting that model size alone does not guarantee superior performance on this task. The per- formance of models varied significantly across different gen- res, reflecting the complexity and diversity of the texts. In the Academic genre, Claude achieves the highest accuracy (0.9500). Meanwhile, in the fantasy genre, Qwen32B sur- passes other models with an F1 score of 0.9427 and the low- est CLME (0), demonstrating its ability to minimize severe misclassifications. 5 Enhancing Capability through Training Table 3: Comparison of Qwen-32B performance before and af- ter LoRA"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 8, "text": "achieves the highest accuracy (0.9500). Meanwhile, in the fantasy genre, Qwen32B sur- passes other models with an F1 score of 0.9427 and the low- est CLME (0), demonstrating its ability to minimize severe misclassifications. 5 Enhancing Capability through Training Table 3: Comparison of Qwen-32B performance before and af- ter LoRA fine-tuning. Setting Accuracy F1 CLME WDBI Zero-Shot 0.6314 0.6696 0.0046 -0.1046 Few-Shot 0.7555 0.7610 0.0396 -0.1930 Fine-Tuned 0.7986 0.6169 0.0467 -0.0502 We conduct fine-tuning on the Qwen32B using Low-Rank Adaptation (LoRA) [22] under ZP setting. The LoRA con- figuration uses a rank of 32, a learning rate of 5×10−4, and a batch size of 8, trained for 5 epochs. Table 3 summarizes the evaluation results after training, compared with Qwen32B’s performance under zero-shot and few-shot settings. The fine-tuned Qwen32B achieves an ac- curacy of 0.7986 in a zero-shot setting, surpassing its previ- ous few-shot performance (0.7555) and significantly improv- ing upon the original zero-shot result (0.6314). The CLME score increases slightly (from 0.0046 to 0.0467), but remains within an acceptable range, indicating no severe misclassifi- cation. Moreover, the WDBI is closer to zero (-0.0502), sug- gesting a reduction in directional bias compared to both prior settings. These results demonstrate that targeted fine-tuning via LoRA can not only bridge the gap between ZP and CL capabilities, but also lead to more balanced model behavior. 6 Discussion Zero-shot performance underestimates models’ latent potential in aligning text with student cognitive levels. Our experiments reveal that ZP performance significantly underestimates the ability of LLMs to assess the suitability of texts for different cognitive levels. When provided with in-context examples, model accuracy improves substantially, Figure 4: Comparative analysis of error type classification. This figure illustrates the misprediction categorization of ten LLMs, where the error attributions were analyzed and classified by GPT-4o based on the input of mispredictions and their under- lying causes. Categories accounting for less than 0.1% are ex- cluded from visualization for clarity. with some models achieving 2-3× higher accuracy com- pared to their ZP baselines. This suggests that LLMs have latent knowledge about text complexity and its alignment with SCA levels but require appropriate contextual guidance to better utilize this capability. The marked improvement through CL indicates that these models likely lack exposure to educational tasks during pretraining, which limits their ability to perform such specialized assessments effectively. Model size and general leaderboard rankings do not consistently predict success in assessing text alignment with SCA. Contrary to expectations, smaller LLMs such as Qwen32B and GPT-4o-mini outperformed larger counter- parts like Qwen-max, and GPT-4o in evaluating text suitabil- ity for different SCA levels. For instance, despite their high rankings on general benchmarks [21, 40, 10, 19, 6], GPT- 4o and Qwen-max struggled to accurately assess text diffi- culty and SCA alignment. This finding highlights that model scale alone is insufficient for success in specialized educa- tional tasks. We hypothesize that larger models, trained on diverse datasets, may inadvertently develop biases that conflict with the nuanced requirements of educational applications fo- cused on student cognitive levels. Their vast parameter spaces and broad exposure to general domains"}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 9, "text": "model scale alone is insufficient for success in specialized educa- tional tasks. We hypothesize that larger models, trained on diverse datasets, may inadvertently develop biases that conflict with the nuanced requirements of educational applications fo- cused on student cognitive levels. Their vast parameter spaces and broad exposure to general domains might lead them to over-rely on patterns irrelevant to the task at hand. In contrast, smaller models, constrained by their limited ca- pacity, appear less influenced by such biases and can focus more effectively on task-specific features, such as aligning texts with cognitive levels. CL has a pronounced impact, particularly on larger mod- els. The transition from ZP to CL yielded particularly significant improvements for larger models. For example, GLM saw its accuracy increase by approximately 2.59× when provided with contextual examples. This suggests that larger models, with their greater parameter space and learned representations, have a stronger capacity to leverage in-context information effectively. However, smaller models 6 like Qwen32B also demonstrated competitive performance, indicating that model size alone is not the sole determinant of success in this task. These results underscore the impor- tance of targeted approaches, such as CL, to unlock the full potential of LLMs in educational contexts. Evidence points to a gap in training for cognitive-level alignment tasks. Our findings collectively indicate that the suboptimal performance of LLMs in assessing text align- ment with student cognitive levels likely results from in- sufficient exposure to relevant educational tasks during pre- training. While LLMs demonstrate latent capabilities that can be enhanced through CL, their current training does not adequately equip them for specialized evaluations of read- ing comprehension difficulty across different developmen- tal stages. This limitation is particularly significant given the critical need to align educational materials with students’ cognitive levels, as highlighted by ZPD theory. In Figure 4, CL proves a certain level of effectiveness in aligning aspects such as “Emotional Complexity” and “Lin- guistic Complexity.” However, it falls short in addressing more intricate dimensions like “Thematic Depth” and “Log- ical Reasoning”, where more targeted training strategies are evidently required. Bridging this gap will necessitate focused training approaches and further exploration into the intersec- tion of LLMs and education. Overall, addressing this issue remains an open challenge and calls for continued research in this interdisciplinary domain. 7 Related Work LLMs as cognitive models of reading. Recent studies in- dicate that LLMs can serve as cognitively plausible proxies for human reading processes [8, 20]. For example, Lopes Rego et al. [34] demonstrated that replacing traditional cloze predictability with LLM-based predictions significantly im- proved a cognitive simulation of eye-movement behavior during reading. LLM-powered educational tools for reading comprehen- sion. A growing body of work leverages LLMs to generate and personalize reading materials for learners [42, 17, 37, 23]. Such personalized or simplified content has been shown to enhance learners’ engagement and understanding. While these LLMs show promise in reducing teacher workload and providing adaptive reading practice, their outputs still require careful educator vetting to ensure appropriate difficulty and alignment with curricular goals. Limitations of LLMs in SCA assessment, particularly in Chinese."}
{"doc_id": "2508.14377v1", "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14377v1", "chunk_id": 10, "text": "simplified content has been shown to enhance learners’ engagement and understanding. While these LLMs show promise in reducing teacher workload and providing adaptive reading practice, their outputs still require careful educator vetting to ensure appropriate difficulty and alignment with curricular goals. Limitations of LLMs in SCA assessment, particularly in Chinese. While existing research has explored the potential of LLMs in educational applications [33, 3, 30, 1], a critical gap remains in their ability to directly assess SCA, particu- larly in the context of reading comprehension across different educational stages. Despite advancements in text processing capabilities [35, 2, 15, 26], most studies have primarily fo- cused on leveraging LLMs for tasks such as text simplifica- tion or personalized content generation, often assuming that these models inherently understand the SCA differences be- tween elementary, middle, and high school. However, this assumption has not been rigorously tested, especially in lan- guages like Chinese, which present unique challenges due to their semantic richness and contextual variability. Although personalized LLMs can estimate individual proficiency lev- els by learning from specific user data, there is also a cru- cial need to delineate the boundaries of group-level cogni- tive abilities. Notably, with appropriate prompting strategies, LLMs may demonstrate some ability to discern the bound- aries of cognitive difficulty levels across different educa- tional stages [43, 24, 31]; however, comprehensively under- standing these broader patterns remains critical for designing equitable and effective educational tools [5, 9, 18]. 8 Conclusion Based on Vygotsky’s Zone of Proximal Development (ZPD) theory, evaluating the alignment between text difficulty and students’ cognitive levels is a core issue in educational ap- plications. However, current large language models (LLMs) generally lack targeted consideration for this task during their training process. Our study introduces a novel task frame- work to assess LLMs’ ability to evaluate Students’ Cognitive Abilities (SCA) in reading comprehension across different educational stages. By constructing a high-quality bench- mark ZPD-SCA annotated by 60 “Special Grade” teachers, we systematically uncover the limitations of existing LLMs in performing this task. Experimental results reveal that while in-context learning significantly improves model per- formance, the low accuracy in zero-shot scenarios, along with cross-level misclassification and directional bias issues, highlights the neglect of educational-specific needs in cur- rent model training. These findings not only provide critical directions for enhancing the adaptability of LLMs in edu- cational contexts but also underscore the importance of in- corporating cognitive alignment tasks into model training to advance their application in education."}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 0, "text": "ISCA: A Framework for Interview-Style Conversational Agents Charles Welch♡and Allison Lahnala♡and Vasudha Varadarajan♥and Lucie Flek♢and Rada Mihalcea♦and J. Lomax Boyd♣and João Sedoc♠ ♡McMaster University, ♥Stony Brook University, ♢University of Bonn, ♦University of Michigan, ♣Johns Hopkins University, ♠New York University cwelch@mcmaster.ca Abstract We present a low-compute non-generative sys- tem for implementing interview-style conver- sational agents which can be used to facilitate qualitative data collection through controlled in- teractions and quantitative analysis. Use cases include applications to tracking attitude for- mation or behavior change, where control or standardization over the conversational flow is desired. We show how our system can be easily adjusted through an online administra- tive panel to create new interviews, making the tool accessible without coding. Two case stud- ies are presented as example applications, one regarding the Expressive Interviewing system for COVID-19 and the other a semi-structured interview to survey public opinion on emerg- ing neurotechnology. Our code is open-source, allowing others to build off of our work and develop extensions for additional functionality. 1 Introduction Conversational agents are increasingly used for applications in healthcare (Valizadeh and Parde, 2022), customer service and engagement (Soni, 2023), and education (Yan et al., 2024). They can help motivate behavior change, assisting with ad- diction (He et al., 2022), medication adherence, and healthy behaviors (Aggarwal et al., 2023). They are also useful tools for understanding thoughts, opinions, and concerns (Ziems et al., 2024). In this paper, we describe our framework ISCA, for interview style conversational agents for appli- cations to health, behavior change, and information gathering. ISCA enables researchers to implement standardized interviews with customizable conver- sation flows. It is deliberately non-generative, al- lowing for more controlled interactions across par- ticipants and avoiding off-topic discussion. The backend of the system includes language detec- tion modules, which the researcher can customize through the admin portal to design rules for trig- gering follow-up questions based on elements of What do you think about chatbots? Interview They’re kinda cool Tell me what you think is cool! About the Study This study is about chatbots! Please read the info to learn about it and hit Submit to begin. Interview Summary happy neutral religion money Post-Interview Survey How much did you enjoy the chat? Pre-Interview Survey Have you ever used a chatbot? Figure 1: ISCA: An Interview-Style Framework the user’s responses. The system is equipped to deploy the studies within a participant-facing in- terface. Language analytics are built-in, both for the researcher’s admin view to gain insight into the study population’s responses and for the par- ticipants to gain insight into their own language. Our system is run as a web server built on the Django framework (Django Software Foundation, 2024) and allows users to easily configure inter- views without coding. ISCA can be used to understand attitude for- mation, attitude polling, and monitoring behav- ior change, using multiple choice survey questions given before and after each interview. We present two case studies: 1) to address stress related to the COVID-19 pandemic (Welch et al., 2020) and 2) to survey public opinion about human brain organoids, an emerging neurotechnology"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 1, "text": "mation, attitude polling, and monitoring behav- ior change, using multiple choice survey questions given before and after each interview. We present two case studies: 1) to address stress related to the COVID-19 pandemic (Welch et al., 2020) and 2) to survey public opinion about human brain organoids, an emerging neurotechnology that raises ethical concerns among segments of the public. Finally, we discuss applications and extensions. We inten- tionally avoid large language models and genera- tive approaches as they are unnecessary and costly. Our code1 and a demo video2 are made publicly available, so that others can easily set up and run servers locally or extend functionality. 1https://github.com/cfwelch/framework-intervi ew-style-agents 2https://youtu.be/_5XvMsZf8dA 2 Related Work Surveys and Interviews Recent research has ex- plored advancements in language-based survey methodologies, particularly focusing on tailoring approaches to enhance response rates and reduce respondent burden (Dillman et al., 2014; Sikström et al., 2023). Computerized adaptive testing (CAT) has long been recognized for its effectiveness in selecting informative questions based on partici- pants’ previous responses. Recently, it has emerged as a viable method to traditional fixed-format sur- veys (Wainer et al., 2000; Varadarajan et al., 2024). While CAT effectively improves engagement and response rates, it faces several limitations including the need for extensive pre-calibration with large but fixed question banks and significant computational resources. Recent solutions employ humans-in-the- loop to aid with question selection to train rein- forcement learning models to dynamically update survey questions (Velez, 2024). Designing interviews with meaningful outcomes requires careful consideration of the context, role dynamics, trust and expectations between the in- volved parties (Schilling, 2013). Distinct interview- ing styles can elicit distinct response patterns from the participants reflective of their cognitive pat- terns (Priede and Farrall, 2011). However, with the advent of conversational agents and chatbot-based interviewing systems, the elicited responses can dif- fer due to the personality of the agent (Xiao et al., 2020), anthropomorphism (Rhim et al., 2022) or the mode of interaction (Oates et al., 2022; Zarouali et al., 2024). AI-powered chatbots have been ex- plored as a means to provide telehealth advice (Xu et al., 2024), mental health assessments (Schick et al., 2022) and facilitating discussions (Nguyen, 2023). AI chatbots present challenges including data privacy concerns, ethical considerations in data handling, and potential impacts on authentic human interactions (Adam et al., 2021; Marks and Haupt, 2023). Conversational Agent Design Several platforms and frameworks exist for developing conversa- tional agents. Rasa, available in both open-source and commercial versions, specializes in machine learning-based dialog management using slot-value systems (Bocklisch et al., 2017). ParlAI provides a research platform specifically designed for dia- log research (Miller et al., 2017), while AIML and ChatScript offer rule-based chatbot development through specialized scripting languages (Ramesh C.P.: What are the major issues in your life right now, especially in light of issues surrounding COVID? User: I am worried about my health and the health of my family. I don’t want to make people sick and I am also having money issues. Lexicons: LIWC WN-Affect MPQA Health Finance Family Health Finance Family General ... Health Reflection I'd like to know"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 2, "text": "light of issues surrounding COVID? User: I am worried about my health and the health of my family. I don’t want to make people sick and I am also having money issues. Lexicons: LIWC WN-Affect MPQA Health Finance Family Health Finance Family General ... Health Reflection I'd like to know more about your feelings surrounding your health and the health of people close to you. What actions can you take to help keep you healthy during these challenging times? Figure 2: Main questions and reflection triggers. et al., 2017). Similar to ISCA, Juji offers conversa- tional capabilities (juj, 2024), while Riff represents a more focused application - a generative chatbot system designed to enhance college students’ learn- ing through reflection (Cavagnaro, 2023), specifi- cally targeting users 18 and older to avoid concerns associated with generative AI. Researchers used Juji for interactive surveying (Xiao et al., 2020), however, this is closed-source limiting the ability to tailor their infrastructure. 3 System Overview We release ISCA, an open framework for re- searchers to create conversational interview-style social science studies. As illustrated in Figure 1, ISCA facilitates the implementation of the inter- view flow and surveys that can be administered before and after the interview. It offers a unique experience for participants by providing analytical summaries and visualizations of what they wrote. Our conversational interview design revolves around main questions and reflections. The main questions will be asked to every participant by com- pletion of the interview, but participants may en- counter them at different times depending on the conversation flow. The conversation flow is im- pacted by the content of the participant’s turns, as the language triggers specific reflections (i.e., follow-up questions) when certain trigger condi- tions are satisfied. Figure 2 shows an example where a participant’s response with a high propor- tion of health topic words triggers a health reflec- tion, prompting the user to reflect more on health. Participant Flow. Participants can be directed to the website where the project is hosted either to a page that shows all topics with active interviews, or directly linked to the start page for a specific topic. The start page shows an informational screen about the purpose of the interview. This can serve as a dis- closure of information about the present study and asking for consent. Next, they are sent to the first multiple choice page. This page can be used for checking pre-interview participant status by asking yes/no or Likert scale questions. After answering questions, they are directed to the conversation. The conversation begins with an introduction from the chatbot and an initial main question. When the participant responds, a reflection may trigger. One reflection can be triggered in between each main question. If no defined reflections trig- ger, preprogrammed generic reflections (e.g. “Tell me a little more about that”) can trigger instead. These will trigger if the participant has taken less than 15 seconds to write their response or if they type less than 100 characters. Only one of these type of triggers can occur in a conversation, as we found that repeatedly"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 3, "text": "me a little more about that”) can trigger instead. These will trigger if the participant has taken less than 15 seconds to write their response or if they type less than 100 characters. Only one of these type of triggers can occur in a conversation, as we found that repeatedly reminding someone to write more tends to be frustrating and causes them to dis- engage. After all the main questions from the active interview are asked, the conversation will end. The participant is then redirected to post-interview mul- tiple choice questions. After they respond to these questions, they are shown a summary of their in- teraction (Figure 14 in the Appendix shows most discussed categories) and have options to download their data, reset the page if they want to start the conversation over, or provide feedback. 4 Administrative Panel Demo The administrative panel consists of seven main pages with distinct functions. Admins can create interview topics, define interview questions for a topic, manage lexicon categories, configure survey questions, view analytics on the dashboard, per- form topic modeling, and manage the FAQ. While pages like the lexicon management are globally managed, other pages, such as the interviews, are specific to a topic. On these pages, the dropdown in the sidebar can be changed at any time to reload the page with information for that topic. Figures showing each of the pages are in Appendix A and step-by-step documentation is provided in the sup- plementary material. Topics. The topics page shows the list of conversa- tion topics (Figure 3). Each topic can have one ac- tive interview at a time. When adding a new topic, admins can provide a name and icon for the topic, the name of the chatbot, and the intro/disclaimer screen that shows before any conversation starts. Interviews. The interview overview page (Fig- ure 4) shows the list of interviews for a topic. A new interview can be created when one wants to update the questions that will be asked in an in- teraction. Any notes on the topic will be shown on this page. When adding a new interview, ad- mins can add questions and reflections. Admins can choose to trigger the reflection when there is a dominant category from the set of lexicons that are active for that topic, sentiment, and whether or not a different reflection has already been triggered. A dominant category is one that occurs more than 50% more often in the response than the next most frequent lexical category. The sentiment option uses VADER to efficiently classify each utterance as positive, negative, or neutral (Hutto and Gilbert, 2014). On the overview page, clicking on any of these interviews shows details and allows setting the active interview (Figure 5). Lexicons. The lexicons page (Figure 6) allows admins to define new lexicon categories and to add/remove words from each category. When adding words, admins provide a comma-separated list of words and word stems. Word stems end with an asterisk and will match words beginning with the preceding stem, while other words must exactly match (case insensitive). When"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 4, "text": "admins to define new lexicon categories and to add/remove words from each category. When adding words, admins provide a comma-separated list of words and word stems. Word stems end with an asterisk and will match words beginning with the preceding stem, while other words must exactly match (case insensitive). When the user visits the lexicon topic management page (Figure 7), they can assign lexicon categories to a topic or remove them. The categories active for each topic are listed on this page. An active category means that those categories can be used as reflection triggers when adding an interview, and that those categories will be detected and recorded for summary statistics and for the administrative dashboard. Surveys. The survey management page (Figure 8) shows the list of questions for a topic. Questions can be added or deleted. The intro and outro boxes allow admins to toggle if the question is asked be- fore or after the interview (or both). Each question can be a yes/no question or a Likert scale question. The responses to questions defined here will be visualized on the administrative dashboard. FAQs. Admins can add or delete frequently asked questions in the FAQ page (Figure 9). This page is accessible to participants who are currently having a conversation either through the side navigation bar or through the conversational interface directly when they ask the bot a question. When a question is detected, a notification will appear that states that the interview was not designed for users to ask the bot questions and instead provides a link to the FAQ page for that topic. Dashboard. The dashboard shows a variety of ag- gregate statistics over interviews for the selected topic. Figure 10 shows the top of the dashboard, which contains a bar chart showing the most fre- quently discussed topics. Charts on this page use lexicon categories that are active for the topic. We also see the total number of conversations, average response length and interview time. The interview time and response length statistics can be selected, which redirects the admin to distribution plots for each statistic. The next plot on the dashboard shows the detected word categories (Figure 11). Unlike the previous plot, which shows the total number of conversations a given category appears in, this plot shows the frequency distribution of each category across conversations. The dashboard also shows a list of summaries and plots. Each summary has a date, word count, and button that redirects to the participant-facing summary for that page. Each survey question is used to generate a plot. Yes/no questions generate bar plots and Likert scale ques- tions generate line plot distributions (Figure 12) with an entry for before and/or after the interview. Topic Modeling. From the dashboard, you can go to separate topic modeling pages for LDA (Blei et al., 2001) and BERTopic (Grootendorst, 2022). The pages are identical except each allows you to run a different topic modeling method, which can be used for exploration of interview responses. The topic modeling pages show an overview of the previous topic modeling runs (Figure"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 5, "text": "modeling pages for LDA (Blei et al., 2001) and BERTopic (Grootendorst, 2022). The pages are identical except each allows you to run a different topic modeling method, which can be used for exploration of interview responses. The topic modeling pages show an overview of the previous topic modeling runs (Figure 15). The admin can choose a number of topics and start a new topic model, which will run as a subprocess. The status of the previous runs is shown below and will update automatically, as it regularly checks the subprocess status. The table of previous runs shows the date, number of topics, how long it took to run, and the topic coherence. Clicking to see the results of the previous topic modeling runs takes the admin to a page specifi- cally for those results (Figure 16). This page shows the distribution of each topic’s frequency and top ten words. Clicking on these topics shows a list of all conversation turns where the words occurred for more context. The bottom of this page contains the visualization generated with pyLDAvis (Sievert and Shirley, 2014), showing topic overlap and a dy- namic visual of the salient terms for each topic (Fig- ure 17). We use the same visual for the BERTopic topics even though it does not use LDA. 5 Case Study 1: COVID-19 The base of ISCA was originally designed for the Expressive Interviewing system as a response to the COVID-19 pandemic (Welch et al., 2020). The system uses a combination of techniques from Ex- pressive Writing (Pennebaker and Beall, 1986) and Motivational Interviewing (Miller and Rollnick, 2012). Expressive writing is a reflective writing technique shown to improve mental and physical health (Frattaroli, 2006). Motivational interviewing is a counseling technique designed to help people change their behavior by eliciting one’s motivation for change and reflective listening. Objective. Our original system aimed to reduce stress related to the pandemic. More recently, Stewart et al. (2023) used ISCA to perform a follow- up study to examine behavior change associated with the use of our system to understand how short- and long-term effects varied based on writing style and demographic factors. Participants were asked about a variety of behaviors, such as how frequently they thought and talked about COVID or went out in public, and used these to determine if long-term behaviors had changed when they returned two weeks later. Interview Configuration. This interview con- sisted of four main questions. The questions cen- tered around (1) the major issues in your life, (2) something you look forward to, (3) advice you would give to others, and (4) something you are grateful for (see Appendix B, for the full ques- tions). The interview contains reflections related to positive and negative emotions, pronoun usage, words related to order, and LIWC (Pennebaker et al., 2015) lexicon words related to money, health, home, and work. Before the interview, participants were asked about their overall life satisfaction. After the inter- view, they were asked how personal and meaning- ful the interaction was. At both times, they were asked about their current stress"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 6, "text": "LIWC (Pennebaker et al., 2015) lexicon words related to money, health, home, and work. Before the interview, participants were asked about their overall life satisfaction. After the inter- view, they were asked how personal and meaning- ful the interaction was. At both times, they were asked about their current stress level. All questions use a 7-point Likert scale. In a behavioral study, they were asked the additional behavior questions separately from our system, though these questions could easily be integrated as additional survey ques- tions. The original study also asked users to com- pare our system to Woebot (Fitzpatrick et al., 2017), a widely-used conversational mental help app. Participants. The original study recruited 174 par- ticipants through social media and announcements through our university. For the follow-up study on behavior change, 200 participants were recruited from Prolific.co to use the system and return two weeks later to use the system again. Findings. In the original study, participants’ self- reported stress levels were assessed before and after the interview, with > 3 on a 7-point Likert scale being high stress. When comparing ISCA to Woe- bot, ISCA resulted in a greater reduction in stress (from 91% to 64%, a 9% absolute decrease over Woebot). When asked to compare the two systems, users found ISCA to be easier to use and, overall, more useful (Welch et al., 2020). When looking at the difference in participants returning two weeks later, writing with more lex- ical diversity was found to be correlated with an increase in social activity. Anxiety words in writing were correlated with stress reduction in the short term, and positive words with a meaningful experi- ence. However, the short-term benefits to mental health did not translate to the long-term. 6 Case Study 2: Neurotechnology - Brain Organoids Another study using ISCA focused on human brain organoids (HBO), as an emerging technology in neuroscience research. Organoids, generally speak- ing, are miniature models of organs grown in a lab from stem cells that mimic the structure and func- tion of actual organs and thus are a valuable tool for studying their development and health (Benito- Kwiecinski and Lancaster, 2020). Objectives. The study aimed to replicate the find- ings from a previous survey on public attitudes to- ward organoid research (Bollinger et al., 2021) and toward xenotransplanted chimeras, an organism composed of cells derived from different species that are though to raise ethical concerns for some individuals (Boyd, 2023). This work primarily sur- veyed the public about their moral attitudes toward brain organoids and/or chimeras. Participants. Students were recruited through the University of Marburg via an internal mailing list. In total, 39 participants used the system. Only five had heard of HBOs before using our system. Interview Configuration. The participants were asked eight main questions, consisting of (1) their initial thoughts about HBOs, (2) justification for their feelings, (3) conflict with moral beliefs, (4) transplanting human brain cells into other animals , (5) impact on disease treatment, (6) views on suf- fering, (7) conflicted views, and (8) consciousness. Reflections related to sentiment and the LIWC cate-"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 7, "text": "of (1) their initial thoughts about HBOs, (2) justification for their feelings, (3) conflict with moral beliefs, (4) transplanting human brain cells into other animals , (5) impact on disease treatment, (6) views on suf- fering, (7) conflicted views, and (8) consciousness. Reflections related to sentiment and the LIWC cate- gories of money and religion were also considered. Before the interview, participants were asked if they had heard of HBOs before (yes/no) and if they were in favor of their use (this and subsequent ques- tions presented on 7-point Likert scale). After the interview, they were asked how conscious HBOs would have to be before they are morally problem- atic, how meaningful and personal the interaction was, and again if they are in favor of their use. Findings. A manual analysis of the conversation topics reveals a large number of similarities with Evans (2022), where people express similar themes related to research benefits, ethical concerns with respect to consciousness, and cautious optimism. They similarly relate to the technology through sci- ence fiction references. They express unease with chimeric research, and religion plays a minor role in shaping opinions. The level of agreement in our study for HBOs and chimeric research were 84% and 62%, respectively, whereas previous work found these levels to be 90% and 68%. The find- ings are similar, though our sample size is small and derived from a different population (German versus USA). In contrast, the participants talked about the socioeconomic impact and how the tech- nology might only be accessible to the wealthy. They also mentioned how the public must be able to give input and compared the technology to artifi- cial intelligence. 7 Discussion We demonstrated ISCA, a system that can be used by researchers to develop and administer research with a conversational interview style design. ISCA facilitates the collection of qualitative data that can provide deeper insight into respondents’ perspec- tives and behaviors. Conducting these interviews through a conversational agent may also help users express perspectives more freely than they would to another person (Lucas et al., 2014; Gratch et al., 2014). By including pre- and post-interview survey options in the design, it offers a suitable framework to investigate the effects of the conversational inter- view. We demonstrated these utilities in two case studies that are contrasted by guiding conversations on familiar versus novel concepts. Utility for chatbot intervention research. The COVID-19 study focused on guiding users to express their thoughts and feelings as a stress- reduction intervention using the expressive inter- viewing technique. ISCA allowed us to under- stand the effects of the technique on reducing stress. Many users who showed reduced stress after the interaction found the conversation mean- ingful. Also, users who found it more personal found it more meaningful as well. This highlights opportunities to research how to personalize the ex- perience (Welch, 2021; Abd-Alrazaq et al., 2021). The Bioethics study aimed to gather ethical per- spectives on a scientific topic that general popula- tions may find novel, playing an educational role to facilitate this. Through the introductory text for the interview,"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 8, "text": "highlights opportunities to research how to personalize the ex- perience (Welch, 2021; Abd-Alrazaq et al., 2021). The Bioethics study aimed to gather ethical per- spectives on a scientific topic that general popula- tions may find novel, playing an educational role to facilitate this. Through the introductory text for the interview, studies can introduce participants to new concepts, or link users to external information or the FAQ section for more details. This allows the system to then capture first impressions of a new concept. The agent is not fluent enough to intro- duce these concepts in conversation, so researchers should take this into consideration. User Expectations. Through interactions in both case studies, we find a small portion of users frus- trated with the lack of natural interaction. We have three mechanisms for setting expectations; (1) the introductory text, (2) redirection to FAQ when a question is asked, and (3) encouraging participants to spend time writing their responses. We want them to view the system as a guide for them to write detailed responses rather than a quick back- and-forth chat. Participants saw significant stress reduction and we were able to collect valuable in- put without a fluent agent, though the impact of fluency on the effectiveness of these methods de- serves attention in future work. Considerations for Integrations with Generative AI. ISCA is engineered as an end-to-end conver- sational system where the research admin has full control over the content the system produces. How- ever, there are benefits that generative techniques could provide. For instance, we found that users of the COVID-19 system were more likely to find it meaningful if also personal, one direction could be to utilize LLMs to adapt to the user. Using LLMs to support personalization may be especially bene- ficial for systems like mental health chatbots (Abd- Alrazaq et al., 2021). However, control over what the system generates is especially crucial for sensitive subjects like those explored in our case studies. Generative models are unpredictable in nature and can produce harm- ful responses (Bommasani et al., 2021). How to mitigate or remove biases of these models remains an open issue (Meade et al., 2022; Blodgett et al., 2020), leading researchers to suggest that they pose significant risks and should be carefully evaluated and monitored (Laranjo et al., 2018; Dinan et al., 2021), and that they may not be ready for many applications, especially those involving topics of a sensitive nature (Lechner et al., 2023). Even if these issues were solved, generative models are still not to be desired for standard- ized conversational interactions. Standardized in- terviews, used for public opinion polling and aca- demic research, influence decision-making in mar- ket research, corporate, and governmental organi- zations (Gwartney, 2007). Often contrasted with surveys that allow for the collection of rich open- ended data (Houtkoop-Steenstra, 2000), interview methodologies are more exploratory – they vary in their level of standardization, with some allow- ing for more open-ended exploration (Priede and Farrall, 2011). This level of control over question standardization and exploration is not possible with language models. One of the salient"}
{"doc_id": "2508.14344v1", "title": "ISCA: A Framework for Interview-Style Conversational Agents", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14344v1", "chunk_id": 9, "text": "rich open- ended data (Houtkoop-Steenstra, 2000), interview methodologies are more exploratory – they vary in their level of standardization, with some allow- ing for more open-ended exploration (Priede and Farrall, 2011). This level of control over question standardization and exploration is not possible with language models. One of the salient problems with open-domain conversational agents is the lack of consistency in the generated output, often involving contradictions (Roller et al., 2020). Extensibility. Our open-source framework can be extended to pursue integration with other models, such as LLMs, that could modify output styles or add flexibility to conversation flow. This feature would be optional and situation-specific as these components are often computationally or mone- tarily expensive (Zhao et al., 2023). Researchers could also extend it to offer human-in-the-loop fea- tures, enabling the use of LLMs to make the system more flexible while ensuring human oversight in sensitive scenarios. 8 Conclusion We introduced ISCA, a framework for interview- style low-compute conversational agents that takes the form of a web interface with admin and par- ticipant facing pages. The interviews can be con- figured for new conversational topics through the administrative panel with no programming expe- rience. By using a rule-based system, we avoid the drawbacks of generative models. By setting user expectations, ISCA can effectively help people reflect on their experiences and monitor behavior change (Case Study 1), and survey public opinion (Case Study 2). We release our code so that others can set up their own servers and extend our system. Ethics statement The COVID-19 case studies were approved by the University of Michigan IRB. Case Study 2: Bioethics of Organoids was approved by the Uni- versity of Marburg and University of Bonn IRBs. Acknowledgements We would like to thank Marlon May for help- ing with code compatibility, Ezzeddine Ben Hadj Yahya for his contributions to the code, and Ten- zin Migmar for her testing and revisions to setup instructions."}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 0, "text": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever Yixin Chen1 , Ying Xiong2† , Shangyu Wu1, Yufei Cui3, Xue Liu2, Nan Guan1, Chun Jason Xue2 1City University of Hong Kong 2Mohamed Bin Zayed University of Aritificial Intelligence 3McGill University Abstract Tool-augmented large language models (LLMs) leverage external functions to extend their capabilities, but inaccurate function calls can lead to inefficiencies and increased costs. Existing methods address this challenge by fine-tuning LLMs or using demonstration- based prompting, yet they often suffer from high training overhead and fail to account for inconsistent demonstration samples, which misguide the model’s invocation behavior. In this paper, we trained a behavior-aligned retriever (BAR), which provides behaviorally consistent demonstrations to help LLMs make more accurate tool-using decisions. To train the BAR, we construct a corpus including different function-calling behaviors, i.e., calling or non-calling. We use the contrastive learning framework to train the BAR with customized positive/negative pairs and a dual-negative contrastive loss, ensuring robust retrieval of behaviorally consistent examples. Experiments demonstrate that our approach significantly reduces erroneous function calls while maintaining high task performance, offering a cost-effective and efficient solution for tool-augmented LLMs. 1 Introduction Tool-augmented models (Tang et al., 2023; Patil et al., 2023; Abdelaziz et al., 2024) have emerged as a promising paradigm for enhancing the capa- bilities of large language models (LLMs), enabling them to interact with external functions such as search engines (Mialon et al., 2023), or domain- specific APIs (Li et al., 2023; Huang et al., 2023). By dynamically invoking functions, LLMs can per- form tasks such as retrieving real-time informa- tion, executing computations, or interacting with databases. However, the reliability of function call- ing remains a critical challenge, as incorrect or un- † Corresponding author. https://anonymous.4open.science/r/BAR-F65E necessary invocations may lead to computational overhead, increased latency, or even financial costs. Ensuring that LLMs accurately decide when and how to call functions is thus essential for deploying these systems efficiently. Prior approaches to this challenge rely on ei- ther pre-training or fine-tuning LLMs on curated function-calling datasets to enhance their tool- using capabilities (Qin et al., 2023; Chen et al., 2024). To better leverage LLMs’ in-context ca- pabilities, these approaches also retrieve simi- lar examples as demonstrations to guide LLMs in generating correct function calls. Although these approaches can alleviate the misinvocation issues (Chen et al., 2024), their training or fine- tuning costs are too high, requiring high-end com- putational resources. Moreover, these works over- look the inconsistency of examples in demonstra- tions, which may confuse or misguide the LLMs in calling the function. The inconsistency issue in demonstrations lies in the lack of a mechanism to ensure that retrieved demonstrations exhibit coherent behavior regarding function calls. Our preliminary experiments also prove that when demonstrations are behaviorally aligned, either uniformly requiring or not requiring function calls, the LLMs can perform significantly better. This observation motivates the need for a specialized retrieval module that can dynamically select the most relevant and consistent demonstra- tions, thereby reducing ambiguity and improving decision-making in tool-augmented LLMs. In this paper, we propose a behavior-aligned retriever, called BAR, which"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 1, "text": "requiring function calls, the LLMs can perform significantly better. This observation motivates the need for a specialized retrieval module that can dynamically select the most relevant and consistent demonstra- tions, thereby reducing ambiguity and improving decision-making in tool-augmented LLMs. In this paper, we propose a behavior-aligned retriever, called BAR, which retrieves examples with consistent behaviors to help LLMs make more accurate tool-using decisions. To train the BAR, we construct a corpus consisting of different be- haviors, e.g., calling or non-calling. We follow the contrastive framework to select positive pairs and negative pairs in the function-calling scenar- ios. We divide negative samples into two sub- 1 LLMs Users What kind of weather do you like? query response results Tool-aware Pretrained/Finetuned Tool DB retrieved samples 1. What's your favorite season and why? (No tool) 2. Can you tell me the weather forecast for tomorrow? (Require tool) 3. Do you prefer rainy or sunny days? (No tool) use tool Retriever Datastore retrieval-augmented prompt Figure 1: Retrieval-augmented function-call pipeline. classes for more fine-grained semantically and be- haviorally discrimination. Finally, we introduce a dual-negative contrastive loss to optimize our BAR. Extensive experiments demonstrate the efficacy of our proposed BAR. Compared to baseline models, BAR improves direct response rate of LLMs by 8.5% on H2A scenarios, and reduces redundant API calls by 4.2% on ToolDEER dataset. The main contributions of this paper are as follows: • We show that demonstrations with aligned be- haviors can guide LLMs correctly utilizing the external tools. • We trained a behavior-aligned retriever with customized positive/negative samples and a dual-negative contrastive loss. • We applied our BAR on various LLMs, and the results show consistent improvements. 2 Background and Motivations 2.1 Retrieval-Augmented Function-Call Pipeline LLMs equipped with function-calling abilities can invoke external APIs to answer queries that re- quire real-time data, computation, or privileged knowledge. Benefited from LLMs’ in-context capabilities, existing works retrieve similar ex- amples as demonstrations to help LLMs make correct function-calling decisions. The retrieval- augmented function-call pipeline is shown in Fig- ure 1, which includes a datastore of labeled ex- amples, a retriever for top-k semantically relevant demonstrations, and a tool-augmented LLM for invoking the functions if needed. For example, given a query from users (“What kind of weather do you like?”), the retriever first re- trieves the top-3 similar examples from the external datastore (i.e., “What’s your favorite season and why?” with the label of no tool calling, “Can you tell me the weather forecast for tomorrow” with the label of tool invocation, and “Do you prefer rainy or sunny days?” with the label of no tool calling). Then the framework concatenates the user query and the corresponding retrievals into one prompt and feeds it into the LLMs. The LLMs would de- termine whether to use the tool or not, and then return the final responses to the users. 2.2 Motivation Retrieval-augmented function-call pipelines still suffer from two key points: high adaptation cost and semantic-only retrieval noise. First, most works still require fine-tuning the LLM on function- call data to ingest the retrieved demonstrations reli- ably. However, fine-tuning LLMs"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 2, "text": "not, and then return the final responses to the users. 2.2 Motivation Retrieval-augmented function-call pipelines still suffer from two key points: high adaptation cost and semantic-only retrieval noise. First, most works still require fine-tuning the LLM on function- call data to ingest the retrieved demonstrations reli- ably. However, fine-tuning LLMs is prohibitively expensive and might be repeatedly tuned when new APIs or domains are introduced. Secondly, ex- isting retrievers rank retrievals solely by seman- tic similarity but overlooking tool-invocation be- haviors. Many queries that are lexically or top- ically close require opposite tool-invocation be- havior. Figure 1 illustrates such a clash: the retrieved example is semantically near the user query yet calls an API, whereas the query should not. Such behavior-mismatched demonstrations systematically mislead the LLM, degrading down- stream accuracy. We quantify this issue on the H2A benchmark (Chen et al., 2024), as shown in Figure 2. Across three query categories, Helpful- ness, Harmfulness, and Autonomy, we measure the behavior-consistency ratio: behavior-consistency ratio = #{retrievals with the same call/non-call behavior} k (1) Our preliminary experiments reveal two key find- ings. Finding 1 is that retrieving semantically related examples can enhance LLMs’ function- calling capabilities. Stronger encoders (e.g., BERT) can provide more similar examples, thus LLMs can learn from them to make function-calling de- cisions. Finding 2 is that LLMs’ function-calling capabilities scale with behavior alignment. The estimated trend indicates that the behaviors of 2 Figure 2: Relationship between behavior-consistency ratio (across different retrievers) and LLM performance on the H2A Dataset. Each point represents an LLM and a retriever combination, where 5 demonstrations are retrieved per query. demonstrations are more consistent, making LLMs more likely to invoke the correct function. These observations expose a fundamental limitation of semantic-only retrieval and motivate us to propose a behavior-aligned retriever that explicitly encodes tool-invocation labels. By selecting demonstrations that are both semantically relevant and behaviorally consistent with the incoming query, the retriever supplies the LLM with noise-free guidance, elimi- nating unnecessary API calls while preserving zero- shot generality, without any further fine-tuning of the backbone model. 3 Behavior-Aligned Retriever In this section, we introduce a behavior-aligned retriever (BAR) that can fetch samples that ex- hibit similar invocation behaviors. First, we de- fine the inputs/outputs of the retriever; Then, we introduce the corpus used for training and retriev- ing; Finally, we present the detailed techniques of training the behavior-aligned retriever, including positive/negative samples and dual-negative con- trastive loss. 3.1 Inputs/Outputs of Retriever The goal of the retriever in tool-augmented LLM is to retrieve similar examples for LLMs, enabling their in-context capabilities. The retriever R takes the user query q as input, and returns top-k behav- iorally compatible examples, R(q) = \b (zi, yi) k i=1, (2) where zi is the similar queries, and yi is the cor- responding behavior. For example, yi can be the specific APIs that need to be called or yi ∈ {call, no_call}. After obtaining the retrievals, all retrievals are concatenated with the user query using a specific prompt template as the final input x to be fed into LLMs. 3.2 Training/Retrieval Corpus"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 3, "text": "responding behavior. For example, yi can be the specific APIs that need to be called or yi ∈ {call, no_call}. After obtaining the retrievals, all retrievals are concatenated with the user query using a specific prompt template as the final input x to be fed into LLMs. 3.2 Training/Retrieval Corpus In this paper, we aim to train a behavior-aligned re- triever with a customized dataset and then retrieve similar examples from the retrieval database during inference. To enhance the awareness of function behavior, we construct the training dataset by merg- ing the function-calling instruction datasets and general question-answering datasets. And we use the dataset of downstream tasks as the retrieval cor- pus. The specific training corpus and the retrieval corpus used in this paper are listed in §4.1. 3.3 Training Details We first train the retriever model as an encoder, i.e., fR : q →hq ∈Rd, then we compute the simi- larities between the query representation and the representations of other examples in the retrieval corpus, finally select the top-k similar examples as the retrievals. During the training stage, we fol- low the representative contrastive framework (Gao et al., 2021) with customized positive/negative sam- ples and a behavior-aligned loss function. Building Positive Pairs. One critical process in contrastive learning is how to construct positive pairs (q, q+). Given an anchor query q, choosing a candidate positive sample q+ follows two criteria: (a) Categorical Matching: q+ has the same function-invocation behavior as that of the anchor query q. (b) Semantic Proximity: they are semantically related, i.e., the similarity δ(q, q+) between them exceeds a predefined threshold t, δ(q, q+) = h⊤ q hq+ ∥hq∥∥hq+∥, (3) where hq is the representation of the query q. With these two constraints, the selected positive pairs 3 would be behavior-consistent and semantically co- herent. Building Negative Pairs. Previous studies (Robin- son et al., 2021) emphasized the critical role of hard negative pairs in contrastive learning. To construct negative samples, we adopt a dual strategy that addresses both fine-grained semantic discrimina- tion between samples with the same behavior and ambiguity of samples across different behaviors. (a) Samples with Same Behaviors. For each an- chor query q, all other queries exhibiting the same behaviors (e.g., requiring function calls), excluding q and its positive pairs, serve as negative samples. This helps the model fur- ther discern subtle semantic variations among queries with identical invocation behaviors. (b) Samples with Different Behaviors. Instead of randomly sampling, we choose other queries with different behaviors (e.g., no function call) but semantically similar to q. This explicitly penalizes confusion between behaviors near decision boundaries. For example, given an anchor query (“What is the weather like today”) which requires an API call, a similar commonsense query (“What kind of weather do you like?”) would be chosen as a neg- ative sample. For an anchor query q, we compute its similarity to all queries with different behav- iors and select the top-l most similar instances as negative samples. Dual-Negative Contrastive Loss With two kinds of negative samples, we design a dual-negative con- trastive loss, which integrates two"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 4, "text": "as a neg- ative sample. For an anchor query q, we compute its similarity to all queries with different behav- iors and select the top-l most similar instances as negative samples. Dual-Negative Contrastive Loss With two kinds of negative samples, we design a dual-negative con- trastive loss, which integrates two kinds of losses corresponding to the above two cases: LDNCL = αLsame + (1 −α)Ldiff (4) where α ∈[0, 1] is a balancing coefficient between these two losses. The first loss for the same behav- iors is based upon the InfoNCE loss (Oord et al., 2018): Lsame = −Ep \" log eδq,p/τ eδq,p/τ + P n eδq,n/τ # (5) where (q, p) is a sampled positive pair, n is the negative sample with the same behavior as that of q. The second loss for different behaviors Ldiff sharpens decision boundaries: Ldiff = −Ep \" log eδq,p/τ eδq,p/τ + P m eδq,m/τ # (6) where m is the negative sample with different behavior. The temperature parameter τ controls the scale of similarity distribution, with lower val- ues producing sharper distributions that emphasize hard negatives. 4 Experiments 4.1 Experimental Setup Model Selection To comprehensively evaluate LLMs’ capability in function calling, we intro- duce twelve advanced models, including three cate- gories: (1) vanilla pre-trained models: Mistral-7B- Instruct (Jiang et al., 2023), Llama-2-7B-chat (Tou- vron et al., 2023), Llama-2-13B-chat (Touvron et al., 2023), Llama-3.1-8B-Instruct (Grattafiori et al., 2024) and Qwen2.5-7B-Instruct (Yang et al., 2024); (2) pretrained model on function calling dataset: Functionary-7B (MeetKai, 2024); (3) fine-tuned models from function calling datasets: gorilla-openfunctions (Patil et al., 2023), granite- 20b-functioncalling (Abdelaziz et al., 2024), Tool- LLaMa (Qin et al., 2023), ToolAlpaca-7B (Tang et al., 2023), ToolAlpaca-13B (Tang et al., 2023) and ToolAlign-DPO (Chen et al., 2024). Datasets We use API-Bank (Li et al., 2023), a benchmark with 2,202 dialogues covering 2,211 APIs from 1,008 domains, for function calling in- structions. For general questions, we select 9,750 high-quality samples from common-sense ques- tions (Talmor et al., 2019) and instructions used in LIMA (Zhou et al., 2023). We evaluate our re- triever on two challenging datasets: H2A (Chen et al., 2024) and ToolDEER (Gui et al., 2024). We choose testset on the H2A, focusing on three di- mensions: single-tool instructions with multi-APIs from helpfulness scenario, harmful instructions, and autonomy instructions. For the ToolDEER dataset, we utilize its validation set, which contains two types of queries: #SearchAPI (queries must be solved by external tools) and #NoSearchAPI (queries can be solved by LLMs without tools). Evaluation Metrics For H2A dataset: (1) Help- fulness: We simulate multi-API scenarios by in- cluding available API lists in prompts. To evaluate performance, we introduce Exact Function Match, measuring whether the model correctly identifies all required APIs without omissions or unnecessary calls. (2) Harmlessness: Following ToolAlign’s prompts (Table 10), we use GPT-4 to judge if the responses reject to answer unsafe instructions, and then we calculate the refusal response rate. (3) 4 Models Helpfulness Harmlessness Autonomy Zero BM25 BERT Contriever BAR Zero BM25 BERT Contriever BAR Zero BM25 BERT Contriever BAR Mistral-7B 25.0% 27.0% 27.0% 27.5% 29.0%"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 5, "text": "(Table 10), we use GPT-4 to judge if the responses reject to answer unsafe instructions, and then we calculate the refusal response rate. (3) 4 Models Helpfulness Harmlessness Autonomy Zero BM25 BERT Contriever BAR Zero BM25 BERT Contriever BAR Zero BM25 BERT Contriever BAR Mistral-7B 25.0% 27.0% 27.0% 27.5% 29.0% 20.6% 39.2% 39.2% 35.6% 35.6% 0.0% 0.0% 1.0% 1.0% 1.0% Qwen2.5-7B 7.0% 11.0% 9.0% 10.5% 11.0% 41.2% 41.8% 38.1% 41.2% 40.2% 38.0% 49.0% 42.0% 42.0% 52.0% Llama-2-7B 5.0% 11.5% 12.0% 11.0% 14.0% 29.9% 58.8% 57.2% 62.4% 60.8% 1.0% 10.0% 14.0% 17.0% 19.0% Llama-2-13B 18.0% 26.0% 28.0% 29.0% 34.0% 36.1% 69.1% 67.5% 69.1% 67.5% 4.0% 13.0% 17.0% 17.0% 21.0% Llama-3.1-8B 4.5% 12.5% 13.0% 12.5% 14.0% 57.2% 56.2% 54.1% 51.0% 52.1% 3.0% 3.0% 2.0% 1.0% 1.0% Functionary-7B 32.5% 22.0% 20.0% 25.5% 25.0% 28.4% 48.5% 54.1% 44.3% 52.1% 2.0% 43.0% 47.0% 47.0% 49.0% Openfunctions 19.0% 16.5% 23.5% 24.5% 27.5% 72.2% 80.9% 83.5% 83.0% 86.6% 20.0% 30.0% 31.0% 33.0% 44.0% Functioncalling 5.0% 7.0% 8.5% 7.0% 10.0% 3.6% 50.0% 47.4% 46.4% 55.7% 4.0% 70.0% 76.0% 84.0% 86.0% ToolLLama-7B 3.5% 3.5% 1.5% 2.0% 2.5% 11.9% 30.9% 35.6% 28.9% 40.7% 58.0% 85.0% 88.0% 93.0% 95.0% ToolAlpaca-7B 10.5% 8.5% 6.0% 8.5% 9.0% 40.7% 64.4% 73.2% 69.1% 75.8% 80.0% 83.0% 89.0% 90.0% 91.0% ToolAlpaca-13B 4.0% 11.0% 11.0% 10.5% 12.0% 40.7% 69.6% 68.0% 57.2% 77.3% 75.0% 79.0% 89.0% 88.0% 94.0% ToolAlign-DPO 2.0% 4.0% 4.0% 3.0% 4.0% 88.1% 94.8% 90.7% 87.1% 85.6% 87.0% 81.0% 83.0% 88.0% 95.0% Average 11.3% 13.4% 13.6% 14.3% 16.0% 39.2% 58.7% 59.1% 56.3% 60.8% 31.0% 45.5% 48.3% 50.1% 54.0% Table 1: The function calling performance of LLMs with 5 demonstrations retrieved by different retrievers on H2A dataset. Zero denotes zero-shot setting. Query Type Similar Query BM25 BERT Contriever BAR Helpfulness Helpfulness 925 956 948 976 Harmlessness 73 36 27 19 Autonomy 2 8 25 5 Harmlessness Helpfulness 352 285 304 276 Harmlessness 600 676 632 684 Autonomy 18 9 34 10 Autonomy Helpfulness 214 174 131 25 Harmlessness 12 7 5 4 Autonomy 274 319 364 471 Table 2: Performance comparison of retrieval models: Distribution of top-5 retrieval queries across three cate- gories (Helpfulness, Harmlessness and Autonomy). Autonomy: We measure the direct response rate without external function calls. For the ToolDEER dataset, we follow the setting of Gui et al. (Gui et al., 2024), and evaluate the vanilla models by calculating the number of correctly predicted re- sponses for NoSearch and Search queries. While the pre-trained and fine-tuned models such as Func- tionary and ToolLlama, we prompt GPT-4o to eval- uate helpfulness of response - whether it provides useful information that meets the task requirements, as shown in Table 11. All detailed prompts are il- lustrated in Appendix A.2. Details for Fine-tuning We initialize the network with the BERT base uncased model(110M) and fine-tune using the AdamW optimizer with a learn- ing rate of 1e-6. We train the model for 20 epochs with a batch size of 64. We set the temperature τ to 0.05 and use α=0.8 to balance between same- behavior and different-behavior loss. Each positive sample is selected based on a similarity threshold of 0.7, and"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 6, "text": "a learn- ing rate of 1e-6. We train the model for 20 epochs with a batch size of 64. We set the temperature τ to 0.05 and use α=0.8 to balance between same- behavior and different-behavior loss. Each positive sample is selected based on a similarity threshold of 0.7, and negatives combine both the same-behavior and top-l (l=10) different-behavior samples via co- sine similarity. All experiments are run on a single https://huggingface.co/google-bert/ bert-base-uncased NVIDIA A100-80G GPU. Baselines We compare our approach with three retrievers: (1) BM25 (Robertson et al., 2009), a traditional probabilistic retrieval model based on term frequency statistics and document length nor- malization; (2) BERT (Devlin et al., 2019), a pre-trained language model where the [CLS] to- ken embedding is for query representation; (3) Contriever (Izacard et al., 2021), a dense trained through contrastive learning in general domain. 4.2 Main Results 4.2.1 Results on H2A Dataset We present our experimental findings in Table 1, comparing the performance of LLMs with 5 demonstrations from three dimensions: Helpful- ness, Harmlessness, and Autonomy. (1) For Helpfulness: Our retrieval-augmented prompt presents higher Exact Function Match ratio than the other baselines, supporting by the consis- tent behavior ratio shown in Table 2. For vanilla models, Mistral-7B-Instruct with BAR achieves a average 4.0% improvement compared to its zero- shot performance. Notably, Llama-2-13B-chat shows remarkable improvement from 26.0% to 34.0% over BM25. For fine-tuned models, Open- functions demonstrates enhancement from 16.5% to 27.5%. However, most fine-tuned LLMs have difficulty to return all necessary APIs, especially for ToolLLaMA and ToolAlign. The reason might be that these LLMs are optimized for function in- vocation, but their over-reliance on fine-tuning data limits generalization, as shown in Table 12. (2) For Harmlessness: Our method presents su- perior ability to refuse harmful instructions. Specif- ically, Openfunctions achieves 86.0% refuse rate In this paper we use the bert-base-uncased version. 5 Models #NoSearchAPI #SearchAPI BM25 BERT Contriever BAR BM25 BERT Contriever BAR Mistral-7B-Instruct 59.6% 50.3% 53.4% 59.1% 79.9% 82.8% 78.6% 82.3% Qwen2.5-7B-Instruct 30.1% 31.1% 40.4% 40.4% 82.3% 83.4% 82.8% 79.5% Llama-2-7B-chat 44.6% 42.5% 42.0% 44.0% 68.9% 70.2% 68.8% 71.2% Llama-2-13B-chat 54.9% 53.9% 52.3% 59.6% 77.8% 78.2% 78.1% 79.2% Llama-3.1-8B-Instruct 63.2% 79.8% 62.7% 64.8% 66.3% 67.9% 69.7% 61.2% Functionary-7B 48.7% 48.2% 50.8% 49.7% 66.1% 72.7% 60.1% 78.3% Openfunctions 60.6% 61.7% 59.6% 56.0% 68.3% 66.2% 68.8% 77.9% Functioncalling-20B 90.2% 89.6% 91.7% 92.2% 61.9% 62.7% 62.3% 62.3% ToolLLama-7B 57.0% 52.3% 53.4% 57.5% 41.6% 40.9% 40.7% 41.6% ToolAlpaca-7B 68.4% 67.4% 67.9% 84.5% 68.8% 69.8% 68.8% 70.5% ToolAlpaca-13B 73.6% 81.9% 81.9% 89.6% 69.0% 72.4% 76.8% 80.2% ToolAlign-DPO 79.3% 87.6% 85.0% 82.4% 80.6% 84.4% 80.3% 80.3% Average 60.8% 62.2% 61.7% 65.0% 69.3% 71.0% 69.6% 72.0% Table 3: The comparison of LLMs decision-making awareness on ToolDEER dataset. We report different retrieval methods (BM25, BERT, Contriever, and BAR) under two scenarios (#NoSearchAPI and #SearchAPI). augmented with BAR, surpassing BM25 by 5.1%. ToolAlpaca-7B shows substantial improvement from 64.4% to 76.7%, as well ToolLLama in- creases from 30.9% to 40.7%. LLMs with BAR reach an average refusal rate of 60.8%, signifi- cantly higher than the zero-shot setting. This out- performance aligns with the retrieval"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 7, "text": "and #SearchAPI). augmented with BAR, surpassing BM25 by 5.1%. ToolAlpaca-7B shows substantial improvement from 64.4% to 76.7%, as well ToolLLama in- creases from 30.9% to 40.7%. LLMs with BAR reach an average refusal rate of 60.8%, signifi- cantly higher than the zero-shot setting. This out- performance aligns with the retrieval distribution in Table 2, indicating that more relevant demonstra- tions contribute to stronger safety awareness. (3) For Autonomy: BAR increases direct re- sponse rate of LLMs, significantly reducing re- dundant API calls. The most notable improve- ments appear in fine-tuned models, where substan- tial gains ranging from 14%-15% for ToolAlign and ToolAlpaca-13B compared to BM25. Func- tionCalling shows remarkable enhancement from 70.0% to 86.0% in direct response rate. Although our methods have strong performance with large models, both Mistral-7B and Llama-7B show lim- ited improvement, which indicates their inherent constrains in tool usage domain. Overall, our ap- proach achieves an 8.5% improvement over base- lines. It aligns with Table 2, where our method retrieves 471 autonomy instructions, a 71.9% in- crease over BM25 (274). The consistent improve- ments among different LLMs confirm its generaliz- ability in enhancing function-calling ability. 4.2.2 Results on ToolDEER Dataset Following ToolDEER (Gui et al., 2024), we select 6 demonstrations within in-context learning. The experimental results presented in Table 3 demon- strate the superior performance of BAR in distin- Query Type CE InfoNCE SCL Triplet DNCL Helpfulness 95.6% 96.4% 96.8% 95.7% 97.6% Harmlessness 68.8% 68.4% 65.6% 65.7% 70.5% Autonomy 93.0% 92.8% 90.2% 82.0% 94.2% Table 4: Behavior-consistency ratio of BAR trained with different loss functions on H2A dataset. Figure 3: Behavior-consistency ratio of BM25, BERT, Contriever, and BAR on ToolDEER dataset. guishing between #NoSearchAPI and #SearchAPI. Using retrieval-augmented in-context learning to enhance prompt relevance, our method achieves significant improvements over both vanilla and tool- augmented models. In the #NoSearchAPI scenario, BAR achieves 95.9% behavior-consistency ratio on the retrieval task with top-6 similar queries (Figure 3), surpass- ing both BM25(81.0%) and Contriever (85.5%). The effectiveness of BAR is particularly evident in the case of ToolAlpaca-13B, demonstrating a 16.0% improvement over BM25, as shown in Ta- 6 Query Type 80% 85% 90% 95% 100% Helpfulness 97.0% 96.2% 96.3% 96.8% 97.6% Harmlessness 66.2% 65.1% 65.4% 68.9% 70.5% Autonomy 93.0% 92.8% 93.6% 92.6% 94.2% Table 5: Impact of training data scaling on behavior consistency ratio on the H2A dataset. ble 3. Regarding the #SearchAPI scenario, BAR maintains the consistent advantages. Notably, Functionary-7B with BAR exhibits stronger dis- criminative capabilities, highlighting its enhanced behavioral consistency in function call necessities detection. 4.3 Ablation Study Impact of different loss function Table 4 presents the ablation study of our retriever using differ- ent loss functions for demonstration retrieval. : Cross-Entropy loss (CE), InfoNCE loss, Super- vised Contrastive Loss (SCL) (Khosla et al., 2020), Triplet loss (Schroff et al., 2015), and our Dual- negative Contrastive Loss (DNCL). BAR trained with DNCL achieves optimal overall robustness, attaining 97.6% on helpfulness and 94.2% on au- tonomy, surpassing CE by 2.0% and 1.2%, re- spectively. For Harmlessness instructions, DNCL maintains stronger behavior consistency, and high- lights the advantage of contrastive"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 8, "text": "et al., 2015), and our Dual- negative Contrastive Loss (DNCL). BAR trained with DNCL achieves optimal overall robustness, attaining 97.6% on helpfulness and 94.2% on au- tonomy, surpassing CE by 2.0% and 1.2%, re- spectively. For Harmlessness instructions, DNCL maintains stronger behavior consistency, and high- lights the advantage of contrastive learning in cap- turing fine-grained semantic differences. Model trained with SCL shows a decline in results due to its focus on class-level rather than instance-level contrasts. The Triplet loss model underperforms DNCL by 16.3% on autonomy decisions, high- lighting its limitations for representation learning. This confirms DNCL’s superiority in distinguishing function-calling behaviors for reliable retrieval. Impact of negative pairs As illustrated in Table 6, We evaluate three negative sampling strategies to validate our dual-negative design: (1) Random Different-Behavior: randomly sampling queries with different behaviors; (2) Top-l Different- Behavior: selecting semantically similar queries but with different behaviors; (3) Dual-Behavior: combining both behavior-consistent and behavior- different negative samples. Random sampling achieves a baseline accuracy of 85.6%. By in- troducing the top-l samples with different behav- ior significantly improves performance to 92.3%, indicating that challenging negative examples en- hance the model to make fine-grained distinctions. The dual-behavior strategy further enhances con- Negative Sampling Strategy Behavior-Consistency Ratio Random Different Behavior 85.6% Top-l Different Behavior 92.3% Dual-Behavior 94.2% Table 6: Ablation study on negative sampling strategies for function retrieval performance on Autonomy query. sistency ratio to 94.2% by combining behavior- consistent and behavior-different negatives, demon- strating the effectiveness of both negative types. Analysis of training data scaling effects To evalu- ate whether our retriever has been trained on a suf- ficiently large dataset, we progressively increased the size of the training corpus from 80% to 100% and measured behavior consistency ratio on the H2A dataset. The results are shown in Table 5. We observe that while retrieval performance improves with more data, the gains plateau after 90–95%, es- pecially for Helpfulness and Autonomy, where con- sistency improves by only 0.8 percentage points or less from 95% to 100%. This trend suggests that be- havioral coverage is largely saturated for these cate- gories, meaning that BAR has already captured the key decision patterns needed for behavior-aligned matching. Although Harmlessness shows slightly larger gains, this is likely due to the inherently noisier and more subjective nature of such queries. This trend suggests that adding more data beyond 90% yields very limited improvement, meaning the current dataset already captures the key behavior patterns needed for stable and accurate retrieval. In practice, further increasing the dataset would incur significant cost without meaningful performance improvement, and our existing corpus strikes a strong balance between coverage, efficiency, and retriever generalization. 4.4 Visualization We analyze retriever effectiveness in semantic rep- resentation learning through t-SNE (Wang and Isola, 2020) visualization on both H2A and ToolD- EER datasets, as shown in Figure 4 and Figure 5. According to Figure 4, we observe that BM25, BERT, and Contriever show scattered distributions with significant overlap between different cate- gories, BAR provides better separation, particu- larly for autonomy instructions, which form a well- separated cluster. However, Helpfulness and Harm-"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 9, "text": "EER datasets, as shown in Figure 4 and Figure 5. According to Figure 4, we observe that BM25, BERT, and Contriever show scattered distributions with significant overlap between different cate- gories, BAR provides better separation, particu- larly for autonomy instructions, which form a well- separated cluster. However, Helpfulness and Harm- lessness queries are hard to separate because they use a similar language, but Harmlessness cases involve unsafe or insecure tools, making the behav- 7 Figure 4: The t-SNE plots of the learned representations with different retrievers on the H2A dataset. Figure 5: The t-SNE plots of the learned representations with different retrievers on the ToolDEER dataset. ioral difference subtle and easy to confuse. Figure 5 on the ToolDEER dataset more clearly illustrates the advantages of BAR. BM25 and Con- triever exhibit substantial overlap in their distribu- tions. BERT improved, but insufficiently discrim- inates between #SearchAPI and #NoSearchAPI categories. In contrast, BAR illustrates a clearer separation, with #NoSearchAPI samples distinctly clustering in the lower-right region. These visu- alizations demonstrate BAR’s stronger ability to capture semantics and enhance in-context learning. 5 Related Work Function Calling by LLMs Recent advances in large language models (LLMs) have signifi- cantly enhanced their ability to perform function calling. Existing approaches broadly fall into two categories: pretrained models with function- calling capabilities (e.g., Functionary (MeetKai, 2024)) and fine-tuned models through supervised and preference learning (e.g., ToolAlpaca (Tang et al., 2023), ToolLLaMA (Qin et al., 2023) and ToolAlign (Chen et al., 2024)). Building on retrieval-augmented generation methods (Wu et al., 2024b; Zhao et al., 2024; Wu et al., 2024a), re- cent studies adapt retrieval to the function-call setting and introduce tool-augmented LLMs (Xu et al., 2024; Qu et al., 2024; Qin et al., 2023). Tool-augmented LLMs interact with users by un- derstanding their intents and needs, concentrat- ing on retrieving recommended tools from a large pool (Xu et al., 2024; Qu et al., 2024; Qin et al., 2023). Although these approaches improve the ac- curacy of function selection and parameter match- ing, they don’t address the challenge of discriminat- ing between queries that require function calls and those that don’t. In contrast, our approach targets at enhancing LLMs’ capability to make decisions on whether or not to use tools. Contrastive Learning Contrastive learning has demonstrated remarkable success with its core ob- jective centered on optimizing feature spaces by leveraging similarities and differences between samples. Self-supervised contrastive learning (He et al., 2020) pioneered the learning of generic rep- resentations from unlabeled data, while supervised contrastive learning (SCL) (Khosla et al., 2020) further enhanced feature discriminability by explic- itly incorporating label information. The key to contrastive learning is how to construct negative samples. Unlike methods (Robinson et al., 2021; Kalantidis et al., 2020) that only consider intra- class negatives, we incorporate inter-class hard neg- atives. This helps LLMs better distinguish instruc- tions with similar semantics but distinct behaviors. 6 Conclusion In this paper, we trained a behavior-aligned re- triever, called BAR, which retrieves semantically coherent and behaviorally related examples as demonstrations for LLMs. Extensive experiments demonstrate that our BAR can consistently guide"}
{"doc_id": "2508.14323v1", "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14323v1", "chunk_id": 10, "text": "hard neg- atives. This helps LLMs better distinguish instruc- tions with similar semantics but distinct behaviors. 6 Conclusion In this paper, we trained a behavior-aligned re- triever, called BAR, which retrieves semantically coherent and behaviorally related examples as demonstrations for LLMs. Extensive experiments demonstrate that our BAR can consistently guide different LLMs to make correct function-calling decisions and reduce unnecessary function calls. 8 Limitations The first limitation is the difficulty in separat- ing risk-aligned behaviors. Both Helpfulness and Harmfulness queries tend to require API calls, but Harmfulness involves requests to unsafe, unethi- cal, or illegal tools (e.g., insecure APIs). Since their language forms are often similar, BAR strug- gles to distinguish them based on behavioral su- pervision alone. Addressing this limitation would require training on more fine-grained, risk-aware annotations that go beyond binary call/no-call la- bels. The second is the dependence on the la- beled behavioral data. Training the retriever re- quires function-call datasets and general question answering datasets. Although lightweight com- pared to LLM fine-tuning, the process still depends on domain-specific labels, which may not be read- ily available for all tools or APIs. Ethical Consideration Our work evaluates LLM’s safety in function call- ing using harmful instructions on publicly ava- iable H2A dataset (Chen et al., 2024), which are modified from helpful instruction and rigorously anonymized to avoid real-world harm. They follow the safetyLLaMA (Bianchi et al., 2023) approach to sample instructions. Besides, our experiments focus on enhancing LLM’s ability to reject unsafe inputs by retrieving demonstrations that reinforce ethical response. Moreover, we provide API lists in prompts to simulate real-world API call scenarios, which prevents potential unsafe API calling."}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 0, "text": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing Jing Chen1,2* , Zhiheng Yang1*, Yixian Shen1, Jie Liu1, Adam Belloum1, Chrysa Papagianni1, Paola Grosso1 1University of Amsterdam, the Netherlands 2Vrije Universiteit Amsterdam, the Netherlands j.chen12@student.vu.nl {z.yang, y.shen, j.liu, a.s.z.belloum, c.papagianni, p.grosso}@uva.nl Abstract Survey papers play a critical role in scien- tific communication by consolidating progress across a field. Recent advances in Large Lan- guage Models (LLMs) offer a promising so- lution by automating key steps in the survey- generation pipeline, such as retrieval, struc- turing, and summarization. However, ex- isting LLM-based approaches often struggle with maintaining coherence across long, multi- section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to- fine retrieval, adaptive planning, and memory- guided generation. SurveyGen-I first performs survey-level retrieval to construct the initial outline and writing plan, and then dynami- cally refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring co- herence across subsections. When the sys- tem detects insufficient context, it triggers fine- grained subsection-level retrieval. During gen- eration, SurveyGen-I leverages this memory mechanism to maintain coherence across sub- sections. Experiments across four scientific domains demonstrate that SurveyGen-I con- sistently outperforms previous works in con- tent quality, consistency, and citation cover- age. The code is available at https://github. com/SurveyGens/SurveyGen-I. 1 Introduction The exponential expansion of scholarly literature, with thousands of new papers published daily, presents significant challenges for researchers to efficiently acquire and synthesize comprehensive knowledge. Consequently, writing survey papers requires substantial expertise and time commitment from researchers, as it traditionally involves an iter- ative and labor-intensive process of reading, note- taking, clustering, and synthesis (Carrera-Rivera * Equal contribution. et al., 2022). Recent advances in Large Language Models (LLMs) offer a promising solution to this bottleneck by enabling the automation of key steps in the survey-writing pipeline, such as retrieving, organizing, and summarizing large volumes of pa- pers (Wang et al., 2024; Liang et al., 2025; Yan et al., 2025; Agarwal et al., 2024a,b). Despite recent advances, current LLM-based sur- vey generation frameworks remain limited in sev- eral key aspects. First, literature retrieval scope and depth remain limited. Most systems rely on embedding-based similarity search over a fixed lo- cal paper database (Wang et al., 2024; Yan et al., 2025). While efficient, such surface-level match- ing often fails to identify important papers with different terminology or at a more conceptual level, resulting in incomplete or biased coverage. Sec- ond, lack of cross-subsection consistency. Most systems generate all subsections in parallel as iso- lated units without modeling dependencies across subsections (Wang et al., 2024; Liang et al., 2025; Yan et al., 2025). This often leads to redundant content, inconsistent terminology, and fragmented discourse. Moreover, they always follow a static, once-for-all outline that cannot adapt to newly gen- erated content, making it difficult to maintain con- tent coherence or integrate emerging insights. Fi- nally, indirect citations are often left unresolved. Retrieval-augmented generation (RAG) typically extracts passages from retrieved papers to support writing. These passages often include indirect cita-"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 1, "text": "a static, once-for-all outline that cannot adapt to newly gen- erated content, making it difficult to maintain con- tent coherence or integrate emerging insights. Fi- nally, indirect citations are often left unresolved. Retrieval-augmented generation (RAG) typically extracts passages from retrieved papers to support writing. These passages often include indirect cita- tions such as \"[23]\" and \"Smith et al., 2022\", which refer to influential prior work not present in the retrieval results. Without tracing these references, the system may miss influential papers, leading to incomplete citation coverage and broken linkage between ideas and their original sources. To address these limitations, we introduce SurveyGen-I, an end-to-end, modular framework for generating academic surveys with consistent content and comprehensive literature coverage. 1 First, SurveyGen-I performs literature retrieval at both the survey-level and subsection-level, aug- mented with citation expansion and LLM-based relevance scoring. This multi-level retrieval strat- egy substantially enhances literature coverage and topical relevance. Second, SurveyGen-I introduces PlanEvo, a dynamic planning mechanism powered by an evolving memory that continuously accumu- lates terminology and content from earlier gener- ated subsections. This memory is used to construct the outline and a dependency-aware writing plan that captures the logical and conceptual relation- ships between subsections, allowing foundational topics to be generated before more advanced or derivative ones. As writing progresses, both the outline and plan are continually refined based on the updated memory, ensuring consistent terminol- ogy and coherent content flow across the survey. Finally, SurveyGen-I introduces CaM-Writing, which combines a citation-tracing module that de- tects indirect references in retrieved passages and resolves them back to their original source papers, with memory-guided generation that uses the evolv- ing memory to maintain coherent terminology and content across the survey. Extensive results highlight the strengths of SurveyGen-I across multiple dimensions of aca- demic survey generation. Compared to the strongest baseline, SurveyGen-I yields an 8.5% im- provement in content quality, a 27% increase in citation density, and more than twice as many dis- tinct references, while also demonstrating signifi- cantly better citation recency. These improvements show the effectiveness of the system in enabling high-quality and consistent survey generation. Our contributions are summarized as follows: • We propose SurveyGen-I, a novel framework for high-quality, reference-rich, and consistent survey generation. • We design a multi-stage Literature Retrieval pipeline that combines keyword search, cita- tion expansion, and LLM-based filtering to construct relevant and comprehensive paper sets at both survey and subsection levels. • We introduce PlanEvo, a dynamic planning mechanism that constructs and continuously refines the outline and writing plan based on inter-subsection dependencies and evolving memory, ensuring coherent survey generation. • We develop a CaM-Writing pipeline that combines citation tracing and memory-guided generation to improve reference coverage and ensure consistent, well-structured writing. 2 Related work Component-Oriented and Hybrid Approaches. A longstanding approach to assisting literature sur- veys has been to tackle the problem in stages, where components handle retrieval, structuring, or writ- ing, etc., independently (Susnjak et al., 2025; Lai et al., 2024; Li et al., 2024c). Early systems orga- nized citation sentences through clustering or clas- sification (Nanba et al., 2000; Wang et"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 2, "text": "literature sur- veys has been to tackle the problem in stages, where components handle retrieval, structuring, or writ- ing, etc., independently (Susnjak et al., 2025; Lai et al., 2024; Li et al., 2024c). Early systems orga- nized citation sentences through clustering or clas- sification (Nanba et al., 2000; Wang et al., 2018), or employed rule-based content models (Hoang and Kan, 2010; Hu and Wan, 2014). These systems of- ten relied on static heuristics or surface-level topic associations, making them difficult to generalize across domains or maintain narrative coherence. The rise of LLMs brought a wave of hybrid de- signs that integrated neural summarization with structured control (Zhang et al., 2024; Fok et al., 2025; An et al., 2024). Template-based gen- eration (Sun and Zhuge, 2019) and extractive- abstractive hybrids (Shinde et al., 2022) introduced more fluent synthesis but retain rigid structures. Meanwhile, RAG-based methods (Lewis et al., 2020; Ali et al., 2024; Agarwal et al., 2024a) en- hanced retrieval fidelity (Gao et al., 2023), and agent-driven systems like the framework proposed by Brett and Myatt (2025), RAAI (Pozzobon and Pinheiro, 2024) and AutoSurveyGPT (Xiao, 2023) broke down the pipeline into retrieval, filtration, and generation stages. More recent works empha- size pre-writing planning, such as COI-Agent (Li et al., 2024b), which organizes references into con- ceptual chains to enhance topic coverage. However, these designs remain fundamentally decomposed: content selection and writing are planned in isolation, and their execution often lacks global coordination across stages. End-to-End Automated Literature Review/Sur- vey Generation. With increasing demand for scalability and consistency, end-to-end frame- works have emerged to streamline the full pipeline from retrieval to synthesis. Multi-agent architec- tures (Sami et al., 2024; Rouzrokh and Shariat- nia, 2025) have been wildly used, and decom- pose the pipeline into specialized agent roles, mim- icking human editorial workflows. Representa- tively, AutoSurvey (Wang et al., 2024) introduces a retrieval-outline-generation sequence that produces entire surveys via section-wise prompting. Survey- Forge (Yan et al., 2025) extends this with memory 2 Final Refinement Current Stage: (Next Stage: ) SAWC: Structure-Aware Writing Controller CaM-Writing LR: Literature Retrieval (Survey) SDP: Structure-Driven Planner Subsection : Subsection : Dispatch all LR: Literature Retrieval (Subsection) LR: Literature Retrieval (Subsection) If If MGSR Trigger replan ( ) replan ( ) memory Memory If Subsection : Final Survey Table Generation Input Update Memory: 1 2 3 4 5 6 7 9 10 All subsections in Stage t finished 8 0 11 12 13 Loop until all stages complete 3 10 Legend: : Subsection description : Retrieval flag : Table flag : Stage index (parallel writing) : Structure memory (drafts, terminology) Key: • Subsections with same are processed in parallel per stage. • Steps 3–10 repeat until all stages complete. • Memory, outline and plan dynamically updated after each stage. • Table generation/global refinement triggered after writing finishes. Best-of-N Drafting & Multi-Stage Refinement Citation Traced Retrieval (RAG + Citation Trace) Memory-Guided Skeleton Gen Figure 1: Overview of the SurveyGen-I pipeline for automatic academic survey generation. The system comprises three stages: (1) multi-stage Literature Retrieval (LR); (2) PlanEvo, a structure planning module integrating"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 3, "text": "generation/global refinement triggered after writing finishes. Best-of-N Drafting & Multi-Stage Refinement Citation Traced Retrieval (RAG + Citation Trace) Memory-Guided Skeleton Gen Figure 1: Overview of the SurveyGen-I pipeline for automatic academic survey generation. The system comprises three stages: (1) multi-stage Literature Retrieval (LR); (2) PlanEvo, a structure planning module integrating SDP (planning), SAWC (scheduling), and MGSR (dynamic replanning); (3) CaM-Writing for citation-aware subsection generation. Final refinement and table generation are performed after writing. Memory M accumulates writing content and terminology across stages to guide planning and ensure consistency. modules and outline heuristics, aiming to enforce consistency across segments. SurveyX (Liang et al., 2025) scales this further by relying on larger mod- els and more complex pipeline, producing more robust and strict step-by-step outputs. Despite these advances, many systems adopt a static and compartmentalized approach. Outlines are typically fixed in advance, with no capacity to revise structure based on intermediate content. Subsections are often generated in parallel, lack- ing shared context, which weakens narrative flow and increases repetition or terminology drift. Cita- tion usage also remains surface-level: references are selected from top-ranked snippets without trac- ing citation chains or enabling expansion (Kasan- ishi et al., 2023). In response, our work views survey writing as a dynamic process, one that re- quires adaptive planning, context-aware memory, and citation-traced retrieval. By continuously re- fining structural plans, maintaining cross-section consistency, and grounding generation in citation chains, we move toward more coherent and adap- tive scientific surveys. 3 Methodology In this section, we propose SurveyGen-I, a novel framework for automatic survey generation. As shown in Figure 1, it consists of three key stages: (1) Literature Retrieval (LR) performs multi- stage literature retrieval at both survey and subsec- tion levels. (2) Structure Planning with Dynamic Outline Evolution (PlanEvo) generates a hierar- chical outline and a dependency-aware writing plan, and dynamically updates both during generation to ensure cross-subsection consistency of the sur- vey. (3) CaM-Writing generates each subsection with strong content consistency and rich citation coverage, combining citation-traced retrieval to re- cover influential references, memory-guided skele- ton planning for content consistency, and best-of-N draft selection to ensure high-quality generation. 3.1 LR: Literature Retrieval To ensure that the generated survey is grounded with the most relevant and comprehensive research, our system adopts a multi-stage literature retrieval strategy that operates at both the survey and sub- section levels. As shown in Figure 1, this retrieval 3 SDP: Structure-Driven Planner LR: Literature Retrieval Keyword Gen Search Semantic Filter Citation Expansion LLM Relevence Rerank Subsection Input: Subsection Literature Set Step 2: Dependency-Aware Writing Plan Input Papers: Extract Review Papers Topic Input: T, E Step 1: Reference-Grounded Outline Generation Extract Non-Review Papers Generate Outline Refine Outline Outline (Example) 1. Intro 1.1 History 1.2 Motivation ... 2. Theory Global Literature Set Same Pipeline Initial Plan Example for Subsection Title : Motivation Subsection description : ... Generate Table : False subsection Retrieval : True Remove Cycles Dependency (Example) A: depends on B B: depends on C C: depends on D D: depends on B Identify Dependency A B C D A B C D DAG Final"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 4, "text": "Example for Subsection Title : Motivation Subsection description : ... Generate Table : False subsection Retrieval : True Remove Cycles Dependency (Example) A: depends on B B: depends on C C: depends on D D: depends on B Identify Dependency A B C D A B C D DAG Final Plan Example for Subsection Title : Motivation Subsection description : ... Generate Table : False subsection Retrieval : True Stage Index τ( ): 1 Initial Plan Input Outline: Topological Sort and Final Plan Figure 2: Details of the Literature Retrieval and Structure-Driven Planner components in SurveyGen-I. process provides the reference foundation for both structure planning (SDP; see Sec. 3.2.1) and writ- ing (CaM-Writing; see Sec. 3.3). The overall work- flow is shown in Figure 2; implementation details are provided in Appendix B.1. 3.1.1 Survey-Level Retrieval for Structure Planning For survey-level literature retrieval, an LLM is first prompted to generate a keyword set K based on the input topic T and its description E (see prompt in Figure 9, Appendix G). These keywords are used to query Semantic Scholar (Ammar et al., 2018), pro- ducing an initial set of papers Pinit. While keyword- based retrieval offers broad initial coverage, it may include irrelevant papers. To enhance topical pre- cision, a semantic filtering step is applied. Specifi- cally, both the input (T, E) and each paper abstract ai are embedded using the all-mpnet-base-v2 sentence transformer (Song et al., 2020). Candi- date papers with high cosine similarity to the input (T, E) are retained, yielding a refined set Psem: Psem = {pi ∈Pinit | cos(eT,E, eai) ≥θ}. (1) To improve coverage and avoid missing influen- tial work, Psem is expanded by retrieving references and citations of top-ranked papers. These expanded papers are again filtered by embedding similarity for topical relevance. Finally, an LLM-based rele- vance scorer assesses all remaining papers with re- spect to (T, E), and the top-ranked literature P∗is retained to support outline generation (see prompt template in Figure 11, Appendix G). 3.1.2 Subsection-Level Retrieval for Writing In addition to survey-level retrieval for structure planning, subsection-level retrieval is optionally triggered during writing. For each subsection si with its description di, a focused paper set Pi is constructed using the same retrieval pipeline as above, with (si, di) as input. Whether this step is performed is controlled by a retrieval flag ri in the dependency-aware writing plan (see Sec. 3.2.1). The final paper set used for writing each subsection is the combination of the survey-level set P∗and the subsection-level set Pi, forming the combined paper set P∗ i . This paper set captures both the global scope of the survey and the specific focus of each subsection. 3.2 PlanEvo: Structure Planning with Dynamic Outline Evolution In this section, we introduce PlanEvo, a planning- centric framework for scalable and coherent survey outline generation and writing plan construction. PlanEvo consists of three tightly integrated com- ponents: the Structure-Driven Planner (SDP), the Structure-Aware Writing Controller (SAWC), and the Memory-Guided Structure Replanner (MGSR). Detailed designs for each component are presented in Sec. 3.2.1, Sec. 3.2.2, and Sec. 3.2.3."}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 5, "text": "framework for scalable and coherent survey outline generation and writing plan construction. PlanEvo consists of three tightly integrated com- ponents: the Structure-Driven Planner (SDP), the Structure-Aware Writing Controller (SAWC), and the Memory-Guided Structure Replanner (MGSR). Detailed designs for each component are presented in Sec. 3.2.1, Sec. 3.2.2, and Sec. 3.2.3. 3.2.1 SDP: Structure-Driven Planner The SDP module serves as the entry point of PlanEvo, transforming a specific research topic (T, E) into a structured, executable plan that guides 4 the full survey generation process. The overall workflow is shown in Figure 2. Reference-Grounded Outline Generation. A literature-grounded outline is essential for generat- ing a coherent and well-structured survey. To build such an outline, the system first identifies review articles within the survey-level literature set P∗by analyzing metadata such as publication type. The structural outlines of these reviews R are then ex- tracted from their full texts using LLMs and used as representative structural patterns to inspire the design of new outlines. The system then collects titles and abstracts of non-review papers in P∗to form the abstract-level content set Cabs, which is then combined with R into a composite context C. Given C and (T, E), an LLM is prompted to generate an initial outline O0: O0 = {(si, di)}N i=1 (2) where each subsection heading si is paired with a brief description di to provide more detailed guid- ance for writing subsections. To improve coher- ence and reduce redundancy, the initial outline O0 is refined by an LLM, yielding the final outline O. Dependency-Aware Writing Plan. To enable logically coherent and coordinated writing across subsections, we construct a dependency-aware writ- ing plan Pdep based on the survey outline O. First, an initial plan Praw is generated by prompt- ing an LLM with O. For each subsection si with its description di, the plan specifies two control signals: whether additional literature retrieval is re- quired (ri), and whether a comparative table should be generated (ti). These signals guide downstream tasks in subsection-level literature retrieval and ta- ble generation. Each subsection takes the form: Praw[si] = (di, ri, ti), (3) Next, a structural dependency graph Graw = (V, E) is constructed by prompting an LLM to iden- tify, for each subsection, its prerequisite subsec- tions (see prompt in Figure 13). Here, V includes all subsections s1, . . . , sn, and E contains edges (si →sj) if si is judged to be a prerequisite of sj. Cycles are resolved by removing one edge per cycle, yielding a Directed Acyclic Graph (DAG) G. A topological sort is then applied to G to deter- mine the writing order. Each subsection s is as- signed a stage index τ(s), representing the length of the longest path ending at s in the DAG: τ(s) =    0 if In(s) = ∅, max s′∈In(s) τ(s′) + 1 otherwise. (4) The final plan Pdep extends Praw by attaching stage indices to each subsection: Pdep[si] = (di, ri, ti, τ(si)). (5) Subsections assigned the same stage index can be written in parallel, enabling multi-stage scheduling. 3.2.2 SAWC: Structure-Aware"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 6, "text": " 0 if In(s) = ∅, max s′∈In(s) τ(s′) + 1 otherwise. (4) The final plan Pdep extends Praw by attaching stage indices to each subsection: Pdep[si] = (di, ri, ti, τ(si)). (5) Subsections assigned the same stage index can be written in parallel, enabling multi-stage scheduling. 3.2.2 SAWC: Structure-Aware Writing Controller The SAWC module serves as the central orchestra- tion engine across the entire writing process in the SurveyGen-I pipeline. Rather than being a single step, SAWC coordinates a sequence of interdepen- dent modules, including writing stage scheduling (Step 4), subsection-level literature retrieval (Step 5), citation-aware writing (Step 6), memory updat- ing (Step 7), dynamic structure replanning (Steps 9–10), global consistency refinement (Step 11), and table generation (Step 12). Its control flow is illus- trated throughout the center path of Figure 1. Parallel Subsection Execution. SAWC executes the dependency-aware writing plan Pdep by acti- vating all subsections with the same writing stage index τ(si) in parallel (Step 4). For each active sub- section si, SAWC first checks the retrieval control flag ri in Pdep[si]. If retrieval is required, SAWC triggers subsection-level literature retrieval (Step 5; see also Sec. 3.1.2). The resulting paper set P∗ i is passed to the writing module (Step 6; see Sec. 3.3) for citation-aware subsection generation. Memory Mechanism for Global Consistency. To ensure structural coherence and terminologi- cal consistency across the survey, SAWC maintains a dynamic structure memory M throughout writ- ing. After each subsection si is written (Step 6), the system extracts key domain-specific terminology using LLMs, and stores both the terminology and the draft content into M (Step 7). This accumu- lated memory is then used to (1) guide subsequent subsection writing by enforcing consistency (see Sec. 3.3.2), and (2) provide feedback for dynamic updates of the outline O and writing plan Pdep dur- ing structure replanning (see Sec. 3.2.3). 5 Dynamic Structure Refinement. At the end of each writing stage, which corresponds to the com- pletion of all subsections si with the same stage index τ(si), SAWC triggers the Memory-Guided Structure Replanner (MGSR; see Sec. 3.2.3) to re- vise the outline and the writing plan based on the accumulated memory M (Step 9–10). This stage- wise feedback loop ensures that structural adjust- ments are continuously informed by prior writing outputs before the next stage begins. Final Refinement and Table Generation. Af- ter all subsections are written, SAWC performs a final refinement step to improve global coherence (Step 11). An LLM analyzes the full draft to detect logical contradictions, redundancy, and terminolog- ical/style inconsistencies. Based on this diagnosis, the system rewrites affected subsections to ensure consistency. Then, for each subsection with the table flag ti enabled in Pdep[si], SAWC generates a structured table based on the retrieved paper set (Step 12). See Appendix B.3 for details. 3.2.3 MGSR: Memory-Guided Structure Replanner After each writing stage, the MGSR module per- forms dynamic refinement of the outline and writ- ing plan based on the accumulated memory M and the current outline O. MGSR prompt an LLM (see prompt in Figure 14) to analyze redundancy, miss- ing conceptual"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 7, "text": "details. 3.2.3 MGSR: Memory-Guided Structure Replanner After each writing stage, the MGSR module per- forms dynamic refinement of the outline and writ- ing plan based on the accumulated memory M and the current outline O. MGSR prompt an LLM (see prompt in Figure 14) to analyze redundancy, miss- ing conceptual gaps, or suboptimal ordering within the unwritten subsections. It produces a set of structured revision actions (merge, delete, rename, reorder, add) applied to the remaining outline. The updated writing plan P′ dep is then derived from the revised outline O′ through the same dependency- aware planning method used in the initial plan Pdep (see Sec. 3.2.1). This enables memory-guided struc- tural evolution throughout writing, ensuring that later sections are adaptively optimized based on prior content while maintaining global consistency. 3.3 CaM-Writing: Citation-Aware Subsection Writing with Memory Guidance This section introduces CaM-Writing, a citation- aware, memory-guided writing pipeline for generat- ing each survey subsection. The pipeline integrates citation-traced retrieval to enhance literature cov- erage and citation diversity, skeleton-based genera- tion guided by accumulated memory M to ensure content consistency, and multi-stage refinement to improve clarity, coherence, and citation integrity. 3.3.1 Context Construction with Citation Tracing To construct a rich and contextually relevant ev- idence set for writing each subsection si with description di, a RAG step is first applied over the contextual paper set P∗ i , which includes both survey-level and subsection-specific literature. Top- ranked passages are selected to form the initial writ- ing context Crag,i. However, the retrieved passages from academic papers often contain indirect cita- tions such as \"[23]\" and \"Ge et al., 2023\". These ci- tations typically refer to influential prior work that is not directly included in the retrieved documents. If the system relies solely on these secondary men- tions without further resolution, it may overlook foundational or highly relevant papers. To address this, we introduce a citation-tracing mechanism that identifies such citations in Crag,i and uses an LLM to determine whether each refers to an original source of a key concept or result. Traceable citations are resolved via the Semantic Scholar API, and their abstracts are appended to the base context Crag,i, forming the citation-enriched context Cenrich,i. To maintain traceability, each en- riched abstract is linked back to the original pas- sage that cited it, allowing the writing model to understand the relationship between the mention and its source. For example, consider the passage: “... recent work has introduced reward- balanced fine-tuning for alignment (Ge et al., 2023), showing improvements over DPO and RLHF ...” The LLM flags “(Ge et al., 2023)” as traceworthy, identifying it as introducing a core method. The system then resolves it to: Title: Preserve Your Own Correlation: A New Reward-Balanced Fine-Tuning Method Abstract: ‘We introduce a reward- balanced fine-tuning (RBF) framework for language model alignment...” This abstract is appended to the context, enabling the system to cite this traced paper directly in the generation. Further implementation details are pro- vided in Appendix B.4. 3.3.2 Memory-Aligned Skeleton-Guided Generation Given the enriched context Cenrich,i, subsection title and description (ti, di), and the accumulated struc-"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 8, "text": "language model alignment...” This abstract is appended to the context, enabling the system to cite this traced paper directly in the generation. Further implementation details are pro- vided in Appendix B.4. 3.3.2 Memory-Aligned Skeleton-Guided Generation Given the enriched context Cenrich,i, subsection title and description (ti, di), and the accumulated struc- 6 ture memory M, the system first uses an LLM (see prompt in Figure 12) to generate a writing skele- ton Si outlining the key conceptual points. The memory M, which includes prior subsections and extracted terminology, ensures content coherence and terminology consistency across the survey. Best-of-N Selection. N candidate drafts are first generated based on the subsection title si, descrip- tion di, writing skeleton Si, and enriched context Cenrich,i (see prompt in Figure 15). An LLM then evaluates the candidates and selects the best version based on alignment with the skeleton, contextual relevance, and overall writing quality. Subsection-Level Refinement. To further im- prove the selected draft, a three-stage refinement is applied. First, the structure is adjusted to better reflect the conceptual flow defined by Si. Second, the draft undergoes citation refinement, where the LLM rewrites the text based on Cenrich,i. Finally, the draft is polished to enhance fluency and clarity. 4 Experiments and Results 4.1 Evaluation Setup. We compare SurveyGen-I with three representative baselines: AutoSurvey (Wang et al., 2024), Survey- Forge (Yan et al., 2025), and SurveyX (Liang et al., 2025). We collect demo reports from SurveyForge and SurveyX official project pages. For AutoSur- vey, we generate reports on matched topics for a fair comparison and used the same model used in SurveyGen-I, with GPT4o-mini (OpenAI, 2024). We also construct a new benchmark covering six major scientific domains, each with ∼30 subtopics. 4.2 Evaluation Metrics We comprehensively evaluate SurveyGen-I against three competitive baselines across two core dimen- sions, content quality and reference quality. Content Quality Evaluation. Measures the structural and semantic strength of the generated survey. This includes five sub-dimensions: cov- erage, relevance, structure, synthesis, and consis- tency. Each aspect is scored by LLM-as-Judge (Li et al., 2024a) (specifically, rated by GPT4o-mini), with explanation-based prompts to reduce variance. This directly reflects the impact of our MGSR and CaM-Writing, which aim to improve global coher- ence, abstraction, and flow. Evaluation criteria can be found in the appendix G. We compute the final content quality score (CQS) as the average of five evaluation dimensions. Reference Quality Evaluation. To assess the ef- fectiveness and recency of reference usage in the generated survey, we adopt three reference-level metrics that reflect citation coverage, intensity, and timeliness. The Number of References (NR) counts the distinct cited works, measuring the breadth of literature coverage. The Citation Density (CD) computes the number of unique citation markers per character of text (excluding the reference sec- tion), reflecting how frequently references are inte- grated into the main narrative. For reporting clarity, we scale CD by a factor of 104. The Recency Ratio (RR@k) measures the proportion of all cited ref- erences that were published within a recent time window (e.g., within the past k=3 years). A higher RR indicates better engagement"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 9, "text": "are inte- grated into the main narrative. For reporting clarity, we scale CD by a factor of 104. The Recency Ratio (RR@k) measures the proportion of all cited ref- erences that were published within a recent time window (e.g., within the past k=3 years). A higher RR indicates better engagement with the latest de- velopments in the field, and reflects the model’s ability to retrieve and integrate timely literature. 4.3 Main Results We report evaluation results across content qual- ity and reference behavior, along with an ablation- based component analysis. SurveyGen-I is com- pared against three state-of-the-art baselines: Au- toSurvey, SurveyX, and SurveyForge. Our results demonstrate that SurveyGen-I achieves significant improvements across all dimensions, showing its effectiveness for automated survey generation. Content Quality. SurveyGen-I achieves consis- tent improvements across all five content quality di- mensions compared to prior systems (Table 1). The overall score reaches 4.59, outperforming the best baseline (SurveyForge: 4.23) by +0.36. Largest gains are observed in structural flow (STRUC: +0.21) and synthesis (SYN: +0.41), indicating that the model maintains a coherent narrative while in- tegrating information from diverse sources. Cover- age (4.72) and relevance (4.76) also lead all base- lines, suggesting high topical breadth and align- ment. Consistency (4.59) improves notably over SurveyX (4.29), reflecting stability in terminology and phrasing across sections. Notably, SurveyX uses GPT4o (Hurst et al., 2024), whereas our sys- tem relies on a smaller and more cost-efficient model, making the performance gap especially sig- nificant. Observed quality gains suggest that sys- tems combining structural adaptivity, iterative re- finement, and citation-tracing can more reliably generate coherent and well-grounded surveys. 7 Model Overall ↑ Cov ↑ Rel ↑ Struc ↑ Syn ↑ Consis ↑ AutoSurvey 4.08 4.10 4.17 4.03 4.10 4.00 SurveyX 4.13 4.10 4.33 4.00 3.95 4.29 SurveyForge 4.23 4.31 4.41 4.07 4.21 4.17 Ours 4.59 4.72 4.76 4.28 4.62 4.59 Table 1: LLM-based evaluation scores across multiple survey quality dimensions. Higher scores reflect bet- ter coverage (COV), relevance (REL), structural flow (STRUC), synthesis (SYN), and consistency (CONSIS). SurveyGen-I leads across all. T ext Length 100 200 300 400 500 Number Of References Ours AutoSurvey SurveyX SurveyForge (a) Ref count vs. length T ext Length 0 5 10 15 20 25 30 35 Citation Density Ours AutoSurvey SurveyX SurveyForge (b) Citation density Figure 3: Citation behavior comparisons across mod- els using KDE-enhanced scatter plots. (a) Number of references vs. text length. SurveyGen-I demonstrates a steeper citation scaling curve, suggesting deeper inte- gration of references even in longer texts. (b) Citation density vs. text length. SurveyGen-I maintains denser citation patterns across all lengths. Reference Quality. In terms of citation qual- ity and scientific grounding, SurveyGen-I exhibits both broader and denser reference usage. It cites 281 unique works per survey on average (Ta- ble 2), representing a sharp increase over SurveyX (102) and AutoSurvey (73). Citation density also rises substantially (17.28), exceeding SurveyForge (5.52) by around 3 times, indicating tighter integra- tion of references into the body text. Importantly, 89.1% of all citations are published within the past 5 years (RR@5), compared to 66.7% in SurveyX and SurveyForge,"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 10, "text": "increase over SurveyX (102) and AutoSurvey (73). Citation density also rises substantially (17.28), exceeding SurveyForge (5.52) by around 3 times, indicating tighter integra- tion of references into the body text. Importantly, 89.1% of all citations are published within the past 5 years (RR@5), compared to 66.7% in SurveyX and SurveyForge, demonstrating significantly im- proved recency alignment. The steep scaling trend between reference count and text length in Figure 3a shows that text length remains relatively stable in SurveyGen-I, reflecting the fixed-length constraint imposed during gener- ation. It also demonstrates that SurveyGen-I in- cludes the most references overall. In contrast, Au- toSurvey consistently generates fewer references, and its citation count remains relatively flat, even as its text length slightly increases, which is un- expected given that the length parameter was con- trolled across all generations. This suggests weaker responsiveness to contextual expansion and under- utilization of available content space. Figure 3b fur- ther shows that SurveyGen-I consistently maintains a high citation density across varying text lengths, indicating robust integration of information-dense content. Model RR@1 RR@3 RR@5 RR@7 RR@10 CD NR AutoSurvey 0.174 0.639 0.837 0.940 0.992 1.54 73 SurveyX 0.239 0.484 0.667 0.792 0.916 13.57 102 SurveyForge 0.137 0.437 0.667 0.824 0.907 5.52 113 Ours 0.478 0.759 0.891 0.955 0.985 17.28 281 Table 2: Performance comparison across models based on recency-focused citation behavior and structural cita- tion metrics. RR@k indicates the proportion of recent references among the top-k citations. CD (scaled by ×104) measures citation density, and NR denotes the total number of cited references. Ablation Analysis. Table 3 reports the impact of removing specific behaviors from SurveyGen- I. The full model yields the highest overall score (4.77), with synthesis and structure both at 4.86 and 4.71, respectively. Disabling final refinement results in the steepest quality drop (Overall: –0.43), particularly in synthesis (–0.43) and structure (– 0.57), indicating that single-pass generation with- out revision is insufficient for maintaining narrative integration. Fixed planning further reduces struc- tural flow (STRUC: –0.42) and consistency (CON- SIS: –0.28), suggesting that static outlines limit the model’s ability to adjust to unfolding content. Removing citation resolution reduces the number of distinct references by 61 and lowers relevance by 0.29, despite stable consistency. Model Overall ↑ Cov ↑ Rel ↑ Struc ↑ Syn ↑ Consis ↑ NR ↑ Ours (w/o Citation Trace) 4.60 4.57 4.57 4.43 4.71 4.71 225 Ours (w/o Plan Update) 4.49 4.57 4.71 4.29 4.43 4.43 212 Ours (w/o Refine) 4.34 4.43 4.43 4.14 4.43 4.29 286 Ours (Full) 4.77 4.71 4.86 4.71 4.86 4.71 286 Table 3: Evaluation results of ablation variants. Each component (Trace, Plan Update, Refine) contributes to overall quality. 5 Conclusion We present SurveyGen-I, a fully automated frame- work for generating academic surveys with high consistency, citation coverage, and structural coher- ence. By integrating multi-level retrieval, adaptive planning, and memory-guided writing, SurveyGen- I effectively captures complex literature landscapes and produces high-quality surveys without manual intervention. Extensive evaluations across six sci- entific domains demonstrate its effectiveness over existing methods, marking a step forward in reli- able and scalable scientific synthesis. 8 Limitations While SurveyGen-I"}
{"doc_id": "2508.14317v1", "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "date": "2025-08-20", "source": "http://arxiv.org/abs/2508.14317v1", "chunk_id": 11, "text": "integrating multi-level retrieval, adaptive planning, and memory-guided writing, SurveyGen- I effectively captures complex literature landscapes and produces high-quality surveys without manual intervention. Extensive evaluations across six sci- entific domains demonstrate its effectiveness over existing methods, marking a step forward in reli- able and scalable scientific synthesis. 8 Limitations While SurveyGen-I shows consistently strong performance across benchmarks, our framework adopts an online retrieval strategy to ensure access to up-to-date literature. However, this design in- troduces network sensitivity, variable latency, and reliance on third-party APIs, which may restrict full-text access due to licensing constraints. Com- pared to offline-indexed corpora used in prior work, our approach trades retrieval speed and infrastruc- ture control for broader coverage and freshness. Additionally, for niche or emerging topics with limited source material, the achievable survey length and depth are naturally constrained. This shows a general challenge in automatic survey gen- eration: content quality is ultimately bounded by the availability and granularity of the source lit- erature. Moreover, some evaluation signals may reflect subjective preferences rather than univer- sal writing standards. We anticipate that broader community adoption and feedback will help guide future enhancements."}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 0, "text": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency Aman Goel∗, Daniel Schwartz∗, Yanjun Qi Amazon Web Services, USA {goelaman, dansw, yanjunqi}@amazon.com Abstract Large language models (LLMs) have demon- strated impressive capabilities across diverse tasks, but they remain susceptible to halluci- nations—generating content that appears plau- sible but contains factual inaccuracies. We present FINCH-ZK, a black-box framework that leverages FINe-grained Cross-model consis- tency to detect and mitigate Hallucinations in LLM outputs without requiring external knowl- edge sources. FINCH-ZK introduces two key in- novations: 1) a cross-model consistency check- ing strategy that reveals fine-grained inaccu- racies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problem- atic segments while preserving accurate con- tent. Experiments on the FELM dataset show FINCH-ZK improves hallucination detection F1 scores by 6-39% compared to existing ap- proaches. For mitigation, FINCH-ZK achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models demonstrates that FINCH-ZK provides a practi- cal, deployment-ready safeguard for enhancing factual reliability in production LLM systems. 1 Introduction With the rapid deployment of large language mod- els (LLMs) across diverse applications, ensuring factual accuracy and reliability has become in- creasingly critical for enterprise systems. LLMs frequently generate plausible-sounding but factu- ally incorrect information—a phenomenon known as hallucination—which poses significant risks in high-stakes domains. Existing black-box hallucination management techniques typically address either detection or mit- *Equal contributions igation, but seldom integrate both. Black-box detec- tion systems when given zero knowledge struggle with single-LLM biases, and coarse outputs lacking interpretability, while mitigation approaches simi- larly over-reformulate, reuse biased models, lack integrated detection-correction pipelines, and offer little transparency (detailed review in §2.1). Our objective is to develop a practical LLM hal- lucination management system that integrates de- tection and targeted mitigation without external knowledge requirements. In this paper, we intro- duce FINCH-ZK, which integrates techniques like consistency checking (Wang et al., 2023; Manakul et al., 2023) with a novel multi-stage mitigation approach that precisely corrects only problematic segments while preserving accurate content and embodying diverse reasoning patterns across model families. Our key contributions include: • We introduce FINCH-ZK, an integrated black- box framework that combines existing detection techniques with a novel multi-stage mitigation process for targeted hallucination correction, ad- dressing a critical gap between detection and mit- igation in existing LLM safeguards. • We demonstrate how leveraging prompting vari- ations (adding dynamic semantic-preserving al- terations to the input prompt) and cross-model consistency checking (comparing outputs across different model architectures) provide more ro- bust detection than single-model approaches, im- proving detection F1 scores by 6-39% on the FELM dataset (Zhao et al., 2023) compared to state-of-the-art methods. • We present an interpretable multi-stage mitiga- tion pipeline that applies targeted corrections only to problematic segments identified through fine-grained analysis while maintaining coher- ence and completeness through cross-model rea- soning feedback, achieving 7-8 absolute percent- age points improvement in answer accuracy on 1 the GPQA-diamond dataset (Rein et al.,"}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 1, "text": "• We present an interpretable multi-stage mitiga- tion pipeline that applies targeted corrections only to problematic segments identified through fine-grained analysis while maintaining coher- ence and completeness through cross-model rea- soning feedback, achieving 7-8 absolute percent- age points improvement in answer accuracy on 1 the GPQA-diamond dataset (Rein et al., 2024). • We provide comprehensive empirical evidence showing that the integration of diverse sampling strategies with targeted correction significantly outperforms existing approaches in the zero- knowledge setting, with quantitative ablation studies identifying the relative contribution of each system component. The framework is designed for practical deploy- ment in production environments, with efficient multi-threaded processing, comprehensive logging support, modular architecture supporting various LLMs, and rich user feedback.1 2 Methodology 2.1 Background and Related Works Existing black-box hallucination management ap- proaches fall into two categories. For detection, techniques include: a) external knowledge-based approaches like RAG (Lewis et al., 2020) that rely on data sources for fact-checking, and b) internal consistency methods like SelfCheckGPT (Man- akul et al., 2023) that analyze variations across model outputs. For mitigation, common tech- niques include self-correction through iterative re- finement (Wang et al., 2023), chain-of-thought reasoning (Wei et al., 2023), and majority vot- ing (Lightman et al., 2023). Detection systems face three primary limita- tions: 1) RAG-based methods require comprehen- sive knowledge bases often unavailable for spe- cialized domains or inaccessible due to privacy concerns; 2) zero-knowledge consistency-based ap- proaches typically rely on a single LLM architec- ture, making them prone to high-certainty halluci- nations due to missing diverse reasoning patterns; and 3) most systems operate at coarse granular- ity, lacking fine-grained analysis and interpretable explanations for flagged content. Mitigation approaches suffer from complemen- tary shortcomings: 1) most systems attempt whole- sale reformulation rather than targeted correction, often modifying accurate content while fixing er- rors; 2) they frequently rely on the same model that produced the hallucination to correct it, per- petuating biases and reasoning patterns; 3) many approaches lack integration between detection and correction mechanisms, resulting in inefficient 1Implementation details and experimental code will be made available to support reproducibility and adoption in industrial settings. pipelines; and 4) few systems provide transparency into why content was flagged and how corrections were determined. 2.2 Proposed: FINCH-ZK To address the above limitations and provide an integrated workflow for hallucination management, we propose FINCH-ZK, a framework for FINe- grained Cross-model consistency for Hallucination detect and mitigate with Zero Knowledge. FINCH- ZK addresses key limitations in existing approaches through two primary innovations: 1) a cross-model consistency checking strategy that leverages di- verse model architectures and prompt formula- tions to reveal fine-grained inaccuracies not de- tectable through single-model analysis, and 2) a targeted mitigation pipeline that applies precise cor- rections to identified problematic segments while preserving accurate content, avoiding the whole- sale response reformulation typical of existing ap- proaches. Figure 1 presents an overview of FINCH-ZK. Given a prompt p, a target LLM T that gener- ates response rT , a set of sampler models M = {m1, m2, . . . , m|M|}, a judge model J, and an im- prover model"}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 2, "text": "reformulation typical of existing ap- proaches. Figure 1 presents an overview of FINCH-ZK. Given a prompt p, a target LLM T that gener- ates response rT , a set of sampler models M = {m1, m2, . . . , m|M|}, a judge model J, and an im- prover model I, FINCH-ZK performs hallucination detection and mitigation in three stages: • Generate diverse samples from different sampler models • Detect fine-grained inaccuracies in the input re- sponse using generated samples • Perform systematic response improvement using detected inaccuracies and generated samples. 2.3 Cross-model Sample Generation As the first component, FINCH-ZK generates di- verse response samples through prompt variations and multi-model sampling to expose hallucinations that may be consistent within a single model but in- consistent across different architectures or prompt formulations. The system applies a set of variations V = {v1, v2, . . . , v|V |} to generate prompt variants {v1(p), v2(p), . . . , v|V |(p)}. These variations in- clude syntactic transformations (rephrasing, expan- sion) and semantic modifications (chain-of-thought prompting, question decomposition) designed to elicit varied reasoning patterns while preserving the original information requirements.2 2Prompt variations are detailed in Appendix A.2. 2 Prompt p + Response rT / Predicted Hallucination Scores Error Summaries Improved Response r’’T Changes Summary Cross-model Sample Generation a. Generate prompt variants v1 . . . vN b. Use Sampler LLMs (M) to generate samples s1 . . . sN 1 Fine-grained Hallucination Detection a. Segment response into blocks b1 . . . bK b. Use Judge LLM (J) to evaluate each block against each sample s1 -> b1? . . . sN -> bK? c. Calculate hallucination scores d. Summarize errors per block with J 2 Multi-stage Hallucination Mitigation a. Apply block correction using Improver LLM (I) on each inaccurate block to generate r’T (with a change summary) b. Perform cross-model response-level reflection on r’T with I to generate r’’T (with a change summary) 3 Figure 1: Overview of FINCH-ZK The system then collects |S| responses by prompting different sampler models in M with dif- ferent variants from V to create the sample set S = {s1, s2, . . . , s|S|}. Each sample si ∈S is generated by randomly selecting a prompt vari- ant vi ∈V and sampler model mi ∈M. This cross-model sampling strategy captures architec- tural differences in reasoning patterns, knowledge representation, and potential systematic biases that single-model approaches cannot detect. 2.4 Fine-grained Hallucination Detection As the second component, FINCH-ZK performs fine-grained hallucination identification through automated cross-consistency evaluation, enabling precise localization of potentially hallucinated con- tent segments. Response Segmentation. Target response rT is segmented into semantic blocks B = {b1, b2, . . . , b|B|} using sentence-level segmenta- tion. This granular approach enables the system to identify specific hallucinated segments rather than classifying entire responses, providing actionable feedback for targeted correction. Cross-consistency Evaluation. Each block bi ∈ B is evaluated against each sample sj ∈S using the judge model J with structured prompts. The judge model classifies each (bi, sj) pair into: • ACCURATE: Block is factually consistent and"}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 3, "text": "segments rather than classifying entire responses, providing actionable feedback for targeted correction. Cross-consistency Evaluation. Each block bi ∈ B is evaluated against each sample sj ∈S using the judge model J with structured prompts. The judge model classifies each (bi, sj) pair into: • ACCURATE: Block is factually consistent and sup- ported by the sample • CONTRADICTION: Direct factual contradiction de- tected between block and sample • NEUTRAL: Insufficient information for definitive assessment. Weighted Scoring. Block-level hallucination scores are computed using weighted aggregation across all samples: score(bi) = P|S| j=1 wj(bi) · score(bi, sj) P|S| j=1 wj(bi) where wj(bi) represents the reliability weight assigned to sample sj for evaluating block bi and score(bi, sj) ∈{0, 0.5, 1} corresponds to ACCURATE, NEUTRAL, and CONTRADICTION labels respectively. Factuality labels are assigned to each block bi based on a threshold τ as: ACCURATE for score(bi) ∈[0, τ], CONTRADICTION for score(bi) ∈ [1 −τ, 1], and NEUTRAL otherwise. Response-level hallucination score is computed as score(rT ) = 1 |B| P|B| i=1 score(bi), with overall response label computed as: NON-FACTUAL if any block in rT is la- belled as CONTRADICTION, and FACTUAL otherwise. Summarize Errors. For blocks identified as poten- tially hallucinated (i.e., labeled as CONTRADICTION or NEUTRAL), the system generates concise error summaries ei using the judge model to characterize the nature and severity of detected inconsistencies, providing interpretable explanations for actionable user feedback and downstream correction. 2.5 Multi-stage Hallucination Mitigation The mitigation component applies targeted correc- tions to identified problematic segments through a two-stage approach: fine-grained block correc- tion followed by response-level coherence improve- ment. Block-level Correction. For each hallucinated block bi with error summary ei, FINCH-ZK gen- erates a corrected version b′ i using an improver model I with a structured correction prompt that includes: 1) the original block text, 2) the auto- matically generated error summary, and 3) detailed contradiction evidence from the cross-consistency analysis. This approach ensures corrections are grounded in specific identified issues rather than generic reformulation. The corrected response is reconstructed as: r′ T = concat(c1, c2, . . . , c|B|) where ci = b′ i if block i was flagged for correction, 3 Table 1: Comparison of hallucination detection methods on FELM (Zhao et al., 2023) dataset. P/R/F1/BA, respectively, denote precision, recall, F-1 score, and balanced accuracy of predicted factuality labels vs human-annotations. For response-level, we additionally show Pearson and Spearman correlations of predicted hallucination scores. GPT-4 Judge variants are from (Zhao et al., 2023) that use GPT-4 for judgment based on the prompt and sentence directly (Vanilla), with chain-of-thought (CoT), or with retrieved content from reference sources (RAG). Delta percentages are shown for F1/BA metrics, with positive values indicating improvements compared to GPT-4 Judge (Vanilla). Method Sentence-level Response-level P R F1 (∆%) BA (∆%) P R F1 (∆%) BA (∆%) Pearson Spearman GPT-4 Judge (Vanilla) 64.0 24.4 35.4 60.7 62.4 39.4 48.3 63.8 — — GPT-4 Judge (CoT) 68.1 30.4 42.0 (+18.6%) 63.7 (+4.9%) 64.7 46.1 53.8 (+11.4%) 66.8 (+4.7%) — — GPT-4 Judge (RAG) 62.9 39.2 48.3 (+36.4%) 67.1 (+10.5%) 64.3 51.1 56.9 (+17.8%) 68.5 (+7.4%)"}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 4, "text": "(∆%) BA (∆%) Pearson Spearman GPT-4 Judge (Vanilla) 64.0 24.4 35.4 60.7 62.4 39.4 48.3 63.8 — — GPT-4 Judge (CoT) 68.1 30.4 42.0 (+18.6%) 63.7 (+4.9%) 64.7 46.1 53.8 (+11.4%) 66.8 (+4.7%) — — GPT-4 Judge (RAG) 62.9 39.2 48.3 (+36.4%) 67.1 (+10.5%) 64.3 51.1 56.9 (+17.8%) 68.5 (+7.4%) — — SelfCheckGPT 41.2 54.1 46.8 (+32.2%) 68.7 (+13.2%) 73.7 53.5 62.0 (+28.4%) 72.0 (+12.9%) 59.5 59.9 FINCH-ZK 45.8 53.1 49.2 (+39.0%) 69.8 (+15.0%) 83.8 53.2 65.1 (+34.8%) 74.0 (+16.0%) 63.1 61.5 and ci = bi otherwise. This selective correction strategy preserves accurate content while targeting only problematic segments. Response-level Improvement. To address broader coherence and completeness issues that may arise from localized corrections, the system performs cross-model reflection by generating an improved response r′′ T that synthesizes insights from all gen- erated samples S. The improver model receives the original prompt, the block-corrected response r′ T , and representative samples from S to produce a final response that maintains factual accuracy while ensuring overall coherence and completeness. This multi-stage approach addresses the key lim- itation of existing mitigation systems that apply wholesale reformulation, often corrupting accurate content while attempting to fix errors. By preserv- ing the structure and content of accurate segments, FINCH-ZK provides targeted correction that main- tains response quality while eliminating identified hallucinations. 3 Experiments We conducted experimental evaluation to answer the following research questions:3 RQ1: How effective is FINCH-ZK at detecting hallucinations compared to other approaches? RQ2: How effective is FINCH-ZK for mitigat- ing hallucinations? RQ3: Which components significantly influ- ence FINCH-ZK’s detection capabilities? RQ4: How does different factors affect FINCH- ZK’s hallucination mitigation? Datasets. We utilize two challenging benchmarks for evaluation: 1) FELM (Zhao et al., 2023) com- posed of 847 questions & responses across di- 3Due to space limits, detailed implementation and setup are described in Appendix A.4 & A.5. verse domains supplemented with fine-grained human-annotated factuality labels, and 2) GPQA- diamond (Rein et al., 2024) composed of 198 graduate-level multiple-choice questions. Baseline Methods. For RQ1 & RQ2, we com- pared against GPT4-based judge variants (Vanilla, CoT, RAG) as utilized in (Zhao et al., 2023) and SelfCheckGPT (Manakul et al., 2023). For RQ2 & RQ4, we compared against SelfCheckGPT and hallucination mitigation techniques: few-shots CoT (Wei et al., 2023) using 5 in-context examples, self-consistency (Wang et al., 2023), a cross-model variant of self-consistency that uses multiple LLMs (call it cross-consistency), and best-of-N majority selection (Lightman et al., 2023).4 RQ1: How effective is FINCH-ZK at detecting hallucinations compared to other approaches? Table 1 presents results for hallucination detec- tion on the FELM dataset. At both fine-grained (i.e., sentence) as well as aggregated response level, FINCH-ZK showed better precision-recall balance, consistently outperforming all baselines. In par- ticular, FINCH-ZK achieved 39% better sentence- level F1-score compared to GPT4-Judge (Vanilla). Surprisingly, FINCH-ZK even outperformed the knowledge-source dependent RAG-based baseline, achieving around 17% better F1-score and 8% bet- ter balanced accuracy respectively at response level. Diverse sample generation through prompt varia- tions and cross-model sampling enabled FINCH- ZK to achieve around 6% better F-1 scores and Pearson correlation compared to SelfCheckGPT."}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 5, "text": "Surprisingly, FINCH-ZK even outperformed the knowledge-source dependent RAG-based baseline, achieving around 17% better F1-score and 8% bet- ter balanced accuracy respectively at response level. Diverse sample generation through prompt varia- tions and cross-model sampling enabled FINCH- ZK to achieve around 6% better F-1 scores and Pearson correlation compared to SelfCheckGPT. RQ2: How effective is FINCH-ZK for mitigating hallucinations? Table 2 presents a comparison of FINCH-ZK against different mitigation baselines on GPQA- 4For a fair comparison, we used equivalent configurations across different techniques (Appendix A.5) and added equiva- lent engineering upgrades (Appendix A.3) to SelfCheckGPT. 4 Table 2: Comparison of hallucination mitigation methods on the GPQA-diamond (Rein et al., 2024) dataset. All methods are evaluated against the same zero-shot CoT baseline. Positive delta percentages indicate improvements compared to the baseline. Regex-based judge compares answer-choice accuracy against ground truth, RAG-based judge uses answer explanations from the dataset as trusted content for LLM-based judgment, FINCH-ZK-based judge is based on §2.4. T Method Regex-based Judge RAG-based LLM Judge FINCH-ZK Judge Answer Acc. ∆% Full Resp. Acc. ∆% Full Resp. Acc. ∆% Claude 4 Sonnet Zero-shot CoT (baseline) 71.7 — 50.0 — 69.7 — Few-shots-CoT 68.2 -4.9% 47.5 -5.1% 70.7 +1.4% Self-Consistency 73.2 +2.1% 48.5 -3.0% 66.7 -4.3% Cross-Consistency 71.2 -0.7% 52.5 +5.1% 71.2 +2.2% Best-of-N 73.7 +2.8% 52.5 +5.1% 69.7 0.0% SelfCheckGPT 71.2 -0.7% 54.5 +9.1% 75.3 +8.0% FINCH-ZK 75.8 +5.6% 59.1 +18.2% 80.3 +15.2% Llama 4 Maverick Zero-shot CoT (baseline) 68.2 — 42.9 — 63.1 — Few-shots-CoT 67.7 -0.7% 43.4 +1.2% 64.7 +2.4% Self-Consistency 67.7 -0.7% 45.0 +4.7% 64.1 +1.6% Cross-Consistency 73.7 +8.2% 50.5 +17.7% 69.7 +10.4% Best-of-N 67.2 -1.5% 41.9 -2.4% 61.1 -3.2% SelfCheckGPT 75.8 +11.1% 84.3 +96.5% 86.9 +37.6% FINCH-ZK 76.8 +12.6% 90.9 +111.8% 92.4 +46.4% diamond dataset. We evaluate performance using three distinct judging methodologies: regex-based answer-choice accuracy, RAG-based LLM judging of the full response, and FINCH-ZK’s based judge. In answer-choice accuracy, FINCH-ZK achieved the best performance—reaching ∼76% accuracy, up +5.6% for Claude 4 Sonnet and +12.6% for Llama 4 Maverick. For full response accuracy, FINCH-ZK outperformed the next best baseline (SelfCheckGPT) by around 9-15% for RAG-based judging and 7-9% for FINCH-ZK-based judge. These results demonstrate that FINCH-ZK’s com- bination of cross-model sampling, fine-grained er- ror detection, and targeted correction offers supe- rior hallucination mitigation compared to existing approaches. The system is particularly effective at improving full response factuality, as evidenced by the substantial gains in RAG-based and FINCH-ZK- based judging metrics. The effectiveness across different model families (Claude and Llama) high- lights FINCH-ZK’s model-agnostic design, making it a versatile solution for production environments with diverse LLM deployments. Notably, while techniques like Self-Consistency and Best-of-N offer modest improvements in an- swer accuracy (2-3%), they often fail to meaning- fully improve full response factuality. This under- scores the limitations of approaches that don’t ex- plicitly target hallucinations at a fine-grained level. RQ3: Which components significantly influence FINCH-ZK’s detection capabilities? Table 3 summarizes ablation studies to understand the influence of each component in FINCH-ZK for hallucination detection. Key observations include: • Detection capabilities do not monotonically in- crease with more samples (G1.a-c vs G0)."}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 6, "text": "ex- plicitly target hallucinations at a fine-grained level. RQ3: Which components significantly influence FINCH-ZK’s detection capabilities? Table 3 summarizes ablation studies to understand the influence of each component in FINCH-ZK for hallucination detection. Key observations include: • Detection capabilities do not monotonically in- crease with more samples (G1.a-c vs G0). • Disabling cross-model sampling (i.e., all sam- ples generated with Claude 4 Sonnet) degraded detection at the response-level (G2.a vs G0). • Adding additional cross-model sampler LLMs, both weaker models (G2.b adds Claude 3.5 Son- net and Llama 4 Scout) or stronger models (G2.c adds Claude 4 Opus), improves detection at the sentence-level. • Using a coarse, response-level judge significantly limits detection due to poor recall (G3.a vs G0). • Using a single judge query to evaluate all blocks together in a batch (instead of separate LLM calls for each sample-block pair) is an effective way to reduce LLM costs for fine-grained as- sessment without compromising detection per- formance (G3.b vs G0). • Judge model can significantly influence detection performance (G3.c-e vs G0). RQ4: How does different factors affect FINCH-ZK’s hallucination mitigation? Table 4 summarizes ablation studies to understand mitigation effectiveness using Claude 4 Sonnet as the target model. Key observations include: 5 Table 3: Ablation studies for hallucination detection on FELM (Zhao et al., 2023) dataset. Group G1 shows the effect of changing number of samples (§2.3), G2 compares the effect of changing sampler LLMs (§2.3), G3 shows the effect of changing LLM-based judge (§2.4). Group Configuration Sentence-level Response-level P R F1 BA P R F1 BA Pearson Spearman G0 FINCH-ZK 45.8 53.1 49.2 69.8 83.8 53.2 65.1 74.0 63.1 61.5 G1 a. 3 samples 46.0 52.0 48.8 69.4 77.6 55.3 64.6 73.7 57.8 55.5 b. 5 samples 43.1 59.5 50.0 71.2 78.7 57.8 66.7 75.0 61.9 59.8 c. 20 samples 48.0 54.5 51.0 70.9 81.2 53.5 64.5 73.7 63.7 61.5 G2 a. (-) cross-model sampling 43.2 55.3 48.5 69.8 77.2 51.8 62.0 72.1 62.1 59.8 b. (+) weak samplers 46.6 56.0 50.9 71.1 81.6 53.5 64.7 73.8 63.0 60.7 c. (+) strong samplers 46.3 56.2 50.7 71.0 79.5 53.5 64.0 73.3 63.2 62.2 G3 a. (-) fine-grained judge — — — — 88.1 31.6 46.5 64.7 58.7 59.7 b. (+) use batch judge 37.2 72.2 49.1 72.9 69.8 73.8 71.7 78.9 63.9 61.5 c. Llama 4 Scout judge 39.6 81.4 53.3 77.3 72.5 80.5 76.3 82.6 71.2 67.9 d. Llama 4 Scout batch judge 35.5 83.2 49.8 75.3 69.4 84.4 76.2 82.9 65.5 64.6 e. Claude 4 Sonnet batch judge 41.5 86.7 56.1 80.1 65.6 85.8 74.3 81.7 69.3 66.9 • FINCH-ZK typically reaches higher accuracy with more samples, though with diminishing re- turns (G1.a-c vs G0). • Disabling cross-model sampling degrades miti- gation capability significantly (G2.a vs G0). • Using coarse response-level judge reduced accu- racy improvements significantly (G3.a vs G0). • Judge variations have modest effects on answer- choice accuracy, but significant impact on full response accuracy (G3.b-d vs G0). • Disabling fine-grained correction drastically lim- its mitigation performance, underscoring the im- portance of targeted correction (G4.a vs"}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 7, "text": "Using coarse response-level judge reduced accu- racy improvements significantly (G3.a vs G0). • Judge variations have modest effects on answer- choice accuracy, but significant impact on full response accuracy (G3.b-d vs G0). • Disabling fine-grained correction drastically lim- its mitigation performance, underscoring the im- portance of targeted correction (G4.a vs G0). • Using Llama 4 Maverick as the improver LLM (instead of Claude 4 Sonnet) significantly im- proved full response accuracy, suggesting cross- model reflection can help remedy perpetuating biases and reasoning patterns inherent in single- model architectures (G4.b vs G0). • FINCH-ZK boosts accuracy even with extended thinking enabled, achieving 80.3% answer- choice accuracy (+11.3% over extended thinking baseline). This demonstrates our proposed tech- niques complements internal extended reasoning, rather than competing with it (G5.b vs G5.a). 4 Conclusions We introduce FINCH-ZK, an integrated black-box framework that closes the gap between hallucina- tion detection and mitigation by combining ad- vanced detection techniques with a novel multi- stage process for targeted hallucination correction. Table 4: Ablation studies for hallucination mitigation on GPQA-diamond (Rein et al., 2024) dataset. Group G1 shows the effect of changing number of samples (§2.3), G2 compares the effect of changing sampler LLMs (§2.3), G3 shows the effect of changing LLM-based judge (§2.4), G4 shows the effect of changing multi-stage mitigation (§2.5), G5 shows the comparison with extended thinking enabled. Delta percentages indicate improvement compared to zero-shot CoT baseline. Group Configuration Regex-based Judge RAG-based LLM Judge FINCH-ZK Judge Answer Acc. ∆% Full Resp. Acc. ∆% Full Resp. Acc. ∆% G0 a. Zero-shot CoT (baseline) 71.7 — 50.0 — 69.7 — b. FINCH-ZK 75.8 +5.6% 59.1 +18.2% 80.3 +15.2% G1 a. 3 samples 69.7 -2.8% 54.0 +8.1% 72.2 +3.6% b. 5 samples 71.2 -0.7% 61.1 +22.2% 76.8 +10.1% c. 20 samples 78.8 +9.9% 59.6 +19.2% 77.8 +11.6% G2 a. (-) cross-model sampling 71.7 0.0% 57.6 +15.2% 74.2 +6.5% b. (+) weak samplers 72.7 +1.4% 56.1 +12.1% 75.3 +8.0% c. (+) strong samplers 75.8 +5.6% 56.1 +12.1% 76.3 +9.4% G3 a. (-) fine-grained judge 74.2 +3.5% 56.6 +13.1% 77.3 +10.9% b. (+) use batch judge 75.8 +5.6% 56.6 +13.1% 79.3 +13.8% c. Llama 4 Scout judge 74.2 +3.5% 56.6 +13.1% 82.8 +18.8% d. Claude 4 Sonnet batch judge 74.2 +3.5% 56.1 +12.1% 78.8 +13.0% G4 a. (-) fine-grained correction 72.7 +1.4% 50.5 +1.0% 76.8 +10.1% b. Llama 4 Maverick improver 74.8 +4.2% 90.4 +80.8% 94.4 +35.5% G5 a. (+) thinking (baseline) 72.2 — 65.7 — 82.3 — b. (+) thinking (FINCH-ZK) 80.3 +11.3% 64.7 -2.0% 90.4 +11.6% 6 Leveraging dynamic prompt variations and cross- model consistency checks, FINCH-ZK delivers sig- nificantly more robust detection than single-model approaches. Its multi-stage mitigation pipeline makes precise, segment-level corrections while maintaining overall coherence. We provide com- prehensive ablation studies underscoring the con- tribution of each component of FINCH-ZK. These ablation results highlight that FINCH-ZK’s effec- tiveness stems from the synergistic combination of multiple components, with fine-grained correction and cross-model sampling being particularly im- pactful. The system’s performance scales with com- putational resources (samples, model capability), but even resource-efficient configurations offer sub- stantial improvements over baseline"}
{"doc_id": "2508.14314v1", "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14314v1", "chunk_id": 8, "text": "component of FINCH-ZK. These ablation results highlight that FINCH-ZK’s effec- tiveness stems from the synergistic combination of multiple components, with fine-grained correction and cross-model sampling being particularly im- pactful. The system’s performance scales with com- putational resources (samples, model capability), but even resource-efficient configurations offer sub- stantial improvements over baseline approaches. Limitations While FINCH-ZK represents a meaningful step to- ward improving the reliability of large language model outputs, it is not without important limi- tations. The underlying approach fundamentally relies on the assumption that a truly reliable an- swer will emerge as the most frequent or stable across repeated sampling. However, for complex or ambiguous queries, models may consistently reproduce similar hallucinated content, leading to a false sense of confidence in its correctness. In such cases, consistency can inadvertently rein- force errors rather than expose them. Additionally, the computational overhead of generating multiple cross-model samples represents a substantial cost increase compared to alternative approaches, which may limit real-time applications. Future work includes exploring extensions not covered in this study like—1) extending hallucina- tion detection and mitigation for languages beyond English, 2) exploring paragraph- and page-level segmentation for very long-form responses, 3) re- ducing computational costs with batch processing, 4) rigorous evaluation on domain-specific bench- marks, and 5) investigating solutions for agentic applications. Ethics Statement Our cross-model approach assumes that consen- sus among different models indicates accuracy, but this may amplify shared biases across model fam- ilies rather than eliminate them. We emphasize that FINCH-ZK should complement, not replace, human oversight in high-stakes applications. Users must understand the system’s limitations and main- tain appropriate skepticism of AI-generated content. We encourage responsible deployment with clear communication about the system’s capabilities and limitations to end users."}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 0, "text": "A Joint Multitask Model for Morpho-Syntactic Parsing Demian Inostroza, Mel Mistica, Ekaterina Vylomova, Chris Guest, Kemal Kurniawan University of Melbourne {inostrozaad, misticam, ekaterina.vylomova, chris.guest, kurniawan.k}@unimelb.edu.au Abstract We present a joint multitask model for the Uni- Dive 2025 Morpho-Syntactic Parsing shared task, where systems predict both morphologi- cal and syntactic analyses following novel UD annotation scheme. Our system uses a shared XLM-RoBERTa encoder with three specialized decoders for content word identification, de- pendency parsing, and morphosyntactic feature prediction. Our model achieves the best overall performance on the shared task’s leaderboard covering nine typologically diverse languages, with an average MSLAS score of 78.7%, LAS of 80.1%, and Feats F1 of 90.3%. Our abla- tion studies show that matching the task’s gold tokenization and content word identification are crucial to model performance. Error analy- sis reveals that our model struggles with core grammatical cases (particularly Nom-Acc) and nominal features across languages.1 1 Introduction The UniDive 2025 Morpho-Syntactic Parsing shared task (Goldman et al., 2025) introduces a novel framework for dependency parsing that seeks to bridge the traditional divide between morpholog- ical and syntactic analysis. In conventional Univer- sal Dependencies (Nivre et al., 2020), morphology and syntax are treated as distinct modules operating at different linguistic levels, with word boundaries serving as the interface between them. However, this separation has led to significant inconsisten- cies in how different languages and even different treebanks for the same language handle word seg- mentation and grammatical analysis. The shared task proposes to address these long-standing chal- lenges by reorganizing grammatical representation around the content-function distinction rather than relying on theoretically problematic word bound- aries, proposing a more typologically consistent 1Our code and models are publicly available: https://github.com/DemianInostrozaAmestica/ shared_task_UD_official ID Token FEATS HEAD DEPREL 1 From _ _ _ 2 the _ _ _ 3 AP Case=Abl|Definite=Def| Number=Sing 4 obl 4 comes Mood=Ind|Polarity=Pos| Tense=Pres|VerbForm=Fin| Voice=Act 0 root 5 this Number=Sing| PronType=Dem 6 det 6 story Number=Sing 4 nsubj 7 : _ _ _ Table 1: Example of the new annotation scheme used in the shared task approach to multi-linguistic parsing. For instance, in the sentence ‘From the AP comes this story’ shown in Table 1, traditional UD treats ‘From’ as a dependent of ‘AP’ with the deprel case, while the new framework transfers the grammatical meaning of ‘From’ as a morphosyntactic feature Case=Abl (Ablative) directly onto the content word ‘AP’. The task requires systems to predict both labeled dependency arcs and morphosyntactic features, but with a difference from standard Universal Depen- dencies parsing: the dependency tree consists only of content words (lexical words carrying semantic meaning like nouns, verbs, and adjectives), while function words (grammatical elements like adpo- sitions, articles, and auxiliaries) contribute their grammatical information as features on related con- tent words. While the content-function distinction is explicit in the training data, systems must identify this dis- tinction themselves at test time from raw text. This identification determines which words participate in the dependency tree and which contribute fea- tures to other words. Additionally, the multi-label nature of features, where a content word can have multiple feature values for"}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 1, "text": "the training data, systems must identify this dis- tinction themselves at test time from raw text. This identification determines which words participate in the dependency tree and which contribute fea- tures to other words. Additionally, the multi-label nature of features, where a content word can have multiple feature values for a given feature class, 1 Figure 1: Joint model architecture for the shared task. like Case=Ine;Atr,2 requires models to learn intri- cate morphosyntactic patterns. We present a joint multitask model (Figure 1) that explicitly addresses these challenges through three specialized decoders sharing a common XLM-RoBERTa encoder (Conneau et al., 2020), initialized from pre-trained multilingual represen- tations. We design content word identification as an explicit task to be learned by the model rather than relying on intuition-driven heuristics. We par- ticipate in the multilingual track, training separate models for each of the nine languages, allowing us to tune hyperparameters specifically for each lan- guage’s characteristics while still benefiting from multilingual pretrained representations. On the shared task’s results, our system achieves the best overall performance with average scores of 78.7% MSLAS, 80.1% LAS, and 90.3% Feats F1 across all languages. Additionally, our model ranks first on each individual language, demonstrating the ef- fectiveness of multitask learning for this task. Our error analysis yields three main observa- tions: (1) errors in tokenization and content word identification cascade through the pipeline, with gold annotations improving MSLAS by up to 12 points; (2) the majority of residual errors lie in nom- inal morphology—Gender, Number, and Case—with 2Ine=Inessive, “inside an enclosed area”; Atr=“complement, attribute”. Both definitions come from the official Case inventory supplied by the shared-task organisers. common Nominative–Accusative swaps; and (3) syn- tactic mislabels are concentrated in the nmod versus obl relation. 2 System Description 2.1 Model Overview We propose a joint multitask model implemented using the Flair framework (Akbik et al., 2019) for morphosyntactic parsing, as shown in Figure 1. Although the evaluation metrics assess only depen- dency arcs and morphosyntactic features, produc- ing these outputs requires distinguishing between content and function words. Because this classifi- cation is not given at test time, we treat it as an ad- ditional prediction task. Our system uses the large version of XLM-RoBERTa augmented with charac- ter embeddings (Akbik et al., 2018) as a shared en- coder, both provided by the Flair framework. This encoder’s output is then passed through a shared in- termediate layer (linear transformation with ReLU and dropout) before being fed to three specialized decoders: content word identification, morphosyn- tactic feature prediction, and dependency parsing. 2.2 Decoders Content word identification. The content word identification decoder accepts tokens as input. Each token’s contextual embedding computed by the shared intermediate layer is passed through a bidi- rectional LSTM (256 hidden units in both direc- tions). The LSTM output is then passed through a linear layer with 2 output units, each corresponds to “content” vs. “function” respectively. Training uses two forms of regularisation: token-level (word) dropout—zeroing the entire embedding of 5% of UD tokens—and locked dropout that masks 50% of the LSTM outputs with the same pattern across all"}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 2, "text": "is then passed through a linear layer with 2 output units, each corresponds to “content” vs. “function” respectively. Training uses two forms of regularisation: token-level (word) dropout—zeroing the entire embedding of 5% of UD tokens—and locked dropout that masks 50% of the LSTM outputs with the same pattern across all timesteps. Class-weighted cross-entropy loss function is then used to compensate for the imbal- ance between the number of content and function tokens. Morphosyntactic features. The morphosyntac- tic features decoder consists of a single linear layer that performs multi-label classification di- rectly from the output of the shared intermediate layer. For each content word, it outputs proba- bilities for all possible feature-value pairs in the vocabulary (e.g., Case=Gen, Number=Sing, Voice=Act). Using sigmoid activation with a 0.5 threshold, the model can predict multiple features per to- ken—for instance, a noun might simultaneously 2 have Number=Plur and Case=Gen. Complex features with multiple values (like Case=Ine;Atr) are handled by predicting each component separately, allowing the model to learn different value combinations. Function words bypass this decoder entirely and receive ‘_’ as their feature value. At training time, we use gold content word (i.e. checking if its fea- ture values exist). In contrast, we use the predicted content words by the content word identification at test time. Dependency parser. The parsing decoder em- ploys separate multilayer perceptrons (MLPs) for arc and relation prediction with biaffine attention mechanisms, following Dozat and Manning (2016). The arc MLPs have 256 hidden units while the rela- tion MLPs use 128 units, both with layer normaliza- tion and ReLU activation. Operating exclusively on content words, we frame the parser as a con- ditional random field over projective dependency trees that we implement using TorchStruct (Rush, 2020). Similar to the morphosyntactic feature de- coder, we use gold and predicted content word at training and test time respectively. 2.3 Data Handling and Inference While the shared task data includes abstract nodes for representing implicit arguments, we initially at- tempted to handle them through sequence labeling by inserting mask tokens at potential abstract node positions. However, this approach introduced noise that degraded performance across all metrics, as incorrect abstract node predictions propagated er- rors to downstream decoders. Therefore, our final system filters out abstract nodes during data load- ing, simplifying the parsing task while improving overall performance. During inference, raw text is first segmented into word tokens using Stanza (Qi et al., 2020). Since tokenization quality impacts downstream per- formance but is not the focus of this shared task, we choose to leverage Stanza’s pre-trained models rather than training custom tokenizers. For each language, we evaluated different Stanza model vari- ants on the development set and selected those that best matched the gold tokenization (e.g., HTB for Hebrew, IMST for Turkish). This selection was done manually by running the full pipeline with each available Stanza model variant and choosing the one that achieved the highest metrics on the official evaluation script. We apply minimal post-processing to ensure valid output. For content word identification, to- kens with confidence below 0.6 that appear be- tween two tokens"}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 3, "text": "manually by running the full pipeline with each available Stanza model variant and choosing the one that achieved the highest metrics on the official evaluation script. We apply minimal post-processing to ensure valid output. For content word identification, to- kens with confidence below 0.6 that appear be- tween two tokens of the opposite type are relabeled to match their context (e.g., a low-confidence func- tion word between two content words becomes content). As a fallback for extreme cases where content word identification predicts all tokens as function words (particularly in very short sentences of 2-3 tokens), we force the first token to be content with deprel=‘root’ and features=‘|’. This ensures every sentence has at least one parseable token. 2.4 Training Objective and Optimization The model is trained end-to-end using a weighted sum of the three decoders’ losses: Ltotal = wparserLparser + wmorphLmorph + wCWILCWI, where the weights are hyperparameters tuned for each lan- guage. The parser uses negative log-likelihood loss over projective trees, the morphosyntactic decoder uses binary cross-entropy for multi-label classifi- cation, and the content word identification uses class-weighted cross-entropy to handle class imbal- ance. 3 Experimental Setup The shared task provided training and development sets for multiple languages. To simulate a realis- tic evaluation scenario, we split the official train- ing data into 90% for training and 10% for de- velopment, using the official development set as our local test set. This allowed us to tune hyper- parameters and select models before the official test release. The languages included in our experi- ments were English, Turkish, Hebrew, Czech, Pol- ish, Portuguese, Italian, Serbian, and Swedish, with training sizes ranging from approximately 3,000 to 10,000 sentences depending on the language. We develop a custom data loader to handle the modified CoNLL-U format used in the shared task. The loader automatically extracts content words by examining the FEATS column, where ‘_’ indi- cates function words and any other value indicates content words. As mentioned before, we filter out abstract nodes during loading. All models are trained using AdamW opti- mizer (Loshchilov and Hutter, 2019) with an initial learning rate of 2 × 10−5 and batch size of 16 for 25 epochs. We employ early stopping with pa- tience of 1 epoch and learning rate reduction by factor 0.5 when validation loss plateaus. Training is performed on a NVIDIA A100 GPU with 32GB 3 RAM on a high-performance computing cluster, with each model taking approximately 1-5 hours to converge. We perform grid search over task-specific loss weights on our development split. The optimal weights varied by language—for example, Turk- ish benefited from weighting parsing and morpho- logical feature losses twice as much as content word identification (2.0:2.0:1.5), while English per- formed better with parsing weighted most heavily, followed by morphological features and content word identification (2.0:1.5:1.0). For each language, we train three models with different random seeds using the same hyperparam- eter configuration to verify training stability and robustness. All three models are evaluated on our local test set (the official development set) using the shared task’s official evaluation script. Once hyperparameters are"}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 4, "text": "word identification (2.0:1.5:1.0). For each language, we train three models with different random seeds using the same hyperparam- eter configuration to verify training stability and robustness. All three models are evaluated on our local test set (the official development set) using the shared task’s official evaluation script. Once hyperparameters are selected, we retrain a single model for each language using the complete official training and development data combined. These final models use the same hyperparameters determined during development. These models are used to generate predictions on the official covered test set, which contains only raw text without anno- tations. Evaluation is performed using the official script which computes three metrics: MSLAS (mor- phosyntactic features F1 only on correctly parsed tokens), LAS (labeled attachment score), and Feats F1 (morphosyntactic features F1). 4 Results This section is divided into two parts: first, we present official test results from models trained on all available data (official train + dev combined) and evaluated on the covered test set; second, we re- port development results using our local data splits (90% train, 10% dev, official dev as test) to analyze design choices and hyperparameter impact. 4.1 Official Test Results Table 2 presents the official test results from models trained on all available data. Our system achieved the highest performance among all submissions with an average MSLAS of 78.7%. The results show strong performance across most languages, with MSLAS scores exceeding 83% for seven of the nine languages. Portuguese (88.9%) and Czech (87.1%) achieved the highest scores, con- sistent with our development results. The morpho- logically complex languages continued to present Language MSLAS LAS Feats Czech 87.1 88.0 95.2 English 83.8 85.1 94.9 Hebrew 68.7 71.4 83.4 Italian 73.0 73.7 84.7 Polish 75.0 76.5 86.2 Portuguese 88.9 89.5 94.8 Serbian 86.6 88.3 95.6 Swedish 86.6 87.7 95.7 Turkish 58.7 60.9 82.1 Average 78.7 80.1 90.3 Table 2: Official test results on the covered test set. Our system achieved the highest average MSLAS score (78.7%) among all submissions. System MSLAS LAS Feats Our model 78.7 80.1 90.3 baseline_multi 47.3 55.4 64.2 baseline_cross 36.7 51.2 50.6 baseline_finetune 33.0 36.1 52.3 Table 3: Comparison with baseline systems (average across all languages). challenges—Turkish (58.7%) and Hebrew (68.7%) showed the lowest performance. The baseline systems provide important context for understanding the task’s difficulty (Table 3). The multilingual few-shot baseline achieved mod- erate performance (average MSLAS 47.3%), while the cross-lingual few-shot approach struggled sig- nificantly (36.7%), highlighting the importance of language-specific examples. The finetuned BERT baseline performed poorest (33.0%), suggesting that the reformulated parsing task with its content- function distinction and expanded feature inventory benefits from specialized modeling approaches. Our 31.4 point improvement over the best base- line (78.7% vs 47.3%) indicates that combining pretrained representations with task-specific archi- tectural components can effectively address the challenges of unified morphosyntactic parsing. 4.2 Development Results The ablations in Figure 2 show that most of the gain comes from using gold tokenization, with a smaller but consistent boost from explicit content/function labeling. Hebrew makes this clear: MSLAS goes from 75.2 (Full) →84.5 (GoldTok, +9.3) →85.7 (GoldWT, +1.2;"}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 5, "text": "the challenges of unified morphosyntactic parsing. 4.2 Development Results The ablations in Figure 2 show that most of the gain comes from using gold tokenization, with a smaller but consistent boost from explicit content/function labeling. Hebrew makes this clear: MSLAS goes from 75.2 (Full) →84.5 (GoldTok, +9.3) →85.7 (GoldWT, +1.2; +10.5 total). This motivates per- language tokenizer selection and modeling content word identification as a dedicated task. Loss-weight tuning largely favored parser=2.0, 4 Full GoldTok GoldWT 60 80 100 MSLAS Hebrew Italian Swedish Turkish Figure 2: MSLAS across setups (Full, GoldTok, GoldWT) for four languages with the largest gains. Full: predicted tokenization and predicted content word iden- tity. GoldTok: gold tokenization with predicted content word identity. GoldWT: gold tokenization plus gold content word identity. Language Parser Morph CWI Czech 2.0 1.5 1.0 English 2.0 1.5 1.0 Hebrew 2.0 1.5 1.0 Italian 2.0 1.5 1.0 Polish 2.0 1.5 1.0 Portuguese 2.0 1.5 1.0 Serbian 2.0 1.5 1.0 Swedish 2.0 1.5 1.5 Turkish 2.0 2.0 1.5 Table 4: Optimal loss weight configurations by language. CWI =content word identification. morph=1.5, CWI=1.0; Turkish and Swedish bene- fited from higher weights on morph/CWI (Table 4). 5 Error Analysis We performed error analysis on the models trained with our local data splits (90% train, 10% dev, of- ficial dev as test). We analyzed only the first seed model for each language, as the low standard de- viations indicate minimal variation across seeds. The analysis uses scripts that replicate the official evaluation logic to ensure our error categorization matches the scoring methodology. 5.1 Nominal Morphology Errors The main feature prediction errors occur in nominal morphology, with Gender, Number, and Case show- ing the highest confusion rates. Since languages have different feature inventories (e.g., Czech in- cludes Dual while others do not), creating a unified confusion matrix is not feasible. We selected Czech as a representative example because it has by far the most training data points, resulting in more stable model behavior. Our analysis of Czech reveals strong overall per- formance, with 99.1% accuracy for Gender and 99.6% for Number predictions. For Gender, the model correctly classifies the vast majority of in- stances, with Feminine (12,405 correct), Mascu- line (14,914 correct), and Neuter (5,325 correct) all showing high diagonal values in the confusion matrix. The annotation scheme includes syncretic forms like \"Fem,Masc\" for grammatically ambigu- ous cases. The most common confusions occur between Masculine and Feminine (110 instances misclassified as Feminine when Masculine was cor- rect), though these remain relatively rare. Simi- larly, for Number, Singular (24,653 correct) and Plural (9,587 correct) are accurately predicted, with minimal confusion between categories (only 44 Singular instances misclassified as Plural, and 85 Plural instances misclassified as Singular). Since our model uses multi-label classification with sigmoid activation (threshold 0.5), it occa- sionally predicts semantically incompatible feature combinations—for instance, simultaneously pre- dicting both a specific gender value (e.g., \"Fem\") and a syncretic form containing that value (e.g., \"Fem,Masc\"). While these semantically nonsen- sical predictions are rare (occurring in fewer than 100 instances out of over 30,000), they suggest that post-processing constraints based on linguistic"}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 6, "text": "semantically incompatible feature combinations—for instance, simultaneously pre- dicting both a specific gender value (e.g., \"Fem\") and a syncretic form containing that value (e.g., \"Fem,Masc\"). While these semantically nonsen- sical predictions are rare (occurring in fewer than 100 instances out of over 30,000), they suggest that post-processing constraints based on linguistic compatibility rules could eliminate such predic- tions and further improve the performance. For Case features, plotting a confusion matrix is impractical due to the >100 possible values in the expanded inventory. While there is some vari- ation across languages, aggregating the most fre- quent errors reveals consistent patterns. Table 5 shows the 10 most common Case confusions aver- aged across all languages. The high frequency of Nom-Acc confusions (154 and 140 instances) reflects both the prevalence of these cases in the data and their potential ambiguity—distinguishing core ar- guments becomes particularly challenging in com- plex sentences with long-distance dependencies or multi-clause structures. This pattern holds across languages despite their individual variations, sug- gesting that even within the expanded Case system, these fundamental grammatical distinctions remain challenging when syntactic complexity increases. These systematic errors in core grammatical cases suggest a targeted improvement strategy: increas- ing loss weights for frequently confused cases (es- pecially Nom/Acc) during training. Given our joint model architecture where all tasks share embed- 5 Count Gold case Predicted case 154 Acc Nom 140 Nom Acc 77 Nom Conj;Nom 47 Nom Gen 44 Conj;Nom Nom 43 Gen Nom 36 Acc Gen 25 Gen Acc 22 Gen Conj;Gen 15 Dat Ins Table 5: Top 10 most frequent case prediction errors (average across all languages). dings, better representation of these central argu- ments could benefit dependency parsing as well. 5.2 Spatial Case Results We evaluate our model’s performance on the fine- grained spatial Case values, a particularly challeng- ing subset due to the numerous possible inflec- tional meanings that this domain contains.3 The complete inventory of spatial cases includes over 40 fine-grained distinctions. Table 6 shows high performance across all languages (F1 scores 89.2- 98.7%), demonstrating that our model successfully learned the unified Case system for spatial mean- ings. This annotation scheme directly names in- flectional meanings regardless of the grammatical markers used - for instance, in Polish, when abla- tive meaning is expressed periphrastically through a clitic (an adposition)4 plus an inflected form (a root with a genitive case affix), the system assigns the inflectional meaning (e.g., Case=Abl) instead of the genitive meaning conveyed by the suffix on its own. Our model’s performance on these distinc- tions suggests it effectively captures the mapping between diverse surface forms and their underly- ing spatial semantics. This opens opportunities for injecting linguistic knowledge about spatial rela- tions in downstream applications, leveraging the semantic transparency of the annotation scheme. 5.3 Dependency Parsing Errors For dependency relation errors, we analyze confu- sions across all languages since the label inventory 3These notions are understood as defined by Haspelmath (2025): “inflectional meaning” designates the specific mean- ing conveyed by an inflected form (for example, ablative), and “inflectional domain” denotes the broader class of related properties in which this meaning"}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 7, "text": "errors, we analyze confu- sions across all languages since the label inventory 3These notions are understood as defined by Haspelmath (2025): “inflectional meaning” designates the specific mean- ing conveyed by an inflected form (for example, ablative), and “inflectional domain” denotes the broader class of related properties in which this meaning is categorized (for example, case). 4The classification of adpositions as clitics follows the definition proposed by Haspelmath (2023). Language Precision Recall F1 Czech 98.2 98.4 98.3 English 93.3 90.3 91.8 Hebrew 88.4 90.0 89.2 Italian 98.0 97.0 97.5 Polish 98.4 97.2 97.8 Portuguese 98.5 99.0 98.7 Serbian 96.4 93.7 95.1 Swedish 98.4 96.1 97.2 Turkish 94.7 96.4 95.6 Table 6: Spatial case performance (%) across languages using micro-averaged metrics. Count Gold label Predicted label 67 obl nmod 63 nmod obl 11 obj nsubj 11 advmod _ 10 nmod flat 10 nmod amod 9 nsubj obj 9 iobj obj 8 obj obl 8 nsubj root Table 7: Top 10 most frequent deprel labeling errors (average across all languages). is universal. Table 7 presents the 10 most frequent labeling errors aggregated across languages. The nmod-obl confusion dominates with 67 and 63 in- stances respectively, accounting for over 40% of the top errors. This pattern is linguistically ex- pected as the boundary between nominal modifiers and oblique arguments could involve borderline cases. Unlike other languages where errors concen- trate on the nmod/obl distinction, Turkish shows a much more dispersed error pattern with confusions spread across many dependency relations. This suggests that our joint architecture may not be opti- mal for Turkish’s non-projective structures and rich morphology. A dedicated non-projective parsing algorithm might better capture Turkish’s complex dependency patterns. Additionally, we analyze attachment distance patterns specifically for parsing errors (i.e., tokens with incorrect head assignments). Figure 3 shows the distribution of attachment distances for Czech parsing errors, comparing gold (blue) versus pre- dicted (orange) distances for these misparsed to- kens. The graph reveals that while most gold at- tachments occur at distances 1-3, the model’s er- rors tend to predict longer distances (note the or- ange bars extending further right). This indicates 6 Figure 3: Distribution of attachment distances for pars- ing errors in Czech. Figure 4: Attachment direction confusion matrix for Czech. the parser frequently overlooks nearby heads in fa- vor of more distant ones when making mistakes. Figure 4 presents a heatmap of misparsed tokens where rows represent gold attachment directions and columns show predicted directions. The strong diagonal (LEFT→LEFT: 1179, RIGHT→RIGHT: 1058) confirms the model correctly identifies at- tachment direction in most error cases. However, within each correct direction, the parser still selects the wrong head - for instance, when it correctly predicts a leftward attachment, it often chooses a head that is too far to the left. 6 Conclusions We present a joint multitask architecture for unified morphosyntactic parsing that achieves first place in the UniDive 2025 shared task. Our key contribution is explicitly modeling content word identification as a classification task, creating a robust cascade where the identification determines parsing and feature assignment. Our analysis reveals systematic error patterns pointing to"}
{"doc_id": "2508.14307v1", "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14307v1", "chunk_id": 8, "text": "joint multitask architecture for unified morphosyntactic parsing that achieves first place in the UniDive 2025 shared task. Our key contribution is explicitly modeling content word identification as a classification task, creating a robust cascade where the identification determines parsing and feature assignment. Our analysis reveals systematic error patterns pointing to specific improvement opportunities. Case confusions concentrate on core grammatical distinctions (Nom-Acc), while dependency errors re- flect the expected challenges at the nmod-obl bound- ary. While these patterns are linguistically under- standable, they suggest potential room for improve- ment through weighted training or specialized han- dling of frequently confused categories, though such optimizations may yield only incremental gains. A more substantial enhancement to the annota- tion scheme could be making explicit which func- tion words contribute features to which content words. Currently, function words are marked with ‘_’ and their grammatical information is incorpo- rated into \"related\" content words, but these re- lationships remain implicit. An indexing system could explicitly link each function word to its tar- get content word. This would not only reduce am- biguity in feature assignment but also make the annotation more transparent for researchers unfa- miliar with specific languages, as they could trace exactly how morphosyntactic information flows from function words to content words in the uni- fied representation. Finally, the 30-point performance gap between Portuguese and Turkish highlights fundamental challenges in handling typologically diverse lan- guages within a unified framework. While the parser excels at the predominantly projective struc- tures, Turkish’s agglutinative morphology and flex- ible word order might be introducing some diffi- culties. The dispersed error patterns observed for Turkish—contrasting with the concentrated confu- sions in other languages—suggest that the current architecture may not be optimal for highly non- projective languages. Future work could explore specialized parsing algorithms designed for non- projective structures or alternative architectures that better handle long-distance dependencies and flexible word order. Despite these challenges, our results across nine languages demonstrate the vi- ability of joint morphosyntactic modeling for the task. Acknowledgments We thank the anonymous reviewers for their con- structive feedback. This research was supported 7 by The University of Melbourne’s Research Com- puting Services and the Petascale Campus Initia- tive. Demian is funded by the Graduate Research Scholarship from the Faculty of Engineering and Information Technology, University of Melbourne. COST (European Cooperation in Science and Tech- nology) kindly provided funding for travel to the event."}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 0, "text": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation Amirmohsen Sattarifard *1, Sepehr Lavasani *1, Ehsan Imani 1, Kunlin Zhang 1, Hanlin Xu 2, Fengyu Sun 2, Negar Hassanpour †1, Chao Gao †1 1Huawei Technologies Canada 2Huawei *Equal Contribution, †Equal Advising Abstract Deploying Large Language Models (LLMs) on edge hard- ware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pat- tern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical re- sults across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free meth- ods, particularly in challenging long-form generation scenar- ios, without relying on auxiliary predictors or adding any in- ference overhead. 1 Introduction Large Language Models (LLMs) have shown remarkable performance in a wide range of natural language applica- tions (Team et al. 2023, 2024a; Touvron et al. 2023; Jiang et al. 2023; Anthropic 2024), leading to increasing demand for deployment on edge devices (Alizadeh et al. 2024; Song et al. 2024; Xue et al. 2024). These models, however, owe their performance to enormous parameter counts and sub- stantial memory and compute requirements which make in- ference on consumer-grade hardware challenging (Xue et al. 2024). An effective on-device model therefore should have low inference latency and memory footprint with little com- promise on output quality. Structured pruning is a well-studied approach to leverage sparsity, especially in feed-forward blocks, and to reduce in- ference time and memory usage. Structured pruning algo- rithms iteratively remove weights or neurons in a hardware- friendly manner based on magnitude or impact, and then fine-tune the pruned model to recover performance (LeCun, Denker, and Solla 1989; Ma, Fang, and Wang 2023; Xia et al. 2023). The actual magnitude or impact depends on the task or even the current input and thus static pruning results in poor quality-efficiency trade-off. Further, iterative fine-tuning is impractical for large-scale models. Predictor-based methods improve upon static pruning by using contextual sparsity (Song et al. 2024; Xue et al. 2024; L0 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22 L23 L24 L25 L26 L27 L28 L29 L30 L31 Global + Local Global only Local only Figure 1: Jaccard similarity (↑) between critical neuron sets identified by the Global-Local Aggregation (green), Global (orange), and Local (blue) methods with the oracle’s as reference, across various layers on Llama 3 8B. By ag- gregating both global and local information, our proposed approach more faithfully identifies the oracle’s critical neu- ron set (see Sec. 4.1 for details). Alizadeh et al. 2024). A predictor is first trained on an of- fline corpus to predict neuron activations using information from a previous layer. During inference time, at each layer"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 1, "text": "and local information, our proposed approach more faithfully identifies the oracle’s critical neu- ron set (see Sec. 4.1 for details). Alizadeh et al. 2024). A predictor is first trained on an of- fline corpus to predict neuron activations using information from a previous layer. During inference time, at each layer the predictor anticipates the active neurons in a subsequent layer to avoid loading and computing the rest of the neurons. Predictor-based methods yield a higher sparsity ratio than static pruning since the number of inactive neurons that can be predicted from a previous layer is higher than the number of consistently inactive neurons. The improvement comes at the cost of (i) offline predictor training, (ii) inference-time predictor overhead, and (iii) extra memory-transfer latency due to loading different neurons for a sequence of generated tokens. Training-free methods aim to alleviate these limitations by obtaining a sparsity mask from the prompt and using it throughout the subsequent generation (Dong, Chen, and Chi 2024; Ma et al. 2025). These algorithms rely on the flocking phenomenon, in which neuron-activation patterns mostly persist throughout the sequence. Since the prompt is fed to the model at once, the entire layer is computed for all prompt tokens and the sparsity patterns are used for finding the set of the most active neurons. This set of neurons will remain fixed throughout the generation phase. These meth- ods, however, perform poorly when the prompt is too short (i.e., there are few samples for deriving the mask), or when the response is relatively long and the actual activation pat- tern drifts. Some recent works have instead proposed deriving a dy- namic mask for every newly generated token to track such drift (Zhang et al. 2025; You et al. 2025; Yin et al. 2025; Stora¨ ı, Lee, and Hwang 2025; Qi et al. 2025; Shin, Yang, and Yi 2025; Lee, Ramachandran, and Krishna 2025; Yang et al. 2025). However, generating a new mask at every decoding step incurs extra I/O overhead. Specifically, on edge devices where inference is memory-bound, the additional overhead may offset the latency gains achieved through sparsifica- tion (Liu et al. 2024). Therefore, we focus on algorithms that form a single static mask during prompt (prefill) phase and reuse it for the entire generation phase, in order to remain edge-friendly. In this work, we propose two methods, Activation- based and Impact-based Global-Local Neural Importance Aggregation for FFN SparSification (A/I-GLASS) that in- corporate model-intrinsic (global) information about neu- ron activation or impact, while respecting document-specific (local) activation patterns. The proposed algorithms Elim- inate the need for offline training stage and inference- time overhead of predictor-based methods while signif- icantly outperforming previous training-free methods in short prompt and long generation scenarios. Contributions. We present three main contributions: (i) we introduce GLASS, a training-free global-local FFN sparsification framework that utilizes local prompt activa- tions as well as precomputed global neural importance to conduct dynamic, inference-time pruning; (ii) we provide a theoretical foundation by deriving GLASS as the maximum- likelihood fusion of two independent Plackett-Luce rank- ings, equivalent to a weighted Borda count, grounding"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 2, "text": "training-free global-local FFN sparsification framework that utilizes local prompt activa- tions as well as precomputed global neural importance to conduct dynamic, inference-time pruning; (ii) we provide a theoretical foundation by deriving GLASS as the maximum- likelihood fusion of two independent Plackett-Luce rank- ings, equivalent to a weighted Borda count, grounding our method in a principled statistical framework; and (iii) we empirically demonstrate up to a 45.10% reduction in per- plexity and a 25.26% reduction in KL divergence over the strongest prior training-free baseline at 50% sparsity across multiple LLMs and benchmarks. 2 Problem Formulation 2.1 Preliminaries and Notation We consider a modern Transformer (Vaswani et al. 2017) block where the Feed-Forward Network (FFN) adopts a gated structure: zu = xWup + bup, au = ϕu(zu) zg = xWgate + bgate, ag = ϕg(zg) h = au ⊙ag y = hWdown + bdown, (1) Gated FFN Parameters and Dimensions Here, x ∈Rd is the input token embedding. The FFN expands the rep- resentation to a higher-dimensional space of width m, per- forms elementwise gating, and projects back to Rd: • Wup, Wgate ∈Rd×m are the expansion matrices. • Wdown ∈Rm×d is the projection matrix. • bup, bgate ∈Rm, and bdown ∈Rd are the bias terms. • ϕu(·) is an activation function and ϕg(·) is a sigmoid-like gating function. • h ∈Rm is the FFN hidden unit vector. Its j-th element is denoted by hj Associate weights of a hidden unit refers to the triplet of parameters and operations indexed by j in the up, gate, and down projections: (W (:,j) up , W (:,j) gate , W (j,:) down) (2) This unit contributes the term yj = ϕu(xW (:,j) up ) · ϕg(xW (:,j) gate ) · W (j,:) down + bj down (3) to the FFN output y ∈Rd. Sparsifying the FFN corresponds to removing some of these hidden units. Problem Statement Our goal is to dynamically prune a subset of FFN hidden units (neurons) at inference-time, to reduce both the parameter count and computational cost, while minimizing degradation in downstream model qual- ity with respect to the base (unpruned) model. Specifically, given a per-layer budget k < m, we aim to select a subset of k hidden units (referred to as critical neurons) to retain, and removing the remaining m −k units. 3 Previous Work Transformer sparsification techniques can be broadly cat- egorized into training-based and training-free methods. Training-based methods, such as DejaVu (Liu et al. 2023), ShadowLLM (Akhauri et al. 2024), and PowerInfer (Song et al. 2024; Xue et al. 2024) rely on auxiliary predictors or routing mechanisms to achieve sparsity. Despite their ef- fectiveness, they introduce inference-time overhead due to runtime decisions or auxiliary computations. In contrast, training-free methods operate post-hoc on pre-trained mod- els, and are therefore better suited for latency-sensitive de- ployments. 3.1 Training-Free Methods GRIFFIN (Dong, Chen, and Chi 2024) leverages the obser- vation that FFN sub-units activated in the prefill phase re- main relevant during generation, enabling single-stage struc- tured pruning based on neuron activation magnitude. TDA (Ma et al. 2025) extends this concept by"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 3, "text": "suited for latency-sensitive de- ployments. 3.1 Training-Free Methods GRIFFIN (Dong, Chen, and Chi 2024) leverages the obser- vation that FFN sub-units activated in the prefill phase re- main relevant during generation, enabling single-stage struc- tured pruning based on neuron activation magnitude. TDA (Ma et al. 2025) extends this concept by applying per- layer activation thresholding during prefill, masking out neu- rons of lower importance dynamically. TEAL (Liu et al. 2024) adopts a similar approach to TDA, and proposes a greedy optimization method for identifying layer-wise sparsity levels. CATS (Lee et al. 2024), in contrast, extracts activations once from offline data (rather than from prompt activations), de- termines a threshold based on the k-th percentile, and zeros out activations below this threshold at inference time. Orthogonal to activation-based sparsification, another re- search direction estimates neuron or channel importance via their impact on model loss, often using Taylor expansions. 3.2 Impact-Based Importance Estimation Optimal Brain Damage (OBD) (LeCun, Denker, and Solla 1989) and Optimal Brain Surgeon (OBS) (Hassibi, Stork, and Wolff 1993) are among the earliest approaches to net- work pruning based on estimating the impact of individual parameters on the training loss. They use a second-order Taylor expansion of the loss with respect to model weights, and compute saliency scores using the diagonal (OBD) or full (OBS) Hessian of the loss. While theoretically grounded, these methods are computationally expensive and scale poorly with model size, limiting their applicability to modern large-scale networks. PerforatedCNNs (Figurnov et al. 2016) is an earlier ap- plication of the first-order Taylor expansion for structured pruning, focusing on reducing spatial redundancy in con- volutional neural networks. This approach demonstrates the general effectiveness of first-order saliency and sets the stage for later neuron- and kernel-level pruning methods. Similarly, Molchanov et al. (2017) applies the first-order Taylor criterion to prune convolutional channels. Molchanov et al. (2019) generalizes this to second-order expansions, and Kwon et al. (2022) introduces a Fisher information- based, training-free pruning framework for Transformers, enhancing inference efficiency. 4 Model-Intrinsic Neural Importance We measure model-intrinsic (also referred to as global) neu- ral importance using any input-independent neural statis- tics that enables accurate identification of critical neurons. If the statistics are extracted from sufficiently large and diverse corpora, we still consider them input-independent and model-intrinsic. In this work, we consider two global neural importance (M g) notions, namely, activation magni- tude (Ag) and impact (Ig). As these are computed only once per model, this global importance profile can be precom- puted and cached at negligible storage cost. 4.1 Activation-based Importance In a similar spirit to GRIFFIN, one method to identify the salient neurons is based on their average activation across a task-agnostic corpus. For each FFN unit j, we define its global activation magnitude as: Ag j := Ex∼D h ˆ hj(x) i , (4) where ˆ h(x) = (ˆ h1(x), ..., ˆ hm(x)) denotes the ℓ2 normal- ized activations over each token x. The ℓ2 normalization is to ensure that activations are comparable across tokens and layers. The resulting Ag j serves as a model-intrinsic impor- tance score for unit"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 4, "text": "hj(x) i , (4) where ˆ h(x) = (ˆ h1(x), ..., ˆ hm(x)) denotes the ℓ2 normal- ized activations over each token x. The ℓ2 normalization is to ensure that activations are comparable across tokens and layers. The resulting Ag j serves as a model-intrinsic impor- tance score for unit j, and will later be used for critical neu- ron selection. Activation Consistency Sparsification methods that rely solely on prompt activations (namely GRIFFIN, TDA, and TEAL) to identify critical neurons are highly sensitive to prompt length. Specifically, as the prompt becomes shorter, the statistic used to select critical neurons (i.e., ˆ h(x)) be- comes more susceptible to error, leading to performance drops in scenarios with short prompts and long genera- tions (Dong, Chen, and Chi 2024). To address this, we studied to what extent critical neurons are input-dependent. If many neurons consistently appear as critical neurons across diverse prompts, it implies existence of a global set of critical neurons that can be identified of- fline and later utilized at inference-time to make the critical neuron selection process more robust to prompt length. We conducted an experiment to assess this hypothesis. From the WikiText data (Merity et al. 2016), we extracted two disjoint sets, each consisting of 100 documents with ex- actly 1024 tokens (constructed by concatenating shorter doc- uments). One set was used to compute the global activation, denoted as Ag. The other set was used to evaluate and com- pare the following methods for extracting critical neurons at 50% sparsity: 1. Oracle: Using the full documents to obtain the ideal crit- ical neurons for the entire document.1 2. Local: Only using Al computed from the first 100 tokens in the corresponding document (representing GRIFFIN, TDA, and TEAL). 3. Global: Only using Ag (representing CATS). 4. Global-Local Aggregation: Using average of Ag and Al, thereby incorporating both global and local signals. To evaluate the effectiveness of aggregating global and local activation statistics, we measure how similar the crit- ical neuron sets obtained from each method (local, global, and global-local aggregation) are to the oracle’s ideal criti- cal neuron sets. Specifically, we compute the Jaccard simi- larity2 between each method’s set of critical neurons and that of the oracle. The results on LLaMA 3 8B (Fig. 1) suggest that (i) global critical neurons do exist; and (ii) this approach yields a set more similar to the ideal critical set than using only global or only local signals, across all layers. 4.2 Impact-based Importance As an alternative to raw activations, we consider the impact of each neuron on the loss, echoing classical Taylor prun- ing (e.g., (Figurnov et al. 2016; Molchanov et al. 2017)). Let fθ(h(x)) be the network output, y the ground-truth token, and L \u0000fθ(h(x)), y \u0001 the cross-entropy loss of the model as a function of the activations of the current FFN layer h(x) = (h1(x), . . . , hm(x)). Ablating neuron j sets 1In practice, this is not feasible, as the oracle requires observing the activations of all neurons not only for the prompt sequence but also for the"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 5, "text": "model as a function of the activations of the current FFN layer h(x) = (h1(x), . . . , hm(x)). Ablating neuron j sets 1In practice, this is not feasible, as the oracle requires observing the activations of all neurons not only for the prompt sequence but also for the response sequence in order to identify the optimal set of critical neurons for that document. Nonetheless, it can serve as a reference in our comparisons. 2The Jaccard similarity between two sets A and B is the size of their intersection divided by the size of their union: J(A, B) = |A∩B| |A∪B|, ranging from 0 (no overlap) to 1 (identical sets). hj 7→0, i.e. ∆hj = −hj. Applying a first-order Taylor ex- pansion of L at h yields ∆Lj ≈∇hL(h) · ∆h = ∂L ∂hj ∆hj = −hj gj, (5) where gj := ∂L/∂hj. Because only the magnitude of ∆Lj in Eq. (5) matters for pruning, we define the (neuron-wise) impact score Ig j = Ex∼D \u0002 |hj(x) gj(x)| \u0003 . (6) Neurons with larger Ig j are predicted to incur a larger dis- turbance in loss when removed. The resulting Ig j serves as another model-intrinsic importance score, and will later be used for critical neuron selection. Impact locality. Consistent with the activation-locality phenomenon reported by Song et al. (2024), we observe that only a small, largely input-independent subset of neurons contributes the majority of the impact (long-tailed distribu- tion). The complete empirical verification, including figures and layer-wise statistics, is presented in Appendix 2.1. 4.3 Null Prompt Stimulation To compute global activations or neural impacts, one ap- proach is to run the model on a dataset and collect the re- spective statistics (Lee et al. 2024). However, standard cor- pora like Wikipedia can bias the statistics toward specific linguistic patterns. We instead propose Null Prompt Stimu- lation (NPS): generating text from the model itself using a null prompt (e.g., empty string or BOS token) under typi- cal sampling settings. This produces self-generated data that better reflects the model’s intrinsic token distribution. Em- pirically (see Fig. 3), NPS yields cleaner and more con- sistent impact estimates, by aligning with the model’s own inference-time behavior and avoiding external corpus bias. 5 GLASS 5.1 Formulation Let each FFN layer contain m neurons, and for each neu- ron i ∈[N], Al j denote the expected activation magnitude of FFN unit j over the prompt tokens, and M g j denote its expected neural importance obtained over D (the offline dataset tokens) Al j, M g j ∈R≥0, (7) Directly utilizing heterogeneous quantities can be prob- lematic because their scales, distributions, and semantics differ, making weighted summation ill-defined. In contrast, ranking each unit within its respective distribution renders the two components comparable by preserving their ordinal preferences. In GLASS, rather than directly combining raw magnitudes, which may be incompatible when the local and global components are of different types (e.g., activation vs. impact), we first convert each to ranks. Rl = Rank \u0000Al\u0001 , Rg = Rank \u0000M g\u0001 , (8) with Rank(Al j) and Rank(M"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 6, "text": "preferences. In GLASS, rather than directly combining raw magnitudes, which may be incompatible when the local and global components are of different types (e.g., activation vs. impact), we first convert each to ranks. Rl = Rank \u0000Al\u0001 , Rg = Rank \u0000M g\u0001 , (8) with Rank(Al j) and Rank(M g j ) denoting the ranks of unit j in local and global importance respectively (with higher rank value indicating higher importance). Figure 2: Overview of GLASS. Let each neuron have a latent utility µi ∈R (i.e., a proxy of importance of the respective neuron), and εi be a sample from the Gumbel distribution (type-I extreme-value distri- bution). The observed (noisy) utilities are Ui = µi + εi, εi i.i.d. ∼Gumbel(0, 1), (9) so the probability of observing a full ranking π = (π1, . . . , πm) ( πt = item placed at rank m −t + 1) follows the Plackett-Luce distribution 3 (Luce et al. 1959; Plackett 1975): P(π | µ) = m Y t=1 exp(µπt) Pm j=t exp(µπj). (10) We treat the global and prompt rankings as independent realizations of Eq. (10). With a uniform prior over permuta- tions, the joint log-likelihood is ℓℓ(µ) = λ log P(π(l) | µ)+(1−λ) log P(π(g) | µ), (11) where λ ∈[0, 1] balances prompt-specific context (λ →1) and model-intrinsic diversity (λ →0). Following Yellott Jr (1977), we expand each log P in Eq. (11) around the “all-utilities-equal” point µ = 0 (full derivation in Appendix 1): log P(π(s) | µ) = Cs+ m X i=1 R(s) i µi+O(∥µ∥2), s ∈{l, g}. (12) Discarding the second-order terms, up to an irrelevant constant, and inserting Eq. (12) into Eq. (11) ℓℓlin(µ) = m X i=1 h λ R(l) i + (1 −λ) R(g) i i µi. (13) 3Drawing each neuron’s observed score following Eq. (9) and then selecting the winner by Gumbel-max (arg maxi{µi + εi}) gives the familiar soft-max/logit choice probabilities (Jang, Gu, and Poole 2016). Repeating that selection step to build an entire ranking produces exactly the Plackett-Luce likelihood in Eq. (10). 5.2 Maximum-Likelihood Ranking Only the ordering of µ matters for maximizing Eq. (13). Choose an arbitrary ordering, e.g., µi = Ri (R ∈{x ∈ Nm | set(x) = {1, . . . , m}}). Under this choice, maximiz- ing Eq. (13) is equivalent to sorting neurons by the scalar weight GLASSi = λ Rank(Al j) + (1 −λ) Rank(M g j ). (14) When M g j represents global activation magnitudes, it is equal to Eq. (4), and we refer to it as A-GLASS; when it represents neural impacts, it is equal to Eq. (6), and we refer to it as I-GLASS. By aggregating ranks, GLASS treats the local and global importances as two independent voters over the criticality of neurons. This ensures that the final selection respects both prompt relevance and model-intrinsic impor- tance while avoiding dominance by any single unnormalized measure. The top-k critical neurons are then selected based on the aggregated ranks (Fig. 2), preserving units critical for interpreting the prompt while injecting model-intrinsic con- texts"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 7, "text": "of neurons. This ensures that the final selection respects both prompt relevance and model-intrinsic impor- tance while avoiding dominance by any single unnormalized measure. The top-k critical neurons are then selected based on the aggregated ranks (Fig. 2), preserving units critical for interpreting the prompt while injecting model-intrinsic con- texts to mitigate overfitting to any single prompt domain. Eq. (14) coincides with a weighted Borda count. Early social-choice work proved that Borda is the Maximum- Likelihood (ML) estimator of the ground-truth ranking in the i.i.d., small-utility, high-noise Plackett-Luce set- ting (Young 1986). Hence GLASS is not a heuristic but the ML (and, with a uniform permutation prior, MAP) solu- tion to fusing two rank observations under a random-utility model. 6 Experiments In this paper, we compare our method with GRIFFIN, which represents a family of sparsification methods that rely solely on prompt statistics. We choose GRIFFIN because deter- mining the sparsity level is outside the scope of this work; our focus is on comparing different neural importance sig- nals, not on deciding how many critical neurons to select. TDA and TEAL, which use thresholding and result in vary- ing sparsity levels across documents, are less suitable for this comparison. However, under fixed sparsity regimes, they are functionally equivalent to GRIFFIN. We also do not include CATS in our experiments, as it has been shown to underper- form compared to methods that use prompt activations for pruning ((Liu et al. 2024)), and our preliminary results (Fig. 1) indicate that it exhibits lower similarity to the ideal set of critical neurons than global-local aggregation. Following GRIFFIN’s protocol, we evaluate our methods on both Classification (C) and Short-form Generation (SG) benchmarks. To more rigorously assess performance under extended decode lengths, we introduce a Long-form Gener- ation (LG) benchmark, that is designed specifically for tasks with brief prompts and lengthy outputs, and demonstrate its suitability for evaluating generation-time sparsification. We note that the benefits of using a static mask during the generation phase become significant only when the gener- ation phase is sufficiently long, since models often operate at full capacity during the prompt processing stage. There- fore, sparsification methods do not meaningfully reduce la- tency for tasks involving long prompts but short genera- tions. Yet GRIFFIN reports results exclusively on C and SG benchmarks, overlooking the short-prompt, long-generation regime in which generation-time sparsification truly shines. 6.1 Datasets The C datasets include HellaSwag (Zellers et al. 2019), PIQA (Bisk et al. 2020), COPA (Roemmele, Bejan, and Gordon 2011), ARC-Easy/Challenge (Clark et al. 2018), and BoolQ (Clark et al. 2019). For the SG task, we use XSum (Narayan, Cohen, and Lapata 2018), CNN/- DailyMail (See, Liu, and Manning 2017; Hermann et al. 2015), CoQA (Reddy, Chen, and Manning 2019), and QASPER (Dasigi et al. 2021). Further details of the C and SG datasets are provided in the Appendix 3.1. For the short-prompt LG task, we used Alpaca (Taori et al. 2023), an instruction-following dataset generated via self-instruction using GPT-3. This dataset covers a wide range of tasks and is commonly used to train and evaluate general-purpose LLMs."}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 8, "text": "of the C and SG datasets are provided in the Appendix 3.1. For the short-prompt LG task, we used Alpaca (Taori et al. 2023), an instruction-following dataset generated via self-instruction using GPT-3. This dataset covers a wide range of tasks and is commonly used to train and evaluate general-purpose LLMs. To ensure sufficiently long genera- tions, we selected samples from the Alpaca dataset whose ground-truth outputs exceed 100 tokens and for which none of the base models that we study in this work generated out- puts shorter than 20 tokens, resulting in 3,602 samples. 6.2 Models For C and SG tasks, we evaluate five publicly available LLMs in the 6-7 billion parameter range: Gemma 7B (Team et al. 2024a), Llama 2 7B (Touvron et al. 2023), Mis- tral 7B (Jiang et al. 2023), OPT 6.7B (Zhang et al. 2022), and Relu-LlaMA 7B (Team 2023), following GRIFFIN and TDA. These models vary in architecture, training data, and optimization strategies, providing a representative set of contemporary foundation models. For the LG task, we adopted a variety of instruction- tuned pretrained models, as base models are generally not capable of generating cohesive long responses and of- ten generate repetitive content. The models under study are: Gemma 7B (Team et al. 2024a), Gemma 2 9B and 27B (Team et al. 2024b), Mistral 7B (Jiang et al. 2023), Qwen 2.5 7B and 14B (Qwen et al. 2025), Phi 3 14B (medium) (Abdin et al. 2024), Llama 3 8B (Dubey et al. 2024), as well as Gemma 3n E2B and E4B, which are based on the MatFormer architec- ture (Devvrit et al. 2024) and already incorporate a built- in sparsification mechanism specifically designed for on- device applications. 6.3 Evaluation Metrics For evaluation of the C and SG tasks, we adopt GRIFFIN’s set of evaluation metrics that includes classification accu- racy, F1-Score, exact match, and ROUGE-1/2/L (see Ap- pendix 3.2 for more details). For the LG task, we argue that evaluating how a sparsi- fication method affects an LLM’s performance using met- rics such as ROUGE or exact match can be misleading. Even small differences in the next-token distribution, when accumulated over many generation steps, can cause out- puts to diverge substantially, resulting in a rapid decline in n-gram-based evaluation scores. Alternatively, one can di- rectly assess changes in the next-token probabilities using 0° 15° 30° 45° 60° 75° 105° 120° 135° 150° 165° 180° 90% 80% 70% 60% 50% 40% 30% 20% 10% (a) Gemma KLD 0° 15° 30° 45° 60° 75° 105° 120° 135° 150° 165° 180° 90% 80% 70% 60% 50% 40% 30% 20% 10% (b) Llama KLD 0° 15° 30° 45° 60° 75° 105° 120° 135° 150° 165° 180° 90% 80% 70% 60% 50% 40% 30% 20% 10% (c) Mistral KLD Figure 3: Visualization of KLD (PPL results in Appendix 3.4) for different sparsification methods across activation densities and models. Each semicircle corresponds to a specific density (10% to 90%), with method values placed according to their relative magnitude within each density (min at the rightmost (angle=0◦) point of each semicircle). The size of each"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 9, "text": "KLD (PPL results in Appendix 3.4) for different sparsification methods across activation densities and models. Each semicircle corresponds to a specific density (10% to 90%), with method values placed according to their relative magnitude within each density (min at the rightmost (angle=0◦) point of each semicircle). The size of each dot is proportional to the absolute metric value relative to the global maximum for the given model, providing an intuitive view of both method ranking and value significance. For all plots, a method closer to the angle=0◦line indicates better performance. metrics like Perplexity (PPL) and Kullback-Leibler Diver- gence (KLD) over the vocabulary probability distributions for the next token. By conditioning on the base model’s gen- erated context, this approach prevents the accumulation of divergence over long sequences and more accurately cap- tures deviations caused by sparsification. Perplexity Perplexity (PPL) gauges how surprised a lan- guage model is when predicting the next token in a sequence. It is the exponentiated average negative log- likelihood per token: PPL = exp \u0010 −1 N N X i=1 log pθ(xi) \u0011 , (15) where xi is the i-th token and pθ(xi) is the model’s out- put probability density under parameters θ. Lower values in- dicate better predictive performance. We use the unpruned model’s generated response as the reference for comput- ing PPL, which effectively measures how unlikely the un- pruned model’s responses are under the sparsified model. This makes sense, as we want to minimize the deviation of the sparsified model from the base model. Kullback-Leibler Divergence The average token-level Kullback-Leibler Divergence (KLD) between the reference distribution Pi and the model distribution Qi is KLD = 1 N N X i=1 X v∈V Pi(v) log Pi(v) Qi(v), (16) where N is the sequence length, v ∈V the vocabulary, Pi the reference token distribution, and Qi = softmax(logitsi) is the model-predicted distribution for the i-th position. Note that since the vocabulary is large, we compute the KLD over only the top-100 tokens with the highest proba- bility under the unpruned model. This approximation is jus- tified as we found empirically that the top-100 tokens of- ten account for nearly the entire probability mass, with their summed probabilities effectively equal to 1. 6.4 Results and Discussions As mentioned earlier, in addition to the C and SG bench- marks (results detailed in the next subsection), we also evaluate our methods on the Alpaca dataset, which pairs short prompts with extended generation lengths, providing a more representative and challenging testbed for assessing generation-time sparsification. Classification and Short Generation Tasks Ta- bles 2 and 3 in Appendix 3.3 indicate that the observed differences in accuracy between I-GLASS and GRIFFIN on the classification tasks were not substantial. Specifically, the largest improvement over GRIFFIN was achieved by Mistral 7B on the ARC-Challenge benchmark, with a gain of 0.68%. Conversely, the greatest margin by which GRIF- FIN outperformed I-GLASS was observed for Gemma 7B on the ARC-Easy benchmark, at 0.30%. Similarly, in short-generation tasks, differences in ROUGE scores and exact match metrics were relatively minor. Long Generation Task We studied the trend of im-"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 10, "text": "with a gain of 0.68%. Conversely, the greatest margin by which GRIF- FIN outperformed I-GLASS was observed for Gemma 7B on the ARC-Easy benchmark, at 0.30%. Similarly, in short-generation tasks, differences in ROUGE scores and exact match metrics were relatively minor. Long Generation Task We studied the trend of im- provement over GRIFFIN as the activation (density) level increases from 10% to 90%, using three instruction- tuned representative models: Llama 3 8B, Gemma 7B, and Mistral 7B (see Fig. 3). To further assess the effectiveness of NPS, we also included variants where the global neural importance was extracted from the WikiText dataset. The results suggest that NPS variants consistently outperform those using WikiText-based importance. Moreover, the re- sults indicate that the performance gap between GRIFFIN and GLASS tends to widen as the activation density level decreases, highlighting the robustness and effectiveness of our approach under more aggressive sparsification regimes. Table 1 compares the performance of GRIFFIN with A-GLASS and I-GLASS across multiple models using PPL and KLD. Overall, both A-GLASS and I-GLASS consis- tently outperform GRIFFIN on most models and metrics, with I-GLASS generally achieving the greatest improve- ments. E.g., on Gemma 2 9B, I-GLASS achieves a 12.91% Model Metric GRIFFIN A-GLASS →Imp% I-GLASS →Imp% Gemma 7B PPL 3.7014 (0.0316) 3.5635 (0.0368) 3.73% 3.3982 (0.0310) 8.19% KLD 0.6453 (0.0033) 0.5998 (0.0037) 7.05% 0.5661 (0.0036) 12.27% Gemma 2 9B PPL 4.0143 (0.0262) 3.5105 (0.0217) 12.55% 3.4962 (0.0218) 12.91% KLD 0.6970 (0.0026) 0.5877 (0.0025) 15.68% 0.5821 (0.0026) 16.48% Gemma 2 27B PPL 2.7729 (0.0190) 2.4860 (0.0165) 10.35% 2.4268 (0.0155) 12.48% KLD 0.4338 (0.0024) 0.3515 (0.0025) 18.97% 0.3349 (0.0024) 22.80% Gemma 3n E2B PPL 23.1611 (0.2938) 12.7155 (0.1806) 45.10% 14.4240 (0.2159) 37.72% KLD 1.9983 (0.0063) 1.4841 (0.0068) 25.73% 1.5677 (0.0072) 21.55% Gemma 3n E4B PPL 8.5984 (0.0920) 5.9075 (0.0707) 31.30% 6.3256 (0.0809) 26.43% KLD 1.3106 (0.0053) 0.9796 (0.0061) 25.26% 1.0190 (0.0066) 22.25% Llama 3 8B PPL 5.2523 (0.0434) 4.0374 (0.0324) 23.13% 4.0295 (0.0331) 23.28% KLD 0.9426 (0.0047) 0.7407 (0.0044) 21.42% 0.7354 (0.0046) 21.98% Mistral 7B PPL 5.0059 (0.0407) 4.4786 (0.0384) 10.53% 4.4860 (0.0387) 10.39% KLD 0.8774 (0.0041) 0.7615 (0.0045) 13.21% 0.7593 (0.0046) 13.46% Qwen 2.5 7B PPL 5.0840 (0.0378) 5.5823 (0.0581) -9.80% 5.5424 (0.0593) -9.02% KLD 0.6772 (0.0028) 0.6880 (0.0039) -1.59% 0.6684 (0.0040) 1.30% Qwen 2.5 14B PPL 5.4330 (0.0882) 5.5823 (0.0677) -2.75% 5.0279 (0.0701) 7.46% KLD 0.6557 (0.0048) 0.5947 (0.0048) 9.30% 0.5680 (0.0050) 13.38% Phi 3 14B (medium) PPL 3.8981 (0.0328) 4.1384 (0.0283) -6.16% 3.7364 (0.0237) 4.15% KLD 0.6763 (0.0034) 0.6928 (0.0035) -2.44% 0.6188 (0.0031) 8.50% Table 1: Perplexity (PPL) and top-100 Kullback-Leibler Divergence (KLD) for GRIFFIN compared to our A/I-GLASS (NPS) methods. The Imp% columns represent the improvement percentage over GRIFFIN. reduction in PPL and a 16.48% reduction in KLD compared to GRIFFIN. Similar trends are observed for larger mod- els such as Gemma 2 27B, where I-GLASS reduces PPL by 12.48% and KLD by 22.80%. A notable exception is Qwen 2.5 7B, where both variants slightly degrade PPL, likely due to the model’s tendency to default to Chinese gen- eration under minimal prompting, which is misaligned with the English Alpaca dataset. Nevertheless,"}
{"doc_id": "2508.14302v1", "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14302v1", "chunk_id": 11, "text": "Gemma 2 27B, where I-GLASS reduces PPL by 12.48% and KLD by 22.80%. A notable exception is Qwen 2.5 7B, where both variants slightly degrade PPL, likely due to the model’s tendency to default to Chinese gen- eration under minimal prompting, which is misaligned with the English Alpaca dataset. Nevertheless, I-GLASS still im- proves KLD by 1.30%, indicating some gain in distribu- tional alignment. These results highlight the effectiveness of our methods in reducing both uncertainty (PPL) and diver- gence (KLD) across diverse model scales and architectures. Another notable observation is the remarkable perfor- mance gap between GRIFFIN and GLASS when applied to the Gemma 3n family, which employs a MatFormer-style ar- chitecture, specifically designed for efficient on-device de- ployment. MatFormer (Devvrit et al. 2024) enables training a large model from which smaller submodels can be ex- tracted at inference time. The larger gap between GLASS and GRIFFIN on Gemma 3n variants, which inherently sup- port structured sparsification, highlights the effectiveness of GLASS in scenarios requiring more aggressive sparsity. For instance, on Gemma 3n E4B, A-GLASS and I-GLASS achieve 31.30% and 26.43% improvements over GRIFFIN in terms of PPL, and 25.26% and 22.25% improvements in KLD, respectively. On the smaller Gemma 3n E2B, the gains are even more pronounced: 45.10% and 37.72% re- ductions in PPL, and 25.73% and 21.55% in KLD, for A-GLASS and I-GLASS, respectively. 7 Conclusion In this paper, we introduced two training-free methods for inference-time sparsification of FFNs in LLMs. By fusing local prompt activations with global statistics, either activa- tion magnitudes (A-GLASS) or impact scores (I-GLASS), our methods strike a balance between context sensitivity and model-intrinsic diversity without requiring offline pre- dictor training or incurring runtime overhead. Extensive ex- periments across multiple tasks and models demonstrate that our methods consistently outperform prior training-free ap- proaches, especially under short prompts and long genera- tions. These results demonstrate that model-intrinsic global knowledge can significantly improve prompt-driven spar- sification, unlocking reliable speed-ups even for the short- prompt long-generation workloads typical of edge devices. Our work opens the door for efficient, adaptive inference in LLMs with minimal changes. Limitations and Future Work Building on the strong performance of GLASS, we identify a couple of opportu- nities to further improve its flexibility and effectiveness: (i) currently we apply a fixed sparsity level uniformly and share a single mixing coefficient λ across all layers; jointly optimizing the sparsity pattern and learning layer-specific λ values could lead to more efficient capacity allocation; (ii) each layer is pruned independently for simplicity and compatibility with standard sparsification frameworks, how- ever, this may neglect potential cross-layer interactions that coordinated pruning strategies could leverage."}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 0, "text": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing Youssef Maklad∗, Fares Wael∗, Ali Hamdi∗, Wael Elsersy∗, Khaled Shaban† ∗Dept. of Computer Science, MSA University, Giza, Egypt {youssef.mohamed88, fares.wael, ahamdi, wfarouk}@msa.edu.eg †Dept. of Computer Science, Qatar University, Doha, Qatar khaled.shaban@qu.edu.qa Abstract—Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications. This paper introduces MultiFuzz, a novel dense retrieval-based multi-agent system designed to overcome these limitations by integrating semantic-aware context retrieval, specialized agents, and structured tool-assisted reasoning. Multi- Fuzz utilizes agentic chunks of protocol documentation (RFC Documents) to build embeddings in a vector database for a retrieval-augmented generation (RAG) pipeline, enabling agents to generate more reliable and structured outputs, enhancing the fuzzer in mutating protocol messages with enhanced state coverage and adherence to syntactic constraints. The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynami- cally adapt fuzzing strategies based on the retrieved contextual knowledge. Experimental evaluations on the Real-Time Stream- ing Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions over state-of-the-art (SOTA) fuzzers such as NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic coordination, and language model reasoning, MultiFuzz establishes a new paradigm in autonomous protocol fuzzing, offering a scalable and extensible foundation for future research in intelligent agentic-based fuzzing systems. Index Terms—Protocol Fuzzing, Network Security, Finite-State Machine, Reverse Engineering, Large Language Models, Multi- Agent Systems, Dense Retrieval, Retrieval-Augmented Genera- tion, Chain-of-Thoughts I. INTRODUCTION Network protocols form the backbone of modern communi- cation systems, yet remain vulnerable to many flaws that can compromise entire infrastructures’ security. Protocol fuzzing has long been an effective technique for uncovering these vulnerabilities through automated test generation, and it has long been recognized as an effective technique for uncovering these software vulnerabilities [1]. As network services grow in complexity and scale, the importance of discovering imple- mentation flaws, especially in stateful protocols with finite- state machines (FSMs), increases. Network protocol fuzzing attempts to systematically test protocol implementations by generating malformed, unexpected, or semi-valid protocol messages to identify anomalous behavior. However, traditional fuzzing methods often struggle with unique protocol chal- lenges, such as handling complex grammar formats, managing deep-protocol state transitions, and maintaining valid session semantics across multi-packet interactions [2]. Recent research highlight multiple directions in the advance- ment of protocol fuzzing, including state-aware input gen- eration and automated reverse engineering of undocumented protocols [3], [4]. Despite these developments, achieving high coverage and deeper state exploration remains difficult, par- ticularly for closed-source or proprietary protocols. This has motivated the integration of more intelligent components into the fuzzing loop. The rise of LLMs has opened new avenues for automating traditionally manual tasks in software"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 1, "text": "protocols [3], [4]. Despite these developments, achieving high coverage and deeper state exploration remains difficult, par- ticularly for closed-source or proprietary protocols. This has motivated the integration of more intelligent components into the fuzzing loop. The rise of LLMs has opened new avenues for automating traditionally manual tasks in software engineering. LLMs have demonstrated strong capabilities in reasoning, code under- standing, and program synthesis [5], [6]. Their potential in security applications, including fuzzing, has begun to evolve. Studies show that LLMs can infer message grammars, generate valid input sequences, and even simulate stateful behavior without access to source code [7], [8]. Recent works such as ChatAFL introduced LLM-guided protocol fuzzing, resulting in improved protocol state coverage [9]. These developments highlight LLMs as promising assistants for fuzzing complex, stateful, and security-critical systems. In this work, we present MultiFuzz, a multi-agent system built on top of ChatAFL, designed to enhance network proto- col fuzzing by unleashing the capabilities of LLMs and dense retrieval [10]. Inspired by recent advancements in retrieval- augmented generation and ReAct-based chain-of-thought rea- soning [11], [12], MultiFuzz is structured around collaborative agents, each responsible for a specific phase of the ChatAFL fuzzing pipeline. Unlike traditional fuzzers or single LLM approaches, MultiFuzz orchestrates tool-augmented agents to maintain semantic context support and protocol-specific infer- ence. The main contributions of this work are as follows: • We propose MultiFuzz, a multi-agent system for protocol fuzzing, integrated with the ChatAFL framework, where each group of agents is dedicated to a specific subtask and enhanced with tool integration, vector database context awareness, and CVE-driven vulnerability knowledge. • We introduce an agentic-chunking method and embed- ding strategy for protocol RFC documents, enabling semantic indexing of protocol knowledge for agent use. • We integrate dense retrieval into the agent reasoning process to maintain a protocol-aware context and guide more effective fuzzing actions. • We evaluate MultiFuzz on stateful protocol targets, and the results demonstrate improvements in branch coverage, number of states explored, and state transitions compared to SOTA fuzzers such as NSFuzz, AFLNet, and ChatAFL. Through MultiFuzz, we aim to bridge the gap between protocol-aware fuzzing needs and the generative coordination capabilities of modern agentic-AI architecture, allowing more intelligent and effective protocol fuzzing. The paper is structured as follows: Section II introduces background on protocol fuzzing, LLMs, and multi-agent sys- tems. Section III reviews the related work. Section IV explains our methodology. Section V explores the research questions, experiments’ setup, and evaluation metrics. Section VI high- lights the experimental results. Finally, section VII concludes the paper and suggests potential directions for future works. II. BACKGROUND The following subsections review key concepts and recent developments in network protocol fuzzing, LLMs, and multi- agent systems, providing background information for their integration in modern frameworks. A. Network Protocol Fuzzing Protocol fuzzing is a specialized security-testing technique that targets the finite-state behavior of communication proto- cols by injecting crafted or mutated packets to uncover flaws. It relies on intelligently generating seed inputs that exercise dif- ferent protocol states, since exploring deep protocol behaviors often exposes hidden vulnerabilities. Network protocol fuzzing focuses on"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 2, "text": "is a specialized security-testing technique that targets the finite-state behavior of communication proto- cols by injecting crafted or mutated packets to uncover flaws. It relies on intelligently generating seed inputs that exercise dif- ferent protocol states, since exploring deep protocol behaviors often exposes hidden vulnerabilities. Network protocol fuzzing focuses on testing stateful network services by feeding packet sequences through the protocol’s FSM in the server under test (SUT). The goal is to traverse unusual protocol paths and trigger implementation bugs or security flaws. Fuzzers can be broadly classified by their test case generation strat- egy. Mutation-based approaches modify existing valid packets using bit-flipping, arithmetic, block-level, or dictionary-based transformations [13]. In contrast, generation-based approaches synthesize packets from protocol specifications or templates. While mutation methods may struggle with diverse data types and protocol constraints, generation-based methods often face difficulties in acquiring or modeling accurate protocol specifi- cations. Additionally, based on the level of knowledge about the target system, fuzzing can be categorized into blackbox, whitebox, and graybox approaches. Blackbox fuzzers operate without internal knowledge of the protocol implementation, relying solely on input/output observation. White-box fuzzers analyze the source code to guide test case generation, while gray-box fuzzers use lightweight instrumentation as code coverage feedback to guide mutations more effectively. In practice, graybox fuzzing offers a balanced trade-off and is widely used due to limited access to source code in real-world protocol implementations [14]. B. Large Language Models LLMs have recently demonstrated powerful capabilities in generating and reasoning over complex inputs, opening new opportunities for automation in domains like software testing and cybersecurity [7], [8]. They are deep transformer-based neural networks [15], trained on massive text corpora, enabling them to generate coherent language and perform complex reasoning [12]. Their rich knowledge and generative power have been harnessed in many domains. In cybersecurity, LLMs have shown remarkable utility. For example, ChatPhishDetec- tor uses an LLM to detect phishing websites [16]. Maklad. Y. et al demonstrated how LLMs, enhanced by RAG and chain- of-thought reasoning, can be used to evaluate seed enrichment tasks and network packet generation [17]. SeedMind explored the use of LLMs for building fuzzing seed generators [18]. Codamosa highlights the use of LLMs in overcoming cov- erage plateaus in test generation [19]. LLMs have also been integrated into automation workflows such as Robotic Process Automation (RPA) and OCR [20]–[22]. These results highlight that LLMs, when incorporated intelligently, can enhance the accuracy and efficiency of automation systems. C. Multi-Agent Systems Multi-agent systems consist of multiple autonomous enti- ties (agents) that interact and collaborate to solve tasks. By harnessing the diverse capabilities and roles of individual agents, multi-agent systems can tackle complex problems more effectively than a single agent could [23]. For example, agents might divide a workflow so that some gather information, others perform reasoning, and yet others execute actions. However, orchestrating agents also introduces challenges like optimal task allocation, sharing complex context information, and memory management, which become critical in LLM- based multi-agent architectures. In security applications, multi- agent LLM architectures have begun to emerge. A recent effort has introduced PentestAgent [24], a framework"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 3, "text": "and yet others execute actions. However, orchestrating agents also introduces challenges like optimal task allocation, sharing complex context information, and memory management, which become critical in LLM- based multi-agent architectures. In security applications, multi- agent LLM architectures have begun to emerge. A recent effort has introduced PentestAgent [24], a framework in which multiple agents collaborate to automate penetration testing and vulnerability analysis. This demonstrates how multi-agent systems can decompose a complex task (like pentesting) into subtasks handled by specialized LLM agents, improving overall efficiency. III. RELATED WORK A. Protocol Fuzzing Fuzzing has proven to be one of the most effective tech- niques for vulnerability discovery. Protocol fuzzing, in partic- ular, poses unique challenges due to its reliance on structured Fig. 1. High-Level System Architecture of the MultiFuzz Framework, based on AFLNet and ChatAFL formats and stateful interactions. Existing techniques outlined earlier in section II, which are blackbox, whitebox, and graybox fuzzing approaches, offer different trade-offs between scalability, precision, and required prior knowledge of the target protocol. 1) Blackbox Fuzzing: Blackbox fuzzers operate without any internal knowledge of the target and typically rely on traffic observation or mutation of recorded protocol messages. Tools like SPIKE and Peach exemplify early blackbox efforts, relying on manual specification of protocol structures [25]. PULSAR and BBuzz extract message formats from captured network traffic using protocol reverse engineering techniques [26], [27]. These methods are simple to deploy but struggle with exploring deeper protocol states, often failing to maintain session validity across multi-message interactions. 2) Whitebox Fuzzing: Whitebox fuzzers leverage full ac- cess to source code or binaries to systematically explore execution paths. Symbolic execution and taint analysis are commonly used to analyze input-dependent behaviors. Polar combines static analysis with dynamic taint tracking to extract input-related conditions from protocol code [14]. While these techniques offer fine-grained insight and deeper coverage, their scalability is limited by path explosion and instrumentation overhead. Whitebox fuzzers are less commonly used in net- work protocol contexts due to the complexity of protocol stacks and message interleaving. 3) Graybox Fuzzing: Graybox fuzzers balance insight and scalability by utilizing lightweight instrumentation to guide input mutations. AFL and its extensibles, like AFL++ and AFLNet, employ coverage feedback to direct test-case gen- eration [28]–[30]. AFLNet extends AFL to stateful network protocols by using response codes to infer protocol state transitions. NSFuzz further improves graybox fuzzing by ex- tracting program variables associated with state changes to better synchronize test inputs with protocol logic [31]. These tools have proven effective on real-world network services and are the foundation for many modern protocol fuzzers. B. Large Language Model-assisted Fuzzing Recent research has explored the integration of LLMs into fuzzing pipelines. These models offer the ability to generate syntactically correct and semantically meaningful inputs by leveraging knowledge learned during pre-training. ChatFuzz uses OpenAI’s ChatGPT to mutate existing seed inputs, re- sulting in improved edge coverage compared to AFL++ [32]. ChatAFL constructs message grammars and predicts the next protocol message using GPT models, achieving significant gains in state and code coverage over AFLNet and NSFuzz [9]. MSFuzz extracts abstract syntax trees from the"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 4, "text": "OpenAI’s ChatGPT to mutate existing seed inputs, re- sulting in improved edge coverage compared to AFL++ [32]. ChatAFL constructs message grammars and predicts the next protocol message using GPT models, achieving significant gains in state and code coverage over AFLNet and NSFuzz [9]. MSFuzz extracts abstract syntax trees from the protocol source code via LLMs to guide syntax-aware mutation [33]. TitanFuzz and FuzzGPT show that LLMs can function as zero- shot fuzzers for deep learning libraries by generating edge case inputs and exploiting rare model behaviors without instru- mentation or prior seeds [34], [35]. These works demonstrate that LLMs can serve as powerful assistants in automating grammar extraction, seed generation, and mutation strategies for protocol fuzzing. IV. METHODOLOGY This section details the design of the MultiFuzz framework and its agent-based workflows. The MultiFuzz framework APIs are integrated in the ChatAFL framework, on top of the AFLNet architecture. The whole system architecture can be shown in Figure 1. MultiFuzz is structured around three specialized crews of agents: the Grammar Extraction Crew, the Seed Enrichment Crew, and the Coverage Plateau Crew. Each crew operates over a shared semantic context retrieved at inference by a common dense retrieval agent, which retrieves the agentic chunked embeddings in the vector store. The workflow begins with preprocessing protocol RFCs, then trans- forms the content into propositional transformation, followed by agentic chunking, and finally, collaborative agent reasoning at inference. System Prompt for Propositional Transformation Decompose the RFC documents’ content given into clear and simple text propositions, ensuring they are interpretable out of context. Rules to follow: 1) Split compound sentences into simple sentences. Maintain the original phrasing whenever possible. 2) For any named entity with descriptive information, separate this information into its own distinct proposition. 3) Decontextualize propositions by adding necessary modifiers and re- placing pronouns (e.g., ”it”, ”they”, ”this”) with the corresponding full entities. 4) Preserve the structure and formatting of any network packet example, protocol message, or code snippet. Do not summarize them. 5) Present the results as a list of strings formatted in JSON. Example: Input: ”The DESCRIBE method retrieves the description of a media object, it accepts application/sdp...” Expected Output: {”sentences”: [”The DESCRIBE method retrieves the description...”, ”The DESCRIBE method accepts application/sdp...”] } Fig. 2. System prompt used for propositional transformation of filtered RFC documents Sample Document Chunk Post Agentic Chunking { ”<chunk id>”: { ”title”: ”RTSP Streaming Protocols and Resource Management”, ”summary”: ”This chunk contains information about RTSP controls in stream- ing protocols, emphasizing session management, RTSP URL semantics, and transmission methods, while including examples and method functionalities.”, ”propositions”: [ ”SETUP starts an RTSP session.”, ”PLAY starts data transmission on a stream allocated via SETUP.”, ”PAUSE does not free server resources.”, ”TEARDOWN causes the RTSP session to cease to exist on the server.”, ”RTSP methods that contribute to state use the Session header field.”, ”The ’rtsp’ scheme requires that commands are issued via a reliable protocol, specifically TCP.”, ”Lines in RTSP messages are terminated by CRLF.”, ”RTSP methods are idempotent unless otherwise noted.”, ”For the scheme ’rtsp’, a persistent connection is assumed.”, ”...”, ]"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 5, "text": "methods that contribute to state use the Session header field.”, ”The ’rtsp’ scheme requires that commands are issued via a reliable protocol, specifically TCP.”, ”Lines in RTSP messages are terminated by CRLF.”, ”RTSP methods are idempotent unless otherwise noted.”, ”For the scheme ’rtsp’, a persistent connection is assumed.”, ”...”, ] } } Fig. 3. Sample document chunk after the agentic chunking phase of text propositions. A. RFC Documents Preprocessing We first process the RFC documents, where each RFC is manually segmented into paragraphs and then passes through a series of filters to extract technical sections, including stateful interactions, command formats, and response rules. We define an RFC as a sequence of paragraphs R = {r1, r2, ..., rn}. A semantic classifier ffilter maps each paragraph to a boolean label: ffilter(ri) = ( 1 if ri is protocol-relevant 0 otherwise (1) Only filtered paragraphs R′ = {ri ∈R | ffilter(ri) = 1} are retained for downstream chunking and proposition extraction. B. Propositional Transformation Once the RFC content has been semantically filtered and structured into coherent sections using specific delimiters (###, ---, @@@), the next step in the pipeline is to trans- form these technical paragraphs into interpretable, context- independent atomic propositions. To perform this transforma- tion, we constructed an LLM-powered pipeline. Each filtered section is first processed using a carefully designed system prompt, shown in Figure 2. The prompt is executed using the gpt-4o-mini model with structured output enforced by a JSON schema. Each chunk of the RFC document is passed through this pipeline, producing a list of minimal, decontextualized statements that accurately capture the semantics of the protocol specification. Formally, for a given input chunk Ci ∈C where C is the set of smart RFC chunks, the transformation function T produces: T(Ci) = {p1, p2, . . . , pk}, where each pj ∈P Here, P denotes the proposition space containing linguis- tically simple, context-independent units of meaning. As a result, each paragraph Ci is mapped to a finite set of logically coherent propositions, and the global proposition set P be- comes the knowledge substrate for subsequent dense retrieval and crew-based inference modules. In our experiments on RFC-2326 (RTSP), this step yielded 445 unique and precise propositions. C. Agentic Chunking Module Following the propositional transformation step, we employ an intelligent chunking mechanism termed the Agentic Chun- ker to group semantically similar propositions into cohesive and operationally meaningful units. This process creates the foundation for precise retrieval and role-specific agent infer- ence in later stages of the MultiFuzz framework. Formally, given a set of propositions P = {p1, p2, . . . , pn} derived from the RFC document, the goal is to partition P into a set of non-overlapping semantic chunks Z = {z1, z2, . . . , zm}, where each zj ⊆P and Sm j=1 zj = P. The chunking objective can be viewed as an unsupervised grouping problem constrained by topic cohesion, guided by an LLM. The chunking process is agentic in nature: each incoming proposition pi is evaluated using a prompt-driven LLM flow by gpt-4o-mini. This LLM first"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 6, "text": "zj ⊆P and Sm j=1 zj = P. The chunking objective can be viewed as an unsupervised grouping problem constrained by topic cohesion, guided by an LLM. The chunking process is agentic in nature: each incoming proposition pi is evaluated using a prompt-driven LLM flow by gpt-4o-mini. This LLM first examines the current set of chunk summaries and determines whether pi semantically aligns with any existing chunk zj. If alignment is detected, pi is appended to that chunk. Otherwise, a new chunk is instantiated. Each chunk zj maintains three evolving elements: Dense Retrieval Agent Grammar Extraction Agent Grammar Formatting Agent Grammar Formatting Tool Grammar Extraction Crew Analysis Agent Vulnerabilities Agent Coverage Surpassing Agent CVEs Retrieval Tool Packet Parsing Tool Coverage Plateau Crew Seeds Enricher Agent Seeds Parsing Tool Seed Enrichment Crew Fig. 4. Summary of the MultiFuzz’s crews, showing each crew’s internal agents and integrated tools. All three crews share a Dense Retrieval Agent for semantic context fetching. • A list of constituent propositions {pk}K k=1, • A concise chunk summary sj generated by the LLM. • A descriptive, technically precise chunk title tj. The internal logic can be modeled as a two-stage LLM pipeline: 1) Chunk Selection: Given current chunk outlines and a new proposition pi, select the most semantically com- patible chunk zj such that: zj = arg max z∈Z sim(pi, sz) If max sim < θ, where θ is a system-defined compati- bility threshold, a new chunk is created. 2) Metadata Refinement: After assignment, the system regenerates the chunk’s summary sj and title tj using structured prompt templates conditioned on the current list of propositions. The final output is a collection of richly annotated document objects, each encapsulating a semantic group of RFC-derived propositions, along with human-readable summaries and titles. These document objects were then embedded using OpenAI’s text-embedding-ada-002 model and indexed into a Chroma- based dense vector database. A sample document object can be shown in Figure 3. D. Dense Retrieval Agent The first common agent in all crews is the dense retrieval agent. This agent is responsible for querying a Chroma-based dense vector store populated with semantically grouped RTSP agentic chunks. It utilizes a Custom RAG Tool, to per- form approximate nearest neighbor search against the indexed chunks. The output of this agent is a context-rich corpus of relevant documents passed to assist all agents with their tasks. E. Grammar Extraction Crew of Agents The Grammar Extraction Crew is the first crew of agents designed to extract structured RTSP client request templates for ChatAFL. It operates through three agents: a dense retrieval agent, a grammar extraction agent, and a grammar formatting agent. Grammar Extraction Agent: This agent uses the re- trieved context to produce JSON-formatted RTSP request templates, where each method (e.g. PLAY, DESCRIBE) maps to a list of headers containing <<VALUE>> placeholders and \\r\\n terminators. Grammar Formatting Agent: It refines the raw JSON output into a clean, numbered textual format using the Grammar Extraction Formatting Tool, making it easier to parse in the ChatAFL grammar parsing module. F. Seed Enrichment Crew of Agents The Seed Enrichment"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 7, "text": "list of headers containing <<VALUE>> placeholders and \\r\\n terminators. Grammar Formatting Agent: It refines the raw JSON output into a clean, numbered textual format using the Grammar Extraction Formatting Tool, making it easier to parse in the ChatAFL grammar parsing module. F. Seed Enrichment Crew of Agents The Seed Enrichment Crew is a two-agent crew designed to enhance a given sequence of RTSP client requests by inserting new protocol-compliant packets at semantically correct posi- tions. This enrichment supports fuzzers by generating deeper, more state-aware input sequences. The first agent is the dense retrieval agent, and the second is the seeds enricher agent. Seeds Enricher Agent: This agent interprets the proto- col’s FSM and uses retrieved context from the dense retrieval agent to insert two desired client requests, typically absent from the original seed into their appropriate positions as adopted in ChatAFL. It ensures server responses are excluded and leverages the Seeds Parsing Tool to generate struc- tured outputs of continuous enriched network packets. These enriched seeds are structured to be easily parsed by the ChatAFL parsing module. G. Coverage Plateau Surpassing Crew of Agents The Coverage Plateau Surpassing Crew is designed to help the fuzzer escape stagnation points during test execution, where no new protocol states or code paths are being explored as observed in ChatAFL. This crew of agents aims to generate packets that can trigger new transitions by analyzing commu- nication history, retrieved context, and optionally exploiting known CVEs. Analysis Agent: This agent performs deep context anal- ysis of the context retrieved from the dense retrieval agent and the fuzzer’s communication history to construct a detailed generation prompt. Rather than producing packets directly, it crafts precise instructions to guide the next agent in generating a coverage-enhancing input. Vulnerabilities Agent: To improve the chance of pro- ducing impactful packets, this agent enriches the genera- tion prompt with insights from real CVEs, fetched using a CVEs Retrieval Tool which uses the NVD (National Vulnerability Database) API to obtain the Live555 server vulnerabilities [36]. If any vulnerability discovered is relevant to the current communication context, the prompt is refined accordingly; otherwise, it is forwarded unchanged. Coverage Surpassing Agent: Finally, this agent con- sumes the refined prompt and generates a valid RTSP client request designed to surpass the coverage plateau. The agent uses a Packet Parsing Tool to structure the final packet and log it along with an explanation of its purpose. A generated sample prompt can be shown in Figure 5. Sample Prompt to generate Coverage Plateau Packet ”prompt”: { ”To surpass the current coverage plateau, Generate a PAUSE request that will transition the server from the Playing state to the Ready state. The PAUSE method should be sent with the appropriate headers, including CSeq: 5, Session: 000022B8, and the method set to PAUSE. This will explore the state transition from Playing to Ready, potentially revealing new server behaviors and increasing coverage.” } Fig. 5. Sample prompt asking the final agent to generate a coverage plateau surpassing packet H. Implementation We have developed MultiFuzz on top of two agentic-AI frameworks: LangChain [37] and"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 8, "text": "PAUSE. This will explore the state transition from Playing to Ready, potentially revealing new server behaviors and increasing coverage.” } Fig. 5. Sample prompt asking the final agent to generate a coverage plateau surpassing packet H. Implementation We have developed MultiFuzz on top of two agentic-AI frameworks: LangChain [37] and CrewAI [38]. LangChain provides abstractions for building applications on top of LLMs. We use the LangChain framework combined with the Chroma vector store for embedding and indexing. We utilize it’s features specifically in the RFC processing stage for RFC document agentic chunking and during the dense retrieval inference in all agent tasks. CrewAI, on the other hand, is used to build autonomous multi-agent systems and provides modular assignment of agents to specific and unique roles. It supports integration with multiple LLM API providers and offers native support for tool-assisted workflows, enabling agents to interact with file systems and vector databases. We use CrewAI in defining and orchestrating the agents that compose the MultiFuzz framework. These structured agent groups, or ”crews”, coordinate within the ChatAFL framework using event-driven task scheduling augmented with custom structured tools. V. EXPERIMENTAL DESIGN AND EVALUATION We evaluate the proposed MultiFuzz framework by measur- ing its effectiveness in fuzzing stateful protocol implementa- tions using a multi-agent-based architecture. Our evaluation aims to answer the following research questions: • RQ1: How effective is MultiFuzz in improving branch coverage and state exploration compared to SOTA proto- col fuzzers? • RQ2: How does the multi-agent collaboration strategy improve over single-LLM approaches such as ChatAFL? To conduct the evaluation, we test MultiFuzz on the RTSP protocol implemented by the Live555 media stream- ing server. RTSP was selected due to its rich stateful be- havior, complexity in session semantics, and widespread use in multimedia transmission. It presents non-trivial state transitions that make it a fitting candidate for state-aware fuzzing. The framework is powered by Llama-based language models obtained via the Groq-Cloud API [39], which are: llama3.3-70b-versatile, deepseek-r1-distill-llama-70b, llama3- 70b-8192, llama-4-scout-17b-16e-instruct, and llama-3.1-8b- instant, chosen for their reasoning abilities, and long-context window capacities. Throughout the experimentation process, we explored different combinations of these models across the various agent groups in the framework. Tasks such as gram- mar extraction, seed enrichment, and plateau surpassing were assigned to different models iteratively until the most effective model was identified for each specific subtask, optimizing the overall performance of MultiFuzz. A. Experiments Setup To evaluate the fuzzing effectiveness of MultiFuzz, we con- ducted a 24-hour three fuzzing sessions using our framework alongside the three SOTA baseline fuzzers: NSFuzz, AFLNet, and ChatAFL under the same experimental conditions. All experiments were performed on a local machine running Ubuntu 24.04.02 LTS, equipped with an Intel Core i5-11300H processor and 16 GB of RAM. The fuzzers were evaluated against the RTSP protocol implemented in the Live555 media streaming server. Each fuzzer was independently executed with default settings. We measured the effectiveness of each fuzzer across several key metrics, including unique crashes, state coverage, branch coverage, and total paths explored. This setup allows us to assess the relative performance of MultiFuzz in contrast with existing"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 9, "text": "the Live555 media streaming server. Each fuzzer was independently executed with default settings. We measured the effectiveness of each fuzzer across several key metrics, including unique crashes, state coverage, branch coverage, and total paths explored. This setup allows us to assess the relative performance of MultiFuzz in contrast with existing approaches. B. Evaluation Metrics To evaluate MultiFuzz’s fuzzing performance, we adopted standard coverage-based metrics inspired by existing works such as AFLNet and ChatAFL: • Branch Coverage: Number of unique conditional branches exercised in the code. • Number of States: Number of FSM states reached and explored during fuzzing. • Number of State Transitions: The total count of valid state transitions triggered within the protocol’s FSM, reflecting the depth of state space exploration. We use ProFuzzBench [40] as our benchmarking platform due to its automated nature in a containerized environment using Docker and to baseline with the previous SOTA pro- tocol fuzzers. All experiments were repeated multiple times to ensure consistency, and results were averaged over time windows to account for variability in execution. This setup allows us to rigorously measure MultiFuzz’s capability to intelligently generate protocol-aware inputs and uncover deep- state vulnerabilities. C. Experimental Results of Fuzzing on Code Coverage Table I presents the branch coverage results, demonstrating MultiFuzz’s substantial superiority in code coverage metrics. MultiFuzz achieves an average branch coverage of 2940 branches, representing dramatic improvements of 1.0% over ChatAFL (2912.67), 2.8% over AFLNet (2860), and 2.3% over NSFuzz (2807). Although these percentage improvements may appear modest, the absolute differences are significant in the context of protocol fuzzing, where each additional branch represents potential discovery of critical vulnerabilities. The consistency of MultiFuzz’s performance is particularly note- worthy, with coverage ranging from 2970 to 2940 branches across experiments, demonstrating reliable and predictable performance. In contrast, ChatAFL shows higher variability (2890-2998 branches), while AFLNet exhibits perfect con- sistency but at significantly lower coverage levels. NSFuzz demonstrates the most variability, with coverage ranging from 2795 to 2826 branches. VI. EXPERIMENTAL RESULTS AND DISCUSSION A. Experimental Results of Fuzzing on State Exploration Table II and Table III demonstrate that compared to NSFuzz, AFLNet, and ChatAFL, MultiFuzz achieves superior perfor- mance in both state transitions and state exploration across all three experimental runs. In terms of state transitions, Multi- Fuzz achieves an average of 163.33 transitions, representing a significant improvement of 2.3% over ChatAFL (159.67), 94.5% over AFLNet (84.0), and 81.2% over NSFuzz (90.33). The state exploration results further validate MultiFuzz’s effec- tiveness, with an average of 14.67 states explored compared to ChatAFL’s 14.33 states, AFLNet’s 10.0 states, and NSFuzz’s 11.7 states. This represents improvements of 2.4%, 46.7%, and 25.4% respectively. TABLE I: Branch coverage achieved by MultiFuzz and baseline SOTA fuzzers Experiment MultiFuzz ChatAFL AFLNet NSFuzz 1 2970 2890 ↑2.8% 2850 ↑4.2% 2800 ↑6.1% 2 2910 2998 ↓-2.9% 2870 ↑1.4% 2795 ↑4.1% 3 2940 2850 ↑3.2% 2860 ↑2.8% 2826 ↑4.0% Average 2940.0 2912.67 ↑0.9% 2860.0 ↑2.8% 2807.0 ↑4.7% TABLE II: Number of state transitions achieved by MultiFuzz and baseline SOTA fuzzers Experiment MultiFuzz ChatAFL AFLNet NSFuzz 1 163 159 ↑2.5% 80 ↑103.8% 88 ↑85.2% 2"}
{"doc_id": "2508.14300v1", "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14300v1", "chunk_id": 10, "text": "2910 2998 ↓-2.9% 2870 ↑1.4% 2795 ↑4.1% 3 2940 2850 ↑3.2% 2860 ↑2.8% 2826 ↑4.0% Average 2940.0 2912.67 ↑0.9% 2860.0 ↑2.8% 2807.0 ↑4.7% TABLE II: Number of state transitions achieved by MultiFuzz and baseline SOTA fuzzers Experiment MultiFuzz ChatAFL AFLNet NSFuzz 1 163 159 ↑2.5% 80 ↑103.8% 88 ↑85.2% 2 165 158 ↑4.4% 85 ↑94.1% 91 ↑81.3% 3 162 162 ↑0.0% 87 ↑86.2% 92 ↑76.1% Average 163.33 159.67 ↑2.3% 84.0 ↑94.4% 90.33 ↑80.8% TABLE III: Number of states explored by MultiFuzz and baseline SOTA fuzzers Experiment MultiFuzz ChatAFL AFLNet NSFuzz 1 14 14 ↑0.0% 9 ↑55.6% 12 ↑16.7% 2 15 14 ↑7.1% 11 ↑36.4% 11 ↑36.4% 3 15 15 ↑0.0% 10 ↑50.0% 12 ↑25.0% Average 14.67 14.33 ↑2.4% 10.0 ↑46.7% 11.7 ↑25.4% B. Observations The dense retrieval-based multi-agent system of MultiFuzz enables more systematic state space exploration by leveraging the agented-chunked and embedded protocol specifications and coordinated agent interactions. Unlike baseline fuzzers that rely on conventional feedback-driven exploration, MultiFuzz’s multi-agent architecture facilitates comprehensive state discov- ery through intelligent coordination and knowledge sharing among specialized agents. The results indicate that MultiFuzz’s dense retrieval mechanism effectively identifies and prioritizes valuable states that serve as critical transition points within the protocol state machine. The multi-agent coordination allows for parallel exploration strategies while maintaining system- atic coverage of the state space. This approach significantly outperforms traditional fuzzing methods that rely on random mutation and single LLM approaches. VII. CONCLUSION AND FUTURE WORK Protocol fuzzing continues to be a foundational technique for uncovering implementation flaws in communication sys- tems. However, traditional fuzzers often face significant limita- tions when applied to stateful or proprietary network protocols, particularly due to difficulties in handling complex message grammars, managing multi-step state transitions, and main- taining valid interactions across sessions. We proposed Multi- Fuzz, a dense retrieval-based multi-agent system designed to address these limitations by leveraging an agentic-RAG-based architecture empowered by chain-of-thought reasoning. Our approach builds upon prior advances in LLM-assisted fuzzing but distinguishes itself by introducing multi-agent coordination instead of a single LLM. It proposes agentic-based chunking of protocol documents and context-aware inference about protocol specifications and vulnerabilities. These additions help overcome key challenges in stateful fuzzing, such as low coverage and stagnation during long-running sessions. Evaluation across real-world protocol implementations has shown that MultiFuzz surpasses existing tools such as NSFuzz, AFLNet, and ChatAFL in terms of state exploration and branch coverage. These findings bridge the gap between tradi- tional fuzzing methodologies and recent advances in agentic- AI, as they open promising opportunities for more effective and adaptive security testing. Looking forward, we suggest several paths to extend this work. Firstly, enhancing automation by tightly integrating re- verse engineering tools, symbolic analyzers, and traffic parsers can further streamline the entire pipeline. Lastly, fine-tuning agent behaviors using domain-specific interaction data could improve their effectiveness in specialized protocol domains. VIII. ACKNOWLEDGMENT Heartfelt gratitude is extended to AiTech AU, AiTech for Ar- tificial Intelligence and Software Development (https://aitech. net.au), for funding this research and enabling its successful completion."}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 0, "text": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP M. Ali Bayram1, Ali Arda Fincan2, Ahmet Semih Gümü¸ s2, Sercan Karaka¸ s3, Banu Diri1, Sava¸ s Yıldırım4, Demircan Çelik2 1Yıldız Technical University, 2Yeditepe University, 3University of Chicago, 4Istanbul Bilgi University malibayram20@gmail.com Abstract Tokenization plays a pivotal role in natural language processing (NLP), shaping how textual data is segmented, interpreted, and processed by language models. Despite the success of subword-based tokenization techniques such as Byte Pair Encoding (BPE) and WordPiece, these methods often fall short in morphologi- cally rich and agglutinative languages due to their reliance on statistical frequency rather than linguistic structure. This paper introduces a linguistically informed hybrid tokenization framework that integrates rule-based morphological analysis with statistical subword segmentation to address these limitations. The proposed approach leverages phonological normalization, root-affix dictionaries, and a novel tokenization algorithm that balances morpheme preservation with vocabulary ef- ficiency. It assigns shared identifiers to phonologically variant affixes (e.g., -ler and -lar) and phonologically altered root forms (e.g., kitap vs. kitabı), significantly reducing redundancy while maintaining semantic integrity. The framework also incorporates special tokens for whitespace and orthographic case, including an <uppercase> token to prevent vocabulary inflation from capitalization. Byte Pair Encoding is integrated to support out-of-vocabulary coverage without compro- mising morphological coherence. Evaluation on the TR-MMLU benchmark—a large-scale, Turkish-specific NLP benchmark—demonstrates that the proposed tokenizer achieves the highest Turkish Token Percentage (90.29%) and Pure To- ken Percentage (85.8%) among all tested models. Comparative analysis against widely used tokenizers from models such as LLaMA, Gemma, and OpenAI’s GPT reveals that the proposed method yields more linguistically meaningful and semantically coherent tokens. A qualitative case study further illustrates improved morpheme segmentation and interpretability in complex Turkish sentences. Al- though the implementation focuses on Turkish, the underlying methodology is language-independent and adaptable to other languages. This work contributes to ongoing efforts to improve tokenizer design through linguistic alignment, of- fering a practical and extensible solution for enhancing both interpretability and performance in multilingual NLP systems. Keywords: Tokenization, Morphologically Rich Languages, Morphological Seg- mentation, Byte Pair Encoding, Turkish NLP, Linguistic Integrity, Low-Resource Languages 1 Introduction Tokenization, the process of segmenting text into smaller linguistic units called tokens, is a founda- tional step in Natural Language Processing (NLP). It has a direct impact on vocabulary construction, model efficiency, semantic interpretation, and the overall performance of downstream tasks such as question answering, sentiment analysis, and machine translation[1]. While traditional tokenization techniques—such as whitespace or rule-based segmentation—have been commonly used in early NLP systems, they fall short in modeling the complex morphological phenomena of many languages, particularly those that exhibit agglutination, inflectional variation, and phonological alternation. Subword-based tokenization methods like Byte Pair Encoding (BPE) [2], WordPiece [3], and Unigram [4] have become the de facto standard in transformer-based language models such as BERT [5] and GPT [6]. These methods address the out-of-vocabulary (OOV) problem by segmenting rare words into frequently occurring subword units, thereby balancing vocabulary size and generalization. However, despite their computational strengths, these frequency-based methods often disregard the linguistic structure of words. As a result, morphologically rich languages such as Turkish,"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 1, "text": "[5] and GPT [6]. These methods address the out-of-vocabulary (OOV) problem by segmenting rare words into frequently occurring subword units, thereby balancing vocabulary size and generalization. However, despite their computational strengths, these frequency-based methods often disregard the linguistic structure of words. As a result, morphologically rich languages such as Turkish, Finnish, and Hungarian are frequently segmented in ways that violate morphemic boundaries, reducing semantic coherence and interpretability [7, 8]. Agglutinative languages like Turkish pose specific challenges for tokenization. Words are formed by appending multiple affixes to a root, producing an expansive set of surface forms that differ only in morphological features. Phonological processes such as vowel harmony and consonant alternation further increase the diversity of surface realizations. For instance, plural suffixes like -lAr or ablative markers like -dAn, -tAn, functionally represent the same morphemes but differ based on the phonological context. Similarly, root alternations like kitap →kitab (book) and gö˘ güs →gö˘ gs (chest) are common in Turkish. Frequency-based subword models fail to account for such variation, resulting in redundant and inconsistent tokenization [9]. Tokenization approaches that ignore these morphological and phonological nuances lead to increased vocabulary size, fragmented representation of morphosyntactic units, and reduced performance in syntactically dependent tasks. Recent benchmark studies, including TR-MMLU [10] and a cross- model tokenizer evaluation [9], have shown that metrics such as Turkish Token Percentage (TR %) and Pure Token Percentage (Pure %) strongly correlate with downstream model performance. These findings underscore the necessity of tokenization strategies that align with linguistic structures. Token purity plays a critical role in the effectiveness of large language models, particularly when applied to morphologically complex languages like Turkish. Since LLMs are fundamentally statistical pattern learners, the quality and clarity of those patterns directly influence their ability to generalize, reason, and generate coherent outputs. Pure tokens—those that cleanly align with complete mor- phemes such as roots or affixes—provide semantically and syntactically consistent input signals. This allows models to recognize grammatical structures, identify morphological relationships, and transfer learned behavior across different word forms (e.g., kitap, kitabı, kitaplık). In contrast, impure tokens—subword units that contain partial or blended morphemes—introduce ambiguity into the token stream. Such noise disrupts the alignment between token boundaries and linguistic meaning, hindering the model’s ability to learn reliable representations. Empirical studies have shown that morphologically aware tokenization can significantly improve model performance, generalization, and interpretability. Hofmann et al. [11] demonstrated that transformer models with derivationally informed vocabularies perform better at interpreting complex word forms, even in English, a language with relatively mild morphological variation. Similarly, Jabbar [12] introduced MorphPiece, a tokenizer that segments text based on morphemes before applying subword encoding. A GPT-style model trained with this tokenizer achieved superior performance across multiple NLP benchmarks—including language modeling, zero-shot GLUE, and text embedding tasks—despite using only half the training iterations of its BPE-based counterpart. These findings provide strong evidence that token purity, grounded in morphological structure, enhances learning efficiency and leads to more transparent and generalizable language models. The importance of token purity is analogous to segmentation practices in other machine learning domains. In computer vision, models such as"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 2, "text": "iterations of its BPE-based counterpart. These findings provide strong evidence that token purity, grounded in morphological structure, enhances learning efficiency and leads to more transparent and generalizable language models. The importance of token purity is analogous to segmentation practices in other machine learning domains. In computer vision, models such as capsule networks [13] and object-centric architectures like Slot Attention [14] show that performance and generalization improve when visual scenes are decomposed into discrete, meaningful entities rather than treated as undifferentiated pixel grids. Capsule networks, for example, represent objects as holistic capsules rather than scattered features, enabling more accurate recognition in complex visual settings. Similarly, Slot Attention learns to bind visual input to abstract object representations, facilitating compositional reasoning and generalization across novel configurations. The same principle applies to language modeling: when token boundaries reflect linguistic structure, the model receives clearer and more interpretable signals. Token purity is thus not merely a linguistic preference—it is a structural requirement for training high-performing, 2 semantically aware language models. This perspective motivates our use of Pure % as a central evaluation metric in this study. In response to these limitations, this paper introduces a linguistically informed, language-independent tokenization framework that integrates rule-based morphological segmentation with statistical sub- word modeling. The approach includes several key innovations: First, phonological normalization is applied so that surface variants of the same morpheme are assigned a unified identifier. This includes mapping affixes with phonological variation triggered by the vowel harmony (e.g., -dAn, -tAn (from)) and roots with final devoicing (e.g., kitap and kitab (book)) to shared token IDs. Second, a special token (<uppercase>) is used to encode orthographic case distinctions, enabling models to differentiate capitalized tokens without duplicating them in the vocabulary. Third, formatting characters such as space, newline, and tab are explicitly tokenized, preserving the structural integrity of the original text for downstream tasks involving structured documents or layout-sensitive processing. Fourth, a hybrid tokenization algorithm is developed, combining dictionary-based morphological analysis with Byte Pair Encoding. While morphological segmentation ensures alignment with linguistic units, BPE provides fallback coverage for unknown words, maintaining efficiency and scalability in large corpora. The proposed tokenizer is evaluated on the TR-MMLU benchmark to test the hypothesis that incorporating linguistic structures—particularly morphological segmentation and phonological nor- malization—into tokenization can significantly enhance semantic alignment and efficiency in mor- phologically rich languages. This hypothesis is grounded in prior empirical evidence that linguistic alignment metrics such as Turkish Token Percentage (TR %) and Pure Token Percentage (Pure %) are correlated with downstream performance on MMLU-style benchmarks [9]. Motivated by these findings, this study aims to develop a tokenization strategy that aligns closely with Turkish mor- phosyntactic structures, minimizes redundancy, and improves interpretability. Empirical results validate this objective: the tokenizer achieved 90.29% TR % and 85.80% Pure %—the highest among all evaluated models—outperforming widely used tokenizers such as those from LLaMA, Gemma, and Qwen. These results demonstrate that tokenizers designed with linguistic integrity in mind can yield tokens that are both semantically meaningful and syntactically coherent, without relying on large vocabularies or excessive computational overhead. While the implementation is tailored to"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 3, "text": "evaluated models—outperforming widely used tokenizers such as those from LLaMA, Gemma, and Qwen. These results demonstrate that tokenizers designed with linguistic integrity in mind can yield tokens that are both semantically meaningful and syntactically coherent, without relying on large vocabularies or excessive computational overhead. While the implementation is tailored to Turkish, the underlying methodology is designed to generalize across other languages. 2 Related Work Tokenization is a fundamental step in NLP, significantly impacting model performance, memory efficiency, and downstream task effectiveness. Tokenization strategies range from character-level segmentation to subword-based methods such as Byte Pair Encoding (BPE) [2], WordPiece [3], and Unigram [15]. The choice of tokenization directly influences the ability of models to capture syntactic, semantic, and morphological structures, especially in morphologically rich languages like Turkish, Finnish, and Hungarian [7, 8]. Recent research has explored alternative tokenization strategies tailored to morphologically rich languages. The morphological tokenizer introduced by [8] outperformed conventional subword tokenization techniques, recovering 97% of the performance of larger BERT-based models while reducing model size by a factor of three. Additionally, tokenization granularity has been extensively examined in [16], which found that Turkish requires nearly 2.5 times more subwords per word than English, emphasizing the importance of vocabulary size in achieving optimal model performance. Tokenization strategies also play a crucial role in machine translation and text generation tasks. [17] demonstrated that morphology-aware segmentation reduces data sparsity in Neural Machine Translation (NMT) for Turkish-English and Uyghur-Chinese translation models. Additionally, [18] investigated target-side word segmentation strategies, showing that morphological segmentation improves translation accuracy by maintaining linguistic consistency between source and target languages. Beyond language modeling and translation, morphological tokenization has been evaluated in abstrac- tive summarization and sentiment analysis tasks. Studies like [7] revealed that morphology-aware tokenization improves summarization quality by preserving semantic information and reducing infor- mation loss. Hybrid tokenization approaches that combine statistical and morphological segmentation 3 have also demonstrated superior performance in multiple NLP tasks, particularly for Named Entity Recognition (NER) and Sentiment Analysis [19]. Despite these advancements, the computational cost of morphological tokenization remains an open challenge. Expanding the vocabulary size in tokenization increases memory consumption and slows down training times. [1] and [5] highlighted that while larger vocabulary sizes enhance performance in morphologically complex languages, they also contribute to increased model size. Furthermore, energy consumption in large-scale NLP models has become a growing concern. As discussed in [20], optimizing tokenization strategies plays a crucial role in improving resource efficiency and minimizing computational costs. To address these challenges, recent research has investigated adaptive tokenization methods that dynamically adjust segmentation strategies based on linguistic context. The EuroLLM project [21] developed multilingual tokenizers optimized for European languages, incorporating language-specific subword segmentation techniques. Similarly, [22] proposed a selective tokenization approach that prioritizes semantically meaningful tokens, demonstrating performance improvements in multilingual NLP tasks. Overall, ongoing research in tokenization strategies continues to evolve, with increasing emphasis on developing efficient, linguistically informed, and adaptive tokenization frameworks. The next section will delve deeper into the role of tokenization in language modeling, pretraining, and benchmark evaluations. Tokenization strategies play a critical role in pretraining large language"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 4, "text": "tasks. Overall, ongoing research in tokenization strategies continues to evolve, with increasing emphasis on developing efficient, linguistically informed, and adaptive tokenization frameworks. The next section will delve deeper into the role of tokenization in language modeling, pretraining, and benchmark evaluations. Tokenization strategies play a critical role in pretraining large language models (LLMs), influencing model efficiency, generalization, and performance across downstream tasks. Transformer-based architectures such as BERT [5], RoBERTa [1], and GPT [6] rely on effective tokenization to balance vocabulary size, sequence length, and computational cost. Studies have shown that inappropriate tokenization choices can introduce biases, degrade semantic coherence, and limit generalization to low-resource languages [23]. A key challenge in tokenization for LLMs is granularity control—striking a balance between exces- sively fragmented sequences and overly coarse segmentation. A comparative study in [16] analyzed tokenization granularity across English and Turkish, revealing that standard subword tokenization strategies result in Turkish words being split into approximately 2.5 times more subwords than English. This discrepancy affects the efficiency of multilingual models, as Turkish texts require longer sequences to encode the same information. Benchmark evaluations such as Massive Multitask Language Understanding (MMLU) [24] and TR-MMLU [10] highlight the shortcomings of existing tokenization techniques for morphologically complex languages. The TR-MMLU benchmark, specifically designed to evaluate Turkish NLP models, demonstrated that token purity—the alignment of tokens with linguistic units—correlates strongly with downstream model performance. The findings suggest that tokenization strategies optimized for English may not be directly transferable to Turkish and similar languages, necessitating morphology-aware adaptations. To address these issues, [9] proposed a novel linguistic integrity framework for evaluating tokenization strategies. This framework introduced token purity and language-specific token percentages (%TR) as critical evaluation metrics, providing a structured approach for assessing how well tokenization preserves morphological structures. Experimental results confirmed that higher %TR values correlate with improved performance on MMLU-style benchmarks, underscoring the importance of preserving language-specific morphemes. Recent efforts to refine tokenization strategies have included hybrid and domain-adaptive approaches. The ITUTurkBERT system [19] explored a hybrid tokenization method, combining whitespace segmentation with BPE and Unigram-based subword representations. This method was particularly beneficial for Named Entity Recognition (NER) and abstractive summarization, where preserving linguistic structure is crucial. Similarly, [25] examined the differences between GPT-4 and GPT-4o, demonstrating that model performance fluctuates depending on tokenization quality, especially in non-English tasks. Beyond model pretraining, tokenization impacts computational efficiency and energy consumption. [20] argued that BPE is suboptimal for pretraining due to inefficient vocabulary utilization, a con- cern echoed in [20]. These studies emphasize the need for tokenization techniques that minimize 4 redundancy and optimize training efficiency. Similarly, research on EuroLLM [21] has focused on developing multilingual tokenizers that adjust dynamically to different languages, reducing processing overhead while improving semantic coherence. Despite these advancements, morphological compositionality remains a challenge for LLMs. [23] found that state-of-the-art models struggle with morphological productivity, particularly when en- countering novel word roots. Their study demonstrated that model performance sharply declines as word complexity increases, a phenomenon that affects agglutinative languages more than English or Chinese. This finding aligns with earlier work by [8], which concluded that morphology-aware tokenization"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 5, "text": "that state-of-the-art models struggle with morphological productivity, particularly when en- countering novel word roots. Their study demonstrated that model performance sharply declines as word complexity increases, a phenomenon that affects agglutinative languages more than English or Chinese. This finding aligns with earlier work by [8], which concluded that morphology-aware tokenization improves semantic alignment, model interpretability, and generalization. The impact of morphological tokenization on NLP pipelines extends beyond text generation and classification. Research in optical character recognition (OCR) and document parsing [26] has demonstrated that custom tokenization tailored to linguistic structures significantly enhances accuracy. The Arabic-Nougat project, for instance, introduced a custom tokenizer, Aranizer-PBE-86k, which improved Markdown structure accuracy and character recognition in Arabic OCR tasks. Further investigations into tokenization adaptation for multilingual models highlight ongoing chal- lenges in cross-linguistic NLP. While standardized tokenization methods enable broad compatibility, they often fail to capture the linguistic diversity of non-English languages. [27] established a bench- mark for Scandinavian tokenizers, identifying key differences in how tokenization strategies affect language understanding. These findings support the argument that morphology-aware tokenization is essential for low-resource and typologically diverse languages. Given these insights, tokenization research continues to evolve toward more adaptive, efficient, and linguistically informed models. The next section will explore cutting-edge developments in tokenizer design, including self-learning tokenization, tokenization-free architectures, and the integration of morphological analysis into transformer-based models. Despite these advancements, morphological segmentation remains underutilized in contemporary LLM architectures. As shown in [23], even state-of-the-art LLMs struggle with compositional morphology, particularly when encountering novel root words. Their analysis found that performance declines sharply as morphological complexity increases, with models failing to generalize across dif- ferent inflected forms. This limitation highlights the need for morphologically informed tokenization that can dynamically adapt to linguistic variations. The integration of linguistic knowledge into tokenizer design has been further explored through morphological tagging and feature-based tokenization. While standard subword tokenization methods tokenize text without explicit linguistic knowledge, recent studies have experimented with incorporat- ing morphological features directly into tokenization schemes [9]. One such approach involves using morphologically tagged tokens instead of raw subwords, preserving grammatical information that is often lost in statistical segmentation. However, experiments with morphological tagging as tokens have yielded mixed results, as excessive granularity can lead to sequence length expansion, reducing model efficiency [16]. An emerging area of interest is dynamic tokenization strategies that adapt based on task requirements. Studies such as [28] have introduced more flexible Byte-Pair Tokenizers, capable of dynamically adjusting segmentation rules based on contextual requirements. This marks a shift away from static, pre-defined vocabularies toward more adaptable tokenization approaches that can optimize model performance dynamically. Despite these advancements, morphological tokenization has yet to become a standard component in mainstream NLP models. While experimental results consistently show that morphology-aware tokenization improves efficiency and accuracy, most large-scale language models still rely on tradi- tional subword segmentation methods. Addressing this gap requires further research into efficient morphological parsing algorithms, lightweight tokenizer architectures, and seamless integration into pretraining pipelines. In conclusion, tokenization research has evolved significantly from simple whitespace-based segmen- tation to more sophisticated subword and morphology-aware techniques. However, the"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 6, "text": "models still rely on tradi- tional subword segmentation methods. Addressing this gap requires further research into efficient morphological parsing algorithms, lightweight tokenizer architectures, and seamless integration into pretraining pipelines. In conclusion, tokenization research has evolved significantly from simple whitespace-based segmen- tation to more sophisticated subword and morphology-aware techniques. However, the limitations of static tokenization—particularly for morphologically rich languages—have spurred interest in self-learning tokenization, hybrid approaches, and tokenization-free architectures. Future research should focus on refining dynamic, language-aware tokenization methods that can enhance NLP 5 models across diverse linguistic contexts, ensuring that tokenization strategies do not become a bottleneck for language model performance. 3 Methodology Traditional NLP models primarily relied on word-level tokenization, where each word was treated as an individual token. However, this approach was inadequate for handling out-of-vocabulary (OOV) words, requiring extensive vocabulary lists that resulted in inefficient memory usage [6]. To address this, subword tokenization methods such as BPE and WordPiece emerged, segmenting rare words into smaller, frequently occurring subunits, thereby improving generalization and reducing OOV occurrences. BPE, originally introduced for data compression [29] and later adapted for NLP by [2], iteratively merges frequent adjacent character pairs into subword units. Similarly, WordPiece, which was initially developed for speech recognition [3], follows a comparable iterative merging approach but optimizes token selection using likelihood-based probability maximization. Morphological complexity presents a significant challenge for NLP tokenization, particularly in ag- glutinative languages such as Turkish, Hungarian, and Finnish. These languages exhibit a high degree of word inflection, resulting in a vast array of surface forms derived from relatively few lemmas [21]. In Turkish, for instance, the word anlayabildiklerimizden (‘from what we were able to understand’) is composed of multiple morphemes: anla- (UNDERSTAND) + -yabil (ABLE) + -dik (NOMINALIZER) + -ler (PLURAL) + -imiz (1PL.POSS) + -den (ABLATIVE). Standard subword tokenization methods such as Byte Pair Encoding (BPE) and WordPiece often fail to capture such rich internal structures, fragmenting words in ways that obscure grammatical function and semantic interpretation [16]. This misalignment reduces linguistic coherence and can negatively impact downstream tasks, highlight- ing the need for tokenizers that are sensitive to language-specific morphological and phonological features. The hybrid tokenization framework combines linguistic knowledge with statistical subword segmen- tation techniques to enhance tokenization performance in morphologically rich languages, using Turkish as a benchmark. The approach integrates rule-based morphological analysis with a structured dictionary of roots and affixes while incorporating Byte Pair Encoding (BPE) to handle out-of- vocabulary (OOV) words and ambiguous segments. The objective is to create a tokenization system that accurately represents linguistic structures while maintaining computational efficiency. The tokenizer is implemented in both Python and Rust, each optimized for different use cases. The Python implementation provides flexibility and ease of integration into NLP pipelines, whereas the Rust implementation prioritizes performance through parallel processing and efficient memory man- agement. The tokenization process follows a structured pipeline consisting of three key components: dictionary-based morphological segmentation, BPE integration for subword tokenization, and the inclusion of special tokens to preserve linguistic and formatting information. Morphological segmentation is a key component of the proposed approach, leveraging a dual- dictionary system"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 7, "text": "memory man- agement. The tokenization process follows a structured pipeline consisting of three key components: dictionary-based morphological segmentation, BPE integration for subword tokenization, and the inclusion of special tokens to preserve linguistic and formatting information. Morphological segmentation is a key component of the proposed approach, leveraging a dual- dictionary system to accurately identify and segment words. The root dictionary is constructed from high-frequency words extracted from large-scale Turkish corpora, ensuring that only base word forms are included. This dictionary is augmented with phonological normalization techniques to prevent vocabulary expansion due to phonological alternations such as final devoicing (kitap → kitabı), haplology (alın →alnı), and vowel hiatus (oyna + yor →oynuyor). Furthermore, frequently used compound words such as \"akarsu\" and \"çama¸ sırhane\" are assigned unique identifiers to ensure they are treated as single tokens rather than being arbitrarily segmented. The affix dictionary consists of approximately 230 linguistic elements, including suffixes, prepositions, and conjunctions. To improve efficiency and reduce redundancy, affixes with identical grammatical functions, such as the plural markers \"-lAr\" or the ablative markers \"-dAn,\" are assigned a common identifier. This approach ensures that morphologically equivalent structures do not inflate the vocabulary size while preserving their grammatical roles in sentence construction. To ensure comprehensive token coverage, the framework integrates Byte Pair Encoding (BPE) to segment words that are not explicitly listed in the morphological dictionaries. The training data for BPE was sourced from large-scale Turkish corpora, specifically umarigan/turkish_corpus_small and kadirnar/combined-turkish-datasets-v4, with a combined size of 8.52 GB. Using the SentencePiece library, a vocabulary of 10,000 subword units was generated and subsequently incorporated into 6 Input Is there any special tokens? Add special token to token list Process word segment Is the word in root list? Add ID of root to token list Iterate through word to find longest matched root Is the root found? Check suffixes Try BPE seg- mentation Are suffixes found? Add IDs of root and suffixes to token list Is remainder a root? Add ID of root to token list Yes No Yes No Yes No Yes No Yes No Figure 1: Tokenization decision flow with root, suffix, and fallback segmentation logic. 7 the tokenizer. This enables the system to process novel words while retaining consistency in morphological decomposition. Special tokens are introduced to handle whitespace, punctuation, capitalization, and unknown words, enhancing the tokenizer’s ability to preserve linguistic structure. A dedicated token for whitespace en- sures that spacing information is explicitly encoded, preserving sentence structure during tokenization. Additionally, an uppercase token is introduced to differentiate capitalized words from their lowercase counterparts without inflating the vocabulary. Additional tokens account for newline characters, tab spaces, and unknown words, preventing tokenization errors when encountering unfamiliar input. The encoding process begins with morphological analysis, where the longest matching root is identified from the dictionary. Once the root is determined, suffix segmentation is performed by iteratively checking for affix matches. If a valid segmentation cannot be identified using the morphological dictionary, the remaining portion of the word is processed using BPE-based subword segmentation. Words that do not match any predefined root, suffix, or subword are"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 8, "text": "Once the root is determined, suffix segmentation is performed by iteratively checking for affix matches. If a valid segmentation cannot be identified using the morphological dictionary, the remaining portion of the word is processed using BPE-based subword segmentation. Words that do not match any predefined root, suffix, or subword are assigned an unknown token, ensuring robustness in handling OOV terms. The decoding process reconstructs tokenized text while adhering to linguistic rules. A reverse mapping mechanism ensures that phonological alternations are restored correctly, preserving morphosyntactic dependencies. Disambiguation rules are applied to select the most probable reconstruction based on phonetic context and grammatical constraints. This process enhances readability while maintaining fidelity to the original text. The proposed framework provides a balance between linguistic integrity and computational effi- ciency. By integrating morphological analysis with BPE-based segmentation, the tokenizer achieves improved performance in capturing linguistic structures while maintaining flexibility for unseen words. Furthermore, the methodology is adaptable to other morphologically complex languages, demonstrating its potential for multilingual applications. The construction of the tokenizer dictionary follows a structured approach that ensures comprehensive coverage of Turkish morphology while maintaining efficiency. The dictionary consists of three primary components: a root word list, an affix list, and a set of functional words such as prepositions and conjunctions. These elements form the basis of the tokenization process, enabling accurate segmentation and linguistic representation. The root dictionary is built from a dataset of high-frequency Turkish words extracted from large-scale corpora. This dataset includes approximately 22,000 roots, ensuring broad lexical coverage. Each root is assigned a unique identifier, allowing for consistent referencing throughout the tokenization process. To improve efficiency, roots are categorized based on their length, enabling a hierarchical lookup mechanism that prioritizes longer roots before shorter alternatives. This method significantly enhances root detection speed by reducing the number of comparisons required. An additional layer of processing is applied to handle phonological alternations in root words, which frequently occur in Turkish due to sound changes triggered by suffixation. To ensure consistency and reduce vocabulary sparsity, different phonetic realizations of the same morphological root are mapped to a single identifier. For example, final devoicing results in surface variations such as kitap (‘book’) and kitabı (‘its book’), both of which are assigned the same root ID. Similarly, haplology in forms like alın (‘forehead’) and alnı (‘his/her forehead’), and vowel hiatus in forms like oyna + yor →oynuyor (‘he/she/it is playing’) are normalized through unified token mappings. This phonological normalization preserves morphological coherence while avoiding unnecessary token duplication. In addition to root words, the dictionary includes a comprehensive inventory of approximately 230 suffixes, prepositions, and conjunctions, compiled from authoritative linguistic sources and organized according to grammatical function. To further optimize vocabulary size without compromising syntactic accuracy, affixes that perform the same grammatical role are assigned a shared identifier. For instance, plural suffixes such as -lAr, or ablative markers like -dAn, -tAn, functionally represent the same morphemes but differ based on phonological context. This strategy is also applied to locative markers like -dA and -tA, which exhibit surface variation due to consonant alternation rules. By merging such"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 9, "text": "shared identifier. For instance, plural suffixes such as -lAr, or ablative markers like -dAn, -tAn, functionally represent the same morphemes but differ based on phonological context. This strategy is also applied to locative markers like -dA and -tA, which exhibit surface variation due to consonant alternation rules. By merging such phonologically conditioned allomorphs, the tokenizer reduces redundancy while maintaining linguistic fidelity. Compound words represent another important aspect of Turkish morphology, wherein multiple roots combine to form a single semantic unit. To prevent incorrect segmentation, frequently used 8 compounds such as akarsu (‘stream’) and çama¸ sırhane (‘laundromat’) are directly included in the dictionary and assigned unique token IDs. This ensures that compound expressions are treated as indivisible lexical items, preserving their semantic integrity and avoiding erroneous decomposition into root-affix pairs. Beyond roots and affixes, the dictionary incorporates functional words such as prepositions and conjunctions, which play a crucial role in sentence structure. These elements are often challenging to tokenize correctly due to their small size and high frequency. By including them explicitly in the dictionary, the tokenizer avoids erroneous segmentations that might result from statistical subword approaches. The integration of Byte Pair Encoding (BPE) further enhances tokenization flexibility. While the dictionary provides structured linguistic segmentation, BPE ensures robust handling of words not explicitly covered in the predefined lexicon. The BPE model is trained on a diverse Turkish corpus, incorporating approximately 10,000 subword units to supplement dictionary-based tokenization. The combined approach enables the tokenizer to efficiently process both frequent and rare words, ensuring comprehensive text coverage. Another important aspect of the proposed framework is its ability to handle case sensitivity without increasing vocabulary size. A dedicated uppercase token is introduced to mark words that were originally capitalized. This avoids the need to store separate tokens for capitalized and lowercase versions of the same word, optimizing storage efficiency while preserving orthographic distinctions. The dictionary-driven approach provides a balance between linguistic accuracy and computational efficiency. By leveraging structured linguistic resources, normalizing phonological variations, and integrating statistical subword segmentation, the tokenizer achieves robust performance across diverse text types. The next section will describe the encoding and decoding processes in detail, outlining how tokenization is applied in practice to segment and reconstruct text. The encoding process follows a hierarchical approach that ensures linguistic consistency while maintaining computational efficiency. The tokenizer operates in a multi-step pipeline that sequentially applies morphological analysis, affix segmentation, and subword processing. This structured approach optimizes tokenization accuracy while preserving essential linguistic features. The encoding process begins with preprocessing, where special characters and formatting elements are replaced with predefined tokens. Whitespace characters such as spaces, newlines, and tab spaces are explicitly encoded using dedicated tokens. This step ensures that text formatting is preserved, preventing information loss in structured text. Additionally, words that begin with capital letters are marked with an uppercase token to maintain case information without inflating the vocabulary. Following preprocessing, the tokenizer applies root detection using a hierarchical lookup strategy. The algorithm first searches for the longest matching root in the dictionary, prioritizing exact matches before considering phonological variants. If a match is"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 10, "text": "letters are marked with an uppercase token to maintain case information without inflating the vocabulary. Following preprocessing, the tokenizer applies root detection using a hierarchical lookup strategy. The algorithm first searches for the longest matching root in the dictionary, prioritizing exact matches before considering phonological variants. If a match is found, the root is assigned its corresponding token ID. In cases where no direct match is identified, alternative scenarios such as compound words or phonologically altered roots are considered. This flexible approach ensures that words are correctly segmented even when phonological modifications are present. Once the root is identified, suffix segmentation is performed iteratively. The algorithm checks for affix matches in the suffix dictionary and assigns token IDs accordingly. Each identified suffix is treated as a separate token, maintaining its grammatical function while ensuring proper segmentation. The suffix matching process continues until no further valid suffixes can be extracted. If an affix is ambiguous or overlaps with multiple possible segmentations, a probabilistic model selects the most likely segmentation based on corpus frequency data. If a word does not match any predefined root or suffix, Byte Pair Encoding (BPE) is applied as a fallback mechanism. The BPE model segments the word into subword units based on a pre-trained vocabulary, ensuring that unknown words are processed effectively. This hybrid approach prevents the tokenizer from failing on unseen words while maintaining the linguistic integrity of known structures. For example, the word kalktı˘ gımızda (‘when we stood up’) is segmented into its root and affix components as follows: Input text: \"Kalktığımızda hep birlikte yürüdük.\" (“When we stood up, we walked to- gether.”) 9 Token sequence: [uppercase], kalk, tığ, ımız, da, [space], hep, [space], birlikte, [space], yürü, dü, k, . Token IDs: 0, 1502, 22280, 22285, 22278, 1, 2300, 1, 4803, 1, 2280, 22296, 22617, 22582 Input Text \"Kalktığımızda hep birlikte yürüdük.\" Tokenized [uppercase], kalk, tığ, ımız, da, [space], hep, [space], birlikte, [space], yürü, dü, k, . Token IDs 0, 1502, 22280, 22285, 22278, 1, 2300, 1, 4803, 1, 2280, 22296, 22617, 22582 Decoded Tokens Output Text \"Kalktığımızda hep birlikte yürüdük.\" Encoding Decoding Figure 2: Encoding and decoding process for the sentence “Kalktı˘ gımızda hep birlikte yürüdük.” This example demonstrates how the encoder accurately identifies the root kalk (“stand up”), segments its suffixes (-tı˘ g “past nominalizer”, -ımız “our”, -da “when/at”), and preserves syntactic structure us- ing dedicated space and punctuation tokens. Each token corresponds to a morphologically meaningful unit, enabling interpretable and reversible text representations. The decoding process reconstructs surface text from tokenized sequences while maintaining linguistic accuracy. Token IDs are mapped back to their textual forms, and affixes are recombined according to their grammatical function. During this step, phonological alternations are reversed: rules for soft consonantization, vowel deletion, and contraction are reapplied to ensure natural word formation. Capitalization is restored using a dedicated [uppercase] token, which automatically capitalizes the first letter of the following word. Space and punctuation tokens ([space], ., etc.) are replaced with their respective characters, maintaining sentence layout. If an unknown or out-of-vocabulary token is encountered, a placeholder is inserted to allow"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 11, "text": "word formation. Capitalization is restored using a dedicated [uppercase] token, which automatically capitalizes the first letter of the following word. Space and punctuation tokens ([space], ., etc.) are replaced with their respective characters, maintaining sentence layout. If an unknown or out-of-vocabulary token is encountered, a placeholder is inserted to allow for post-processing or human review. Consider another example: Token sequence: [uppercase], kitap, [space], okuma, yı, [space], sev, i, yor, um, . Decoded output: \"Kitap okumayı seviyorum.\" (“I like reading books.”) This process demonstrates how the tokenizer ensures both accuracy and efficiency in text reconstruc- tion, preserving morphological structure while maintaining fluency. The proposed framework successfully integrates morphological analysis with subword segmentation, creating a robust tokenizer optimized for morphologically complex languages. By balancing linguistic integrity and computational efficiency, this approach offers a scalable solution adaptable to multiple 10 languages. Future work may explore extending this framework to other agglutinative languages, optimizing it for real-time applications, and integrating additional linguistic features for enhanced performance. 4 Results and Analysis The performance of the proposed morphological tokenizer was evaluated using the TR-MMLU benchmark dataset, which comprises over 1.6 million characters and approximately 200,000 words curated specifically for Turkish [10]. This dataset is designed to reflect the linguistic complexity of Turkish, including its rich morphology, agglutinative structures, and diverse syntactic constructions. As such, it provides a rigorous basis for assessing tokenization quality in morphologically complex languages. The evaluation compared five different tokenizers: google/gemma-2-9b, meta-llama/Llama-3.2-3B, Qwen/Qwen2.5-7B-Instruct, CohereForAI/aya-expanse-8b, and the proposed turkish_tokenizer. Each tokenizer was assessed using a consistent set of linguistic and computational metrics introduced in [9]. These metrics include total token count, vocabulary size, number of unique tokens, Turkish Token Percentage (TR %), and Pure Token Percentage (Pure %). TR % quantifies the proportion of tokens that correspond to valid Turkish words or morphemes, while Pure % measures the proportion of tokens that fully align with unambiguous root or affix boundaries, thus reflecting morphological integrity. Table 1: Performance of the proposed turkish_tokenizer on the TR-MMLU dataset. Metric Value Vocabulary Size 32,768 Total Token Count 707,727 Processing Time (s) 0.6714 Unique Token Count 11,144 Turkish Token Count 10,062 Turkish Token Percentage (TR %) 90.29% Pure Token Count 9,562 Pure Token Percentage (Pure %) 85.80% The proposed turkish_tokenizer demonstrated the highest linguistic alignment across all evalu- ated metrics. It achieved a TR % of 90.29% and a Pure % of 85.80%, substantially outperforming all competing tokenizers. In comparison, google/gemma-2-9b reached a TR % of only 40.96% and a Pure % of 28.49%, indicating that the majority of its tokens do not represent full morphemes. Similarly, meta-llama/Llama-3.2-3B produced a TR % of 45.77% and a Pure % of 31.45%, while Qwen2.5 and aya-expanse achieved TR % values of 40.39% and 53.48%, respectively. Despite employing significantly smaller vocabulary sizes, the proposed tokenizer demonstrated better linguistic segmentation. With a vocabulary of 32,768 tokens and 11,144 unique tokens used during evaluation, it balanced generalization and expressiveness more effectively than models such as gemma-2-9b and aya-expanse, which rely on vocabularies of over 255,000 tokens. These large-vocabulary tokenizers, rooted in frequency-based subword segmentation, tend to"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 12, "text": "tokenizer demonstrated better linguistic segmentation. With a vocabulary of 32,768 tokens and 11,144 unique tokens used during evaluation, it balanced generalization and expressiveness more effectively than models such as gemma-2-9b and aya-expanse, which rely on vocabularies of over 255,000 tokens. These large-vocabulary tokenizers, rooted in frequency-based subword segmentation, tend to fragment morphologically rich expressions and introduce ambiguity in downstream tasks. In contrast, the morphological awareness of the turkish_tokenizer enables semantically coherent token formation and more consistent syntactic parsing. Although the total token count generated by the proposed tokenizer (707,727) exceeds those of the other models—for instance, aya-expanse produced 434,526 tokens—this increase is offset by gains in interpretability and linguistic fidelity. High TR % and Pure % scores suggest reduced reliance on spurious subword splits and improved preservation of morphosyntactic structure. This is particularly beneficial for tasks such as syntactic parsing, translation, summarization, and question answering, where semantic consistency across tokens is essential. These findings support the hypothesis introduced in [9], which argues that high linguistic alignment in tokenization correlates strongly with downstream model performance in morphologically rich and low- resource languages. While conventional subword tokenizers may suffice for high-resource languages like English, they exhibit clear limitations in Turkish unless informed by morphological structure. 11 The results presented here highlight the effectiveness of combining rule-based linguistic analysis with subword strategies to produce tokenizers that are both accurate and efficient in morphologically complex settings. To illustrate the linguistic fidelity of different tokenization strategies, we present a qualitative compar- ison using the Turkish sentence: \"Atasözleri geçmi¸ sten günümüze kadar ula¸ san anlamı bakımından mecazlı bir mana kazanan kalı- pla¸ smı¸ s sözlerdir.\" (“Proverbs are fixed expressions passed down from the past to the present that acquire a metaphorical meaning in terms of their significance.”) This sentence contains a wide range of morphological features, including compound words, multiple derivational and inflectional suffixes, and root forms that undergo phonological alternations. These properties make it an ideal test case for evaluating the morphological sensitivity of different tokenizers. Proposed Hybrid Tokenizer: The hybrid morphological tokenizer segments the sentence into linguistically meaningful units with high fidelity. It produces: [\"<uppercase>\", \"atasöz\", \"ler\", \"i\", \"<space>\", \"geçmiş\", \"ten\", \"<space>\", \"gün\", \"üm\", \"üz\", \"e\", \"<space>\", \"kadar\", \"<space>\", \"ulaş\", \"an\", \"<space>\", \"anlam\", \"ı\", \"<space>\", \"bakım\", \"ın\", \"dan\", \"<space>\", \"mecaz\", \"lı\", \"<space>\", \"bir\", \"<space>\", \"mana\", \"<space>\", \"kazan\", \"an\", \"<space>\", \"kalıp\", \"laş\", \"mış\", \"<space>\", \"sözle\", \"r\", \"dir\", \".\"] It correctly separates suffixes (\"ler\", \"i\", \"ın\", \"dan\", \"lı\", \"an\", \"mış\", \"dir\"), extracts root forms such as \"atasöz\", \"gün\", \"mana\", and employs special tokens like \"<uppercase>\" and \"<space>\" to preserve orthographic structure. Gemma-3: The tokenizer google/gemma-3 segments the sentence as: [\"<bos>\", \"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" gün\", \"ümü\", \"ze\", \" kadar\", \" ulaş\", \"an\", \" anlam\", \"ı\", \" bakım\", \"ından\", \" mec\", \"az\", \"lı\", \" bir\", \" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"mış\", \" söz\", \"lerdir\", \".\"] Although it captures some suffixes like \"ten\" and \"ından\", it fragments common roots (\"At\", \"as\", \"öz\" instead of \"atasöz\") and fails to isolate inner morphemes in forms such as \"lerdir\" and \"kazanan\", limiting morphological interpretability."}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 13, "text": "\" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"mış\", \" söz\", \"lerdir\", \".\"] Although it captures some suffixes like \"ten\" and \"ından\", it fragments common roots (\"At\", \"as\", \"öz\" instead of \"atasöz\") and fails to isolate inner morphemes in forms such as \"lerdir\" and \"kazanan\", limiting morphological interpretability. LLaMA-3.2: The tokenizer meta-llama/Llama-3.2-3B yields: [\"<|begin_of_text|>\", \"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" gün\", \"ümü\", \"ze\", \" kadar\", \" \", \"ula\", \"ş\", \"an\", \" anlam\", \"ı\", \" bakımından\", \" me\", \"ca\", \"z\", \"lı\", \" bir\", \" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"mış\", \" söz\", \"lerdir\", \".\"] This tokenizer combines morphologically valid segments like \"bakımından\" and \"kazanan\" with fragmented roots like \"At\", \"as\", \"öz\", creating inconsistency in morpheme alignment. Qwen2.5: The tokenizer Qwen/Qwen2.5 outputs: [\"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" gün\", \"üm\", \"ü\", \"ze\", \" kadar\", \" ulaş\", \"an\", \" anlamı\", \" bakım\", \"ından\", \" me\", \"ca\", \"z\", \"lı\", \" bir\", \" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"mış\", \" söz\", \"ler\", \"dir\", \".\"] While suffixes such as \"ten\" and \"ından\" are recognized, the tokenizer introduces redundant splits like \"üm\", \"ü\", \"ze\", reducing the linguistic coherence of the token stream. 12 Aya-Expanse: The tokenizer CohereForAI/aya-expanse returns: [\"<BOS_TOKEN>\", \"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" günümüze\", \" kadar\", \" ulaşan\", \" anlamı\", \" bakımından\", \" mec\", \"az\", \"lı\", \" bir\", \" mana\", \" kazanan\", \" kalı\", \"pl\", \"aş\", \"mış\", \" söz\", \"lerdir\", \".\"] It retains some complete word forms such as \"günümüze\" and \"ulaşan\", but still fragments compounds like \"kalıplaşmış\" and splits the root \"atasöz\", reducing morphological traceability. Phi-4: The tokenizer microsoft/phi-4 produces: [\"At\", \"as\", \"ö\", \"z\", \"leri\", \" geç\", \"mi\", \"ş\", \"ten\", \" gün\", \"üm\", \"ü\", \"ze\", \" kadar\", \" \", \"ula\", \"ş\", \"an\", \" an\", \"lam\", \"ı\", \" bak\", \"ım\", \"ından\", \" me\", \"ca\", \"z\", \"lı\", \" bir\", \" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"m\", \"ış\", \" sö\", \"z\", \"ler\", \"dir\", \".\"] This tokenizer over-fragments even basic stems like \"geçmiş\" into \"geç\", \"mi\", \"ş\" and \"anlam\" into \"an\", \"lam\", increasing token count and reducing interpretability. YTU Turkish GPT-2: The tokenizer ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1, trained on Turkish corpora, yields: [\"At\", \"as\", \"öz\", \"leri\", \" geçmişten\", \" günümüze\", \" kadar\", \" ulaşan\", \" anlamı\", \" bakımından\", \" mec\", \"az\", \"lı\", \" bir\", \" mana\", \" kazanan\", \" kalıp\", \"laşmış\", \" söz\", \"lerdir\", \".\"] Although it still segments \"atasözleri\" incorrectly, it performs well with forms like \"geçmişten\", \"günümüze\", and \"bakımından\", showing the advantage of Turkish-specific pretraining. GPT-4o: The tokenizer gpt-4o-o200k_base generates: [\"At\", \"as\", \"öz\", \"leri\", \" geçmiş\", \"ten\", \" gün\", \"ümü\", \"ze\", \" kadar\", \" ulaş\", \"an\", \" anlam\", \"ı\", \" bakım\", \"ından\", \" mec\", \"az\", \"lı\", \" bir\", \" mana\", \" kaz\", \"anan\", \" kal\", \"ı\", \"pla\", \"ş\", \"mış\", \" söz\", \"ler\", \"dir\", \".\"] Its segmentation strategy is similar to LLaMA and Qwen—partially aware of Turkish morphemes but limited by frequent over-segmentation of compound and derived forms. The results presented in this section provide strong empirical support for the hypothesis introduced in the introduction: tokenizers that explicitly incorporate morphological and phonological"}
{"doc_id": "2508.14292v1", "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14292v1", "chunk_id": 14, "text": "\"ler\", \"dir\", \".\"] Its segmentation strategy is similar to LLaMA and Qwen—partially aware of Turkish morphemes but limited by frequent over-segmentation of compound and derived forms. The results presented in this section provide strong empirical support for the hypothesis introduced in the introduction: tokenizers that explicitly incorporate morphological and phonological knowledge of Turkish can outperform general-purpose models in both segmentation accuracy and linguistic coher- ence. While most state-of-the-art tokenizers struggle with root-fragmentation, over-segmentation, and inconsistent affix treatment, the proposed hybrid tokenizer consistently identifies morpheme boundaries, preserves semantically meaningful units, and reduces vocabulary redundancy. These findings validate the motivation behind this work: morphologically informed tokenization is essential for robust and interpretable NLP in agglutinative languages like Turkish. The qualitative compar- isons presented here illustrate not only the performance gap between general and language-specific tokenizers, but also the need for tokenizer architectures that respect language-internal rules. 5 Conclusion In this study, we introduced a linguistically-informed hybrid tokenization framework specifically designed to address the challenges posed by morphologically rich and low-resource languages, with Turkish serving as the primary case study. By integrating rule-based morphological analysis with subword segmentation techniques such as Byte Pair Encoding (BPE), our approach seeks to preserve morpheme boundaries, minimize vocabulary redundancy, and improve syntactic and semantic coherence during tokenization. Empirical evaluations on the TR-MMLU dataset demonstrated that the proposed turkish_tokenizer significantly outperforms existing state-of-the-art tokenizers—including 13 gemma-2, llama-3, qwen2.5, and aya-expanse—in both Turkish Token Percentage (TR %) and Pure Token Percentage (Pure %), achieving 90.29% and 85.80%, respectively. These metrics reflect the tokenizer’s strong alignment with the linguistic structure of Turkish, a crucial factor for downstream NLP tasks. The tokenizer also exhibited efficient vocabulary utilization with only 32,768 entries and showed robust performance in handling morphosyntactic structures across diverse sentence types. Qualitative analyses further reinforced the superiority of our approach, revealing that the proposed tok- enizer segments text into linguistically meaningful units and accurately preserves suffixes, compound forms, and phonologically altered variants—challenges frequently mishandled by general-purpose, frequency-driven tokenization strategies. The findings presented here reaffirm the thesis proposed in [9], namely that tokenization strategies rooted in linguistic structure are not only desirable but necessary for accurate and efficient language modeling in morphologically complex settings. As NLP continues to evolve toward inclusive, multilingual systems, the development of linguistically aware tokenization methods will be critical for ensuring equity in language technologies. Future directions include extending this hybrid framework to other agglutinative and typologically diverse languages, refining the morphological rules through semi-supervised learning, and exploring integration with multilingual LLM pretraining pipelines to optimize performance in low-resource language environments."}
{"doc_id": "2508.14288v1", "title": "Measuring LLM Code Generation Stability via Structural Entropy", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14288v1", "chunk_id": 0, "text": "Measuring LLM Code Generation Stability via Structural Entropy Yewei Song, Tiezhu Sun, Xunzhu Tang, Prateek Kumar Rajput, Tegawend´ e F. Bissyand´ e Jacques Klein, The Interdisciplinary Centre for Security, Reliability and Trust University of Luxembourg Abstract—Assessing the stability of code generation from large language models (LLMs) is essential for judging their reliability in real-world development. We extend prior “structural- entropy” concepts to the program domain by pairing entropy with abstract-syntax-tree (AST) analysis. For any fixed prompt, we collect the multiset of depth-bounded subtrees of AST in each generated program and treat their relative frequencies as a probability distribution. We then measure stability in two complementary ways: (i) Jensen–Shannon divergence, a symmetric, bounded indicator of structural overlap, and (ii) a Structural Cross- Entropy ratio that highlights missing high-probability patterns. Both metrics admit structural-only and token-aware variants, enabling separate views on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or CodeBLEU, our metrics are reference-free, language-agnostic, and execution-independent. We benchmark several leading LLMs on standard code generation tasks, demonstrating that AST-driven structural entropy reveals nuances in model consistency and robustness. The method runs in O(n, d) time with no external tests, providing a lightweight addition to the code-generation evaluation toolkit. Index Terms—Large Language Models, Code Generation, Structural Entropy, Evaluation Metrics, Stability Test I. INTRODUCTION The advent of powerful Large Language Models (LLMs) has enabled remarkable capabilities in automated code generation. However, a notable challenge is the high variability of generated code: identical prompts can yield substantially different code snippets across runs or across models. Prior studies observe that even with fixed inputs and hyperparameters, state-of-the-art LLMs are rarely deterministic at the output level [1]. For example, ChatGPT produces completely different code on repeated queries for the same programming task in the majority of cases (e.g. 75.8% of tasks showed zero identical test outputs across runs) [2]. Crucially, setting the sampling temperature to zero (greedy decoding) did not guarantee consistency. This output instability undermines developer trust and makes reproducibility of code-generation research difficult. In safety- critical or collaborative software settings, unpredictable LLM suggestions can harm reliability. Hence there is a pressing need to rigorously quantify the structural stability of generated code, beyond simply assessing correctness. Current evaluation of LLM code outputs largely focuses on functional correctness or textual similarity to reference solutions. The pass@k metric, for example, checks if any of k samples passes the unit tests [3], thereby measuring functional success. While pass@k and related metrics (e.g. the unbiased pass-ratio@n) are valuable for overall performance [4], they do not address variability: different outputs may all pass the tests but differ substantially in structure. Likewise, traditional NLP- style metrics (BLEU, ROUGE, METEOR, etc. [5]–[7]) assess n-gram overlap with a reference solution, but have known limitations for code [8], [9]. Ren et al. in the CodeBLEU work show that BLEU correlates poorly with code semantics and cannot account for the many functionally equivalent programs that do not share surface tokens [10]. Code-specific metrics such as CodeBLEU have been proposed to address syntax and semantics: CodeBLEU augments n-gram matches with AST-based syntax weighting and data-flow features [10]."}
{"doc_id": "2508.14288v1", "title": "Measuring LLM Code Generation Stability via Structural Entropy", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14288v1", "chunk_id": 1, "text": "work show that BLEU correlates poorly with code semantics and cannot account for the many functionally equivalent programs that do not share surface tokens [10]. Code-specific metrics such as CodeBLEU have been proposed to address syntax and semantics: CodeBLEU augments n-gram matches with AST-based syntax weighting and data-flow features [10]. These improvements help correlate with human judgments, but CodeBLEU and similar metrics still measure pairwise similarity to a single reference output rather than the consistency across multiple samples. We list some metrics that are currently used for measuring LLM stability in Table I. In short, existing metrics focus on correctness or reference fidelity, but they do not capture structural/topological similarity of different outputs from the same prompt. Although structural entropy—measuring uncertainty or vari- ability in structural choices—is not widely used in LLM code-generation literature, related concepts have been explored in software engineering contexts. For example, Torres et al. applied structural entropy metrics to study software evolution, quantifying how changes affect the organization and complexity of code structures over time [11]. Their method captures how transformations impact predictability and structural information in software systems. A closely analogous idea in LLM text generation is Semantic Entropy (SE), proposed by Kossen et al., which assesses model uncertainty by clustering multiple generated answers by meaning; high SE indicates diverse or inconsistent outputs, signaling possible hallucinations [12]. Inspired by this, our work adapts the entropy concept to code generation, analyzing variability in generated code AST structures. Low structural entropy reflects consistent outputs, whereas high entropy indicates the model frequently alternates between different programming structures or approaches. II. STRUCTURAL ENTROPY AND SIMILARITY Our pipeline consists of three phases: subtree extraction from ASTs, constructing empirical distributions from these subtrees, 1Proposed by this paper. Readers can check the code with the link: https://github.com/Etamin/SCE.git TABLE I COMPARISON OF CURRENT STABILITY METRICS WITH OUR APPROACHES. Evaluation Method Description Structure Ref. BLEU/ ROUGE-L/ METEOR N-gram overlap or sequence matching to reference code. No [5]–[7] Exact Match Checks if generated code exactly matches the reference. No - Pass@k Runs up to k generated outputs; succeeds if any pass all tests. Indirect [13] CodeBLEU Combines BLEU with AST subtree match and data-flow match. Yes [10] RUBY Compare Program Dependency Graph of output vs. reference. Yes [14] TSED Computes the tree edit distance between AST of output and AST of reference. Yes [15] Semantic Entropy Clusters multiple answers to the same question, then calculates entropy. Indirect [16] Structural Entropy1 Proposed: Parse the answers’ AST structure and computes the subtree entropy between answers, with KL or JS divergence. Yes - and computing similarity metrics. We parse code outputs into ASTs and extract depth-bounded subtrees, transforming them into canonical encodings. These encodings are then used to construct empirical distributions that capture structural variations between code outputs. Finally, we calculate stability using two entropy-based metrics: Jensen–Shannon divergence, providing a symmetric measure of structural similarity, and Structural Cross-Entropy, which emphasizes missing high- probability patterns. Phase 1 : Depth-bounded Subtree Extraction: Let TA and TB denote the ASTs parsed from two code snippets. Fix a depth parameter d ∈N. For every"}
{"doc_id": "2508.14288v1", "title": "Measuring LLM Code Generation Stability via Structural Entropy", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14288v1", "chunk_id": 2, "text": "calculate stability using two entropy-based metrics: Jensen–Shannon divergence, providing a symmetric measure of structural similarity, and Structural Cross-Entropy, which emphasizes missing high- probability patterns. Phase 1 : Depth-bounded Subtree Extraction: Let TA and TB denote the ASTs parsed from two code snippets. Fix a depth parameter d ∈N. For every node v in an AST, we consider the rooted subtree sub(v; d) which is the fragment of T rooted at v containing all descendants up to depth d. To transform such subtrees into hashable symbols, we define two canonical encodings: (a) Structure-only encoding σstruct : sub(v; d) 7→ (node-type(v), \u0000node-type(c1), . . . , node-type(ck) \u0001 ), where c1, . . . , ck are the immediate children of v. (b) Structure+value encoding σvalue : sub(v; d) 7→ (node-type(v), lexeme(v), \u0000node-type(c1), . . . , node-type(ck) \u0001 ), where lexeme(v) is the exact source text covered by v (for leaves) or a sentinel ∅(for internal nodes). These two approaches trade off generality vs. specificity. Structure-only patterns may capture common coding patterns and yield higher overlap between different programs, but they lose fine-grained information. Structure-with-value patterns are more discriminative (sensitive to exact code), but they also increase the size of the pattern vocabulary and may yield sparser overlap. In practice, one can choose the representation depending on whether value information is important to the similarity task. For either choice of σ, we enumerate the multisets of depth- bounded subtrees extracted from TA and TB: SA = \b σ \u0000sub(v; d) \u0001 v ∈TA , SB = \b σ \u0000sub(v; d) \u0001 v ∈TB . (1) Let nA = |SA| and nB = |SB| be the total numbers of (possibly repeated) subtree symbols harvested. In (1) we treat SA, SB as multisets symbols may appear with multiplicity. Phase 2 : Constructing Empirical Distributions: Let the multisets of subtree symbols extracted in Phase 1 be SA ⊆Σ ∗ and SB ⊆Σ ∗, where Σ ∗denotes the countable universe of canonical subtree encodings (either structure–only or struc- ture–with–value). Define the joint support U = SA ∪SB = \b u1, . . . , um , m = |U|. (2) Each ui ∈U is a distinct subtree symbol. Multiplicity functions.: For u ∈U define cA(u) = # \b s ∈SA : s = u , cB(u) = # \b s ∈SB : s = u , (3) i.e. the number of occurrences of u in the respective multiset. Let nA = X u∈U cA(u), nB = X u∈U cB(u) (4) be the total numbers of (depth-bounded) subtrees harvested from TA and TB, respectively. Empirical probability distributions.: We define the fre- quency distributions P, Q : U →[0, 1] by P(u) = cA(u) nA , Q(u) = max \u0010cB(u) nB , ε \u0011 , X u∈U P(u) = X u∈U Q(u) = 1, (5) where 0 < ε ≪1 is a fixed smoothing constant ensuring Q(u) > 0 for every u ∈U. (The summation constraint on Q can be restored by a final renormalisation, but in practice ε≪1/nB suffices and the effect on P u Q(u) is negligible.)"}
{"doc_id": "2508.14288v1", "title": "Measuring LLM Code Generation Stability via Structural Entropy", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14288v1", "chunk_id": 3, "text": "X u∈U Q(u) = 1, (5) where 0 < ε ≪1 is a fixed smoothing constant ensuring Q(u) > 0 for every u ∈U. (The summation constraint on Q can be restored by a final renormalisation, but in practice ε≪1/nB suffices and the effect on P u Q(u) is negligible.) In vector notation let p = \u0000P(u1), . . . , P(um) \u0001 T, q = \u0000Q(u1), . . . , Q(um) \u0001 T ∈[0, 1]m (6) so ∥p∥1 = ∥q∥1 = 1. These two probability vectors fully characterize the structural ’vocabularies’ of AST TA and TB and constitute the input for the similarity metrics of Phase 3. Remark. The inclusion of a smoothing parameter ε is only required for the directed cross-entropy in Phase 3 A to avoid undefined log Q(u) when Q(u) = 0. The symmetric Jensen–Shannon divergence of Phase 3 B remains finite without explicit smoothing as long as P and Q are defined on the common support U. Phase 3 : Entropy–Based Similarity Metrics: With P and Q defined in (5), we present two similarity measures. (A) Structural Cross-Entropy (SCE): Define the cross- entropy of P relative to Q and the Shannon entropy of Q as H(P, Q) = − X u∈U P(u) log Q(u), H(Q) = − X u∈U Q(u) log Q(u). (7) 2 We normalise by taking the ratio SCE(P, Q) = H(Q) H(P, Q), 0 < SCE ≤1. SCE(P, Q) = 1 ⇐ ⇒P = Q; SCE →0 as Q fails to explain P. (8) Because the numerator and denominator share the same logarithmic base, the score is scale–independent and monotone in the KL divergence DKL(P ∥Q) = H(P, Q) −H(P). In code, compute H(Q) and H(P, Q) directly with base-2 logs; skip terms with P(u) = 0 and apply ε smoothing on Q(u). This ratio acts as a normalized structural cross-entropy. (B) Jensen–Shannon Divergence (JSD): Let M = 1 2(P + Q) denote the midpoint distribution. The Jensen–Shannon divergence is DJS(P ∥Q) = 1 2DKL(P ∥M) + 1 2DKL(Q ∥M) = H(M) −1 2 \u0002 H(P) + H(Q) \u0003 . (9) With logarithm base 2, DJS ∈[0, 1] bits and √DJS is a metric [17]. We use SJSD(P, Q) = 1 −DJS(P ∥Q) ∈[0, 1], (10) where larger values again indicate greater similarity. Unlike SCE, JSD is symmetric and finite without smoothing so long as P, Q share support. a) Computational cost: Both scores require O(|U|) arith- metic once P and Q are formed. When depth d is fixed, |U| grows at most linearly in the number of AST nodes, the overall pipeline O(n d) for subtree extraction plus O(|U|) for scoring. b) Summary: SCE (Equation 8) offers a directed mea- sure—“how efficiently does distribution Q encode P?”—while SJS (Equation 10) gives a symmetric, bounded diver- gence–to–similarity conversion. Either can be deployed de- pending on whether directionality or symmetry is desired; both map naturally into the [0, 1] range, facilitating thresholding and comparative analysis of program structures. III. EXPERIMENTS AND RESULTS To evaluate the effectiveness and interpretability of both entropy-based stability metrics we proposed,"}
{"doc_id": "2508.14288v1", "title": "Measuring LLM Code Generation Stability via Structural Entropy", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14288v1", "chunk_id": 4, "text": "bounded diver- gence–to–similarity conversion. Either can be deployed de- pending on whether directionality or symmetry is desired; both map naturally into the [0, 1] range, facilitating thresholding and comparative analysis of program structures. III. EXPERIMENTS AND RESULTS To evaluate the effectiveness and interpretability of both entropy-based stability metrics we proposed, (1) Jensen–Shannon Divergence (JSD) and (2) Structural Cross Entropy (SCE), we conducted experiments focusing on the stability of LLM-generated code. Specifically, we provided each prompt to the LLM five times independently, generating five distinct outputs per prompt. We then measured stability by computing similarity or divergence scores pairwise among these five outputs, using standard evaluation metrics (BLEU, CodeBLEU, TSED) and our proposed entropy-based metrics (JSD and SCE). The final stability score for each metric was obtained by averaging all pairwise comparisons. For comparison, we also included pass@k, a metric eval- uating functional correctness, computed by executing each generated code output against provided test cases. Unlike other metrics, pass@k directly reflects correctness rather than stability but serves as an important baseline for understanding Text to SQL Python Code Generation Fig. 1. Heatmap of Pearson Correlation Coefficient between Stability Evaluation Metrics, JSD: Jensen-Shannon Divergence; SCE: Structural Cross Entropy, Structural means value in subtree is ignored. practical implications of structural stability. Additionally, we analyzed structural-only variants – JSD (structural) and SCE (structural) – (cf. Section II.1(a)) by abstracting away token values, thus isolating structural differences to examine their effect on stability. A. Experimental Setup We performed our evaluations using two widely recognized benchmarks across two programming languages: • Python (BigCodeBench): A dataset widely adopted for general code generation tasks [18]. • SQL (Spider): A comprehensive benchmark for SQL query generation [19]. We evaluated three representative models: LLaMA 3.1 (8B, instruct), Qwen-2.5(7B, instruct), and Qwen-2.5-Coder(7B). Our analysis focuses on understanding how our entropy-based measures reflect code stability and their correlations with existing metrics. B. Quantitative Results The results are summarized in Table II. For the Python tasks, while the Qwen-2.5 models exhibit stronger stability in lexical metrics such as BLEU and CodeBLEU compared to LLaMA 3.1, a notable discrepancy between their pass@1 and pass@5 indicates considerable variability. Specifically, Qwen-2.5-Coder achieves pass@1 of 0.373 and pass@5 of 0.517, suggesting 3 TABLE II COMPARISON OF CURRENT STABILITY METRICS. JSD: JENSEN-SHANNON DIVERGENCE; SCE: STRUCTURAL CROSS ENTROPY; STRUCTURAL MEANS ONLY COMPARES THE SUBTREE STRUCTURE. Model Language(Task) Avg. BLEU Code BLEU Pass@1 Pass@5 TSED SCE(structural) SCE JSD(structural) JSD LLaMA-3.1 8B it Python BigCodeBench 0.428 0.669 0.289 0.481 0.785 0.798 0.656 0.940 0.898 Qwen-2.5 7B it 0.596 0.716 0.339 0.482 0.765 0.823 0.726 0.947 0.913 Qwen-2.5-Coder 7B 0.614 0.715 0.373 0.517 0.764 0.823 0.722 0.951 0.918 LLaMA-3.1 8B it SQL Spider 0.495 N/A 0.673 0.824 0.832 0.729 0.669 0.934 0.905 Qwen-2.5 7B it 0.770 N/A 0.716 0.790 0.921 0.886 0.850 0.971 0.958 Qwen-2.5-Coder 7B 0.664 N/A 0.787 0.860 0.927 0.822 0.781 0.962 0.946 that multiple sampled outputs differ substantially in correctness, reflecting inherent instability. For SQL (Spider), this instability is also evident. Although Qwen models achieve higher overall correctness and stability, they demonstrate substantial differences between pass@1 and pass@5 (e.g., Qwen-2.5-Coder: pass@1"}
{"doc_id": "2508.14288v1", "title": "Measuring LLM Code Generation Stability via Structural Entropy", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14288v1", "chunk_id": 5, "text": "Qwen-2.5-Coder 7B 0.664 N/A 0.787 0.860 0.927 0.822 0.781 0.962 0.946 that multiple sampled outputs differ substantially in correctness, reflecting inherent instability. For SQL (Spider), this instability is also evident. Although Qwen models achieve higher overall correctness and stability, they demonstrate substantial differences between pass@1 and pass@5 (e.g., Qwen-2.5-Coder: pass@1 = 0.787, pass@5 = 0.860), further highlighting output variability. Our proposed entropy-based stability metrics (JSD and SCE) complement these findings. JSD values remain consistently high (above 0.9), suggesting that structurally, outputs are broadly similar across samplings. However, SCE scores (especially when considering token values explicitly) are consistently lower, indicating sensitivity to subtle variations overlooked by purely structural metrics. This contrast highlights the utility of SCE in detecting fine-grained token-level variability. Moreover, the structural-only variants of both metrics gener- ally produce higher scores, as expected, reflecting their insensi- tivity to identifier or literal changes. Thus, these structural-only scores represent a baseline stability that isolates deeper syntactic patterns from superficial token-level differences. Overall, our metrics successfully quantify and clarify the nature of stability in generated code, complementing traditional correctness-based metrics like pass@k, which indirectly reflect variability through the disparity between single- and multi- sample correctness evaluations. C. Correlation Analysis Figure 1 illustrates the Pearson correlation coefficient across stability metrics: a) Python (BigCodeBench): We observe high correlations between JSD and JSD(structural) (0.94), indicating structural abstraction maintains strong stability signals. The correlation between SCE and JSD metrics is moderately strong (0.85-0.87), though lower than the near-perfect correlation between JSD and its structural variant (0.94). TSED shows good correlation with JSD (0.85–0.86), reinforcing its utility for capturing structural differences. Notably, SCE structural variants correlate less strongly with TSED (0.74), highlighting differences in sensitivity to structural variations. b) SQL (Spider): On SQL tasks, the correlations between JSD and JSD(structural) are even stronger (0.98), underscoring the robustness of structural abstraction in capturing stability. Interestingly, BLEU’s correlation is lower with JSD metrics (around 0.51), emphasizing the limitation of token-based metrics in SQL’s structural context. CodeBLEU is not valid for SQL, hence excluded. TSED correlates strongly with JSD (0.94), confirming their structural similarity evaluation alignment. The SCE metrics correlate moderately (0.72–0.88) with others, again highlighting their distinctive sensitivity to token-level discrepancies. IV. THREATS TO VALIDITY Several threats may impact the validity of our findings. Firstly, our experiments were conducted on specific benchmarks (Python BigCodeBench and SQL Spider), potentially limiting generalizability across other languages and tasks. Second, the chosen depth parameter in subtree extraction might influence stability measures; different values could yield varied results. Additionally, while our entropy-based metrics effectively capture syntactic variability, they currently do not explicitly address semantic equivalences or execution behaviors. Lastly, the limited number of models evaluated and the constrained dataset size may affect the robustness and external validity of our conclusions. Future work will address these limitations by expanding evaluations across diverse programming languages, incorporating broader semantic and behavioral analyses, and employing larger-scale benchmarks. V. CONCLUSION In this paper, we introduced entropy-based metrics—Jensen- Shannon divergence (JSD) and structural cross-entropy (SCE)—to quantify the stability of LLM-generated code using Abstract Syntax Trees (ASTs). Our experiments demonstrate that these metrics:"}
{"doc_id": "2508.14288v1", "title": "Measuring LLM Code Generation Stability via Structural Entropy", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14288v1", "chunk_id": 6, "text": "by expanding evaluations across diverse programming languages, incorporating broader semantic and behavioral analyses, and employing larger-scale benchmarks. V. CONCLUSION In this paper, we introduced entropy-based metrics—Jensen- Shannon divergence (JSD) and structural cross-entropy (SCE)—to quantify the stability of LLM-generated code using Abstract Syntax Trees (ASTs). Our experiments demonstrate that these metrics: 1) Provide insights beyond functional correctness (pass@k) and lexical-syntactic metrics (BLEU, CodeBLEU). 2) Correlate strongly with AST-based structural metrics (e.g., TSED), effectively capturing structural stability. 3) Offer significant advantages with structural-only abstrac- tion by reducing token-value noise. Distinguishing structural-only from full-subtree encodings highlights deep syntactic stability versus token-specific varia- tions. These metrics are lightweight, language-agnostic, and generalizable, complementing existing stability assessments. Future work will incorporate semantic relationships, such as data- and control-flow dependencies, to extend analysis to behavioral stability. We also plan adaptive subtree-weighting schemes and benchmarking on larger, multi-module codebases to study long-range structural variability. ACKNOWLEDGEMENT The FNR funded this research under grants NCER22/IS/16570468/NCERFT. 4"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 0, "text": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs Adrian-Marius Dumitran* Alexandra-Mihaela Danila* Angela-Liliana Dumitran University of Bucharest Faculty of Mathematics and Computer Science Abstract LLMs (Large language models) have revolu- tionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admis- sions). GRILE enables us to probe two com- plementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) produc- ing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or peda- gogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings ex- pose open challenges for trustworthy educa- tional NLP in low-resource settings and estab- lish GRILE as a new test-bed for controllable explanation generation and evaluation. 1 Introduction Recent advancements in NLP, particularly through LLMs, have unlocked new opportuni- ties for intelligent computer-assisted language learning. Although extensive benchmarks exist for English and other high-resource languages (Hendrycks et al., 2021; OpenAI, 2024), their efficacy in low-resource contexts remains under- explored—especially for tasks that demand fine-grained grammatical reasoning. Romanian, spoken by roughly 24 million people world- wide, illustrates this gap: it is comparatively underserved by both annotated corpora and NLP *Equal contribution. research. (P˘ ai¸ s and Tufi¸ s, 2023) mention that the number of identified NLP tools for Romanian represent only 15% of the tools available for English. This paper bridges that gap through an inter- disciplinary collaboration between linguists and computer scientists. We introduce GRILE, a new benchmark of 1,151 single-answer multiple- choice questions (MCQs) extracted from high- stakes Romanian examinations, including the Na- tional Evaluation, the Baccalaureate, and several university entrance tests (e.g., Law School, Police Academy). Our investigation pursues two principal goals. First, we quantitatively assess how accurately state-of-the-art multilingual and Romanian- specific LLMs answer the questions in our dataset. Second, we examine their pedagogical potential by requiring each model to justify its choice with a concise grammatical explanation. The evaluation covers commercial systems such as Gemini 2.5 Pro Experimental and open-weight models such as DeepSeek V3-0324. Initial results reveal substantial variation, with answer accuracy spanning from 38% to 85%, and chain-of-thought prompting generally yield- ing higher scores. Beyond answer prediction, we instruct Gemini 2.5 Pro Experimental to cat- egorise every question as Lexical, Morpholog- ical, Syntactic, or Phonetic. Manual verifica- tion confirms more than 99% agreement, offer- ing an additional lens for analysing error patterns. A qualitative evaluation of the generated expla- nations—conducted by a specialist in Romanian grammar—highlights persistent issues in termino- logical precision and normative compliance, un- derscoring the challenges of deploying LLMs as trustworthy tutors for low-resource languages. Our main contributions are: • GRILE, the first public Romanian grammar benchmark paired"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 1, "text": "patterns. A qualitative evaluation of the generated expla- nations—conducted by a specialist in Romanian grammar—highlights persistent issues in termino- logical precision and normative compliance, un- derscoring the challenges of deploying LLMs as trustworthy tutors for low-resource languages. Our main contributions are: • GRILE, the first public Romanian grammar benchmark paired with expert-validated ex- planations; • a comprehensive evaluation of multilingual and Romanian-specific LLMs on both an- swering and explaining high-stakes grammar questions; • a linguist-driven qualitative study that un- covers systematic weaknesses and normative misalignments; • an open-access dataset and interactive web application, released under a permissive li- cence on Zenodo1, to catalyse future research and educational use. These resources and findings advance the study of educational NLP in low-resource settings and lay the groundwork for more reliable, Romanian- focused grammatical tutoring and assessment tools. 2 Related Work Benchmarking LLMs on Multiple-Choice Grammar Tasks. Recent studies have shown that large language models (LLMs) can achieve impressive results on standardized multiple-choice tests across various domains (Hendrycks et al., 2021; OpenAI, 2024). However, fine-grained grammatical assessments remain challenging for LLMs. For example, multilingual benchmarks focusing on grammar, such as the TELEIA (Mayor-Rocher et al., 2024b) dataset for Span- ish, demonstrate that LLMs struggle to reach native-like accuracy in nuanced grammar tasks (Mayor-Rocher et al., 2024a). This motivates specialized evaluations of grammatical knowl- edge, especially for lower-resource languages like Romanian. Multilingual vs. Language-Specific LLMs. Most large language models are predominantly trained on English data, which results in signifi- cantly better performance for English compared to other languages (Hu et al., 2020). Consequently, this has encouraged the development of monolin- gual or language-specific NLP models that often outperform multilingual counterparts on local lin- guistic tasks. For instance, Masala et al. (2020) introduced RoBERT, a Romanian-specific BERT- based model, demonstrating superior results over 1https://zenodo.org/records/15504323 multilingual BERT across various Romanian NLP tasks such as sentiment analysis, dialect identifica- tion, and diacritic restoration. Similar outcomes have been consistently re- ported for other languages, including French, Polish, and Japanese, where monolingual trans- former models regularly surpass general multi- lingual baselines in language-specific evaluations (Martin et al., 2020; Kuratov and Arkhipov, 2019; Conneau et al., 2020). More recently, the NLP community has seen efforts toward developing large-scale LLMs tailored specifically for individ- ual languages, such as Finnish GPT-style models (Luukkonen et al., 2023) and Chinese LLaMA2- based models (Cui et al., 2024), achieving state- of-the-art performance on localized benchmarks. In the Romanian context, Masala et al. (2024) released RoLLaMA2, the first open-source Romanian-specific large language model based on LLaMA-2, trained on curated Romanian cor- pora and fine-tuned through instruction tuning. RoLLaMA2 achieved leading performance across multiple Romanian NLP benchmarks, underscor- ing the effectiveness of language-specific train- ing. Our study contributes to this growing body of literature by directly comparing multilingual models against Romanian-specific models like RoLLaMA2, focusing particularly on grammati- cal tasks to evaluate the effectiveness and viability of smaller, targeted language models against more extensive multilingual counterparts. LLM Explanation Generation and Educational Feedback. Generating explanations for educa- tional purposes extends beyond correctness, en- compassing the quality and pedagogical sound- ness"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 2, "text": "against Romanian-specific models like RoLLaMA2, focusing particularly on grammati- cal tasks to evaluate the effectiveness and viability of smaller, targeted language models against more extensive multilingual counterparts. LLM Explanation Generation and Educational Feedback. Generating explanations for educa- tional purposes extends beyond correctness, en- compassing the quality and pedagogical sound- ness of explanations. Prior works in educational NLP formalized feedback comment generation tasks, highlighting the need for meaningful auto- mated feedback (Nagata, 2019). Recently, gen- erative LLMs like GPT-4 demonstrated poten- tial in automatically generating high-quality ex- planations in science education, closely matching human-generated feedback (López Cortez et al., 2024). Human-in-the-loop validation remains cru- cial to ensure explanation accuracy and educa- tional value. Our work builds upon these directions, bench- marking Romanian-specific and general LLMs against a Romanian grammar multiple-choice dataset, examining common errors, and assess- ing explanation quality through expert validation. Figure 1: Dataset Sources This contributes toward understanding LLM capa- bilities and limitations within educational contexts for low-resource languages. 3 Dataset and Methodology In this section we explain how we collect the data and the models we decided to benchmark in our test. 3.1 Data Collection and Structure Figure 1 charts the distribution of our 1 151 single- answer multiple-choice questions (MCQs) were harvested from publicly available high-stakes Romanian examinations—the National Evalu- ation, Baccalaureate, and university entrance tests for Law School and the Police Academy (2010–2024). Source PDFs and scans were con- verted with Tesseract OCR, then parsed by rule- based scripts and manually spot-checked. Each record is stored as JSON with fields question, options, answer, source, and year; under- lined focus words are preserved via surrounding underscores. Full OCR pipeline details and an ex- ample entry appear in Appendix A.2 2https://zenodo.org/records/15504323 3.2 LLM Selection Our model selection aimed to evaluate both a state-of-the-art model known for strong reasoning capabilities and several other prominent models readily accessible for research purposes. Specifi- cally, we included Gemini 2.5 Pro Experimental (Team et al., 2024) as it represented the leading edge of publicly available models at the time of our experiments. The other benchmarked models, including variants of DeepSeek, Mistral, Llama 3.3, and Qwen, were chosen primarily for their strong performance in various benchmarks and their convenient accessibility via the Together AI API3, which facilitated systematic evaluation. • DeepSeek V3-0324 (DeepSeek-AI et al., 2025) • Mistral Small 24B • Llama 3.3 70B Instruct (Grattafiori et al., 2024) • Qwen 2.5 Coder 32B (Yang et al., 2025) • Gemini 2.5 Pro Experimental, Gemini 2.0 Flash (Team et al., 2024) Furthermore, our few-shot prompting experi- ments (Section 4.2) involved evaluating multi- ple fine-tuned Romanian variants (e.g., RoL- lama, RoMistral, RoGemma based on OpenLLM- Ro (Masala et al., 2024)) alongside their respective base models to analyze the interaction between fine-tuning and in-context learning. The full list of models and variants tested in the few-shot set- ting is provided in Appendix B. 3.3 Prompting Strategies We evaluated LLM performance using two pri- mary prompting approaches applied to each MCQ in the dataset: 3.3.1 Direct Questioning (Zero-Shot) This baseline approach involved providing the model directly with the question text"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 3, "text": "models and variants tested in the few-shot set- ting is provided in Appendix B. 3.3 Prompting Strategies We evaluated LLM performance using two pri- mary prompting approaches applied to each MCQ in the dataset: 3.3.1 Direct Questioning (Zero-Shot) This baseline approach involved providing the model directly with the question text and multiple- choice options, instructing it to select the correct answer. The prompt template was: Question: [Question text here] Choices: (A) [Choice A text] (B) [Choice B text] (C) [Choice C text] 3https://www.together.ai/ (D) [Choice D text, if applicable] Select the correct answer. Ensure that the final answer is only the letter of the correct option, without any additional text or symbols. The final instruction was added to minimize pars- ing errors during evaluation. 3.3.2 Chain-of-Thought (CoT) Prompting To encourage step-by-step reasoning and generate explanations, we employed a CoT approach. The core CoT instruction added was: Let’s think step by step. Provide your reasoning in several steps, and then output the final answer on a new line starting with \"Final Answer:\". Ensure that the final answer is only the letter of the correct option, without any additional text or symbols. This strategy aimed to improve answer accuracy by forcing a reasoning process and simultaneously generating explanatory text for qualitative analy- sis. 3.3.3 Few-Shot Prompting To assess the impact of in-context examples, we also evaluated models using few-shot prompting. This involved prepending the Direct Questioning or CoT prompt (as applicable) with 1, 3, or 5 randomly selected question-answer pairs from the dataset (excluding the question being tested) for- matted similarly to the main task. 3.4 Explanation Generation and Validation Setup To facilitate efficient linguistic expert validation of both the answers and reasoning of the best- performing model identified in preliminary tests (Gemini 2.5 Pro Experimental), we used a simple, direct prompt asking it to provide its chosen an- swer, the correct answer (based on its knowledge), its reasoning, and a category classification. This generated a structured output specifically format- ted for review, exemplified below: Question 10: Din campul lexical al cuvantului \" electorat\" fac parte: Options: a) electron, neutron, proton b) electric, energetic, curent c) senator, primar, magistrat d) alegeri, vot, candidat Gemini’s Answer: d Correct Answer: d Explanation: Campul lexical include cuvinte inrudite ca sens. \"Electorat\" se refera la totalitatea alegatorilor, fiind direct legat de \"alegeri\", \"vot\", \"candidat\". Category: Lexical Listing 1: Example structured output from Gemini 2.5 Pro for validation. This structured format, including the category classification (Lexical, Morphological, Syntactic, Phonetic) assigned by Gemini 2.5 Pro Experimen- tal, was used by the linguist expert to validate the model’s answer, reasoning quality, and classifica- tion accuracy (which exceeded 99% for the cate- gorization task itself). 3.5 Evaluation Metrics Performance was primarily measured by answer accuracy. Additionally, a qualitative linguistic analysis was conducted on the explanations gen- erated via CoT (facilitated by the structured output from Gemini 2.5 Pro for validation) to assess their correctness, precision, and pedagogical suitability. 4 Quantitative Results This section presents quantitative findings from our benchmarking evaluations. We analyze the impact of different prompting strategies (Chain- of-Thought vs. Direct,"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 4, "text": "on the explanations gen- erated via CoT (facilitated by the structured output from Gemini 2.5 Pro for validation) to assess their correctness, precision, and pedagogical suitability. 4 Quantitative Results This section presents quantitative findings from our benchmarking evaluations. We analyze the impact of different prompting strategies (Chain- of-Thought vs. Direct, Few-Shot vs. Zero- Shot) and compare the performance of Romanian- specific fine-tuned models against their multilin- gual base versions. 4.1 Impact of Chain-of-Thought Prompting We first compared model accuracy using direct zero-shot questioning versus Chain-of-Thought (CoT) prompting, which instructs the model to provide reasoning steps. Table 1 summarizes these results. Model Accuracy without CoT Accuracy with CoT DeepSeek V3.0324 53.26% 64.62% Mistral small 24B 39.10% 38.31% Llama 3.3 70B 49.70% 55.60% Qwen 2.5 Coder 32B 48.22% 51.61% Gemini 2.5 Pro — 82.8% Table 1: Comparison of model accuracy (%) with and without Chain-of-Thought (CoT) prompting. Top score in each column is bolded. Gemini 2.5 Pro was evalu- ated only with CoT. CoT prompting generally led to notable accu- racy improvements, particularly for DeepSeek V3 (+11.36 pp) and Llama 3.3 70B (+5.9 pp), confirming its benefit for reasoning on this task. An exception was Mistral Small 24B, which saw a slight decrease, possibly due to diffi- culties following complex instructions. Gemini 2.5 Pro, tested only with CoT, achieved the highest score (82.8%), significantly outperform- ing others. However, accuracies for most models (38-65% range with CoT) remain substantially be- low proficient human levels (>80%) on compara- ble exams4, highlighting a remaining gap in nu- anced Romanian grammar understanding for most LLMs. 4.2 Impact of Few-Shot Prompting We also investigated whether providing in-context examples (1, 3, or 5 shots) could improve perfor- mance compared to zero-shot prompting. The re- sults, detailed fully in Appendix B , show mixed and generally modest benefits. While accuracy often increased slightly, gains were typically small (2-5 pp) and inconsistent across models and shot counts. Table 2 illustrates this variability for the Llama 3.1 8B family. Llama 3.1 8B Variant FS=0 FS=1 FS=3 FS=5 Base Instruct 35.62 38.84 40.75 39.70 Ro-Instruct (25-04) 36.06 37.71 40.49 39.36 Ro-Instruct-DPO (25-04) 38.49 37.53 39.53 39.27 Table 2: Illustrative accuracy (%) for Llama 3.1 8B variants with Few-Shot (FS) prompting. Best few-shot score per model in bold. Full results in Appendix B . Few-shot prompting did not consistently fa- vor base models over fine-tuned ones, nor did a clear optimal number of shots emerge. Crucially, even with examples, peak accuracies for the mod- els tested in this setting (detailed in Appendix B) remained significantly below top performers like Gemini 2.5 Pro and human levels. This sug- gests limited in-context examples are insufficient to overcome the core challenges of this task for these models. 4.3 Performance of Romanian Fine-tuned Models vs. Base Models A key goal was to assess the impact of Romanian- specific fine-tuning. We compared the perfor- mance of various RoLLM variants (developed by 4Example results: https://cdn.edupedu. ro/wp-content/uploads/2023/01/lista_ anonimizata_rezultate_finale_LC.pdf Masala et al. (2024)) against their respective base multilingual models using data from our few-shot experiments (details in Appendix B). The results indicate that"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 5, "text": "goal was to assess the impact of Romanian- specific fine-tuning. We compared the perfor- mance of various RoLLM variants (developed by 4Example results: https://cdn.edupedu. ro/wp-content/uploads/2023/01/lista_ anonimizata_rezultate_finale_LC.pdf Masala et al. (2024)) against their respective base multilingual models using data from our few-shot experiments (details in Appendix B). The results indicate that Romanian-specific fine-tuning does not consistently yield signifi- cant improvements on this specific grammatical MCQ task, especially compared to strong base models. • For the Llama family (Llama-2, Llama- 3, Llama-3.1), the RoLLM variants gener- ally performed similarly to or sometimes slightly worse than their corresponding base Instruct models in zero-shot settings. Few- shot prompting occasionally led to RoLLM variants slightly outperforming their base, but the difference was often marginal. DPO fine- tuning on RoLLM variants did not produce consistent gains over standard RoLLM in- struction tuning for this task. • For Mistral-7B, the RoMistral variants showed slightly better average performance than the base Mistral-7B Instruct v0.3, but the difference was small (within 1 pp on average across FS settings). • For Gemma models, the results were particularly striking. The base google/gemma-2-9b-it significantly outperformed all its RoGemma2-9B fine- tuned variants (including DPO versions) across all few-shot settings, achieving an average accuracy of 47.13% compared to averages around 39-42% for the RoGemma2 variants. A similar, though less pronounced, trend was observed for Gemma-1.1-7b. These findings suggest that for complex gram- matical reasoning tasks like those in our dataset, the benefits of general multilingual pre-training in strong base models (like Llama 3.1 Instruct or Gemma-2-9B-it) might outweigh the current Romanian-specific fine-tuning approaches applied in the RoLLM project, at least for zero-shot and few-shot settings. The fine-tuning might be more beneficial for other NLP tasks or require different strategies (e.g., incorporating grammatical knowl- edge more explicitly) to show significant gains on this benchmark. The relatively low overall scores for both base and fine-tuned models (excluding Gemini) further emphasize the difficulty of this task. 5 Qualitative Analysis: Explanations, Categorization, and Dataset Challenges Beyond quantitative accuracy, understanding LLM capabilities for educational use requires qualitative assessment. This section presents find- ings from an in-depth analysis focusing primarily on explanations and category classifications gen- erated by Gemini 2.5 Pro Experimental (using CoT prompting). The analysis, conducted by a linguistic expert, evaluated 200 explanations in detail and reviewed LLM category assignments across the full dataset (1151 items). This revealed insights into LLM reasoning, limitations in han- dling linguistic nuances, and characteristics of the benchmark dataset itself. The involvement of a linguistic expert was es- sential to the present study due to the complex and nuanced nature of the materials under anal- ysis. The apparent uniformity of item structure did not preclude the presence of subtle or highly context-dependent linguistic phenomena. Certain errors identified in the dataset were marked by fine semantic or syntactic distinctions that would likely elude detection without expert philological judg- ment, such as putting the error into the correct lan- guage category (see questions 51, 77, 211, 228, etc. where the lexical errors are considered mor- phological and the opposite) or presenting the ex- planation in an inadequate"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 6, "text": "or syntactic distinctions that would likely elude detection without expert philological judg- ment, such as putting the error into the correct lan- guage category (see questions 51, 77, 211, 228, etc. where the lexical errors are considered mor- phological and the opposite) or presenting the ex- planation in an inadequate style or erroneously. 5.1 Quality of LLM-Generated Explanations While CoT prompting improved answer accuracy (Table 1), the quality of the accompanying expla- nations varied significantly. A detailed review of 200 explanations revealed that 48% (96 explana- tions) were problematic (incomplete, incorrect, misleading, or containing flaws). Key issues in- cluded: • Incorrect Justifications (67/96): Explana- tions frequently provided factually incorrect grammatical reasoning, even when the se- lected MCQ answer was correct. • Imprecision and Irrelevance (29/96): Ex- planations often suffered from redundancy, imprecise terminology, irrelevant points, or incoherent concluding statements that under- mined the reasoning (e.g., the concluding phrase for Question 26 regarding pronoun agreement). This high error rate in explanations limits the re- liability of current LLMs as standalone grammar tutors without expert validation. 5.2 LLM Performance on Linguistic Categorization We also evaluated Gemini 2.5 Pro Experimental’s ability to classify each of the 1151 questions into four linguistic domains: Lexical, Morphological, Syntactic, or Phonetic. While the overall accuracy on this task was very high (over 99% agreement with expert classification), the five instances of misclassification are revealing: • Morphology vs. Lexicon: Question 51 (ab- breviation of dumneavoastr˘ a) was misclas- sified as \"Lexical\" instead of \"Morphologi- cal\". Question 77 (use of optim creating a pleonasm) was misclassified as \"Morpholog- ical\" instead of \"Lexical-Semantic\". • Syntax vs. Morphology: Question 211 (mor- phological value of pronoun ce) and Question 228 (identifying an interjection) were mis- classified as \"Syntactic\". Question 252 (fo- cusing on the preposition dup˘ a) was also mis- classified as \"Syntactic\" instead of \"Morpho- logical\". These errors typically occurred in borderline cases involving multiple linguistic layers (form, func- tion, meaning), suggesting LLMs may struggle to pinpoint the primary linguistic concept being tested beyond surface cues. 5.3 Dataset Characteristics and Normative Alignment The qualitative analysis also highlighted charac- teristics and potential issues within the source ma- terials: • Unclear/Flawed Prompts: Seven questions out of the 200 deeply analyzed contained am- biguities or errors affecting comprehensibil- ity and reliable evaluation. • Incorrect/Incomplete Official Answers: Four analyzed items had designated correct answers conflicting with current standards. • DOOM 3 Misalignment: 5This was a no- table issue. Analysis of the 19 DOOM- related questions in the dataset revealed: – LLM Errors: In six explanations for DOOM-related questions, Gemini 2.5 Pro provided false or incorrect informa- tion regarding DOOM 3 (Institutul de Lingvistic˘ a “Iorgu Iordan – Alexandru Rosetti”, 2021) norms. Examples in- clude incorrectly stating DOOM 3 \"ac- cepts\" both forms for cofeina/cafeina (Q626) or biscuit/biscuite (Q1128), mis- stating the preferred form for baroc˘ a (Q733), inaccurately describing the abbreviation rule for dumneavoastr˘ a (Q51), or misrepresenting the recom- mendation for cuvânt-înainte (Q896). – Outdated Source Questions: Some source questions themselves appeared outdated relative to DOOM 3 (2021 edi- tion). For example, Question 144 (plural of"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 7, "text": "(Q1128), mis- stating the preferred form for baroc˘ a (Q733), inaccurately describing the abbreviation rule for dumneavoastr˘ a (Q51), or misrepresenting the recom- mendation for cuvânt-înainte (Q896). – Outdated Source Questions: Some source questions themselves appeared outdated relative to DOOM 3 (2021 edi- tion). For example, Question 144 (plural of proroc) lacked the standard DOOM 3 form among its options, suggesting the question predates the latest norma- tive update. These findings underscore the need for careful curation and normative alignment of benchmark datasets, especially when evaluating performance on rule-based linguistic tasks. 5.4 Implications The qualitative results emphasize the need for evaluation metrics beyond accuracy, particularly when assessing LLMs for educational roles re- quiring explanations. The inconsistencies found highlight the importance of dataset quality con- trol and alignment with current linguistic stan- dards (like DOOM 3) for reliable benchmarking in high-stakes domains like language assessment. 5DOOM3, published in 2021, serves as the current stan- dard reference for Romanian spelling (ortografic), pronun- ciation (ortoepic), and morphology (morfologic). While the core orthographic, ortoepic, and morphological norms largely remain unchanged, DOOM3 introduces over 3,000 discrete normative updates. These include the addition or removal of definite/article forms, select flexional variants, and ad- justments to accentuation in compound words. It intention- ally curtails the proliferation of orthographic variants, helping users to clearly identify the recommended form. It represents the official norms, reason why explanations such as “DOOM 3 prefers” or “recommends” from questions 733 or 896 are not adequate. 6 Application In order to explore the applicability of the cre- ated dataset and the generated explanations, we developed a publicly accessible web application designed as a learning tool for Romanian language proficiency. Figure 2: User interface of the web application, allow- ing practice with dataset questions. The application utilizes the benchmark dataset as its question bank. Users can navigate ques- tions filtered by source (allowing targeted prac- tice based on exam type/difficulty proxy) and sub- mit their answers within a timed or practice mode. Upon submission, the application reveals the cor- rect answer and, importantly, displays the corre- sponding LLM-generated grammatical expla- nation (specifically, those generated by Gemini 2.5 Pro and validated for quality by our linguistic expert, as discussed in Section 5). Figure 3: User interface of the web application, dis- playing explanations for the correct answers. This feature allows learners to not only test their knowledge but also receive immediate feedback and reasoning, leveraging the LLM’s explanatory capabilities in a practical educational context. Al- though explanation quality still requires ongoing validation, providing expert-vetted explanations offers significantly more pedagogical value than simple answer keys. The application is imple- mented in React and hosted statically, ensuring easy access and deployment. 7 Conclusion and Future Work This study evaluated state-of-the-art LLMs on a novel dataset of Romanian language MCQs sourced from high-stakes exams, focusing on an- swer accuracy and explanation quality. Our bench- marking, utilizing direct and Chain-of-Thought (CoT) prompting, revealed significant perfor- mance variations and a considerable gap between most LLMs and proficient human performance, although Gemini 2.5 Pro Experimental achieved high accuracy ( 83%) using CoT. Qualitative"}
{"doc_id": "2508.14279v1", "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14279v1", "chunk_id": 8, "text": "sourced from high-stakes exams, focusing on an- swer accuracy and explanation quality. Our bench- marking, utilizing direct and Chain-of-Thought (CoT) prompting, revealed significant perfor- mance variations and a considerable gap between most LLMs and proficient human performance, although Gemini 2.5 Pro Experimental achieved high accuracy ( 83%) using CoT. Qualitative analysis of Gemini 2.5 Pro’s expla- nations, while revealing flaws in nearly half, also found that the majority were linguistically valu- able and useful as a starting point for pedagogical feedback, confirmed by expert assessment. This potential was demonstrated through their integra- tion into our publicly available educational web application, providing learners with practice op- portunities augmented by AI-generated, expert- validated grammatical reasoning. However, the analysis also highlighted challenges related to dataset quality and normative alignment (e.g., with DOOM 3). Our findings underscore the need for both quan- titative and qualitative metrics when evaluating LLMs for educational roles, particularly concern- ing explanation generation in low-resource lan- guages. While current LLMs show promise as assistive tools, significant work remains to ensure consistent accuracy and pedagogical soundness. Future work should proceed along several key directions. Firstly, expanding the bench- mark scope is essential: incorporating more ques- tions, annotations (like difficulty levels or specific grammar subsections), evaluating a broader range of SOTA models (including newer Romanian- specific ones), and performing detailed cross- lingual analyses. Secondly, significant potential lies in enhancing model performance and expla- nation quality through advanced techniques. This includes exploring Retrieval-Augmented Gener- ation (RAG) leveraging Romanian language man- uals or grammar resources as external knowledge, targeted fine-tuning using a subset of the dataset questions, and experimenting with more sophisti- cated prompting strategies such as few-shot learn- ing to potentially create more effective AI tutors. Finally, addressing the identified dataset limita- tions through continued curation and normative alignment remains vital for building truly robust evaluation benchmarks for Romanian educational NLP. 8 Limitations Our dataset of 1 151 Romanian multiple-choice questions (MCQs) is smaller than the large benchmarks available for high-resource languages and represents mainly high-stakes exam material. Items were extracted with Tesseract OCR and then spot-checked, yet some annotation noise may re- main. Question categorisation relied on an LLM- assisted workflow that, despite manual review, can still inherit model errors. While we have categorized the questions when answering them and obtained the following split syntactic 382 questions, morphological 364, lex- ical 319 and phonetic 86, we have not annotated the initial data with this information. Methodologically, the study is restricted to MCQs; open-ended generation and deeper dis- course skills are outside our present scope. 9 Ethical Considerations This research uses publicly available Romanian multiple-choice exam questions; all sources are free of licensing or privacy constraints. We re- lease the dataset on Hugging Face to foster trans- parency, reproducibility, and further work on low- resource educational NLP. The free web applica- tion stores no PII. Dataset content mirrors the fo- cus of the original exams. LLM-generated expla- nations were validated by a linguist to prevent mis- information. We acknowledge possible training- data biases and welcome community scrutiny; users should respect original copyrights."}
{"doc_id": "2508.14275v1", "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14275v1", "chunk_id": 0, "text": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders Cliff O’Reilly Department of Computer Science City St George’s, University of London cliff.oreilly@citystgeorges.ac.uk Ernesto Jim´ enez-Ruiz Department of Computer Science City St George’s, University of London ernesto.jimenez-ruiz@citystgeorges.ac.uk Tillman Weyde Department of Computer Science City St George’s, University of London t.e.weyde@citystgeorges.ac.uk Abstract Connecting LLMs with formal knowledge representation and reasoning is a promising ap- proach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text repre- sentations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy. 1 Introduction The combination of LLMs with formal reasoning and knowledge representation has become a topic of increased interest recently. While the improvements of LLMs have enabled new applications of machine learning and artificial intelligence, LLMs have weaknesses, such as hallucinations and reasoning errors. The combination of LLMs with formal knowledge representation, such as ontologies, has the potential to address these problems but it requires bridging the gap between text and formal semantics. Sparse autoencoders can help model semantic content in an interpretable way by disentangling components in text embeddings. However, embeddings contain not only information on concept semantics, but also syntactic and language-specific aspects. In this work, we propose the use of multi-lingual aggregation to extract concept semantics and evaluate this method in ontology alignment tasks. Our question is: can multi-lingual representations be used to isolate semantic aspects in sparse embedding vec- tors. This will help both mechanistic interpretability, but also practical tasks like ontology alignment, and more generally the integration between LLMs and formal sematic systems. 1 In our method, we parse OWL ontology classes into text representations and use these as prompts for an LLM with Sparse Autoencoders (SAEs), which produces concept activations. We use existing class similarity mapping from the ontology suite1 as ground truth, and compare the similarity of concept activations with the ground truth. With single language prompts in English, concept activations are noisy, in that they show low correlation to the ground truth. To address this, we perform a natural language translation to produce multiple concept activation vectors. The average of these vectors (which we term the conceptual average) shows a clearly stronger correlation to ground truth. We interpret this as evidence for syntactic and language-specific information being suppressed by"}
{"doc_id": "2508.14275v1", "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14275v1", "chunk_id": 1, "text": "the ground truth. To address this, we perform a natural language translation to produce multiple concept activation vectors. The average of these vectors (which we term the conceptual average) shows a clearly stronger correlation to ground truth. We interpret this as evidence for syntactic and language-specific information being suppressed by the multi-lingual approach. The rest of this paper is organised as follows: In section 2 we present the context of our idea and related work, in section 3, we introduce our method followed by results, in section 4, and a discussion in section 5. 2 Related Work An ontology in general terms can be thought of as a formal representation of a domain of knowledge. Any representation has to be subjective and, in practice, ontologies tend to be bespoke to a domain or application. 2.1 Ontology Parsing OWL2 ontologies are a standard, machine-readable and flexible format for representing any domain. Due to the subjective nature of semantic representation, our goal of creating a text prompt from an OWL ontology is also subjective. The extraction of OWL classes, properties and relationships can be performed with libraries for various programming languages (we used OWLAPI [1]), and tools have been created to generalise text extrac- tion, e.g. NaturalOWL [2] and OWL Verbalizer [3]. The recursive concept verbaliser approach for ontology subsumption inference [4] presents a toolbox for OWL ontology analysis (OntoLAMA). 2.2 Ontology Alignment The challenge of matching concepts between ontology representations is as old as the representations themselves. Since 2004, the Ontology Alignment Evaluation Initiative3 has provided a framework for evaluating various approaches. From straightforward lexical approaches, through structural and semantic techniques to more recent innovations with machine learning [5] (and a multitude of hybrid methods ([6]; [7]; [8])), we believe our research is novel in approaching the problem with analyses of LLM internal concept states. 2.3 Mechanistic Interpretability and Sparse Autoencoders Mechanistic Interpretability (MI) is a domain which aims to interpret the internal activation states of neural networks for various purposes such as AI safety, neural network decision-making and improving network design [9]. Our interest in MI is for the learned concept activations — the correlation between conceptual semantics and node activations of LLMs. When applied to language models, Sparse Autoencoders are unsupervised algorithms that learn to map from latent representations to interpretable concepts (also called features). An SAE is a pair of encoder and decoder functions that compresses an input into a hidden representation and tries to reconstruct the input from the hidden representation — thereby learning a set of activation features which can be correlated via techniques such as Dictionary Learning [10] to a vocabulary of human understandable concepts. The sparsity controls that are applied during training result in a reduced set of activations that are more easily computed (compared with billions of activations in a full LLM). Gemma Scope [11] is an open suite of SAEs trained on Google’s Gemma 2 LLM — at every layer and sublayer. 1https://oaei.ontologymatching.org/2024/conference/index.html 2https://www.w3.org/OWL/ 3https://oaei.ontologymatching.org/ 2 3 Method The corpus used in this experiment comes from the conference track of the Ontology Alignment Evaluation"}
{"doc_id": "2508.14275v1", "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14275v1", "chunk_id": 2, "text": "billions of activations in a full LLM). Gemma Scope [11] is an open suite of SAEs trained on Google’s Gemma 2 LLM — at every layer and sublayer. 1https://oaei.ontologymatching.org/2024/conference/index.html 2https://www.w3.org/OWL/ 3https://oaei.ontologymatching.org/ 2 3 Method The corpus used in this experiment comes from the conference track of the Ontology Alignment Evaluation Initiative 2024. Across the 16 ontologies, there are 867 class definitions and a set of 174 reference class mappings as ground truth alignments. The alignment mappings do not cover all the 16 ontologies so we cut down the analysis to only those ontologies with a target mapping available. The method we apply can be broken down into stages, described below. 1. We use the Java library OWLAPI4 to parse each owl ontology file. Due to the nature of owl representations, each ontology can take a different format and so the Java script has bespoke logic that extracts classes, any related subclasses, superclasses, object properties and data properties. Some manipulation of the representation is needed. There are two styles of output we create: a summary and a verbose version — summary and verbose examples for the class Author (from the edas ontology) are shown in quotes (A) and (B), below. The verbose output is a text string which encapsulates a description of the class and includes connecting and descriptive words, but the summary version is a concatenation of the target class name, associated class names and object properties. (A) Author is a SuperClassOf Presenter and hasRelatedPaper Paper (B) Author is a SubClassOf some writes Contribution and is a SubClassOf Person and is a SubClassOf only writes Contribution and writes Contribution 2. We use the googletrans Python library5 to perform a natural language translation from English to French and English to Simplified Chinese. There are no parameters supplied to this process - it is a straightforward translation service. Examples of French and Chinese translations of summary and verbose representations are shown below: (C) Personne auteur uniquement Contribution Certaines ´ ecritures Contribution Contribution (D) L’auteur est une sous-classe de contribution des ´ ecritures et est une personne sous-classe et est une sous-classe unique en r´ edaction de contribution et ´ ecrit la contribution (E) 作者有些人写贡献只写贡献 (F) 作者是一个子类人，是一个仅写作贡献的子阶级，并且是某些撰写贡献的子类 别，并写下了贡献 3. Using the huggingface library6 [12] to access the Gemma Scope open suite of sparse autoencoders, we process each text representation as a prompt to a PyTorch neural network (using a Jump ReLu activation function). The particular SAE set used here is the 2 billion parameter model based on Google’s Gemma 2 Large Language Model. We take every layer (0 to 25) of the 16.4k width model and we take the L0 Norm variant for model regularisation (the number of non-zero elements in the activation vector) where the average is between 13 and 23 active features (e.g. 13 out of 16.4k on average). If L0 is set too high then features overlap and interpretability breaks down. Set L0 too low and the network underfits and misses important structures. The output is a set of concepts and an activation weighting. An example tensor with 7 concept identifiers and"}
{"doc_id": "2508.14275v1", "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14275v1", "chunk_id": 3, "text": "13 out of 16.4k on average). If L0 is set too high then features overlap and interpretability breaks down. Set L0 too low and the network underfits and misses important structures. The output is a set of concepts and an activation weighting. An example tensor with 7 concept identifiers and activation values is shown in Table 1. 4https://github.com/owlcs/owlapi 5https://pypi.org/project/googletrans/ 6https://huggingface.co/google/gemma-scope 3 Concept ID Activation Weight 2446 57.0846 3391 47.3293 3752 37.2378 5327 79.6517 6035 70.1694 6035 71.6481 7234 36.0779 8816 46.0310 9823 57.1111 10144 43.9628 12529 49.0565 14829 61.3937 Table 1: Example concept activation set 1 (for the verbose English edas-Author class) 4. The same process as in (3) is repeated for the translated (French and Chinese) texts. 5. For each class representation, the English and the translated concept activation sets are averaged. For concepts shared between the English and translated sets the simple average of the weighting is taken. Concepts not shared are removed from the output. This results in a much reduced set of concepts for each class. The resulting average class representation we call the conceptual average. An example is shown in Figure 1. Figure 1: Converting two concept sets into a conceptual average 6. Every class average within each of the three groups (English only, English to French combined conceptual average and English to Chinese conceptual average) is compared with every other class average in the same group using a Cosine Similarity function (function represented in Figure 2. Where the class comparison has a pre-defined mapping in the ground truth dataset, we align the similarity score with a target variable value of 1, else it is set to 0. An example record, showing the similarity score of the Author classes from the emt and edas ontologies, with a target of 1 is shown below, at (H). 4 Figure 2: Computing the class concept representation similarity (H) cmt-Author,edas-Author,0.8362799,1 7. The resulting output from the previous steps is a set of differences between representations of classes from the source ontologies. The ground truth class reference mappings are used to create a correlation between the correct relationship and the conceptual difference. The correlation is a measure of the accuracy of conceptual representation from the sparse autoencoder, and by creating the conceptual average we hope to improve the accuracy. The correlation algorithm used is the Point-Biserial Correlation, which is ideal for correlations between binary and continuous variables. Due to the nature of the corpus, the layer-by-layer analysis only has 174 ground truth mappings (from a total of 95,000 class comparisons) and hence there is a large class imbalance for each layer. We reduce the imbalance by using a random re-sampling to reduce the false target variable size to be the same as the true size. A further analysis was undertaken using the same approach, but instead of translating to French, Simplified Chinese was used. Example code is available for validation7. 4 Results The corpus used in this experiment contains 16 OWL ontology files, comprising 867 classes. The corpus also includes 174 class mappings which form the ground truth of this experiment. From"}
{"doc_id": "2508.14275v1", "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14275v1", "chunk_id": 4, "text": "same approach, but instead of translating to French, Simplified Chinese was used. Example code is available for validation7. 4 Results The corpus used in this experiment contains 16 OWL ontology files, comprising 867 classes. The corpus also includes 174 class mappings which form the ground truth of this experiment. From this set of files, we follow the method described above and record the results as shown below. The outputs consist of a correlation between ontology classes and over a number of variations such as the nature of the text representation (either a summary or verbose representation) and the layer of concept activation (0-25). These results are then compared at the level of language, i.e. are the English-only concept sets different from the French/Chinese conceptual averages. Results are shown in Figure 3, for the summary text representation, and Figure 4 for the verbose prompt. Both results show a clear improvement in correlation comparing English only and the translation conceptual average. There is a small increase in correlation when we look at averages of translations between English and Simplified Chinese for summary texts, but a reduction for the verbose texts. 7https://github.com/cliffore/millms 5 Figure 3: Summary prompt - conceptual average correlation vs English-only (higher is better) Figure 4: Verbose prompt - conceptual average correlation vs English-only (higher is better) Table 2 shows the correlations compared. Text Version Language Correlation Summary English only 0.09 Summary Avg Eng/French 0.39 Summary Avg Eng/Chinese 0.33 Verbose English only 0.18 Verbose Avg Eng/French 0.20 Verbose Avg Eng/Chinese 0.35 Table 2: Average correlations for each text version and the translated language 6 5 Discussion After we extract an English text version of an ontology class, we put that string through a set of SAE neural net- works and compare the concept activations between other classes from related ontologies. We have a (small) set of ground truth correspondences between classes and we see a fairly weak correlation emerge. This is potentially due to the small size of the corpus and also the relatively subjective extraction from OWL representation to a string of words. We notice that there is a difference in overall correlations between the summary extracted text and the more verbose version. We take the same English text and translate it to French and Simplified Chinese and put these two prompts through the same SAEs. The resulting concept activation sets are averaged for each ontology class between the English and French and between the English and Chinese versions. This represents a different concept activation set for each ontology class. When we calculate the same correlations as with the English only activations, we see a significant improvement in correspondence for the French translation and an even stronger correlation for the Chinese translations. The difference in average correlations between the French and Chinese (from Table 2) is small, however the average percentage difference is 9% (summary). In the same way that “Conceptual Semantics takes the meanings of words and sentences to be structures in the minds of language users” [13], we might assume that LLMs have structures which represent the meaning of words"}
{"doc_id": "2508.14275v1", "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14275v1", "chunk_id": 5, "text": "(from Table 2) is small, however the average percentage difference is 9% (summary). In the same way that “Conceptual Semantics takes the meanings of words and sentences to be structures in the minds of language users” [13], we might assume that LLMs have structures which represent the meaning of words that are processed through their neural network states. We might also assume that LLMs don’t have concepts of meaning in themselves, but instead are learning and storing correspondences between symbolic and linguistics structures upon which LLMs are trained. Arguments are emerging, however, which show that LLMs do represent real world concepts ([14] and [15]) beyond the purely linguistic. When we peek into the network internals, using Sparse Autoencoders, we often see that concepts are activated which relate to surface cues such as syntactic and linguistic semantics, but our results show that we can reduce the symbolic concept space for a set of activations and isolate concepts that reflect a purer semantic representation. Following the example from before, we had an Author class that was represented as English, French and Chinese. The verbose representation concept activation set is shown in Table 1. In Table 3, we show the Chinese translation concept activations, and in Table 4 the conceptual average output. Concept ID Activation Weight 2446 21.1420 5327 44.4837 6035 39.4718 6035 39.7229 7748 30.9887 8920 22.0814 9967 73.0786 13833 15.4156 14763 19.7685 Table 3: Example concept activation set 1 (for the verbose Chinese translation of edas-Author class) Concept ID Activation Weight 2446 39.1133 5327 62.0677] 6035 55.6855 Table 4: Conceptual average activation set (for the verbose English and Chinese translation of edas-Author class) Concepts can be interpreted and given human-readable representations [16]; concept ID 3391 in this analysis can be interpreted as (G), below (more detail can be seen on the Neuronpedia website e.g. for concept 6035 on layer 0: https://www.neuronpedia.org/gemma-2-2b/0-gemmascope-att-16k/6035. This concept is removed during the averaging process for the edas-Author class because it is only activated by the English text. The output conceptual average has interpretations shown in Table 5 below. 7 (G) attends to key parameters denoted by brackets from associated values in the same context Concept ID Automatic interpretation 2446 attends to the token ”Sant” from related tokens concerning ”orum” and ”ana.” 5327 attends to numerical outputs and associated metadata from other mathematical or technical symbols and structures 6035 attends to specific instances of the token ”N” from various indexed digits appearing later in the sequence Table 5: Conceptual average activation set (for the verbose English and Chinese translation of edas-Author class) The interpretation representations don’t intuitively correspond with a human-readable assessment of similarity or conceptual semantics, however we suggest that our results show some significance to the process of removing concepts that are not shared between translated representations. The translation and averaging process is remov- ing linguistic specific concepts and leaving concepts that are a purer representation of the core semantics of the original prompt. The overall average results (Table 2 indicate a trend for the Chinese conceptual average to correlate more strongly with the ground truth when"}
{"doc_id": "2508.14275v1", "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14275v1", "chunk_id": 6, "text": "The translation and averaging process is remov- ing linguistic specific concepts and leaving concepts that are a purer representation of the core semantics of the original prompt. The overall average results (Table 2 indicate a trend for the Chinese conceptual average to correlate more strongly with the ground truth when compared with the French translation (assuming that the Summary versions (0.39 and 0.33) are too similar to be divided). This pattern adds credence to the argument since Chinese language tokens contains fewer syntactic elements, “e.g. more frequent functional words in English texts” [17]. Given that the dataset is small and the representations and translations relatively subjective, this result should be validated. This slightly unexpected result hints at a new technique for improving conceptual analyses of LLMs, especially via SAEs. We expect future research to confirm and extend this result. 6 Future Work We highlight some problems which we hope to address in future versions of the research. The accuracy of extraction of class representations is exposed to problems of subjectivity. Both the conceptual model used to create an OWL ontology and the extraction process to create a text string version are prone to idiosyncrasies in design. The corpus used is relatively small, having a low number of OWL classes. There is also a class imbalance because there are missing ground truth mappings for many OWL classes. The ground truth is also liable to potential bias. The use of SAEs for interpretability is a relatively novel approach and there are known challenges e.g. feature splitting [18], terse concept dictionaries and reconstruction errors [19]. The following areas are in scope for next steps: (i) Extend the corpora to confirm and explore this result, (ii) Explore a generalised ontology class extraction process, (iii) Analyse concept features for a common sense analysis, (iv) Apply improvements in interpreting conceptual semantics to Ontology Alignment tasks."}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 0, "text": "Let’s Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper Krishna Garg1*, Firoz Shaik1*, Sambaran Bandyopadhyay2, Cornelia Caragea1 1University of Illinois Chicago 2Adobe Research kgarg8@uic.edu, fshaik8@uic.edu, sambaranb@adobe.com, cornelia@uic.edu Abstract As researchers increasingly adopt LLMs as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Intro- duction Generation (SciIG), a task that evaluates LLMs’ abil- ity to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art mod- els—including both open-source (DeepSeek-v3, Gemma-3- 12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed- source GPT-4o systems across multiple dimensions: lexical overlap, semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA- 4 Maverick’s superior performance on most metrics, par- ticularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot ap- proaches. These findings provide practical insights into de- veloping effective research writing assistants and set realistic expectations for LLM-assisted academic writing. To foster re- producibility and future research, we will publicly release all code and datasets. 1 Introduction The rapid advancement of generative artificial intelligence, particularly large language models (LLMs), like OpenAI’s GPT series (Brown et al. 2020; Achiam et al. 2023), has sparked significant interest in their potential to assist in various complex tasks, including academic writing. As re- searchers and academics strive to balance the increasing de- mands of scholarly communication with the need for high- quality writing, the importance of using LLMs to generate coherent, engaging, and accurate research paper introduc- tions becomes evident. Despite the impressive capabilities demonstrated by these models, creating an introduction that encapsulates the essence of a research paper, its motivation, scope, and significance remain a formidable challenge. This challenge is amplified by the need for the generated text to be not only contextually relevant but also to exhibit a high degree of narrative quality and academic rigor, as the facts mentioned in a research introduction should be grounded on state-of-the-art literature. *Equal contribution Existing works in the realm of LLM-assisted academic writing have made strides in various domains, from drafting essays to summarizing scientific papers (Xiao et al. 2022a; Nandy and Bandyopadhyay 2025). However, these efforts often fall short in several critical aspects. Current models of- ten struggle with maintaining faithfulness to the source ma- terial, ensuring consistency in narrative flow, and accurately incorporating citations (Huang et al. 2025). Moreover, while there have been attempts to measure the quality of gener- ated texts using automated metrics, these evaluations often overlook nuanced aspects such as the coherence of the intro- duction and its alignment with academic standards (Li et al. 2024). There are a plethora of blogs and videos on quick suggestions on how to use LLMs to write different parts of a research paper, but none of those shows a systematic anal- ysis of the capabilities of current LLMs. There are a few publicly available datasets that maintain a collection of re- search papers for various"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 1, "text": "and videos on quick suggestions on how to use LLMs to write different parts of a research paper, but none of those shows a systematic anal- ysis of the capabilities of current LLMs. There are a few publicly available datasets that maintain a collection of re- search papers for various generation tasks (Lu, Dong, and Charlin 2020; LIU et al. 2022). However, they are not ap- propriate for use with the current LLMs since they all have probably appeared in the LLM training set. Consequently, the potential of LLMs in generating high-quality research paper introductions remains underexplored and inadequately benchmarked. To address these research gaps, we propose a comprehen- sive framework for the generation of scientific introductions. This framework aims to rigorously evaluate the capability of LLMs to produce introductions of research papers using new datasets curated from the NAACL 2025 and ICLR 2025 ac- cepted papers. Our approach involves evaluating multiple di- mensions of generated introductions, including lexical over- lap, semantic similarity, content coverage, faithfulness, con- sistency, citation correctness, and narrative quality. By com- bining automated metrics with LLM-as-a-judge evaluations (Liu et al. 2023), we seek to provide a holistic understand- ing of the strengths and limitations of current state-of-the-art models. Finally, a small scale but rigorous human evaluation is conducted to understand the gap between the generated introductions and the expected standard. The following are the contributions we have made in this paper. First, we introduce a novel task and curate datasets specifically designed for evaluating the generation of re- search paper introductions by state-of-the-art LLMs. Sec- ond, we present a comprehensive evaluation framework that integrates automated and human-in-the-loop assessments to offer robust insights into model performance. Third, we conduct extensive experiments with five open and closed- source state-of-the-art LLMs, highlighting the superior per- formance of LLaMA-4-Maverick, particularly in terms of semantic similarity and faithfulness. Lastly, we demonstrate the effectiveness of three-shot prompting over fewer-shot approaches, providing practical guidelines for optimizing LLM-assisted writing processes. These contributions col- lectively advance our understanding of LLM capabilities in academic writing and set the stage for developing more ef- fective and reliable research writing assistants. 2 Related Works Multi-Document Summarization (MDS) A significant body of work in multi-document summarization (MDS) has explored techniques to distill information from multi- ple related sources into coherent summaries. Models like PRIMERA (Xiao et al. 2022b) have introduced tailored pre- training strategies that enhance a model’s ability to integrate information from concatenated documents using encoder- decoder transformers. Meanwhile, graph-based methods (Liao, Lebanoff, and Liu 2018; Li et al. 2020; Pasunuru et al. 2021; Nandy and Bandyopadhyay 2025) leverage structured representations such as Abstract Meaning Representation (AMR) and discourse relations to encode inter-document connections. However, these approaches often require ad- ditional linguistic annotations or parsing pipelines, limiting their applicability in general settings. Meta-Review Generation Meta-review generation—a task involving the synthesis of multiple peer reviews into a single coherent summary—has been tackled using a range of strategies. Early systems employed rule-based logic and shallow neural components (Bhatia, Pradhan, and Pal 2020; Pradhan et al. 2021), while more recent work adopted encoder-decoder"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 2, "text": "general settings. Meta-Review Generation Meta-review generation—a task involving the synthesis of multiple peer reviews into a single coherent summary—has been tackled using a range of strategies. Early systems employed rule-based logic and shallow neural components (Bhatia, Pradhan, and Pal 2020; Pradhan et al. 2021), while more recent work adopted encoder-decoder frameworks for abstracting feedback (Ku- mar, Ghosal, and Ekbal 2021; Li, Hovy, and Lau 2023). With the emergence of powerful language models, several studies have begun evaluating their suitability for peer-review gen- eration. Du et al. (2024) and Tyser et al. (2024) investigated how well LLMs perform in generating reviews and meta- reviews compared to human-written outputs. Zeng et al. (2024) proposed CGI2, a structured prompting technique that guides LLMs through iterative steps for producing more grounded scientific assessments. Large Language Models for Scientific Summarization General-purpose LLMs such as GPT-3.5 (Arshad, Yaqoob, and Mehmood 2023), GPT-4 (Achiam et al. 2023; Ouyang et al. 2022), PaLM (Chowdhery et al. 2023), LLaMA (Tou- vron et al. 2023), Bloom (Le Scao et al. 2023), and GLaM (Du et al. 2022) have demonstrated impressive performance across summarization benchmarks. While these models ex- cel in single-document generation tasks, their application to multi-input, constraint-driven generation—such as meta- review synthesis or domain-specific aggregation—remains under-investigated (Bansal, Akter, and Karmaker Santu 2022). While systems like AutoSurvey (Wang et al. 2024) aim to synthesize prior work into broad surveys, they fall short in addressing the rhetorically intricate and concise nature of re- search paper introductions, which demand precise argumen- tation and contextual positioning rather than comprehensive coverage. In contrast, our work formally defines the task of Scientific Introduction Generation and focuses on generat- ing coherent, well-grounded introductions from a paper’s Title, Abstract, and citations. This targeted formulation en- ables deeper exploration of the unique challenges in rhetori- cal planning and academic discourse, laying the groundwork for future extensions toward full research paper generation. 3 Task 3.1 Problem Formulation We formulate the task of Scientific Introduction Generation (SciIG) as a conditional generation problem. Given a re- search paper’s Title T , Abstract A, and a set of k Related Papers R = {r1, r2, ..., rk}, where each ri contains a title, abstract, and author list, the objective is to generate a co- herent, academically styled Introduction I. Formally, the model does a mapping: f : (T , A, R) 7→I where f is instantiated by a large language model (LLM) conditioned on a task-specific prompt. We choose Title, Abstract, and Related Work as inputs because they collectively provide the essential ingredients for generating effective scientific introductions. According to Swales’ widely adopted CARS model (Swales 1990), in- troductions typically serve three rhetorical purposes: (1) es- tablishing the research territory, (2) identifying the niche, and (3) occupying that niche. In this context, the related work provides a foundation for establishing the territory and highlighting gaps in the literature; the abstract conveys the core contribution and its significance, aligning with niche occupation; and the title frames the overall research focus. 3.2 Datasets To support the SciIG task, we construct two datasets1 from accepted papers of"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 3, "text": "work provides a foundation for establishing the territory and highlighting gaps in the literature; the abstract conveys the core contribution and its significance, aligning with niche occupation; and the title frames the overall research focus. 3.2 Datasets To support the SciIG task, we construct two datasets1 from accepted papers of the NAACL 2025 and ICLR 2025 con- ferences, comprising 800 and 3100 samples, respectively. These datasets, focused on natural language processing (NAACL) and machine learning (ICLR), provide diverse, high-quality research papers for training and evaluating models to generate coherent and contextually relevant in- troductions. The creation process involved five key steps to extract titles, abstracts, introductions, authors, and citation details, ensuring rich data for the task. Table 1 summarizes text length and citation statistics, highlighting the datasets’ variability and suitability. The creation process began with Step 1: we retrieve PDF files of accepted NAACL 2025 and ICLR 2025 papers from web sources, including official conference proceedings and repositories like ArXiv. For each conference, we aggregated all available PDFs to ensure comprehensive coverage of that venue’s publications. In Step 2, we process the PDFs using the grobid2json parsing tool provided by the S2ORC project (Lo et al. 2020). 1We will make the datasets publicly available under CC BY 4.0. Introductions (words) Abstracts (words) Citations Dataset Samples Mean Median Range Mean Median Range Average Range NAACL 2025 800 597.1 ± 228.4 610.0 66–1,546 169.3 ± 76.7 162.0 25–1,748 13.4 1–45 ICLR 2025 3100 771.1 ± 414.3 747.5 42–13,675 210.4 ± 101.4 194.0 6–1,063 15.3 1–58 Table 1: Statistics for the NAACL 2025 (800 samples) and ICLR 2025 (3100 samples) datasets, including number of samples, text length, and citation metrics, highlighting variability in introduction lengths and citation patterns relevant to the Introduction Generation task. This tool internally leverages the Grobid library to first con- vert PDFs into structured TEI-XML. Subsequently, it ex- tracts essential components such as titles, abstracts, intro- ductions, author details, and bibliographic references, and organizes them into a structured JSON format. This struc- ture representation provides a robust foundation for subse- quent tasks in our dataset. For Step 3, we develop a regular expression to identify Related Works cited within each paper’s introduction. The regex targeted citation patterns (e.g., “Author et al., Year”) to extract references relevant to the introductory context, cap- turing the scholarly connections embedded in the text. In Step 4, we employ the LLaMA 4-Maverick model to map extracted Related Works citations (e.g., “et al.” strings) to entries in the references section, retrieving the corre- sponding titles. The prompt used for this citation mapping is provided in Table 8 in Appendix. This step ensures accurate linkage between in-text citations and their full bibliographic details. Finally, in Step 5, we use the SemanticScholar API, com- bining official2 (Kinney et al. 2023) and unofficial3 end- points, to fetch abstracts and author details for each Related Works title. This enriched the dataset with contextual cita- tion information, supporting tasks requiring citation-aware generation. The resulting datasets exhibit significant diversity, as shown in Table 1. NAACL 2025 introductions average 597.1 ± 228.4 words, while"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 4, "text": "2023) and unofficial3 end- points, to fetch abstracts and author details for each Related Works title. This enriched the dataset with contextual cita- tion information, supporting tasks requiring citation-aware generation. The resulting datasets exhibit significant diversity, as shown in Table 1. NAACL 2025 introductions average 597.1 ± 228.4 words, while ICLR 2025 introductions are longer at 771.1 ± 414.3 words, with an outlier maximum of 13,675 words, posing challenges for generation. Citation patterns within the Introductions also vary, with NAACL 2025 averaging 13.4 citations per paper and ICLR 2025 av- eraging 15.3, reflecting dense scholarly connections. These characteristics make the datasets ideal for evaluating the Scientific Introduction Generation task across varied text lengths and citation contexts. In our experiments, we use models with sufficient context length to process the entire metadata, accommodating the datasets’ variable introduc- tion lengths and citation-rich content. We limit this initial effort to the CS domain since re- cruiting domain-expert annotators is challenging. As ML researchers, we annotated the outputs ourselves. However, since no model was trained on this data, and the study fo- cuses purely on benchmarking, the risk of author bias is min- imal. 2https://www.semanticscholar.org/ 3https://github.com/danielnsilva/semanticscholar 4 Methods and Evaluation 4.1 Methods To evaluate the Scientific Introduction Generation task on the NAACL 2025 and ICLR 2025 datasets, we employ five state-of-the-art language models with knowledge cutoffs be- fore January 2025: Deepseek-V3-0324 (Liu et al. 2024), LLaMA 4-Maverick 4, Mistral-Small-3.1-24b 5, Gemma- 3-12b-it (Team et al. 2025), and GPT-4o (Hurst et al. 2024). These models, selected for their diverse architectures and ro- bust text generation capabilities, include a combination of open-source and commercialized systems. They are well- suited to handle the datasets’ variable introduction lengths (up to 13,675 words) and citation-rich metadata. Each model has sufficient context length to process the entire input, in- cluding titles, abstracts, and related paper details, ensuring comprehensive generation. We design seven prompting strategies to guide the mod- els in generating introductions, tailored to the task’s require- ments and the datasets’ characteristics. These strategies, in- cluding SHORT, MEDIUM, ELABORATE, ONE SHOT, TWO SHOT, THREE SHOT, and AUTOCOT, vary in com- plexity and context, incorporating the target paper’s title, abstract, and JSON-formatted related papers with APA ci- tations. The SHORT prompt provides minimal context for concise generation, while MEDIUM emphasizes research gaps and citation integration. ELABORATE enforces a strict four-paragraph structure (each 100–150 words), detail- ing context, gaps, contributions, and impact. ONE SHOT, TWO SHOT, and THREE SHOT incorporate 1–3 example introductions from a held-out set6, enabling few-shot learn- ing. AUTOCOT uses iterative, self-refining prompts to en- hance reasoning for complex scenarios. All prompts main- tain a formal academic tone, enforce APA citation accuracy, and leverage the datasets’ citation-rich metadata, ensuring robust and contextually relevant introductions. The prompts for each strategy are discussed in detail in Appendix in Ta- bles 9, 10, and 11. 4https://huggingface.co/meta-llama/Llama-4-Maverick-17B- 128E-Instruct 5https://huggingface.co/mistralai/Mistral-Small-3.1-24B- Instruct-2503 6Held-out set is created using additional 54 samples for NAACL 2025 and 52 samples for ICLR 2025. It is used to select random examples provided in the prompt for few-shot experiments. 4.2 Evaluation The evaluation"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 5, "text": "in detail in Appendix in Ta- bles 9, 10, and 11. 4https://huggingface.co/meta-llama/Llama-4-Maverick-17B- 128E-Instruct 5https://huggingface.co/mistralai/Mistral-Small-3.1-24B- Instruct-2503 6Held-out set is created using additional 54 samples for NAACL 2025 and 52 samples for ICLR 2025. It is used to select random examples provided in the prompt for few-shot experiments. 4.2 Evaluation The evaluation of the Scientific Introduction Generation task employs a comprehensive set of metrics organized into six key categories: Lexical Overlap, Semantic Similarity, Content Coverage, Faithfulness, Consistency, and Narrative Quality, as detailed in Table 2. Below, we describe the com- putation and purpose of each category’s metrics, followed by a discussion of the LLM-as-a-Judge metric, highlighting their roles in assessing model performance. Lexical Overlap evaluates surface-level similarity be- tween generated and reference introductions using ROUGE- 1, ROUGE-2, ROUGE-L (Lin 2004), BLEU (Papineni et al. 2002), and METEOR (Banerjee and Lavie 2005). ROUGE-1 and ROUGE-2 compute unigram and bigram overlap as the ratio of matching n-grams, balancing precision and recall. ROUGE-L measures the longest common subsequence, cap- turing structural similarity. BLEU calculates n-gram preci- sion with a brevity penalty for length differences. METEOR incorporates synonymy and stemming, aligning words by semantic similarity to enhance sensitivity to linguistic vari- ations.7 Semantic Similarity metrics, including BERTScore (Zhang* et al. 2020), BLEURT(Sellam, Das, and Parikh 2020), and our proposed contextual embedding-based met- ric Contextual Relevance, assess the alignment of meaning between the generated and reference texts. BERTScore computes cosine similarity between contex- tual token embeddings from a pretrained BERT model, al- lowing soft matching across paraphrases. BLEURT8 uses a BERT-based regression model fine-tuned on human ratings to predict similarity with greater nuance. CONTEXTUAL RELEVANCE measures the average cosine similarity between the embedding of the generated introduc- tion and the embeddings of its associated context elements, which include the paper’s title, abstract, and each of the cited papers. These individual components form the input context used to evaluate how topically aligned the generated intro- duction is with the source material. Let g ∈Rd denote the embedding of the generated introduction, and {c1, . . . , cn} the context embeddings. The relevance is computed as: Contextual Relevance = 1 n n X i=1 g · ci ∥g∥· ∥ci∥+ ϵ where ϵ is a small constant for numerical stability. Content Coverage evaluates how well the generated in- troduction captures key content, either by matching the gold (ground truth) introduction or by grounding in the input con- text (Title and Abstract). We define two complementary met- rics: REFERENCE-BASED COVERAGE. Let Kgen be the set of keyphrases extracted from the generated introduction, and KGT be the set of keyphrases from the ground truth introduc- tion. Define semantic match ∼based on cosine similarity of phrase embeddings (e.g., using Sentence-BERT), such that: 7Implementations sourced from https://github.com/ huggingface/evaluate 8Implementation sourced from https://github.com/lucadiliello/ bleurt-pytorch kGT ∼kgen ⇐ ⇒cos (e(kGT), e(kgen)) ≥τ for a threshold τ (e.g., 0.7). Then, Reference-based Cov- erage (RBC) is computed as: RBC = |{k ∈KGT | ∃k′ ∈Kgen, k ∼k′}| |KGT| REFERENCE-FREE COVERAGE. Let Kctx denote the set of keyphrases extracted from the Title and Abstract (input context). Then, Reference-free Coverage (RFC)"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 6, "text": "⇐ ⇒cos (e(kGT), e(kgen)) ≥τ for a threshold τ (e.g., 0.7). Then, Reference-based Cov- erage (RBC) is computed as: RBC = |{k ∈KGT | ∃k′ ∈Kgen, k ∼k′}| |KGT| REFERENCE-FREE COVERAGE. Let Kctx denote the set of keyphrases extracted from the Title and Abstract (input context). Then, Reference-free Coverage (RFC) is defined similarly: RFC = |{k ∈Kctx | ∃k′ ∈Kgen, k ∼k′}| |Kctx| Faithfulness assesses the factual alignment between the generated Introduction and its source inputs. We implement two automatic metrics and one LLM-based judge: QA-BASED FAITHFULNESS (Q2). We adopt a Q2-style (Honovich et al. 2021) metric to evaluate the factual consis- tency of a generated introduction with respect to its source context (i.e., the title, abstract, and cited works). Please refer to Appendix A for the formal derivation of the metric. ENTAILMENT-BASED FAITHFULNESS. We employ SummaC-ZS (Laban et al. 2022), a zero-shot Natural Language Inference (NLI)-based model, to assess factual consistency between the generated introduction and its source context (i.e., title, abstract, and cited papers). The model treats the source context as a premise and the generated introduction as a hypothesis, computing a scalar consistency score: FaithfulnessEntail(G, C) = SummaC ZS(C, G) Here, SummaC ZS(·) returns an alignment score in [−1, 1], where higher values indicate stronger entailment and factual alignment. This metric effectively evaluates faithfulness by detecting hallucinations and contradictions in summarization outputs. KEYPHRASE-BASED FAITHFULNESS. Let Kgen be the set of keyphrases extracted from the generated introduction, and Kctx the set extracted from the Title and Abstract (serving as factual ground truth). Define semantic match ∼based on a similarity thresh- old over embeddings (e.g., cosine similarity ≥0.7). Then, Keyphrase-based Precision for Faithfulness is: FaithfulnessKP = |{k ∈Kgen | ∃k′ ∈Kctx, k ∼k′}| |Kgen| This metric captures the proportion of generated keyphrases that are factually grounded in the source context. Consistency metrics, including QA-based, Entailment- based, and Keyphrase-based Consistency, evaluate inter- nal coherence. These metrics mirror the implementation of Faithfulness metrics but differ in their reference point: while Faithfulness is computed with respect to the title and ab- stract, Consistency is calculated with respect to the ground truth introduction. They assess whether the generated text Category Metric NAACL 2025 ICLR 2025 Deepseek Gemma LLaMA4 Mistral GPT-4o Deepseek Gemma LLaMA4 Mistral GPT-4o Lexical Overlap ROUGE-1 0.4161 0.4361 0.4402 0.4388 0.4282 0.3863 0.4342 0.4298 0.4326 0.4154 ROUGE-2 0.1232 0.1304 0.1509 0.1340 0.1310 0.1195 0.1340 0.1537 0.1344 0.1289 ROUGE-L 0.1649 0.1725 0.1894 0.1765 0.1754 0.1531 0.1700 0.1830 0.1715 0.1689 BLEU 0.1175 0.1232 0.1198 0.1278 0.1043 0.0781 0.0979 0.0874 0.0977 0.0762 METEOR 0.2669 0.2851 0.2700 0.2900 0.2658 0.2245 0.2600 0.2418 0.2592 0.2358 Semantic Similarity BERTScore 0.8415 0.8475 0.8448 0.8467 0.8422 0.8377 0.8449 0.8403 0.8427 0.8377 BLEURT 0.3200 0.3088 0.3339 0.3236 0.3262 0.3171 0.3032 0.3290 0.3194 0.3225 Contextual Relevance 0.9691 0.9713 0.9684 0.9694 0.9696 0.9687 0.9729 0.9696 0.9700 0.9707 Content Coverage Reference-based Coverage 0.4435 0.4513 0.4845 0.4692 0.4713 0.4461 0.4732 0.4794 0.4725 0.4697 Reference-free Coverage 0.3913 0.3666 0.4367 0.3959 0.4035 0.3547 0.3448 0.4097 0.3847 0.3664 LLM-as-a-Judge 0.6907 0.7051 0.7019 0.7008 0.7054 0.6567 0.6970 0.6809 0.6833 0.6879 Faithfulness QA-based Faithfulness 0.7794 0.7794 0.7794 0.7846 0.7762 0.7761 0.7814"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 7, "text": "0.9696 0.9700 0.9707 Content Coverage Reference-based Coverage 0.4435 0.4513 0.4845 0.4692 0.4713 0.4461 0.4732 0.4794 0.4725 0.4697 Reference-free Coverage 0.3913 0.3666 0.4367 0.3959 0.4035 0.3547 0.3448 0.4097 0.3847 0.3664 LLM-as-a-Judge 0.6907 0.7051 0.7019 0.7008 0.7054 0.6567 0.6970 0.6809 0.6833 0.6879 Faithfulness QA-based Faithfulness 0.7794 0.7794 0.7794 0.7846 0.7762 0.7761 0.7814 0.7772 0.7790 0.7751 Entailment-based Faithfulness -0.0364 0.1461 0.1077 0.1533 0.0937 -0.0276 0.1647 0.1280 0.1660 0.1095 Keyphrase-based Faithfulness 0.3138 0.2933 0.3455 0.3230 0.3263 0.2892 0.2786 0.3318 0.3067 0.2941 LLM-as-a-Judge 0.7873 0.8042 0.8157 0.7874 0.8141 0.7825 0.8088 0.8149 0.7861 0.8138 Consistency QA-based Consistency 0.7820 0.7876 0.7860 0.7861 0.7873 0.7810 0.7891 0.7846 0.7842 0.7847 Entailment-based Consistency -0.0151 0.1339 0.1077 0.1319 0.0774 -0.0552 0.0923 0.0758 0.1015 0.0465 Keyphrase-based Consistency 0.2133 0.2170 0.2393 0.2230 0.2248 0.2121 0.2197 0.2214 0.2187 0.2208 LLM-as-a-Judge 0.8788 0.8910 0.8954 0.8850 0.8945 0.8583 0.8873 0.8826 0.8761 0.8868 Citation Correctness Recall 0.5158 0.5763 0.5574 0.4914 0.4826 0.4954 0.5413 0.5505 0.4671 0.4657 Precision 0.8819 0.9049 0.9357 0.8889 0.9097 0.8686 0.8965 0.9238 0.8822 0.8942 LLM-as-a-Judge (Citation Quality) 0.7683 0.7959 0.8051 0.7658 0.8050 0.7575 0.8021 0.8019 0.7597 0.7968 Narrative Quality Perplexity ↓ 28.5031 32.3897 18.4870 25.6458 21.8494 36.9546 29.1004 21.1194 29.2585 22.0046 LLM-as-a-Judge 0.8892 0.8888 0.9001 0.8928 0.9004 0.8835 0.8899 0.8986 0.8907 0.9006 Table 2: Evaluation metrics categorized across standard and LLM-as-a-Judge criteria for NAACL 2025 and ICLR 2025. Highest values are bolded. Note: Lower perplexity values indicate better performance. maintains logical and factual consistency within itself, en- suring a cohesive introduction. Citation Correctness metrics include Precision, Recall, and LLM-as-a-Judge metric. We extract citations (e.g., “et al.” strings) from the generated introduction with the LLaMA 4-Maverick model and match them against the orig- inal Related Works citation list to compute the scores. The prompt used is given in Table 8 in the appendix. Narrative Quality is evaluated using Perplexity and LLM-as-a-Judge. Perplexity, computed as the exponential of the average negative log-likelihood under a language model (GPT-2) (Radford et al. 2019), indicates fluency, with lower values reflecting more natural text. The LLM-as-a-Judge metric (Liu et al. 2023), using GPT-4o, evaluates Content Coverage, Faithfulness, Consis- tency, Citation Contextual Quality, and Narrative Quality. GPT-4o is prompted to score generated introductions on a numerical scale for specific qualities as discussed above. Its contextual reasoning captures nuanced aspects, comple- menting automated metrics. The exact prompts for each cat- egory are provided in Table 6 in the appendix. This met- ric improves the framework by integrating human-like judg- ment with computational precision. These metrics collectively form a robust evaluation frame- work, capturing lexical, semantic, content, factual, and nar- rative dimensions of the SciIG task, ensuring a thorough assessment of model performance across diverse require- ments. 5 Results and Analysis Table 2 presents the performance of five state-of-the-art models (Deepseek, Gemma, LLaMA4, Mistral, GPT-4o) on the NAACL 2025 (800 samples) and ICLR 2025 (3100 samples) datasets for the SciIG task. The results highlight several important observations. First, LLaMA4 achieves consistently high lexical over- lap and semantic similarity scores, particularly evident in its superior performance on both datasets. This indicates LLaMA4’s strong capability in closely replicating the orig- inal reference content, likely due to optimized"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 8, "text": "samples) datasets for the SciIG task. The results highlight several important observations. First, LLaMA4 achieves consistently high lexical over- lap and semantic similarity scores, particularly evident in its superior performance on both datasets. This indicates LLaMA4’s strong capability in closely replicating the orig- inal reference content, likely due to optimized pretraining and instruction-following mechanisms. Second, although Gemma demonstrates exceptional recall in Citation Correctness, it shows relatively lower precision compared to LLaMA4, suggesting effective reference incor- poration but occasional introduction of extraneous citations. Third, Mistral shows remarkable entailment-based con- sistency scores, surpassing other models on both datasets, indicating its strength in maintaining logical alignment be- tween input contexts and generated text. However, its higher perplexity suggests a trade-off with fluency. Fourth, Gemma’s superior Contextual Relevance scores, especially on the ICLR dataset, imply that it captures deeper contextual nuances within research topics. Lastly, de- spite GPT-4o’s competitive QA-based consistency scores, its lower lexical overlap and citation recall metrics suggest challenges in precisely capturing explicit references. The LLM-as-a-Judge metrics reveal nuanced strengths among the models. LLaMA4 stands out for its robust faith- fulness and consistency, while GPT-4o shines in content coverage and narrative quality. Gemma’s edge in citation Category Metric Instruction-Level Few-Shot Short Medium Elaborate AutoCoT Zero-Shot One-Shot Two-Shot Three-Shot Lexical Overlap ROUGE-1 0.4261 0.4513 0.4402 0.4233 0.4402 0.4418 0.4440 0.4460 ROUGE-2 0.1622 0.1625 0.1509 0.1361 0.1509 0.1607 0.1645 0.1647 ROUGE-L 0.1911 0.1940 0.1894 0.1701 0.1894 0.1942 0.1965 0.1972 BLEU 0.1117 0.1345 0.1198 0.1353 0.1198 0.1182 0.1193 0.1204 METEOR 0.2516 0.2797 0.2700 0.3132 0.2700 0.2669 0.2679 0.2695 Semantic Similarity BERTScore 0.8480 0.8471 0.8448 0.8352 0.8448 0.8486 0.8496 0.8492 BLEURT 0.3347 0.3312 0.3339 0.3201 0.3339 0.3431 0.3432 0.3452 Contextual Relevance 0.9694 0.9690 0.9684 0.9698 0.9684 0.9677 0.9674 0.9681 Content Coverage Reference-based Coverage 0.4875 0.4697 0.4845 0.4347 0.4845 0.4935 0.4913 0.4985 Reference-free Coverage 0.4313 0.4231 0.4367 0.4556 0.4367 0.4622 0.4632 0.4755 LLM-as-a-Judge 0.7084 0.7101 0.7019 0.7340 0.7019 0.7047 0.7081 0.7107 Faithfulness QA-based Faithfulness 0.7840 0.7767 0.7794 0.7627 0.7794 0.7829 0.7780 0.7833 Entailment-based Faithfulness 0.0119 0.0505 0.1077 -0.1368 0.1077 0.1292 0.1361 0.1432 Keyphrase-based Faithfulness 0.2338 0.2183 0.2393 0.3610 0.2393 0.2353 0.2303 0.2366 LLM-as-a-Judge 0.8462 0.8382 0.8157 0.8603 0.8157 0.8077 0.8146 0.8094 Consistency QA-based Consistency 0.7870 0.7786 0.7860 0.7660 0.7860 0.7864 0.7827 0.7873 Entailement-based Consistency 0.0762 0.0802 0.1077 -0.0521 0.1077 0.0955 0.0902 0.0852 Keyphrase-based Consistency 0.3450 0.3348 0.3455 0.2060 0.3455 0.3640 0.3688 0.3687 LLM-as-a-Judge 0.9045 0.8959 0.8954 0.9177 0.8954 0.8945 0.8954 0.8998 Citation Correctness Citation Recall 0.6326 0.6193 0.5574 0.5350 0.5574 0.5930 0.6083 0.6076 Citation Precision 0.8382 0.8548 0.9357 0.8942 0.9357 0.9506 0.9470 0.9515 LLM-as-a-Judge (Citation Quality) 0.8492 0.8388 0.8051 0.8243 0.8051 0.7901 0.7855 0.7823 Narrative Quality Perplexity ↓ 21.4601 26.2687 18.4870 19.0518 18.4870 19.1426 23.0976 19.6214 LLM-as-a-Judge 0.8997 0.8950 0.9001 0.8976 0.9001 0.8999 0.8965 0.8987 Table 3: Performance of LLaMA 4-Maverick on NAACL dataset under two prompting paradigms: Instruction-Level (Short, Medium, Elaborate, AutoCoT) and Few-Shot (Zero-Shot, One-Shot, Two-Shot, Three-Shot). Best scores in each paradigm are bolded. Note: Lower perplexity values indicate better performance. correctness for ICLR points to its strength in precise ref- erencing. However, the tight score ranges across models highlight a limitation of LLM-as-a-Judge: it may"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 9, "text": "Instruction-Level (Short, Medium, Elaborate, AutoCoT) and Few-Shot (Zero-Shot, One-Shot, Two-Shot, Three-Shot). Best scores in each paradigm are bolded. Note: Lower perplexity values indicate better performance. correctness for ICLR points to its strength in precise ref- erencing. However, the tight score ranges across models highlight a limitation of LLM-as-a-Judge: it may struggle to discern fine-grained differences, potentially underesti- mating the unique capabilities of open-source models like LLaMA4, which rival the closed-source GPT-4o in rigorous academic settings. 5.1 Analysis on Few-shot Experiments In Table 3, we present the results for few-shot prompting experiments (Zero-Shot, One-Shot, Two-Shot, Three-Shot) on the NAACL dataset. The results reveal noticeable im- provements with an increasing number of examples. Specif- ically, Three-Shot prompting consistently outperforms other few-shot settings in lexical overlap (ROUGE-1, ROUGE-2, ROUGE-L) and reference-free content coverage, suggesting that additional exemplars guide the model toward capturing finer lexical details and comprehensive content. However, improvements plateau after Two-Shot, indicating diminish- ing returns beyond a certain number of examples. Moreover, higher SummaC-ZS faithfulness scores in Three-Shot setups reflect that additional contexts help the model better adhere to the factual correctness of the input, highlighting the ben- efit of illustrative examples in complex generation tasks. 5.2 Analysis with Various Prompt Strategies In Table 3, we present the results depicting the impact of prompt elaboration (Short, Medium, Elaborate, AutoCoT) on the NAACL dataset. Medium-length prompts achieve higher lexical overlap (ROUGE scores), suggesting that moderate complexity is optimal for balancing detail with model understanding. Conversely, AutoCoT prompts re- sult in superior METEOR and Keyphrase-based faithfulness scores, implying that models benefit significantly from de- tailed reasoning steps when precision in conceptual align- ment is required. However, AutoCoT notably increases per- plexity, indicating potential trade-offs between detailed in- structional clarity and narrative fluency. Thus, prompt com- plexity should be carefully adjusted depending on whether the task prioritizes lexical fidelity, conceptual alignment, or narrative coherence. 6 Ablation Study We conduct an ablation study to examine the effect of vary- ing input configurations on the quality of generated intro- ductions. Specifically, we compare three setups: (i) Title- only, (ii) Title + Abstract, and (iii) Title + Abstract + Re- lated Work, under two prompting strategies: ELABORATE and MEDIUM. As shown in Table 4, we evaluate perfor- mance using a comprehensive set of lexical, semantic, and LLM-based metrics. The results show a consistent trend: including more con- text leads to better generation quality. The T+A+Related configuration achieves the best scores across nearly all met- rics and prompting strategies, with notable gains in se- mantic similarity (e.g., BERTScore: 0.8448 →0.8471), ci- Category Metric ELABORATE MEDIUM Title-only Title+Abstract T+A+Related Title-only Title+Abstract T+A+Related Lexical Overlap ROUGE-1 0.3545 0.4093 0.4402 0.3356 0.3949 0.4513 ROUGE-2 0.0772 0.1202 0.1509 0.0660 0.1084 0.1625 ROUGE-L 0.1553 0.1784 0.1894 0.1436 0.1700 0.1940 BLEU 0.0503 0.0729 0.1198 0.0223 0.0472 0.1345 METEOR 0.2150 0.2428 0.2700 0.1816 0.2185 0.2797 Semantic Similarity BERTScore 0.8324 0.8408 0.8448 0.8235 0.8320 0.8471 BLEURT 0.3189 0.3333 0.3339 0.3303 0.3365 0.3312 Contextual Relevance 0.9651 0.9657 0.9684 0.9633 0.9650 0.9690 Content Coverage Reference-based Coverage 0.3918 0.4805 0.4845 0.3878 0.4715 0.4697 Reference-free Coverage 0.4504 0.4771 0.4367 0.4602 0.4953 0.4231"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 10, "text": "0.1345 METEOR 0.2150 0.2428 0.2700 0.1816 0.2185 0.2797 Semantic Similarity BERTScore 0.8324 0.8408 0.8448 0.8235 0.8320 0.8471 BLEURT 0.3189 0.3333 0.3339 0.3303 0.3365 0.3312 Contextual Relevance 0.9651 0.9657 0.9684 0.9633 0.9650 0.9690 Content Coverage Reference-based Coverage 0.3918 0.4805 0.4845 0.3878 0.4715 0.4697 Reference-free Coverage 0.4504 0.4771 0.4367 0.4602 0.4953 0.4231 LLM-as-a-Judge 0.5425 0.6916 0.7019 0.5783 0.6916 0.7101 Faithfulness QA-based Faithfulness 0.7662 0.7808 0.7794 0.7849 0.7858 0.7767 Entailment-based Faitfulness 0.0238 0.1430 0.1077 -0.0854 0.0120 0.0505 Keyphrase-based Faithfulness 0.3523 0.3788 0.3455 0.3495 0.3898 0.2183 LLM-as-a-Judge 0.7288 0.8557 0.8157 0.8110 0.8557 0.8382 Consistency QA-based Consistency 0.7780 0.7849 0.7860 0.7863 0.7889 0.7786 Entailment-based Consistency 0.0881 0.1261 0.1077 0.0223 0.0579 0.0802 Keyphrase-based Consistency 0.1843 0.2210 0.2393 0.1780 0.3898 0.3348 LLM-as-a-Judge 0.7872 0.8977 0.8954 0.8430 0.8977 0.8959 Citation Correctness Recall 0.0918 0.1199 0.5574 0.0001 0.0039 0.6193 Precision 0.2399 0.3610 0.9357 0.0012 0.0175 0.8548 LLM-as-a-Judge (Citation Quality) 0.5174 0.5184 0.8051 0.3696 0.5184 0.8388 Narrative Quality Perplexity ↓ 14.5978 19.9132 18.4870 16.6799 20.7426 26.2687 LLM-as-a-Judge 0.9000 0.8986 0.9001 0.9010 0.8986 0.8950 Table 4: Ablation Study. Performance of LLaMA 4-Maverick on NAACL 2025 dataset with different input configurations: Title-only, Title+Abstract, and Title+Abstract+Related Work. Note: Lower perplexity values indicate better performance. tation precision (e.g., 0.3610 →0.9357), citation recall (e.g., 0.1199 →0.5574) when related work is added. LLM- as-a-Judge citation quality also increases from 0.5184 to 0.8051. These findings underscore the importance of richer input—particularly related work—in enabling LLMs to gen- erate more accurate, well-grounded, and coherent introduc- tions. 7 Human Evaluation Metric LLaMA4 0-shot LLaMA4 3-shot LLaMA4 AutoCoT GPT-4o 0-shot Faithfulness 4.20 4.70 4.11 4.35 Consistency 3.80 4.50 3.84 4.10 Content Coverage 4.05 4.50 3.74 4.05 Flow of Ideas 4.30 4.60 4.26 4.55 Citation Contextual Quality 3.40 4.20 3.32 3.60 Hallucination Resistance 4.60 5.00 4.37 4.60 Literature Context 3.85 4.40 3.95 4.00 Motivation Clarity 4.15 4.50 4.21 4.40 Methods Summary 3.65 4.25 3.79 3.85 Contributions Summary 3.30 4.20 3.37 3.74 Table 5: Human evaluation of different models across ten qualitative criteria. We conducted a human evaluation on 20 source exam- ples, each with four model generations, LLaMA 4-Maverick (0-shot, 3-shot, AutoCoT) and GPT-4o (0-shot), totaling 80 outputs. The three authors performed the annotations them- selves, each annotating 6–7 papers (four generations each). Due to the unavailability of domain expert annotators, no paper was annotated by multiple annotators. This single- annotation approach is common in recent benchmarking studies such as Hossain et al. (2025) and appropriate here since the task is purely benchmarking, not proposing new models. Evaluating 50–100 outputs aligns with established LLM evaluation norms. The annotation form is included in Appendix 7, and results are summarized in Table 5. Across comparisons, we observe several clear trends. First, GPT-4o outperforms 0-shot LLaMA-4 on most criteria—particularly consistency, citation contextual qual- ity, and contribution summary—though LLaMA-4 shows stronger hallucination resistance. Second, 3-shot prompt- ing leads to the strongest overall performance, achiev- ing the highest scores in content coverage, flow of ideas, and clarity of methods and contributions. Finally, ELAB- ORATE 0-shot prompting surpasses AutoCoT across all dimensions, with notable gains in citation quality and faith- fulness, emphasizing the value of carefully designed instruc- tion"}
{"doc_id": "2508.14273v1", "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14273v1", "chunk_id": 11, "text": "leads to the strongest overall performance, achiev- ing the highest scores in content coverage, flow of ideas, and clarity of methods and contributions. Finally, ELAB- ORATE 0-shot prompting surpasses AutoCoT across all dimensions, with notable gains in citation quality and faith- fulness, emphasizing the value of carefully designed instruc- tion formats. These results highlight the importance of both model selection and prompting strategy in improving the quality and reliability of LLM-generated scientific introduc- tions. Refer Appendix B to see the generated samples. 8 Discussion Building on the findings presented in this paper, we address two central questions regarding the role of large language models (LLMs) in scientific writing. First, are LLMs good enough to generate the first draft of a research paper’s in- troduction? Our automated and human evaluations indicate that, when provided with structured prompts—including a clear set of instructions, a meaningful title and abstract, and a relevant set of related works—LLMs are capable of gen- erating coherent, well-structured, and stylistically appropri- ate introductions that align with academic norms. Second, can the LLM-generated introduction be used as-is? Not en- tirely. While LLMs produce high-quality drafts that serve as strong starting points, they fall short in several critical aspects, including the incorporation of fine-grained techni- cal details from cited literature, accurate contextualization of citations, and precise articulation of the target paper’s con- tributions. As such, the generated introductions require sub- stantial post-editing and expert input. These findings suggest that LLMs are promising assistants in the scholarly writing pipeline, but they are not yet suitable replacements for hu- man authorship. 9 Ethical Statement This study investigates the capabilities of LLMs for gener- ating scientific paper introductions, with the goal of under- standing their potential as assistive tools for researchers. We explicitly do not advocate for autonomous use of LLMs in scholarly writing. While LLMs can produce coherent and stylistically appropriate drafts, our evaluation reveals persis- tent challenges such as factual inaccuracies, improper cita- tion use, and limited domain grounding, all of which neces- sitate human validation. We emphasize that any generated content should be re- viewed and revised by human authors to ensure accuracy, integrity, and alignment with academic standards. This work is intended to benchmark current model capabilities and highlight where human involvement remains indispensable. We support responsible deployment of LLMs and caution against their use in critical academic workflows without ex- pert oversight."}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 0, "text": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text Zixin Rao1*, Youssef Mohamed2*, Shang Liu3, and Zeyan Liu3** 1 University of Georgia zr04546@uga.edu 2 Egypt-Japan University of Science and Technology youssef.khalil@ejust.edu.eg 3 University of Louisville {shang.liu, zeyan.liu}@louisville.edu Abstract. Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated remarkable abilities in generating natural language. However, they also pose security and integrity challenges. Existing coun- termeasures primarily focus on distinguishing AI-generated content from human-written text, with most solutions tailored for English. Mean- while, authorship attribution–determining which specific LLM produced a given text–has received comparatively little attention despite its impor- tance in forensic analysis. In this paper, we present DA-MTL, a multi-task learning framework that simultaneously addresses both text detection and authorship attribution. We evaluate DA-MTL on nine datasets and four backbone models, demonstrating its strong performance across mul- tiple languages and LLM sources. Our framework captures each task’s unique characteristics and shares insights between them, which boosts performance in both tasks. Additionally, we conduct a thorough anal- ysis of cross-modal and cross-lingual patterns and assess the frame- work’s robustness against adversarial obfuscation techniques. Our find- ings offer valuable insights into LLM behavior and the generalization of both detection and authorship attribution. Our source code is available at https://github.com/youssefkhalil320/MTL_training_two_birds_ with_one_stone. Keywords: AIGC Detection · Forensics · Responsible AI · Large Lan- guage Models 1 Introduction Neural language generation has revolutionized natural language processing, en- abling the creation of highly coherent, contextually relevant, and human-like text [70]. Powered by advances in Large Language Models (LLMs), such as ⋆These authors contributed equally to this work, which was conducted during their internship at the University of Louisville. ⋆⋆Corresponding author. GPT-4 [1] and Llama [61,62], these models have achieved extraordinary perfor- mance across diverse applications, including education [71], medicine[57], soft- ware engineering[22], creative industries [17], and so on. However, the prolifer- ation of LLM-generated content has also raised widespread concerns related to integrity, security, creativity, and ethics [72,3,11,35]. Meanwhile, the accessibility and ease of use of commercial tools such as ChatGPT [43] and Google Gemini [15] have further magnified these risks. For example, the proportion of articles generated by LLMs on misinformation websites has grown rapidly [19], and as of December 2023, 26.1% of arXiv submissions included ChatGPT-generated content [34]. In addition, threat actors have exploited LLMs for malicious pur- poses such as phishing and website hacking [46,16,12]. These trends highlight the urgent need for effective mechanisms to mitigate the social impacts of LLM- generated content. Detecting LLM-generated text has emerged as a critical and commonly used solution to address these challenges, both in research and in practice [58,55]. Existing studies primarily focus on identifying whether a given text is human- written or AI-generated, with most studies centered on English or closely related languages and GPT models [74,32,18,33,63,34]. However, these approaches leave gaps in understanding how detection generalizes across diverse languages and LLM architectures. Recent works, such as MULTITuDE[36] and M4 [68,69], have started to explore multi-lingual and multi-modal detection, but questions regarding cross-linguistic and cross-modal generalization remain unanswered. Authorship attribution, which identifies the"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 1, "text": "and GPT models [74,32,18,33,63,34]. However, these approaches leave gaps in understanding how detection generalizes across diverse languages and LLM architectures. Recent works, such as MULTITuDE[36] and M4 [68,69], have started to explore multi-lingual and multi-modal detection, but questions regarding cross-linguistic and cross-modal generalization remain unanswered. Authorship attribution, which identifies the specific AI model responsible for generating a piece of text, is an equally important but underexplored challenge. With the rapid proliferation of LLMs-hundreds being developed each week-and the growing sophistication of generated text [13], accurately attributing text to its source model is becoming increasingly difficult. At the same time, this task is critical for applications such as intellectual property protection [24]. Unlike detection, attribution requires a more nuanced analysis of characteristics. This makes it a technically complex but essential problem to regulate and understand AI-generated content. However, most existing studies focus only on binary classi- fication between human-written and machine-generated text. In addition, many recent studies on authorship attribution overlook human-written text or simply treat it on par with individual LLMs [64,41,7,52]. In this paper, we introduce a new LLM forensics framework called DA-MTL, which simultaneously tackles text detection and authorship attribution using multi-task learning. Our approach uses shared representations to solve two tasks: (1) distinguishing between human-written and machine-generated text, and (2) identifying the specific source LLM of machine-generated text. We evaluate the framework on multiple backbone models, datasets, and languages, demonstrating its superior performance. Notably, our framework improves both tasks. Based on our accurate models, we explore cross-modal similarities, investigating how shared patterns among LLMs influence their proximity in feature space and their attribution. We also examine how LLMs differ using stylometric analysis and how our forensics models trained in one language family generalize to others. Finally, we test the robustness of the proposed framework against obfuscation techniques, showing its ability to handle content post-processed by adversaries. Specifically, we aim to answer the following research questions: – RQ1: Can multi-task learning improve text detection and attribution per- formance compared to single-task baselines? – RQ2: Which LLMs are similar, and how do these similarities impact attri- bution performance? – RQ3: How does DA-MTL generalize across languages? Our main contributions include: 1. We present a multi-task learning framework, DA-MTL, to jointly solve text detection and attribution, which shows outstanding performance. 2. We comprehensively analyze cross-modal and cross-lingual generalization, which provides deep insights into LLM proximity and behaviors. 3. We demonstrate the robustness of DA-MTL against adversarial obfuscation techniques, showcasing its effectiveness in complex real-world scenarios. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 formalizes the problem and presents our DA-MTL framework. Section 4 shows experimental results and analysis, followed by our conclusions in Section 5. 2 Related Work Machine-Generated Text Detection: The growing capabilities of language models to generate realistic text have led to a significant increase in research efforts for machine-generated text (MGT) detection. Typically framed as bi- nary classification, MGT detection distinguishes between machine-generated and human-written text. Early methods relied on linguistic and statistical discrep- ancies, employing classical feature-based classifiers [14,39,54]. Recent"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 2, "text": "of language models to generate realistic text have led to a significant increase in research efforts for machine-generated text (MGT) detection. Typically framed as bi- nary classification, MGT detection distinguishes between machine-generated and human-written text. Early methods relied on linguistic and statistical discrep- ancies, employing classical feature-based classifiers [14,39,54]. Recent advances have embraced fine-tuned transformer models such as RoBERTa, achieving per- formance improvements in identifying subtle stylistic and structural differences [50,18,34,33]. Furthermore, the introduction of comprehensive datasets and cor- pora, such as those featuring AI-generated responses to consistent instructions, has significantly advanced this field by offering benchmarks [33,34,36,68,18,20]. Authorship Attribution: Authorship attribution is a traditional problem in NLP, which identifies the author of a text based on linguistic and stylistic cues [51]. In the context of LLMs, attributing machine-generated texts is particu- larly important to ensure proprietary and ethical responsibility when using AI- generated content [64]. LLMs often embed unique stylistic signatures, which have been utilized by many methods [64,66,23]. Furthermore, advanced models such as XLM-R and RoBERTa also demonstrated effectiveness [67]. However, there are still challenges like cross-domain attribution and complex datasets, emphasizing the need for continued advances in this area [53,28]. In adversarial scenarios, it is also crucial that attribution techniques be robust against intentional obfuscation techniques employed to conceal or mask authorship [29,2,37,38]. Multi-task Learning for NLP: Multi-task learning (MTL) has emerged as a foundational approach to improve performance across related tasks. Initially introduced by Caruana [6], MTL has been shown to improve generalization and reduce overfitting by regularizing models through shared knowledge [47,49]. In NLP, early applications demonstrated its utility for tasks such as part-of-speech tagging and sentiment analysis [4,10], while language models like T5 [45] suc- cessfully framed multiple NLP tasks as text generation problems. Cross-lingual tasks have also benefited from MTL [56], while optimization techniques have been developed to address cross-task gradient conflicts [73]. In this paper, we extend MTL by applying it to MGT detection and attribution. 3 The DA-MTL Framework 3.1 Problem Definition and Assumptions We specify two tasks in our paper: LLM Detection (Task 1 or TD) and LLM Attribution (Task 2 or TA). Specifically, Task 1 determines whether a given text is human-written or generated by an LLM. Task 2 identifies the specific LLM responsible for generating the text, which involves multiple classes. Formally, given a dataset D = {(xi, yTD,i, yTA,i)}N i=1, where xi is a piece of text, yTD,i ∈{0, 1} is the binary label for detection, and yTA,i ∈{1, 2, . . . , C} is the multi-class label for attribution, and N is the total number of samples. We assume that the data are sampled i.i.d. from a joint distribution P(x, yTD, yTA), or P in short. Our goal is to optimize a multi-task classifier with parameters θ by maximizing the likelihood for both TA and TD, which can be approximated by joint optimization as: min θ L = Ex∼P(x)   X T ∈{TD,TA} EyT ∼P(yT |x) \u0002 LT (fT (x; θ), yT ) \u0003   (1) where LD is the binary cross-entropy loss for detection"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 3, "text": "by maximizing the likelihood for both TA and TD, which can be approximated by joint optimization as: min θ L = Ex∼P(x)   X T ∈{TD,TA} EyT ∼P(yT |x) \u0002 LT (fT (x; θ), yT ) \u0003   (1) where LD is the binary cross-entropy loss for detection and LA is the categorical cross-entropy loss for attribution. Our primary objective is to determine whether a given text was written by human beings or by one of several possible LLM candidates. We assume a black-box defender, which is widely adopted for AI-generated content detection [58,9,40]. Specifically, we have no access to the LLM’s internal weights, archi- tecture, or gradients. Instead, our analysis relies solely on the model’s output. This assumption is necessary and practical for two key reasons: (1) Many lead- ing commercial LLMs, including OpenAI’s GPT-4 [1,43], Google’s Gemini [15], and Anthropic’s Claude [5], are proprietary and closed-source. Meanwhile, these models also have the greatest real-world impact with the largest user base. (2) Security-critical AI-generated content may appear in public domains without clear authorship, such as social media posts or research articles. In such cases, a black-box approach is the most effective and feasible way to analyze and verify content in real-world forensic scenarios. 3.2 Key Observations and Motivation LLM detection and attribution inherently involve differences in granularity. Al- though optimized to mimic human distribution, LLM outputs typically present distinct differences from natural human data [18], making Task 1 relatively coarse-grained and straightforward. In contrast, Task 2 could be subtle (e.g., ChatGPT vs. GPT4All), as different LLMs sometimes share architectural sim- ilarities and stylistic traits [42]. Toward a better understanding, we showcased two key observations on the Essay dataset in MGTBench [20]: – Observation I: Task 1 is easier. We trained two binary classifiers for simplified Task 1 and 2. The training loss curves, shown in Figure 1, reveal a significant gap in dynamics. The loss for the human versus ChatGPT classifier decreases rapidly and stabilizes early, indicating that this task is much easier to learn. In contrast, the loss for the GPT4All versus ChatGPT classifier decreases more slowly and exhibits greater variability, indicating increased difficulty. – Observation II: Human data attribution is distinct. We trained a single multi-class classifier for text generated by human beings as well as different LLMs. We visualized the feature representations using PCA. As shown in Figure 1, the human data form a distinct cluster, while many LLMs’ representations overlap significantly. This suggests that a single classifier might struggle to reconcile both coarse and fine-grained distinctions. One alternative solution is to remove human data from attribution and build a separate, dedicated classifier for detection. However, this approach has two limitations: Firstly, in real-world applications, attributing the authorship of un- labeled text to either a human or one of the LLMs in a unified way is more interesting and practically useful. Secondly, two tasks can hypothetically benefit from each other: (1) understanding subtle stylistic and distributional differences among LLMs enriches the representation, and (2) human-generated content can be a regularizer for attribution, helping with capturing broader"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 4, "text": "or one of the LLMs in a unified way is more interesting and practically useful. Secondly, two tasks can hypothetically benefit from each other: (1) understanding subtle stylistic and distributional differences among LLMs enriches the representation, and (2) human-generated content can be a regularizer for attribution, helping with capturing broader linguistic fea- tures. We validate these hypotheses in Sec 4.3 by showing how joint training improves both tasks. These observations motivate us to decompose tasks, where we explicitly re- tain human-written text in the label space and design a framework to jointly handle detection and attribution. Our goal is to achieve a dual benefit by taking advantage of both human and MGT data to maximize overall performance. 3.3 Methodology Our framework consists of a shared encoder and two task-specific heads for detection and attribution. Given an input text x, the shared encoder produces a d-dimensional feature vector h. h = SharedEncoder(x, a) (2) Fig. 1. Left: The training losses for ChatGPT-Human (the orange line) and ChatGPT- GPT4All (the blue line). The training losses decrease much more rapidly when distin- guishing human data. Right: PCA visualization of the features, showing a distinct and separate cluster for human data, far from the clusters of LLMs. Next, each task-specific head applies a linear transformation to h, yielding logits for the respective classification tasks. For detection (Task 1), we have a binary classification head, and for attribution (Task 2), we have a C-ary classi- fication head: pD(x) = σ(WDh + bD), pA,j(x) = eWA,jh+bA,j PC k=1 eWA,kh+bA,k (3) where WD ∈R1×d, WA ∈RC×d are the weights for detection and attribution heads, and bD, bA ∈R are the biases. C is the number of LLM candidates. σ(·) denotes the sigmoid function. The cross-entropy losses for Task 1 and 2 can be computed as: LD = −1 N N X i=1 h yTD,i log(pD(xi)) + (1 −yTD,i) log(1 −pD(xi)) i LA = −1 N N X i=1 C X j=1 yTA,i,j log(pA,j(xi)) (4) During optimization, gradients for each task are computed independently as ∇θLD and ∇θLA and then combined to update the model parameters: θ ←θ −η (λD · ∇θLD + λA · ∇θLA) (5) where η is the learning rate. In our paper, we set λD = λA = 0.5. Design Choice. Conflicting optimization objectives of different tasks is a com- mon issue in multi-task learning, which can hinder performance. However, we found that adding PCGrad [73], which projects TD’s gradients onto the nor- mal plane of TA’s gradients, only yields minimal improvements but significantly increases the time and GPU memory costs. Therefore, we choose not to use PCGrad. 4 Experiments We implemented DA-MTL with PyTorch 2.10.2, Transformers 4.44.2, and Python 3.10.2 on Ubuntu 22.04.3. The experiments are conducted using an Nvidia L4 GPU and an Intel Xeon CPU. Pre-trained model parameters are obtained from Huggingface. For training, we adopt an AdamW optimizer with an initial learn- ing rate of 1e-5, a maximum of 30 epochs, and an early-stopping strategy to stop when the validation F1 score does not increase for six epochs. Detailed experiment settings are"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 5, "text": "Xeon CPU. Pre-trained model parameters are obtained from Huggingface. For training, we adopt an AdamW optimizer with an initial learn- ing rate of 1e-5, a maximum of 30 epochs, and an early-stopping strategy to stop when the validation F1 score does not increase for six epochs. Detailed experiment settings are listed in Table 1. Table 1. Experiment Settings and Details. Setting Details Environment Ubuntu 22.04.3, Python 3.10.2, PyTorch v2.4.1+cu121, Transformers=4.44.2 Hardware NVIDIA L4 GPU & Intel Xeon CPU @ 2.20GHz Optimizer AdamW Scheduler Linear Train/Val Split 80/20 (if not specified by the dataset) Hyperparameters LR = 1×10−5, Batch Size=8 to 32, Dropout=0.5, Max Epochs=30, Weight Decay=0.01, Random Seed=42 Early Stopping Val F1-score does not increase for 6 epochs 4.1 Basic Results We first evaluate DA-MTL on mono-lingual tasks. We select English, as it is the most widely studied language in NLP. Specifically, we used nine different datasets from three benchmarks: – MGTBench: A benchmark for detecting AI-generated text across multiple genres. It includes three domains: Essay, WritingPrompt (WP), and Reuters, which cover diverse writing styles, from formal essays to creative writing and news articles [20]. The dataset features text from six major LLMs: ChatGPT-turbo, ChatGLM, Dolly, GPT4All, StableLM, and Claude, with each domain containing 1,000 samples per generator. – MULTITuDE: A large-scale dataset comprising both AI-generated and human-written news articles [36]. It includes 74,081 samples spanning 11 languages, generated by eight multilingual LLMs, namely GPT-3, ChatGPT, GPT-4, Llama-65B, Alpaca-LoRa-30B, Vicuna-13B, OPT-66B, and OPT- IML-Max-1.3B. – M4: A benchmark for multi-generator, multi-domain, and multi-source AI- generated text detection. It includes model outputs across various domains such as Wikipedia, Reddit, WikiHow, PeerRead, and arXiv paper ab- stracts. The dataset features text from six LLMs: GPT-4, ChatGPT, GPT- 3.5, Cohere, Dolly-v2, and BLOOMz-176B, which contains 2,344 to 3,000 samples for each generator and domain. The selected datasets comprehensively span a wide range of doamins, includ- ing academic writing, news articles, open-ended question-and-answer tasks, user reviews, and instructional materials. They also represent text generated by two categories of LLMs: (1) Conversational Agents: MGTBench and M4 feature responses from application-level LLMs such as ChatGPT, Claude, ChatGLM, and GPT4All, which are designed for real-world deployment via user interface or APIs. These models are more likely to be used in practical settings, rais- ing security concerns; and (2) Foundational NLP Models: MULTITuDE and M4 include text data composed by cutting-edge open-source language models like LLaMA, Vicuna, OPT, and Alpaca. These models are widely employed for research and development, offering a flexible foundation for advanced NLP tasks. We compare DA-MTL with two categories of baseline models that are com- monly adopted in the literature: (1) Feature-based ML Classifiers: These classifiers rely on traditional text embeddings, such as TF-IDF and Bag-of- Words. They have been widely used for various text classification tasks, includ- ing MGT detection [25,26,32,34,31,76,30]. We evaluated four machine learning models: Multinomial Naive Bayes (MNB), Linear Regression (LR), Support Vec- tor Machines (SVM), and Random Forests (RF). (2) Fine-tuned Language Models These models are fully fine-tuned on each dataset. We select BERT [27,74,25,65,32], RoBERTa [65,31,18,33,36,68], DistilBERT[48,20], and GPT-2 [74,34]."}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 6, "text": "tasks, includ- ing MGT detection [25,26,32,34,31,76,30]. We evaluated four machine learning models: Multinomial Naive Bayes (MNB), Linear Regression (LR), Support Vec- tor Machines (SVM), and Random Forests (RF). (2) Fine-tuned Language Models These models are fully fine-tuned on each dataset. We select BERT [27,74,25,65,32], RoBERTa [65,31,18,33,36,68], DistilBERT[48,20], and GPT-2 [74,34]. We also include mBERT [27] and XLM-R [8], which are designed for multi-lingual problems. In total, we evaluate 10 baseline models across nine datasets, resulting in 90 trained baseline classifiers. For these baseline models, human-written text is treated equally as a separate category alongside LLM- generated content, framing the problem as a single multi-class classification task. To answer RQ1, we choose four backbone models for comparison with vary- ing parameter sizes, ranging from 67 million to 278 million parameters: Distil- BERT, mBERT, XLM-R, and RoBERTa. These models are trained using DA- MTL under the same settings as the baselines. This helps evaluate the general applicability of our DA-MTL framework. As shown in Table 2, we have the following findings: (1) Feature-based ma- chine learning classifiers perform poorly. Especially on the MULTITuDE dataset, they achieve an average F1 score of just 46.8%. In contrast, fine-tuned lan- guage models perform significantly better, with F1 scores exceeding 86% on all datasets except MULTITuDE. However, even for MULTITuDE, their perfor- mance remains unsatisfactory, with an average F1 score of 71%. (2) All four DA-MTL models outperform their corresponding baselines trained without DA- MTL. On the Essay, WP, and Reuters datasets, DA-MTL yields average F1 score improvements of 2.7%, 2.1%, and 1.2%, respectively. The largest improvement is observed on MULTITuDE, where DA-MTL boosts performance by an average of 4.2%. It also enhances F1 scores on M4, improving by 3.1%, 0.5%, 1.7%, 1.2%, and 1.7% for arXiv, PeerRead, Reddit, Wikihow, and Wikipedia, respectively. (3) We find that DA-MTL is particularly effective on more challenging datasets (such as Essay, MULTITuDE, and arXiv) and with less optimal backbone models (such as XLM-R). The most substantial single improvement occurs with XLM-R on MULTITuDE, where DA-MTL increases the F1 score by 6.5%. Across all nine datasets, DA-MTL improves F1 scores by an average of 1.9% for DistilBERT, 2.0% for mBERT, 2.7% for XLM-R, and 1.6% for RoBERTa. This aligns with our expectation that multi-task learning mitigates the dominance of either task, which is more pronounced in overfitted models with larger parameter sizes. (4) Among all classifiers, RoBERTa with DA-MTL achieves the best performance on most of the datasets, except on MULTITuDE, PeerRead and WikiHow, where XLM-R performs best. Takeaway I DA-MTL is generally applicable and effective across different LLMs and domains. Efficiency: DA-MTL is notably lightweight, adding only a few thousand param- eters compared to the hundreds of millions in the encoder. Although training time per epoch is roughly doubled (because the model is invoked twice), DA- MTL achieves good performance in just a few epochs. For instance, DistilBERT reaches its optimal performance in 11 epochs, but with DA-MTL, it achieves the same in just 3 epochs. XLM-R with DA-MTL takes 4 epochs to match the perfor- mance of the baseline trained"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 7, "text": "model is invoked twice), DA- MTL achieves good performance in just a few epochs. For instance, DistilBERT reaches its optimal performance in 11 epochs, but with DA-MTL, it achieves the same in just 3 epochs. XLM-R with DA-MTL takes 4 epochs to match the perfor- mance of the baseline trained with 8 epochs on WP. These results demonstrate that DA-MTL is both effective and efficient in terms of resource usage, time, and storage. Ablation Study: The balance between detection loss (LD) and attribution loss(LA) is crucial for optimizing DA-MTL. By default, we set their weighting factors (λD and λA, as defined in Eq. 5) to 0.5 each. However, our framework allows users to adjust these weights based on their specific needs and task pri- orities. To analyze the impact of this weighting choice, we conduct an ablation study by varying λD and λA. We evaluate RoBERTa on six datasets where DA- MTL achieves the most significant improvements: Essay, WP, and Reuters from MGTBench, MULTITuDE, as well as arXiv and Reddit from M4. The results are summarized in Table 3. Increasing the detection weight (λD = 0.7, λA = 0.3, λD > λA) slightly improves binary detection performance but significantly reduces attribution accuracy. This supports our observation that detection is generally easier than authorship attribution. Conversely, prior- itizing attribution (λD = 0.3, λA = 0.7, λD < λA) enhances LLM differentiation but weakens detection performance. Notably, on three of the six datasets (WP, Reuters, and Reddit), attribution performance with a higher λA is even lower than our default setting. These findings suggest that attribution benefits from strong detection, and setting λD = λA = 0.5 provides a balanced trade-off, ensuring high detection accuracy while maintaining reliable attribution perfor- mance. Table 2. Classification F1 scores (%), number of parameters, and model sizes (GB) of DA-MTL and SOTA models. WP: WritingPrompt. RT: Reuters. MU: MULTITuDE. PR: PeerRead. RD: Reddit. WH: Wikihow. Wiki: Wikipedia. Para.: Number of model parameters. Model Essay WP RT MU arXiv PR RD WH Wiki Para. Size (a) Feature-based ML Classifiers TF-IDF + MNB 33.2 49.1 36.9 34.5 56.5 56.0 33.0 31.0 38.6 - 0.12 TF-IDF + LR 58.6 72.5 71.4 55.6 88.2 94.4 77.0 76.8 77.6 - 0.06 BoW + SVM 68.9 68.8 81.9 54.3 88.9 95.0 81.9 76.9 76.7 - 0.02 BoW + RF 66.3 62.8 76.7 42.6 82.1 94.3 76.0 75.9 78.6 - 0.05 (b) Fine-tuned Language Models BERT 88.6 86.3 90.2 69.1 95.7 98.8 87.7 94.0 90.0 110M 0.42 GPT-2 89.8 94.1 94.9 74.9 97.5 99.3 93.4 96.3 92.5 124M 0.55 DistilBERT 87.2 87.1 91.8 70.4 96.4 98.8 92.0 95.1 90.4 67M 0.26 mBERT 90.4 89.9 92.8 70.2 94.8 99.0 91.4 94.8 91.7 178M 0.68 XLM-R 87.3 87.2 92.1 70.6 95.2 98.8 91.8 94.7 90.0 278M 1.08 RoBERTa 95.7 94.0 95.2 73.3 94.4 99.0 93.2 96.0 92.7 125M 0.48 (c) DA-MTL DistilBERT 91.6 89.5 93.6 72.8 98.2 99.4 92.9 95.9 91.9 67M 0.26 mBERT 92.7 90.8 93.1 75.5 98.4 99.3 94.3 95.9 92.9 178M 0.68 XLM-R 90.3 90.7 93.2 77.1 98.1 99.4 92.6 96.9 93.3 278M 1.08 RoBERTa"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 8, "text": "95.7 94.0 95.2 73.3 94.4 99.0 93.2 96.0 92.7 125M 0.48 (c) DA-MTL DistilBERT 91.6 89.5 93.6 72.8 98.2 99.4 92.9 95.9 91.9 67M 0.26 mBERT 92.7 90.8 93.1 75.5 98.4 99.3 94.3 95.9 92.9 178M 0.68 XLM-R 90.3 90.7 93.2 77.1 98.1 99.4 92.6 96.9 93.3 278M 1.08 RoBERTa 96.6 95.6 96.6 75.9 98.4 99.3 95.4 96.5 93.3 125M 0.48 Table 3. F1 scores (%) of RoBERTa trained with DA-MTL and different λD, λA weight values. WP: WritingPrompt. RT: Reuters. MU: MULTITuDE. RD: Reddit. The highest F-1 score result for each dataset is marked in bold. λD / λA TD TA Essay WP RT MU arXiv RD Essay WP RT MU arXiv RD 0.3 / 0.7 99.7 99.1 99.8 95.5 97.8 99.4 96.7 93.0 95.7 74.5 99.0 92.7 0.7 / 0.3 99.5 98.6 99.8 96.7 97.8 99.7 94.6 93.8 95.3 72.1 98.4 92.3 0.5 / 0.5 99.8 99.1 99.9 96.3 98.4 99.3 96.0 95.0 96.1 73.4 98.4 94.7 4.2 Cross-modal Proximity Analysis To better understand LLM proximity and address RQ2, we examine the error patterns through the confusion matrices of MULTITuDE, which is the most chal- lenging dataset. We evaluate DistilBERT and XLM-R, which have the largest and smallest parameter sizes, respectively. As shown in Figure 2, our analysis re- veals consistent error patterns across classifiers: Alpaca-LoRA-30B is frequently misclassified as Text-DaVinci-003 and GPT-3.5, while Vicuna-13B and GPT-3.5 often confuse classifiers. Interestingly, relatively advanced models like OPT-66B are sometimes misclassified as smaller models, like OPT-IML-MAX-1.3B, with LLaMA-65B also being a common error. Conversely, smaller models, including OPT-IML-MAX-1.3B and those in the GPT family, are less likely to be misclas- sified as their more advanced counterparts. Fig. 2. Confusion matrices of DistilBERT and XLM-R on MULTITuDE. The lineage of LLMs provides valuable insight into their similarities and differences: (1) OPT and LLaMA (\"OPT-like\"): Both families were de- veloped by Meta (formerly Facebook AI Research). Their base models focus on pure next-token prediction without specific optimization for user interac- tions, making their responses less structured and user-friendly. Some variants of OPT models, such as OPT-IML and OPT-IML-Max, were fine-tuned by instruc- tion following, but they still retain much of OPT-66B’s behavior patterns. Both OPT and LLaMA adopt a decoder-only transformer-based architecture, while LLaMA achieves higher training efficiency and lower perplexity through bet- ter stability and data curation. Considering data, both of them leverage large, curated datasets from web crawls, academic corpora, and structured sources like Wikipedia, Reddit, and GitHub [61,75]. (2) Alpaca-LoRA, GPT-3.5s, and Vicuna (\"Davinci-like\"): Both Text-Davinci-003 and GPT-3.5 belong to OpenAI’s InstructGPT series. Although their exact architectures and hyper- parameters remain proprietary, they were known to be trained with large-scale Reinforcement Learning from Human Feedback (RLHF) [44], which improves their ability to follow instructions, generate clear responses, and avoid harmful content. Community-driven models like Alpaca and Vicuna are built on them: Alpaca (e.g., Alpaca-LoRA-30B) was fine-tuned using Text-Davinci-003 outputs, and Vicuna (e.g., Vicuna-13B) was trained on ShareGPT dialogues, which often contain ChatGPT responses. As a result, these models inherit the stylistic, for- matting, and instructional traits of OpenAI’s models. (3) GPT-4:"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 9, "text": "Community-driven models like Alpaca and Vicuna are built on them: Alpaca (e.g., Alpaca-LoRA-30B) was fine-tuned using Text-Davinci-003 outputs, and Vicuna (e.g., Vicuna-13B) was trained on ShareGPT dialogues, which often contain ChatGPT responses. As a result, these models inherit the stylistic, for- matting, and instructional traits of OpenAI’s models. (3) GPT-4: GPT-4 is a more advanced LLM with significant breakthroughs and distinctness. It excels at handling complex tasks with stronger reasoning, coherence, and problem-solving abilities than its predecessors. Takeaway II DA-MTL’s misclassification patterns reveal relationships between LLMs, which often align with their lineage. Stylometric Analysis. To better understand the proximities of the LLMs, we analyze their writing styles using NELA [21], a framework that evaluates text based on 87 features across six categories: style, complexity, bias, affect, morality, and events. From each LLM-generated sample, we extracted a NELA feature vector. We identified the eight most important features using the Chi- squared test: singular proper nouns (NNP) count, past tense verbs (VBD) count, total word count, Flesch-Kincaid readability score, positive opinion word count, Word-level Positive Sentiment Score (wpos), Sentence-level Negative Sentiment Score (sneg), and explicit date mentions. Figure 3 illustrates the average feature values of LLM-generated text in MULTITuDE dataset. Fig. 3. The average normalized values of the eight most important NELA features are shown. The features are listed in order of importance, from left (blue) to right (gray): NNP, VBD, total word count, Flesch-Kincaid readability score, positive opinion word count, wpos, sneg, and explicit date count. Our analysis reveals that OPT-66B and OPT-IML-MAX-1.3B are the most similar, while Vicuna-13B, GPT-3.5, and Text-Davinci-003 form another closely related group. However, notable intra-group differences exist among the \"OPT- like\" models, \"Davinci-like\" models, and GPT-4. These align with our obser- vations derived from our DA-MTL model’s confusion matrices. Specifically, we have the following findings: – OPT and LLaMA: OPT models frequently use singular proper nouns, past tense verbs, and explicit dates, while showing minimal sentiment word usage. Their responses are lengthy, though shorter than the ones generated by GPT-4, and are complex and harder to read. LLaMA-65B shares these patterns but with an even stronger preference for proper nouns and explicit dates while avoiding sentiment expressions. LLaMA’s responses are slightly easier to read than OPT’s but still more complex than those of the other LLMs. – Vicuna, GPT-3.5, Text-Davinci-003, and Alpaca: These models ex- hibit highly similar patterns: they use far fewer singular proper nouns, past tense verbs, and explicit dates, which is nearly half as frequently as OPT models. Instead, they incorporate significantly more sentiment and opinion words, which is almost 50% more than OPT and LLaMA. They generate shorter and more readable responses. Alpaca follows this trend but stands out with the shortest responses (under 150 words on average) and the highest readability. – GPT-4: GPT-4 shares some similarities with \"Davinci-like\" models but stands out with significantly longer responses, averaging nearly 300 words. Despite this, its readability remains low, comparable to Text-Davinci-003. Additionally, GPT-4 uses more sentiment and opinion words than Vicuna, GPT-3.5, Text-Davinci-003, and Alpaca. However, while NELA feature analysis provides valuable insights into"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 10, "text": "GPT-4 shares some similarities with \"Davinci-like\" models but stands out with significantly longer responses, averaging nearly 300 words. Despite this, its readability remains low, comparable to Text-Davinci-003. Additionally, GPT-4 uses more sentiment and opinion words than Vicuna, GPT-3.5, Text-Davinci-003, and Alpaca. However, while NELA feature analysis provides valuable insights into LLM behaviors, it does not fully explain their differences. To validate this, we trained two ML models, SVM and LR, using NELA features as text embeddings. Both models performed poorly, with F1 scores of only 41.9% (SVM) and 31.6% (LR). This suggests that LLM stylistic differences involve more subtle factors beyond what NELA features can represent. 4.3 Cross-Lingual Analysis Multi-Lingual Text Detection and Attribution. We use mBERT and XLM- R for multi-lingual tasks. To better analyze the impact of different languages, we exclude highly correlated and potentially confusing models (e.g., OPT-66B versus OPT-IML-MAX-1.3B) and focus on five representative models: OPT- IML-MAX-1.3B, LLaMA-65B, GPT-4, Text-DaVinci-003, and Vicuna-13B. As shown in Table 4, the classifier’s F1 scores improve for both Task 1 and 2 after applying DA-MTL. Across nine languages, mBERT gains an average increase of 2.96% for Task 1 and 2.67% for Task 2, while XLM-R shows improvements of 1.78% and 2.6%, respectively. These results confirm that the two tasks com- plement each other. Among the languages, Dutch (NL) and Portuguese (PT) proved to be the most challenging for both tasks. However, incorporating DA- MTL significantly improved detection F1 scores for these two languages, reaching F-1 scores of 99.83% and 98.17%, respectively. Moreover, no signs of overfitting are shown for either task when applying DA-MTL. This again aligns with our hypothesis that DA-MTL serves as a form of regularization to prevent overfitting to single tasks. Takeaway III DA-MTL significantly improves multilingual detection and attribution, especially in challenging languages. Table 4. F1 scores (%) of mBERT and XLM-R w/o and with DA-MTL by language. Blue and red numbers show the differences. Gains larger than 3% are in bold. Tasks Models EN ES RU UK CS DE NL CA PT TD mBERT 98.02 97.93 96.33 96.49 98.45 96.24 96.13 97.93 95.17 +MTL (+2.5) (+1.5) (+0.5) (+1.6) (+3.1) (+3.9) (+3.7) (+2.9) (+3.0) XLM-R 97.25 97.87 98.80 96.34 96.19 96.87 95.53 97.76 95.36 +MTL (+1.5) (+1.5) (+2.3) (+1.6) (+0.6) (+2.1) (+2.0) (+2.0) (+2.4) TA mBERT 91.45 94.60 95.73 91.20 91.43 89.39 87.60 90.03 87.69 +MTL (+2.9) (+4.6) (+3.5) (+6.7) (+2.5) (+1.8) (-0.1) (-0.3) (+2.4) XLM-R 91.36 93.84 96.58 90.92 91.21 87.80 88.76 90.89 82.54 +MTL (+2.1) (+3.4) (+3.5) (+3.7) (+3.9) (+1.0) (+4.9) (-0.4) (+1.3) Cross-Lingual Generalization. We follow [36] to evaluate cross-lingual gen- eralization, addressing RQ3. Figure 4 presents the cross-validation performance of mBERT and XLM-R on Tasks 1 and 2. Fig. 4. Cross-Lingual validations of models fine-tuned on MULTITuDE. From left to right: mBERT for Task1 & Task 2 and XLM-R for Task 1 & Task 2. Each model was fine-tuned on individual source languages, and results show that they can generalize to other languages. However, their performance is gen- erally lower than when fine-tuned directly on the target languages. Notably, XLM-R fine-tuned on Ukrainian transfers"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 11, "text": "& Task 2 and XLM-R for Task 1 & Task 2. Each model was fine-tuned on individual source languages, and results show that they can generalize to other languages. However, their performance is gen- erally lower than when fine-tuned directly on the target languages. Notably, XLM-R fine-tuned on Ukrainian transfers well to Russian and vice versa, demon- strating strong cross-lingual performance among closely related Slavic languages that share the Cyrillic script. In Task 2, performance varies widely, with mBERT achieving accuracies between 21.31% and 88.7%, while XLM-R ranges from 23.58% to 91.07%. Further analysis reveals that language families and writing systems signifi- cantly impact cross-lingual transfer. For instance, models generalize well among Romance languages like Spanish, Portuguese, and Catalan. Most source lan- guages transfer effectively to Czech, which uses a Latin script. Fine-tuning on English or Spanish typically results in strong generalization across multiple lan- guages, whereas models fine-tuned on Ukrainian or Russian exhibit poor trans- ferability except to Czech. Interestingly, models trained on English show moder- ate performance on other Germanic languages, particularly Dutch and German, likely due to English’s historical divergence and influence from Latin-based lan- guages. 4.4 Robustness We assess the robustness of DA-MTL against two widely used authorship obfus- cation techniques: – Mutant-X (MX): A stylometric obfuscation method that uses genetic al- gorithms [38]. It alters text by replacing words with synonyms and rearrang- ing phrases while maintaining semantic similarity. These modifications are guided by a fitness function that balances authorship attribution probability with meaning preservation. We use the publicly available implementation with its default settings †. – Back-Translation (BT): A technique that translates text into another language and then back to the original [29,2]. This process changes word choice and sentence structure while preserving meaning. For our experi- ments, we use French as the intermediate language, utilizing Opus MT mod- els for English-to-French (\"en-fr\") and French-to-English (\"fr-en\") transla- tion [59,60]. Table 5 shows how applying these obfuscation techniques affects the accuracy of our XLM-R classifier trained with DA-MTL. We conduct this evaluation us- ing 500 randomly selected English text samples from MULTITuDE. Our results indicate that while the model remains generally robust, the extent of accuracy degradation varies across LLMs. For Mutant-X, the classifier is most resistant when analyzing text from OPT-66B, where accuracy decreases by only 0.61%, suggesting that text generated by OPT-66B has fewer distinct stylistic features, † https://github.com/asad1996172/Mutant-X making it harder to manipulate. In contrast, text from GPT-4 is the most af- fected, with accuracy dropping by 9.03%, indicating that its unique vocabulary and structured writing style make it more vulnerable to synonym substitutions and word-order changes. For Back-Translation, the classifier performs best on text from Vicuna-13B, with only a 4.38% accuracy drop. However, it struggles significantly with text generated by GPT-3.5, Text-DaVinci-003, and LLaMA- 65B, where accuracy declines by 14% to 17%. Interestingly, we find a negative correlation between the model’s initial classification accuracy and its resilience to Back-Translation. Texts that were harder to classify originally suffer the most after translation, likely because the process disrupts subtle linguistic patterns. Table 5. Accuracy differences"}
{"doc_id": "2508.14190v1", "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14190v1", "chunk_id": 12, "text": "LLaMA- 65B, where accuracy declines by 14% to 17%. Interestingly, we find a negative correlation between the model’s initial classification accuracy and its resilience to Back-Translation. Texts that were harder to classify originally suffer the most after translation, likely because the process disrupts subtle linguistic patterns. Table 5. Accuracy differences (%) against authorship obfuscation. N/A: Original ac- curacy on original text. MX: Mutant-X. BT: Back-Translation. Atk. GPT-3.5 GPT-4 DaVinci Vicuna OPT OPT-IML Alpaca LLaMA N/A 86.43 91.70 59.57 91.75 52.18 91.31 67.29 57.40 MX -2.68 -9.03 -2.53 -4.75 -0.61 -2.86 -2.67 -5.22 BT -14.2 -8.31 -14.5 -4.38 -10.4 -8.64 -14.9 -17.0 These findings demonstrate that the robustness of DA-MTL varies depending on the LLM source of the analyzed text. While some LLM-generated texts are naturally more resistant to stylometric obfuscation, others are more susceptible, making it easier for adversaries to evade attribution. Although DA-MTL exhibits strong real-world applicability against mild obfuscation techniques, it is not ex- plicitly designed to maximize robustness. We do not claim that DA-MTL offers superior resistance to adversarial attacks. In future work, we aim to explore adaptive defenses that dynamically adjust to different obfuscation strategies and develop more resilient authorship attribution methods that leverage deeper lin- guistic and structural features. 5 Conclusion In this paper, we introduce DA-MTL, a multi-task learning framework designed to detect and attribute LLM-generated text jointly. We highlight the benefits of multi-task learning by evaluating on nine datasets and with four different backbone models. The results show that multi-task learning improves perfor- mance for both tasks and in both English and multi-lingual contexts. Using our accurate models, we further analyze cross-modal and cross-lingual similarities and evaluate DA-MTL against various authorship obfuscation techniques. Our work offers an effective tool as well as valuable insight into LLM behaviors and proximity, contributing to a deeper understanding of AI-generated content and its associated security and safety risks."}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 0, "text": "Comparing energy consumption and accuracy in text classification inference Johannes Zschache and Tilman Hartwig Application Lab for AI and Big Data, German Environment Agency, Alte Messe 6, Leipzig, 04103, Saxony, Germany. *Corresponding author(s). E-mail(s): tilman.hartwig@uba.de; Contributing authors: johannes.zschache@uba.de; Abstract The increasing deployment of large language models (LLMs) in natural language processing (NLP) tasks raises concerns about energy efficiency and sustainabil- ity. While prior research has largely focused on energy consumption during model training, the inference phase has received comparatively less attention. This study systematically evaluates the trade-offs between model accuracy and energy consumption in text classification inference across various model archi- tectures and hardware configurations. Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classifica- tion accuracy. We observe substantial variability in inference energy consumption (<mWh to >kWh), influenced by model type, model size, and hardware spec- ifications. Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible. These findings have implications for sustainable AI development, provid- ing actionable insights for researchers, industry practitioners, and policymakers seeking to balance performance and resource efficiency in NLP applications. Keywords: NLP, Large Language Model, Resource Efficiency, Sustainable AI 1 Introduction Artificial intelligence (AI) systems, particularly large language models (LLMs), have driven remarkable progress in Natural Language Processing (NLP) applications. This 1 development has been enabled by the Transformer architecture (Vaswani et al., 2017) and exemplified by the emergence of large-scale models such as GPT-3 (Brown et al., 2020), which have significantly advanced task performance. However, this progress has come at a cost: the escalating energy demands of AI systems pose significant environmental and computational challenges. Data centers that support AI com- putations are major electricity consumers, often dependent on fossil fuels, thereby contributing to greenhouse gas emissions (Lacoste et al., 2019; Axenbeck et al., 2025). This increasing energy demand challenges global climate objectives such as the Paris Agreement (United Nations, 2015a) and the United Nations’ Sustainable Development Goals (SDGs), specifically Goal 13 on climate action (United Nations, 2015b). Consequently, designing energy-efficient AI systems is imperative for aligning technological advancements with sustainability goals. Moreover, beyond sustainabil- ity, energy-efficient models offer additional advantages, including reduced hardware requirements, lower financial costs, and faster inference times. When evaluating machine learning models, most studies concentrate on the quality of the model responses by tracking e.g. the accuracy, the RMSE, or other measures. And even if the energy consumption is taken into account, prior research has mainly focused on the training phase (Strubell et al., 2019; Patterson et al., 2021; Luccioni and Hernandez-Garcia, 2023). The inference phase, which is repeatedly executed in real world deployments, has received comparatively less attention. However, energy efficiency during the operational phase is an increasingly relevant topic as LLM appli- cations become ubiquitous and LLM models are trained to use additional test-time compute to improve performance (OpenAI, 2024; DeepSeek-AI, 2025). Addressing this gap, we"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 1, "text": "repeatedly executed in real world deployments, has received comparatively less attention. However, energy efficiency during the operational phase is an increasingly relevant topic as LLM appli- cations become ubiquitous and LLM models are trained to use additional test-time compute to improve performance (OpenAI, 2024; DeepSeek-AI, 2025). Addressing this gap, we present a systematic study on the energy consumption of language models during inference, providing actionable insights for balancing accuracy with efficiency. A particularly popular machine learning task is text categorization, a task that lightweight models have been shown to handle effectively. For instance, Joulin et al. (2017) show that a simple classifier built on word embeddings is often as accurate as deep learning classifiers. Despite this, some authors argue for the use of pre-trained LLMs for text classification because it reduces the need for model training and sim- plifies data preprocessing (Wang et al., 2024). Additionally, popular software tutorials promote LLMs for classification tasks (LangChain Team, 2023; Lamini Team, 2023), further encouraging their use even when more efficient alternatives exist. In order to justify the usage of LLM in relatively simple tasks such as text categorization, we advocate a consequent comparison of a model’s response quality to its energy efficiency. Given a practical use case that is occurring in public administration, our study empirically analyzes trade-offs between model accuracy and energy consumption across various language models and hardware configurations. We find that the best perform- ing model is energy efficient while LLMs show higher energy usage with lower accuracy. Generally, we see significant variability in inference energy consumption, influenced by model type, model size, and hardware specifications. Additionally, the energy con- sumption during inference is shown to highly correlate with the model’s runtime. This makes the duration of computations a valuable proxy measure for energy consump- tion in settings where the latter cannot be traced. Our findings have implications for researchers, industry practitioners, and policymakers advocating for sustainable 2 AI development (Kaack et al.; Luccioni et al., 2025). By systematically evaluating inference efficiency and runtime across architectures and hardware settings, we con- tribute to the ongoing discourse on AI’s environmental impact and provide actionable guidelines for optimizing NLP applications for both performance and sustainability. 2 Previous research Research on the environmental impact of machine learning (ML) has primarily focused on the energy consumption and carbon emissions produced during the training phase of large-scale models. Most famously, Strubell et al. (2019) quantify the carbon footprint of NLP models, revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes (their measurements include thousands of hyperparameter tuning jobs, which makes it difficult to disen- tangle model-inherent efficiency from experimental setup). This seminal work spurred further investigations into the environmental costs of training neural networks, includ- ing large language models (Patterson et al., 2021; Luccioni and Hernandez-Garcia, 2023; Patterson et al., 2022). While training remains a significant contributor to energy consumption, recent studies have begun to focus on the inference phase. Samsi et al. (2023) highlighted the substantial energy demands of LLM inference but"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 2, "text": "networks, includ- ing large language models (Patterson et al., 2021; Luccioni and Hernandez-Garcia, 2023; Patterson et al., 2022). While training remains a significant contributor to energy consumption, recent studies have begun to focus on the inference phase. Samsi et al. (2023) highlighted the substantial energy demands of LLM inference but did not explore the relationship between energy consumption and task-specific performance. Liu et al. (2022) under- score the importance of evaluating NLP models not just on efficiency metrics but also on accuracy by introducing the Efficient Language Understanding Evaluation (ELUE) benchmark. ELUE aims to establish a Pareto frontier that balances performance and efficiency. It includes various language understanding tasks, facilitating fair and com- prehensive comparisons among models. However, the framework adopts number of parameters and FLOPs as the metrics for model efficiency, disregarding hardware specific factors. Similarly, Chien et al. (2023) estimate the energy consumption associ- ated with the inference phase of generative AI applications based on the output word count and several assumptions about the application such as the number of FLOPS per inference and the sampling rate. In contrast, we promote energy-efficient NLP models by the direct measurement of the power consumed during inference. Hence, our work follows the approach of the SustaiNLP 2020 shared task (Wang and Wolf, 2020). SustaiNLP demonstrated that substantial energy savings are achievable with minimal performance loss. While this study was limited to the performance of a couple of small language models on a single benchmark, we extend these efforts to a greater number of partially very large models deployed to a practical inference scenario. This makes our study very similar to the one by Alizadeh et al. (2025), who inves- tigated the trade-offs between accuracy and energy consumption when deploying large language models (LLMs) for software development tasks. Besides the finding that larger LLMs with higher energy consumption do not always yield significantly bet- ter accuracy, the authors demonstrated that architectural factors, such as feedforward layer size and transformer block count, directly correlate with energy usage. 3 Finally, Luccioni et al. (2024) provide one of the most comprehensive analyses of energy consumption during ML model inference. Their study systematically com- pared the energy costs of 88 models across 10 tasks and 30 datasets, including both smaller task-specific and larger multi-purpose models. They found that the larger models are orders of magnitude more energy-intensive than smaller task-specific ones, especially for tasks involving text and image generation. Furthermore, their research underscores the variability in energy consumption across tasks and model architec- tures. The authors advocate for increased transparency and sustainable deployment practices, emphasizing that the environmental costs of deploying large, multi-purpose AI systems must be carefully weighed against their utility. 3 Data and methods Our experiments are inspired by an occasionally occurring use case in public admin- istration: the management of objections that are submitted by the population. Due to a potentially very large amount of submissions, an automatic preprocessing of the objections is of high value. One of the possible steps of an automated workflow is to categorize each submission for optimal forwarding to the"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 3, "text": "admin- istration: the management of objections that are submitted by the population. Due to a potentially very large amount of submissions, an automatic preprocessing of the objections is of high value. One of the possible steps of an automated workflow is to categorize each submission for optimal forwarding to the responsible department. The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany. During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php). The text of the submission is given by the column ‘Beitrag’ (input). The column ‘Themenkomplex’ (topic) contains the category of the text. We scraped the dataset from the website and restricted it to entries for which the topic occurs at least 10 times. The remaining 378 entries were split into half: 189 entries for training and 189 entries for testing. This unusual 50:50 split was done so that the test set should be sufficiently representative by containing enough examples of each of the 14 categories. Each of the following experiments was repeated 10 times with different train-test-splits. To increase comparability, every experiment was run with the same 10 train-test-splits. An experiment run consists of a training phase and a testing phase. Since large language models have been argued to be applicable to text categorization without training (zero-shot), we omit the training phase for these models and apply LLMs without fine-tuning. We report the energy consumption and accuracy only for the test phase as averages over all runs. 3.1 Traditional models Besides LLMs, we initially run the experiments with lightweight NLP models that we call traditional because they have been used for categorization tasks long before LLMs existed. Specifically, we use a linear model (logistic regression) and a gradient boosting algorithm (xgboost). Logistic regression is a simple, interpretable model that estimates the probability of a class based on a linear combination of input features. XGBoost 4 (Extreme Gradient Boosting) is an efficient, scalable machine-learning algorithm that combines predictions from multiple decision trees to improve accuracy. We consider three different types of features: bag-of-words (BoW), term frequency- inverse document frequency (TF-IDF), and a pretrained multilingual sentence embedding. BoW represents text by counting word occurrences without consider- ing order, while TF-IDF adjusts word counts by their importance across documents, capturing rare but informative terms. The TF-IDF features are calculated on all 2-gram and 3-gram character sequences, which capture local patterns in the text. The multilingual sentence embedding (https://huggingface.co/sentence-transformers/ paraphrase-multilingual-mpnet-base-v2) provides dense vector representations of text, preserving semantic meaning across languages. This embedding is not fine-tuned on the training data. Both models are trained using the default parameters provided by sklearn.linear model.LogisticRegression and xgboost.XGBClassifier. 3.2 Large language models Large language models (LLMs) were applied without training (zero-shot) using the test set only. Table 1 gives the names and sources of the models used. The LLMs were selected by the following criteria: • availability on Huggingface • support of german language • capability of processing"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 4, "text": "3.2 Large language models Large language models (LLMs) were applied without training (zero-shot) using the test set only. Table 1 gives the names and sources of the models used. The LLMs were selected by the following criteria: • availability on Huggingface • support of german language • capability of processing the dspy-prompt (see appendix A) Additionally, Jamba Mini 1.5 was chosen as model with an alternative architec- ture that includes next to Transformer also Mamba layers (a state-space model). The Deepseek distillations (DS) were added to include models with reasoning capabilities (test-time compute). Model Link Llama 3.1 8B https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct Llama 3.1 70B https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct Qwen 2.5 7B https://huggingface.co/Qwen/Qwen2-7B-Instruct Qwen 2.5 72B https://huggingface.co/Qwen/Qwen2-72B-Instruct Phi 3.5 Mini https://huggingface.co/microsoft/Phi-3.5-mini-instruct Phi 3.5 MoE https://huggingface.co/microsoft/Phi-3.5-MoE-instruct Jamba Mini 1.5 https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini DS Qwen 14B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B DS Qwen 32B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B DS Llama 8B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B DS Llama 70B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B Table 1 Selection of large language models 3.3 Computing Resources We used different computing systems for a comparative analysis of energy efficiency across diverse hardware architectures. This enables the assessment of how architectural 5 differences - especially GPU tensor core capabilities - affect the inference speed and power usage. A diversity in computational infrastructure is crucial for generalizing findings across different environments and ensuring the validity and replicability of experimental results in machine learning research. Furthermore, insights gained from using multiple platforms contribute to optimizing resource allocation strategies and improving cost-effectiveness in large-scale machine learning projects. To run our experiments, we were granted access to the high-performance computing (HPC) systems of TUD Dresden University of Technology (https://doc.zih.tu-dresden. de/) and Leipzig University (https://www.sc.uni-leipzig.de/). For GPU-accelerated computing, three different systems are available named Capella, Paula, and Clara (see Table 2). The main difference for our study is the GPU: while a node on the Capella cluster is equipped with 4 x H100, there are 8 x A30 on each node on Paula and 4 x V100 on Clara. This means that a large model such as Llama 3.1 70B or Qwen 2.5 72B fits on a single node of Capella (requiring 2 GPUs) or Paula (requiring all 8 GPUs) but takes up two nodes of the Clara cluster (assuming a 16-bit floating point representation of the parameters). Cluster Capella Paula Clara HPC center TUD Dresden University of Technology Leipzig University Leipzig University number of nodes 144 12 6 CPU per node 2 x AMD (32 cores) 2.7GHz 2 x AMD (64 cores) 2.0GHz 1 x AMD (32 cores) 2.0GHz RAM per node 768 GB 1 TB 512 GB GPU per node 4 x NVIDIA H100 (94GB) 8 x NVIDIA A30 (24 GB) 4 x NVIDIA V100 (32GB) single GPU max power consumption 700W 165W 250W Table 2 HPC Resources LLMs were deployed using the vllm library (https://github.com/vllm-project/ vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations. If a model is too large to be deployed on a single GPU, the model weights are dis- tributed over multiple GPUs, which allow for a parallel computation of the activations (c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two computing"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 5, "text": "for multi-node computations. If a model is too large to be deployed on a single GPU, the model weights are dis- tributed over multiple GPUs, which allow for a parallel computation of the activations (c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two computing nodes are needed, the model is split into two parts and executed sequen- tially (c.f. pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model part on the first node and then the model part on the second node. The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon). This package uses the NVIDIA Management Library (NVML) and the Intel RAPL files to track the power usage of GPU and CPU (https://mlco2.github.io/codecarbon/methodology. html#power-usage). The power consumption of the memory is flatly added with 0.375W/GB of memory used. In settings where the model is deployed on more than one node, the inference duration is taken as the maximum and the energy as the sum over all nodes. 6 Various software tools have been created to monitor energy consump- tion during the application of machine learning models (https://github.com/ tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF% B8%8F-energy-metrics). Similar to CodeCarbon, Carbontracker (Anthony et al., 2020) and experiment-impact-tracker (Henderson et al., 2020) estimate energy con- sumption by monitoring hardware usage. In some settings, CodeCarbon is considered more accurate, yielding values closer to those obtained via physical wattmeters (Bouza et al., 2023). Comparing different tools of energy monitoring is beyond the scope of our paper. 4 Results For each model, we report accuracy, energy consumption, and inference duration. The energy consumption and duration were measured only for the inference step, i.e., after the model and data were already loaded. One inference run involves classifying 189 text samples from a test set. All tables and figures present the average results over 10 runs on different test sets, with the same 10 test sets used for each model. Measurement variance was generally low: < 0.002 for accuracy, and < 0.2 dex for both energy consumption and duration (logarithmically scaled to base 10). Figure 1 illustrates the trade-off between energy consumption and accuracy across all models. For these experiments, a single node of the Capella system was used. The minimum number of H100 GPUs required varies by model (see Table B1). The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. Notably, even the most energy-efficient model - a linear model with TF-IDF features - outperformed several large language models (LLMs). Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points. Deepseek models, despite their extensive reasoning processes during inference, exhibit lower accuracy than non-reasoning LLMs while consuming significantly more energy and taking longer to complete inference. 4.1 Analysis of hardware settings This section analyzes the impact of different hardware configurations (see Tab. 2) on energy consumption. We focus on GPU usage due to its dominant role"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 6, "text": "processes during inference, exhibit lower accuracy than non-reasoning LLMs while consuming significantly more energy and taking longer to complete inference. 4.1 Analysis of hardware settings This section analyzes the impact of different hardware configurations (see Tab. 2) on energy consumption. We focus on GPU usage due to its dominant role in machine learning inference. As shown in Figure 2, GPU consumption accounts for the largest share of total energy usage in all experiments. The only exceptions are traditional models without embeddings, which do not use the GPU during inference. 4.1.1 Varying the Number of GPUs We examined the effect of varying the number of GPUs on energy consumption and inference duration. Most LLMs were tested on 1, 2, or 4 GPUs on a single Capella system node. Larger models (Qwen 72B, Phi MoE, Llama 70B, Jamba Mini, and DS Llama 70B) required either 2 or 4 GPUs. Increasing the number of GPUs consistently 7 Fig. 1 Accuracy-energy-trade-off of all models for the inference task on the Capella system (single node). The energy consumption for the same task spans over six orders of magnitude with traditional models being the most energy-efficient models and reasoning models are most energy-consuming. The best model for this specific task is a traditional model (Linear Embedding) with moderate energy consumption. reduced inference duration but did not reduce energy consumption. In some cases, energy consumption increased due to the additional GPUs in operation (see Figure 3). 4.1.2 Varying the Number of Nodes While large models can often be executed on a single computing node, certain hardware limitations or shared high-performance computing (HPC) environments may necessi- tate using multiple nodes. In shared systems, it is often easier to access two nodes with half the number of available GPUs than a single node with all its GPUs, due to scheduling constraints and resource allocation policies. However, deploying mod- els across multiple nodes increases network communication overhead and significantly raises energy consumption. We evaluated this effect for the largest models on the Capella system by comparing a ‘single-node’ configuration (2 GPUs on one node) with a ‘double-node’ configuration (1 GPU on each of two nodes). For the double-node configuration, energy consumption was summed across both nodes and averaged over 10 runs, while the reported duration reflects the average of the maximum value between the two nodes. As shown in Figure 4, using two nodes increased energy consumption by a factor that depends on the model (see also Table B2). This increase stems from the overhead 8 Fig. 2 Energy consumption of all models for the inference task on the Capella system (single node) Fig. 3 Effects of the number of GPUs on the runtime and consumed energy (Capella, single node). Deepseek models are not shown. of coordinating across nodes. Inference duration also increased by the same factor due to the sequential execution of model components and the required inter-node communication. 4.1.3 Comparing GPU Architectures Finally, we compared the energy efficiency of different GPU architectures (see Figure 5 and Table B3). Interestingly, the expected efficiency gains from using the more pow- erful H100"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 7, "text": "increased by the same factor due to the sequential execution of model components and the required inter-node communication. 4.1.3 Comparing GPU Architectures Finally, we compared the energy efficiency of different GPU architectures (see Figure 5 and Table B3). Interestingly, the expected efficiency gains from using the more pow- erful H100 instead of V100 or A30 GPUs were only observed for the Deepseek models. This discrepancy is likely to arise because Deepseek models engage in extended rea- soning by generating a larger output of words before making a classification decision. Consequently, the efficiency of H100 GPUs becomes evident only when substantial 9 Fig. 4 Comparison single node vs. double node deployment (Capella). text is generated. For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. Fig. 5 Comparison of different GPU cards: four exemplary LLMs. Single node deployment. 4.2 Linear relationship between duration and energy In most of the tables in appendix B, we report both the duration of each inference run and its corresponding energy consumption. Since energy is the integral of power over time, these two measures exhibit a strong correlation. If the power is constant over time, this correlation should be linear. Figure 6 illustrates this relationship for 10 all experiments conducted on a single node of the Capella cluster. When controlling for the number of GPUs used for model deployment, the relation between duration and energy is approximately linear. Therefore, the duration appears to serve as a good proxy for the energy consumed. Fig. 6 Plotting the relationship between duration and energy consumption (single node on Capella). The lines are added by running a linear regression model. To further quantify the relationship between duration and energy consumption, we performed a linear regression analysis for each hardware configuration (see Table 3). This analysis includes all experiments, regardless of the number of nodes used for model deployment. The consistently high R2 values across all configurations indi- cate that, for a given hardware setup, duration and energy consumption are nearly interchangeable as measures of computational effort. Moreover, when the regression coefficients are known for a specific computing sys- tem, energy consumption can be reliably estimated from the duration and the number of GPUs. Only the coefficients of duration (a) and of the interaction term dura- tion:GPUs (c) are statistically significant. The other coefficients (b and d) are omitted from the approximation: Energy ≈(a + c · GPUs) · Duration. (1) For instance, on the Capella system, the following approximation holds for any computation: 11 Energy 1 Wh ≈(0.1 + 0.09 · GPUs) · Duration 1 s . (2) This relationship suggests that, under fixed hardware conditions, monitoring the duration of computations provides an efficient means of estimating energy usage with minimal additional measurement overhead. Dependent variable: Energy Capella Clara Paula (1) (2) (3) Duration (a) 0.097∗∗∗ 0.061∗∗∗ 0.079∗∗∗ (0.008) (0.002) (0.026) GPUs (b) −0.500 0.048 −2.195 (2.297) (0.339) (3.472) Duration:GPUs (c) 0.090∗∗∗ 0.036∗∗∗ 0.054∗∗∗ (0.004) (0.0002) (0.004) Constant (d) −6.205 −0.826 3.328 (5.725) (1.368) (17.220) Observations 44 19 23 R2 0.998 1.000"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 8, "text": "measurement overhead. Dependent variable: Energy Capella Clara Paula (1) (2) (3) Duration (a) 0.097∗∗∗ 0.061∗∗∗ 0.079∗∗∗ (0.008) (0.002) (0.026) GPUs (b) −0.500 0.048 −2.195 (2.297) (0.339) (3.472) Duration:GPUs (c) 0.090∗∗∗ 0.036∗∗∗ 0.054∗∗∗ (0.004) (0.0002) (0.004) Constant (d) −6.205 −0.826 3.328 (5.725) (1.368) (17.220) Observations 44 19 23 R2 0.998 1.000 0.989 Adjusted R2 0.998 1.000 0.987 Note: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01 Table 3 Linear regression of energy consumption on duration (table format by Hlavac, 2022). The numbers (coefficients) give the estimated effects of each predictor on the dependent variable. A positive coefficient means the variable increases the outcome, while a negative coefficient means it decreases the outcome. The standard error (in parenthesis) estimates the variability of the coefficient estimate. The p-value (given by the asterisks) indicates whether the predictor is statistically significant (different from zero). 5 Discussion We would like to mention the limitations of our study, which also point to the areas of future research. First, while traditional models were trained on approximately 200 examples, the large language models (LLMs) were applied in a zero-shot setting, meaning they had no access to labeled examples. Previous research has shown that few-shot prompting - where representative examples are included in the prompt - can improve performance (Brown et al., 2020). For the present study, we kept the prompt 12 as simple as possible (see section A). But in an actual application, we would add back- ground information about the data and the categories. In general, prompt engineering, the addition of representative examples to the prompt, or even fine-tuning an LLM could yield higher accuracy rates. On the other hand, energy efficiency in LLMs can be improved through model quantization, which reduces computational demands by compressing model parameters (Jacob et al., 2017). Second, we do not account for the energy costs associated with training the tradi- tional models because it is infeasible to compare them to the training costs of LLMs. The LLMs used in this study were pre-trained by external organizations and made publicly available. As a result, the energy costs of training are distributed among all users, making it difficult to estimate per-user energy consumption. Even if training energy costs for an LLM were known, the number of users remains uncertain. Addi- tionally, hosting LLMs (e.g., on Hugging Face) and managing network traffic also contribute to energy consumption. Deploying an LLM on a dedicated server (e.g., using vLLM) requires setup time and additional energy. Beyond inference, significant time and computational resources are also required for development tasks, including data processing, testing different models and prompts, parameter tuning, and debug- ging - workloads that apply to both traditional models and LLMs. The measurement of additional related energy consumptions (such as network traffic or disk storage) is beyond the scope of this paper. Third, energy consumption was measured using CodeCarbon, a tool recognized for providing reliable estimates of a machine’s total energy use (Bouza et al., 2023). However, it does not allow for precise measurement of energy consumption at the level of individual processes. Moreover, power intake was recorded at 15-second intervals, meaning the accuracy"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 9, "text": "consumption was measured using CodeCarbon, a tool recognized for providing reliable estimates of a machine’s total energy use (Bouza et al., 2023). However, it does not allow for precise measurement of energy consumption at the level of individual processes. Moreover, power intake was recorded at 15-second intervals, meaning the accuracy of energy estimates improves with longer-running processes. Another limitation of CodeCarbon is that RAM energy consumption is approximated at 0.375W per GB of memory used. While the Running Average Power Limit (RAPL) framework can directly measure RAM power consumption, it is not supported on all CPUs (https://github.com/mlco2/codecarbon/issues/717# issuecomment-2589805160). Additionally, in shared computing environments such as high-performance computing (HPC) clusters, measurements may be affected by other users’ activities. Especially when an LLM was deployed across multiple nodes, varia- tions in network traffic at different times may have influenced energy measurements. A more precise assessment of energy efficiency would benefit from using dedicated com- puting resources with physical wattmeters and high-resolution energy measurement tools(e.g. Ilsche et al., 2019). In the following, we assess further limitations of the present study in more detail. More specifically, we address our focus on a single dataset in section 5.1 and the limitation to the text categorisation task in section 5.2. Subsequently, we contextualise our work in the broader context of planet-centered LLMs (section 5.3). 5.1 Analysis on other datasets Our analysis was conducted on a highly specialized dataset. To assess the generaliz- ability of our findings, we replicated the experiments using four additional, widely used datasets (see table 4). These datasets were selected from the HuggingFace platform 13 based on popularity and had to meet two criteria: suitability for text classification and inclusion of two columns - text and label. To maintain comparability with our initial analysis, we randomly sampled 200 training examples and 200 test examples from each dataset. Using a slightly larger training set might have provided an advantage to tra- ditional models, as the LLMs were applied in a zero-shot setting without fine-tuning. Each model experiment was repeated 10 times with different samples, ensuring that each model was tested on the same 10 sets. Name Classification Task ID on https://huggingface.co/datasets news news topics: World, Sports, Business, Sci/Tech fancyzhx/ag news yelp sentiment: 1-5 stars Yelp/yelp review full tomatoes sentiment: pos, neg cornell-movie-review-data/rotten tomatoes emotion emotion: anger, fear, joy, love, sadness, surprise dair-ai/emotion Table 4 Selection of datasets for text classification tasks. Figure 7 visualizes the relationship between accuracy and energy consumption for these additional text classification tasks. For clarity, we restricted the visualization to the models with the three highest accuracy scores and included the linear model with sentence embeddings for comparison (see Tables B4 and B5 for details). Similar to our findings with the FKTG dataset, the DeepSeek models do not outperform the best non-reasoning models in most cases. The only exception is the emotion dataset, where DeepSeek Llama 70B achieves an accuracy of 0.61, slightly surpassing the best non-reasoning model, Phi 3.5 MoE (0.60). However, unlike in the previous analysis, for every dataset, at least one LLM outperforms the best traditional model. For the"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 10, "text": "models in most cases. The only exception is the emotion dataset, where DeepSeek Llama 70B achieves an accuracy of 0.61, slightly surpassing the best non-reasoning model, Phi 3.5 MoE (0.60). However, unlike in the previous analysis, for every dataset, at least one LLM outperforms the best traditional model. For the news dataset, Llama 3.1 70B achieves an accuracy 0.05 points higher than the best linear model (0.88 vs. 0.83). However, this comes at the cost of signif- icantly higher energy consumption (34.15 Wh vs. 0.0021 Wh), highlighting the need for careful trade-off considerations. In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. In some cases, a smaller model, such as Qwen 2.5 7B, may be sufficient. While its accuracy is slightly lower than the version with 72B parameters (0.60 vs. 0.68), it consumes only one-eighth of the energy. A similar pattern is observed for sentiment analysis on the Rotten Tomatoes dataset, where traditional models fail to match LLM performance. Among the larger models, Jamba Mini 1.5 stands out as one of the most efficient choices, offering strong accuracy while consuming significantly less energy. Notably, despite having nearly as many parameters as Llama 3.1 70B and Qwen 2.5 72B (51.6B vs. 70B/72B), Jamba Mini 1.5 requires only a quarter of the energy for the same task. Finally, for emotion classification, the linear model with sentence embeddings is among the top-performing models. In this case, a traditional model provides the most efficient solution. Hence, accuracy-energy trade-offs must be assessed on a case-by- case basis. In some scenarios, traditional models are sufficient, while in others, LLMs offer justifiable benefits despite higher energy consumption. However, a reason for the 14 Fig. 7 Accuracy-energy-trade-off of the best models for the inference task on different datasets (the Linear Embedding model was added for comparison), Capella system, single node. See Tables B4 and B5 for results of all models. superior performance of LLMs on some datasets might be that the data were included in the model’s training data. Our study uses data that are probably not part of any LLM training set. Nevertheless, test-time compute, as featured by the Deepseek models, has no benefits in text classification tasks, and the linear relationship between computation runtime and energy consumption holds across all datasets (see Table B6). 5.2 Transferability to other tasks Another limitation of the present study is its exclusive focus on the categorization task, which confines the analysis to a narrow subset of machine learning challenges. While this focus allows for a straightforward measurement of a model’s performance (using the accuracy metric), it neglects the applicability of the results to other tasks. Recent studies suggest that similar comparisons in terms of efficiency and accuracy can be insightful in a variety of domains beyond categorization. For instance, Clavi´ e et al. 15 (2025) demonstrate that simple encoder-based models can effectively tackle generative tasks, expanding the potential applications of smaller, less energy-hungry models. Moreover, a growing body of research highlights the advantages of fine-tuned small models"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 11, "text": "can be insightful in a variety of domains beyond categorization. For instance, Clavi´ e et al. 15 (2025) demonstrate that simple encoder-based models can effectively tackle generative tasks, expanding the potential applications of smaller, less energy-hungry models. Moreover, a growing body of research highlights the advantages of fine-tuned small models for specialized tasks, where they often outperform larger models (Savvov, 2024). This trend is evident in studies such as Wei et al. (2024), where an diabetes- specific LLM - despite having significantly fewer parameters - outperforms both GPT-4 and Claude-3.5 in processing various diabetes tasks. Similarly, Lu et al. (2023) report that their fine-tuned models achieve performance levels comparable to GPT-4 on domain-specific annotation tasks, yet with hundreds of times fewer parameters and significantly reduced computational costs. Zhan et al. (2025) further emphasize the superior performance of fine-tuned small models over zero-shot LLMs, particularly in in-domain content moderation tasks. The study by Luccioni et al. (2024) provides additional insights into the balance between model size and efficiency while looking at ten different machine learning tasks including image classification and captioning, question answering, summarization, as well as image and text generation. The authors demonstrate that smaller models can achieve high performance with considerably less resource consumption. Their initiative resulted into the AI Energy Score (https://huggingface.co/AIEnergyScore), a tool designed to assess the environmental impact of AI models on a range of tasks, and reinforces the growing importance of considering energy efficiency in model evaluation. 5.3 Further Requirements of Planet-Centered LLMs While energy consumption and the associated carbon footprint remain crucial con- siderations for sustainable AI, truly planet-centered LLMs must meet a broader set of requirements that go beyond mere efficiency. These include other limited resources (water, rare-earth metals, landuse,...), transparency, accessibility, ethical considerations, and technical adaptability to ensure responsible and sustainable AI deployment. Transparency in AI models is essential for trust and reproducibility (Raji et al., 2020). The predictions of traditional LLM models are generally more transparent than those of LLMs. Open-source LLMs, where both model architectures and training data are publicly available, contribute to scientific progress, allow for direct model compar- isons such as this present study, and reduce dependency on proprietary technologies (Wei et al., 2023). Furthermore, the ability to inspect training data is crucial to assess potential biases and copyright compliance (Bender et al., 2021). Many proprietary models, such as GPT-4, lack such transparency, making it difficult to evaluate their fairness and ethical considerations. The EU AI Act will require providers of general- purpose AI models to publish a sufficiently detailed summary of their training data starting in August 2025, which further highlights the call for transparency. LLMs vary significantly in size, ranging from lightweight models such as fast- Text (Joulin et al., 2017) to massive architectures like BLOOM-176B, which require substantial GPU memory and network bandwidth (Luccioni et al., 2023). These com- putational demands translate into high operational costs and environmental impacts. Moreover, some models require proprietary hardware, limiting their accessibility and 16 long-term sustainability. Future AI systems should prioritize modularity and adapt- ability, enabling efficient integration into diverse infrastructures"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 12, "text": "substantial GPU memory and network bandwidth (Luccioni et al., 2023). These com- putational demands translate into high operational costs and environmental impacts. Moreover, some models require proprietary hardware, limiting their accessibility and 16 long-term sustainability. Future AI systems should prioritize modularity and adapt- ability, enabling efficient integration into diverse infrastructures without excessive resource demands. The relevance and fairness of AI-generated outputs depend on the quality and recency of training data. Stale or biased datasets can lead to misleading results and reinforce harmful stereotypes (Bender et al., 2021; Gehman et al., 2020). In par- ticular, the presence of toxic content or hate speech in training data can result in models generating harmful or discriminatory outputs, which poses serious challenges for their deployment in sensitive contexts such as education, healthcare, or public administration. Moreover, safety concerns—such as the risk of models producing fac- tually incorrect, manipulative, or otherwise harmful content—are especially critical in public-sector applications, where accountability and trust are paramount (Weidinger et al., 2021). Addressing these challenges requires robust bias-mitigation strategies and transparent documentation of model behavior. To align with global sustainability and ethical AI principles, future research should emphasize the development of adaptable, transparent, and energy-efficient LLMs. By integrating principles of openness, fairness, and regulatory compliance, we can foster AI systems that not only minimize environmental impact but also promote responsible and equitable usage across sectors. Acknowledgements. We gratefully acknowledge the support provided by the Fed- eral Ministry for the Environment, Nature Conservation and Nuclear Safety (BMUV). Additionally, we thank colleagues from Z 2.3 and the entire AI-Lab team for their support and inspiration. This work was supported by high-performance computer time and resources from the Center for Information Services and High Performance Computing (ZIH) of TUD Dresden University of Technology and the systems for scientific computing of Leipzig University. We thank the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI Dresden/Leipzig) for their support in the acquisition process. The tool ChatGPT (OpenAI) was used to revise the text of the paper. Author contribution statements. T.H. conceived the study, initiated the project, led the research effort, and contributed to the literature review and manuscript writing. J.Z. designed and implemented the experiments, developed the codebase, conducted data analysis, and contributed to drafting the manuscript. Competing interests. There are no competing interests. Availability of data and code. All underlying data will be shared upon reasonable request to the corresponding author. The source code will be made public. 17 Appendix A LLM prompt For the zero-shot classification, we prompted the LLM with the following instruction (originally in German): C l a s s i f y the t e x t as one of the f o l l o w i n g c a t e g o r i e s : −<category 1> −<category 2> − . . . The categories were a fixed set of 14 options that occurred in the train- ing as well as the test dataset: ‘geoWK’, ‘Tongestein’, ‘Aktive St¨ orungszonen’, ‘¨ Offentlichkeitsbeteiligung’, ‘Kristallingestein’, ‘FEP/Szenarien/Entwicklungen des Endlagersystems’, ‘Anwendung geoWK’, ‘Mindestanforderungen’, ‘Steinsalz in steiler Lagerung’, ‘Datenverf¨ ugbarkeit’,"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 13, "text": ": −<category 1> −<category 2> − . . . The categories were a fixed set of 14 options that occurred in the train- ing as well as the test dataset: ‘geoWK’, ‘Tongestein’, ‘Aktive St¨ orungszonen’, ‘¨ Offentlichkeitsbeteiligung’, ‘Kristallingestein’, ‘FEP/Szenarien/Entwicklungen des Endlagersystems’, ‘Anwendung geoWK’, ‘Mindestanforderungen’, ‘Steinsalz in steiler Lagerung’, ‘Datenverf¨ ugbarkeit’, ‘Modellierung’, ‘Referenzdatens¨ atze’, ‘Bereitstellung der Daten’, ‘Ausschlusskriterien’. Since we deployed the dspy framework (https://dspy.ai/) to query the LLMs, the final prompt was automatically extended to the following: −r o l e : system content : |− Your input f i e l d s are : 1. ‘ text ‘ ( s t r ) Your output f i e l d s are : 1. ‘ category ‘ ( s t r ) A l l i n t e r a c t i o n s w i l l be s t r u c t u r e d i n the f o l l o w i n g way , with the a p p r o p r i a t e v a l u e s f i l l e d i n . [ [ # # t e x t # # ] ] { t e x t } [ [ # # category # # ] ] { category } [ [ # # completed # # ] ] In adhering to t h i s s t r u c t u r e , your o b j e c t i v e i s : C l a s s i f y the t e x t as one of the f o l l o w i n g c a t e g o r i e s : −<category 1> −<category 2> − . . . −r o l e : user content : |− [ [ # # t e x t # # ] ] <text > Respond with the co r r e s p o n d i n g output f i e l d s , s t a r t i n g with the f i e l d ‘ [ [ # # category # # ] ] ‘ , and then ending with the marker f o r ‘ [ [ # # completed # # ] ] ‘ . 18 Appendix B Tables Model GPUs Energy (Wh) Accuracy Duration (s) Average Power (W) Linear BoW 1 <0.01 0.43 0.01 139.96 Linear Tf-idf 1 <0.01 0.41 0.01 43.72 Linear Embedding 1 0.12 0.57 1.64 259.41 XGBoost BoW 1 <0.01 0.35 0.01 63.32 XGBoost Tf-idf 1 <0.01 0.47 0.01 67.77 XGBoost Embedding 1 0.21 0.47 2.87 259.94 Llama 3.1 8B 1 5.86 0.35 36.88 572.49 Llama 3.1 70B 2 48.60 0.48 161.59 1082.82 Qwen 2.5 7B 1 5.58 0.45 36.28 553.84 Qwen 2.5 72B 2 48.66 0.51 164.44 1065.31 Phi 3.5 Mini 1 5.74 0.30 41.45 498.46 Phi 3.5 MoE 2 11.00 0.40 55.51 713.34 Jamba Mini 1.5 2"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 14, "text": "8B 1 5.86 0.35 36.88 572.49 Llama 3.1 70B 2 48.60 0.48 161.59 1082.82 Qwen 2.5 7B 1 5.58 0.45 36.28 553.84 Qwen 2.5 72B 2 48.66 0.51 164.44 1065.31 Phi 3.5 Mini 1 5.74 0.30 41.45 498.46 Phi 3.5 MoE 2 11.00 0.40 55.51 713.34 Jamba Mini 1.5 2 17.42 0.34 78.61 797.94 DS Llama 8B 1 79.64 0.37 517.83 553.67 DS Llama 70B 2 702.06 0.46 2543.47 993.68 DS Qwen 14B 1 155.20 0.39 981.35 569.33 DS Qwen 32B 1 373.56 0.45 2255.99 596.11 Table B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs Model Duration (s) Energy consumed (Wh) single double ratio single double ratio Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95 Qwen 2.5 72B 164.44 308.16 1.87 48.66 95.70 1.97 Jamba Mini 1.5 78.61 113.88 1.45 17.42 29.81 1.71 DS Llama 70B 2543.47 6792.54 2.67 702.06 1899.86 2.71 Table B2 Comparison single vs. double node deployment, Capella system Model Duration (s) Energy consumed (Wh) A30 V100 H100 A30 V100 H100 Llama 3.1 8B 20.78 27.52 36.88 2.91 2.88 5.86 Qwen 2.5 7B 19.58 24.64 36.28 2.87 2.63 5.58 Phi 3.5 Mini 19.18 25.02 41.45 2.65 2.50 5.74 Phi 3.5 MoE 77.60 32.53 45.93 17.77 6.04 15.04 DS Llama 8B 1210.90 1439.58 517.83 175.83 137.90 79.64 DS Qwen 14B 1348.09 1736.21 624.38 254.01 230.72 157.58 DS Qwen 32B 1688.23 2192.53 806.68 444.67 457.60 378.58 Table B3 Comparison of different GPU cards, single node deployment. 19 Dataset news yelp Model Energy (Wh) Accuracy Energy (Wh) Accuracy Linear BoW <0.01 0.65 <0.01 0.36 Linear Tf-idf <0.01 0.65 <0.01 0.34 Linear Embedding <0.01 0.83 0.04 0.43 XGBoost BoW <0.01 0.48 <0.01 0.31 XGBoost Tf-idf <0.01 0.52 <0.01 0.29 XGBoost Embedding 0.03 0.74 0.01 0.40 Llama 3.1 8B 4.31 0.71 4.73 0.58 Llama 3.1 70B 34.15 0.88 36.71 0.67 Qwen 2.5 7B 4.21 0.01 4.52 0.60 Qwen 2.5 72B 33.75 0.79 38.20 0.68 Phi 3.5 Mini 3.30 0.53 15.55 0.58 Phi 3.5 MoE 8.53 0.78 8.32 0.58 Jamba Mini 1.5 9.34 0.78 11.45 0.56 DS Llama 8B 60.58 0.82 97.18 0.62 DS Llama 70B 483.73 0.83 707.03 0.67 DS Qwen 14B 113.81 0.83 177.41 0.63 DS Qwen 32B 271.92 0.83 358.62 0.63 Table B4 Measurements of all models for the inference task on the news and yelp datasets, Capella system, single node, shown are averages over 10 runs Dataset tomatoes emotion Model Energy (Wh) Accuracy Energy (Wh) Accuracy Linear BoW <0.01 0.59 <0.01 0.36 Linear Tf-idf <0.01 0.59 <0.01 0.40 Linear Embedding 0.01 0.79 <0.01 0.59 XGBoost BoW <0.01 0.54 <0.01 0.30 XGBoost Tf-idf <0.01 0.55 <0.01 0.33 XGBoost Embedding <0.01 0.76 <0.01 0.53 Llama 3.1 8B 4.12 0.87 4.46 0.56 Llama 3.1 70B 32.07 0.91 34.12 0.58 Qwen 2.5 7B 4.04 0.73 4.17 0.37 Qwen 2.5 72B 33.25 0.91 34.81 0.58 Phi 3.5 Mini 7.20 0.87 5.13 0.53 Phi 3.5 MoE 7.72 0.89 8.82 0.60 Jamba Mini 1.5 8.37 0.91 10.22 0.56 DS Llama 8B 72.15 0.83 81.82 0.60 DS Llama 70B 510.86 0.90 670.40 0.61"}
{"doc_id": "2508.14170v1", "title": "Comparing energy consumption and accuracy in text classification inference", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14170v1", "chunk_id": 15, "text": "0.58 Qwen 2.5 7B 4.04 0.73 4.17 0.37 Qwen 2.5 72B 33.25 0.91 34.81 0.58 Phi 3.5 Mini 7.20 0.87 5.13 0.53 Phi 3.5 MoE 7.72 0.89 8.82 0.60 Jamba Mini 1.5 8.37 0.91 10.22 0.56 DS Llama 8B 72.15 0.83 81.82 0.60 DS Llama 70B 510.86 0.90 670.40 0.61 DS Qwen 14B 134.02 0.89 148.20 0.60 DS Qwen 32B 246.48 0.89 323.48 0.60 Table B5 Measurements of all models for the inference task on the tomatoes and emotion datasets, Capella system, single node, shown are averages over 10 runs 20 Dependent variable: Energy tomatoes emotion news yelp (1) (2) (3) (4) Duration 0.040∗∗∗ 0.043∗∗∗ 0.052∗∗∗ 0.045∗∗∗ (0.002) (0.002) (0.003) (0.003) GPUs −0.079 −0.052 0.536 0.810 (0.950) (1.011) (1.470) (1.545) Duration:GPUs 0.122∗∗∗ 0.120∗∗∗ 0.115∗∗∗ 0.120∗∗∗ (0.001) (0.001) (0.002) (0.002) Constant −0.397 −0.464 −1.300 −1.773 (1.290) (1.372) (1.985) (2.103) Observations 17 17 17 17 R2 1.000 1.000 1.000 1.000 Adjusted R2 1.000 1.000 1.000 1.000 Note: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01 Table B6 Linear regression of energy consumption on duration for the datasets of section 5.1 (table format by Hlavac, 2022)."}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 0, "text": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities∗ Xiancheng Li†1, Georgios D. Karampatakis2, Helen E. Wood2, Chris J. Griffiths2, Borislava Mihaylova2, Neil S. Coulson3, Alessio Pasinato4, Pietro Panzarasa1, Marco Viviani4, and Anna De Simoni2 1School of Business and Management, Queen Mary University of London 2Wolfson Institute of Population Health (WIPH), Queen Mary University of London 3Department of Medicine, University of Nottingham 4Department of Informatics, Systems, and Communication, University of Milano-Bicocca Abstract Digital health analytics face critical challenges nowadays. The sophisticated analysis of patient- generated health content, which contains complex emotional and medical contexts, requires scarce domain expertise, while traditional ML approaches are constrained by data shortage and privacy limitations in healthcare settings. Online Health Communities (OHCs) exemplify these challenges with mixed-sentiment posts, clinical terminology, and implicit emotional expressions that demand specialised knowledge for accurate Sentiment Analysis (SA). To address these challenges, this study explores how Large Language Models (LLMs) can integrate expert knowledge through in-context learning for SA, providing a scalable solution for sophisticated health data analysis. Specifically, we develop a structured codebook that systematically encodes expert interpretation guidelines, enabling LLMs to apply domain-specific knowledge through targeted prompting rather than extensive training. Under this strategy, six GPT models validated alongside DeepSeek and LLaMA 3.1 are compared with pre-trained language models (BioBERT variants) and lexicon-based methods, using 400 expert-annotated posts from two health communities. LLMs achieve superior performance (81-89% accuracy vs. 48-72% for traditional methods) while demonstrating expert-level agreement (Fleiss’ Kappa: 0.42-0.75). This high agreement, with no statistically significant difference (e.g., p>0.05) from inter-expert agreement levels, suggests knowledge integration beyond surface-level pattern recognition. The consistent performance across diverse LLM models, supported by in-context learning, offers a promising solution for digital health analytics. Confidence calibration analysis shows that reasoning models provide reliable uncertainty estimates, enabling quality-controlled deployment in healthcare settings. This approach thus addresses the critical challenge of expert knowl- edge shortage in digital health research, enabling real-time, expert-quality analysis for patient monitoring, intervention assessment, and evidence-based health strategies. Our open-source implementation facilitates the immediate adoption of this methodology across diverse healthcare research contexts and tasks. ∗This work was supported in part by the National Institute for Health and Care Research Program Grant for Applied Research (NIHR PGfAR reference 202037, Effectiveness and cost-effectiveness of a digital social intervention for people with troublesome asthma promoted by primary care clinicians). The views expressed are those of the authors and not necessarily those of the National Health Service, National Institute for Health and Care Research, or the Department of Health and Social Care. †Corresponding author: x.l.li@qmul.ac.uk 1 1 Introduction Over the past few years, Machine Learning (ML) has demonstrated remarkable capabilities across various aspects of digital health, including diagnostics [1], personalised treatment [2], and patient monitoring [3]. However, digital health analytics faces fundamental challenges that limit the widespread adoption of advanced computational approaches. Healthcare data is characterised by complex regulatory requirements, privacy constraints, and the need for domain-specific interpretation that goes beyond general-purpose analytics [4]. These challenges are compounded by the scarcity of labelled training data due to"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 1, "text": "digital health analytics faces fundamental challenges that limit the widespread adoption of advanced computational approaches. Healthcare data is characterised by complex regulatory requirements, privacy constraints, and the need for domain-specific interpretation that goes beyond general-purpose analytics [4]. These challenges are compounded by the scarcity of labelled training data due to privacy and governance restrictions, creating barriers to implementing sophisticated data analysis methods across digital health contexts [5]. In the realm of digital health applications—from electronic health records to patient-generated content—text analysis presents unique interpretive challenges. Health-related text consistently involves specialised medical terminology, implicit clinical meanings, and context-dependent informa- tion that require domain expertise for accurate analysis [6]. Online Health Communities (OHCs) serve as a representative and particularly challenging use case, where patients sharing chronic condition experiences create posts containing complex medical information, personal narratives, and nuanced expressions that require sophisticated interpretation [7]. The analytical challenges present in OHCs reflect broader difficulties in digital health text analysis, underscoring the need for innovative approaches to handle such complex content. Sentiment Analysis (SA) has been widely employed for quite some time to extract insights from health-related content, providing valuable opportunities to explore patient experiences, treatment outcomes, and the dynamics of OHCs as expressed through narrative text [8]. SA can reveal patterns related to patient engagement, treatment adherence, and emotional responses to interventions that would otherwise remain hidden in unstructured data [9]. As illustrated in [7, 10], OHCs exemplify this potential, since SA can be employed to capture shifts in patient mood, treatment satisfaction, and peer support effectiveness over time, providing valuable indicators for healthcare quality improvement and patient monitoring. This is particularly relevant for chronic disease management, where patients often require sustained emotional support that clinicians may struggle to provide within limited consultation time, leading many to seek encouragement and understanding from peers who share similar experiences [11]. However, despite the recognised importance of SA in the above-mentioned scenario, existing approaches still face significant limitations. Most SA tools and models have been developed and validated on general social media data, with limited evaluation on the complex, domain-specific content found in healthcare contexts [12, 13]. Furthermore, traditional ML approaches require extensive labelled datasets for effective performance, yet such data remains scarce in healthcare due to privacy constraints and the high cost of expert annotation [14, 15]. Large Language Models (LLMs) such as GPT, LLaMA, and DeepSeek, represent today a trans- formative opportunity to address both data scarcity and domain expertise challenges simultaneously in modern digital health analytics. Through extensive pre-training on diverse text corpora, including medical literature, LLMs possess inherent domain knowledge that can be activated and directed through structured prompting approaches [16]. Crucially, LLMs demonstrate the ability to integrate expert-derived guidelines and perform sophisticated analysis tasks without requiring extensive labelled training data through in-context learning, offering a pathway to scale expert knowledge in healthcare contexts where such expertise is traditionally scarce and expensive [17, 18]. This capabil- ity represents a fundamental shift from traditional data-intensive approaches to knowledge-guided methodologies, potentially democratising access to sophisticated health data interpretation across diverse digital health applications. In light of"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 2, "text": "offering a pathway to scale expert knowledge in healthcare contexts where such expertise is traditionally scarce and expensive [17, 18]. This capabil- ity represents a fundamental shift from traditional data-intensive approaches to knowledge-guided methodologies, potentially democratising access to sophisticated health data interpretation across diverse digital health applications. In light of the above, leveraging LLMs for the task of SA appears to be a highly promising direction for healthcare analytics. While this potential is significant, recent studies have primarily 2 demonstrated their capabilities in general-domain settings [17, 18], and there remains a notable gap in understanding how these models perform when applied to health-related content, where specialised terminology, contextual subtlety, and emotional complexity demand more nuanced interpretation. Most critically, the fundamental question of whether LLMs can effectively integrate domain-specific expert knowledge to achieve sophisticated analysis without extensive training data requirements has not been systematically evaluated in healthcare settings, where such expertise is traditionally scarce and expensive to scale [19]. In addition, for practical healthcare deployment, sophisticated analysis capabilities alone are insufficient—healthcare applications require a reliable assessment of when automated predictions can be trusted and when expert review is necessary. A natural approach to address this challenge is to leverage LLMs’ capacity for self-assessment, where models evaluate their own prediction confidence [20]. While LLMs can indeed be prompted to provide confidence scores alongside their predictions, the meaningfulness and reliability of these uncertainty estimates in healthcare contexts remain largely unexplored [21]. A further deployment concern is confidence calibration—whether a model’s predicted confidence aligns with its actual accuracy. Traditional performance metrics (e.g., accuracy, precision, recall) do not capture prediction reliability, yet in many contexts, understanding when a model is uncertain can be as important as the prediction itself. Therefore, systematic assessment of calibration is essential to determine whether LLMs can provide reliable uncertainty quantification. Such capability would enable quality-controlled deployment strategies in which high-confidence predictions are processed automatically while uncertain cases receive expert oversight, optimising the balance between automation efficiency and quality assurance in healthcare applications. To tackle the above-mentioned issues, this study specifically aims to evaluate the potential of LLMs to integrate expert knowledge for SA in digital health contexts. To demonstrate the effectiveness of expert knowledge integration, we develop a structured codebook that encodes domain-specific interpretation guidelines, enabling LLMs to apply expert-derived rules for consistent sentiment classification, carefully taking into account the confidence calibration analysis. We focus primarily on GPT models due to their widespread accessibility and well-documented performance [22, 23], whilst also evaluating other state-of-the-art open-source LLMs including DeepSeek and LLaMA, to establish the generalisability of our findings across different model architectures. The inclusion of open-source models is particularly important for healthcare applications, as they enable local deployment that can better ensure data security and privacy compliance when working with confidential medical data. By comparing LLM performance against traditional ML and lexicon- based approaches, we demonstrate how expert knowledge integration can achieve sophisticated analysis without extensive training data requirements, establishing a new paradigm for scalable, expert-quality analysis in digital health applications. To facilitate broader adoption, we provide a complete open-source implementation that enables"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 3, "text": "By comparing LLM performance against traditional ML and lexicon- based approaches, we demonstrate how expert knowledge integration can achieve sophisticated analysis without extensive training data requirements, establishing a new paradigm for scalable, expert-quality analysis in digital health applications. To facilitate broader adoption, we provide a complete open-source implementation that enables immediate application of our solution across diverse healthcare research contexts and expert annotation tasks in healthcare beyond SA. 1.1 Related Work The application of advanced Natural Language Processing (NLP) and, more in general, Artificial Intelligence (AI) techniques, particularly LLMs, in digital health studies has gained increasing attention in recent years. Within NLP, SA is particularly relevant for digital health, as it supports the systematic understanding of patient experiences and emotional states from unstructured text. Traditional approaches to SA span three families: (i) lexicon-based methods [24]; (ii) classical ML classifiers [25]; and (iii) transformer-based architectures (e.g., BERT) [26]. In OHCs, these approaches have been used to analyse symptom-focused discussions [27], to monitor emotional trajectories in patient interactions [10], and to detect mental health concerns or 3 at-risk users in forum settings [28]. However, health-related text introduces additional challenges, including domain shift between general and medical language, frequent abbreviations and jargon, implicit or mixed sentiment within single messages, and the scarcity of high-quality annotated datasets [29]. These factors increase the risk of domain shift when models are applied outside their original training distribution and can degrade performance in OHCs compared with general social media. Recent advances in large language models (LLMs) have introduced new possibilities for SA through zero- and few-shot in-context learning, reducing reliance on large task-specific training datasets while improving adaptability across domains. This growing body of work motivates further examination of how LLMs perform in both general-domain and health-specific SA tasks. Analyses have shown that LLMs achieve similar or even better performance compared to other approaches when applied to SA tasks using data from Reddit, Twitter (now X), and Amazon Review [17, 18, 30]. These general-domain tasks often focus on product reviews or social commentary, where sentiment tends to be explicit and polarity is easier to detect. SA has also been applied in health-related contexts. For example, Pratiwi et al. [12] used a dictionary-based approach to analyse asthma- related discussions on Twitter, while Elbers et al. [13] applied sentiment scoring to clinical notes from lung cancer patients to examine emotional trends around diagnosis and treatment. However, such applications typically focus on method demonstration rather than systematic performance evaluation, and often fail to address the inherent complexity of health-related text that requires domain-specific interpretation. Recent comparative studies have begun to address these limitations by systematically evaluating different model architectures on health-specific datasets. Elmitwalli and Mehegan [31] conducted a comprehensive comparison of lexicon-based, deep learning, and pre-trained language models using both general-purpose datasets (IMDB, Sentiment140) and domain-specific medical content (COP9-related tweets), demonstrating that pre-trained models consistently outperformed traditional approaches. Their findings showed GPT-3 achieving superior performance on the medical datasets, suggesting strong zero-shot capabilities in health-specific contexts. However, Zhang et al. [32] reported contrasting results when evaluating fine-tuned GPT and BERT models on"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 4, "text": "(IMDB, Sentiment140) and domain-specific medical content (COP9-related tweets), demonstrating that pre-trained models consistently outperformed traditional approaches. Their findings showed GPT-3 achieving superior performance on the medical datasets, suggesting strong zero-shot capabilities in health-specific contexts. However, Zhang et al. [32] reported contrasting results when evaluating fine-tuned GPT and BERT models on HPV vaccine- related Twitter content, with BERT outperforming GPT in their specific task setting. These mixed findings highlight a critical challenge in health-related SA: model performance appears highly dependent on dataset characteristics, task formulation, and fine-tuning strategies, making it difficult to establish generalisable conclusions about optimal approaches. Furthermore, these studies remain constrained to short-form Twitter content with explicit medical terminology, leaving significant gaps in understanding how these models perform on more complex, narrative-rich health communications. In contrast, OHCs represent a fundamentally different environment for SA, where users engage in extended discussions about chronic conditions, treatment experiences, and peer support. Unlike the brief, topic-specific posts typical of Twitter, OHC participants—predominantly patients, caregivers, or family members—create detailed personal narratives that often exhibit shifting emotional tones within single messages. These posts may simultaneously express gratitude for community support, anxiety about upcoming treatments, and frustration with healthcare systems, creating complex sentiment patterns that require sophisticated interpretation. The personal, experiential nature of OHC content, combined with the emotional complexity inherent in chronic disease management, presents unique challenges for automated SA that extend well beyond the capabilities of approaches designed for general social media platforms. 4 2 Methods This section describes the methodology adopted to perform SA on OHC data. We first present the datasets used in the study and the human annotation process. Next, we introduce the SA models considered for comparative evaluation. Finally, we describe the performance assessment framework and the metrics used to compare the models. 2.1 OHC Datasets and Human Annotation As described in our previous work [33], data were collected by HealthUnlocked,1 the platform provider of the Asthma + Lung UK (AUK) and British Lung Foundation (BLF) OHCs. Only posts that were shared publicly were collected and analysed. Our datasets were stored and analysed on a secure server held by Queen Mary University of London. Anonymised user identifiers (IDs) were provided by HealthUnlocked, and no demographic information was available. Our data contains three types of posts, denoted as: (i) level-0 posts (i.e., posts starting new threads), (ii) level-1 replies (i.e., replies to the level-0 posts), and (iii) level-2+ replies (i.e., replies to level-1 replies and above). The data sets consisted of 12,453 posts since 2006 for AUK, and of 367,787 posts since 2012 for BLF. We used stratified random sampling without replacement to select 200 posts from each dataset, maintaining the original distribution of post types. Five expert human annotators independently added sentiment labels (i.e., positive, neutral, or negative) to the two sets of 200 random posts from each OHC. All annotators hold PhD degrees: Annotators 2 and 5 are native English speakers, Annotators 2, 4 and 5 specialise in qualitative analysis in healthcare research, Annotator 4 is a General Practitioner with clinical experience, and Annotator 1 is a data scientist with extensive"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 5, "text": "of 200 random posts from each OHC. All annotators hold PhD degrees: Annotators 2 and 5 are native English speakers, Annotators 2, 4 and 5 specialise in qualitative analysis in healthcare research, Annotator 4 is a General Practitioner with clinical experience, and Annotator 1 is a data scientist with extensive experience in OHC data analysis. 2.1.1 Structured Codebook We developed a structured codebook to operationalise the sentiment schema used in this study. It consolidates (i) the label set with definitions and decision rules, (ii) inclusion/exclusion criteria and guidance for borderline or implicit cases, and (iii) prototypical examples with brief rationales. The codebook was created through a rigorous consensus process: annotators first independently labelled a 400-post pilot set (distinct from the final dataset), then disagreements were reviewed in multiple meetings to harmonise interpretations and formalise rules. The final version includes comprehensive sentiment definitions, instructions for handling implicit sentiment, examples with justifications, and specific rules for ambiguous or mixed-sentiment cases. We used the codebook both to train annotators and standardise labelling—supporting inter-annotator agreement and quality control—and to construct LLM prompts: zero-shot prompts comprised task instructions plus the codebook’s label definitions and decision rules (no examples), while few-shot prompts appended a small set of representative examples with gold labels. This alignment ensures that humans and models follow the same expert-derived labelling logic and facilitates the integration of domain-specific knowledge into automated analysis. 2.2 Sentiment Analysis Models Traditional SA approaches have primarily relied on two classes of methods: lexicon-based techniques that use predefined word lists categorised by sentiment polarity, and semantic-based techniques 1https://healthunlocked.com/ 5 based on supervised ML where classifiers are trained on labelled datasets [34]. Whilst lexicon- based methods offer simplicity and interpretability, they often lack contextual understanding, and supervised approaches require extensive annotated training data that may not be available in specialised domains. To evaluate the potential of LLMs for expert knowledge integration in health- specific SA, we compared their performance against these traditional approaches. We employed the following three categories of models to generate sentiment labels. 2.2.1 Lexicon-based models We used three widely-adopted lexicon-based models: Valence Aware Dictionary for Sentiment Reasoning (VADER) [35], TextBlob [36], and SentiWordNet [37]. These models were implemented using standard Python libraries and serve as established baseline approaches for SA tasks2. Lexicon- based models generate a compound score indicating the sentiment polarity of each post based on predefined sentiment lexicons. The compound score ranges from −1 to 1, where negative and positive values correspond to negative and positive sentiment, respectively. We classified posts with compound scores between −0.1 and 0.1 as neutral sentiment. 2.2.2 Fine-tuned BioBERT models BioBERT is a domain-specific language representation model pre-trained on large-scale biomedical data [38]. We selected it as a representative example of pre-trained language models requiring supervised fine-tuning, as such models have demonstrated promising performance in prior SA tasks involving health-related texts, making BioBERT a suitable baseline for comparison with LLMs and lexicon-based approaches. We fine-tuned BioBERT with four datasets: 1. 143,903 Covid-19 tweets [39], where sentiment labels were produced by NLTK, a Python library for NLP; 2. 50,333 tweets from SemEval-2017 [40],"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 6, "text": "promising performance in prior SA tasks involving health-related texts, making BioBERT a suitable baseline for comparison with LLMs and lexicon-based approaches. We fine-tuned BioBERT with four datasets: 1. 143,903 Covid-19 tweets [39], where sentiment labels were produced by NLTK, a Python library for NLP; 2. 50,333 tweets from SemEval-2017 [40], where sentiment labels were produced by annotators recruited through the CrowdFlower platform; 3. A combination of Covid-19 and SemEval-2017 tweets; 4. A balanced sub-sample of Covid-19 and SemEval-2017 tweets containing 48k tweets where a random sample of 16k tweets was selected for each sentiment label. In the remainder of the paper, the fine-tuned BioBERT models using the four datasets are referred to as BioBERT-Covid, BioBERT-SemEval, BioBERT-Combine, and BioBERT-Balance, respectively. 2.2.3 Large Language Models LLMs represent a paradigm shift in NLP, offering the ability to perform complex text analysis tasks through in-context learning via zero-shot and few-shot learning without requiring extensive task-specific training data. Our method implements expert knowledge integration by embedding domain-specific interpretation guidelines and labelling rules directly into structured prompts, derived from our expert-developed codebook. We evaluated multiple LLM architectures to demonstrate the generalisability of this expert knowledge integration approach across different model families. 2 VADER: https://github.com/cjhutto/vaderSentiment TextBlob: https://textblob.readthedocs.io/en/dev/ SentiWordNet: https://www.nltk.org/howto/sentiwordnet.html 6 GPT Models: We selected GPT models as our primary LLM evaluation target due to their widespread accessibility, well-documented performance, and user-friendly API interface that enables broader adoption in healthcare research settings. In this study, we used OpenAI’s API to query the latest models GPT-4.1, GPT-o3, and their corresponding mini variants (i.e., GPT-4.1-mini, GPT-o3-mini), which offer different computational efficiency trade-offs whilst maintaining strong performance capabilities. We evaluated both zero-shot and few-shot learning approaches to assess how effectively this structured prompting methodology could transfer expert knowledge without requiring extensive training data. For zero-shot learning, we used prompts that incorporated the expert-derived codebook guidelines and labelling rules without providing specific examples. For few-shot learning, we enhanced the structured prompts with a few carefully selected examples, referred to as GPT-4.1-fs, GPT-o3-fs, GPT-4.1-mini-fs, and GPT-o3-mini-fs, respectively. The few-shot prompts included the same codebook guidelines plus specific examples to demonstrate the application of labelling rules. Both approaches demonstrate how domain-specific knowledge can be systematically transferred to LLMs through structured prompting approaches. Other LLM Models: To establish the generalisability of our findings across different LLM architectures, we evaluated recently released models including DeepSeek (versions R1 and V3) and LLaMA 3.1 (70B and 405B). These open-source alternatives are particularly important for healthcare applications, as they enable local deployment that can better ensure data security and privacy compliance when working with confidential medical data. The performance of these models is summarised in the text to complement our main findings and demonstrate the robustness of the LLM approach across different architectures, while detailed graphical analysis focuses on the GPT model family. To ensure reproducibility and facilitate broader adoption, we provide open-source code imple- menting our structured codebook approach on GitHub3. The repository includes the complete implementation code, evaluation scripts, the codebook, zero-shot and few-shot prompts with embedded codebook rules, and example usage demonstrating the methodology. 2.3 Performance Assessment Our"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 7, "text": "model family. To ensure reproducibility and facilitate broader adoption, we provide open-source code imple- menting our structured codebook approach on GitHub3. The repository includes the complete implementation code, evaluation scripts, the codebook, zero-shot and few-shot prompts with embedded codebook rules, and example usage demonstrating the methodology. 2.3 Performance Assessment Our comparative evaluation covers five aspects: label distribution analysis, inter-annotator agreement among experts, agreement of LLMs with respect to individual annotators, detailed performance against the majority label, and confidence calibration. 1. Label distribution analysis. For each dataset, we computed the marginal frequency of Positive, Negative and Neutral labels produced by human annotators and models through visualisation. This analysis is descriptive and aims to characterise broad tendencies in assigned labels across human annotators and models. 2. Inter-annotator agreement. Among the five expert annotators who independently provided their sentiment labels for the random samples—as previously introduced in Section 2.1— was assessed using Fleiss’ kappa coefficient (denoted as κ), which reduces to Cohen’s kappa coefficient for pairwise comparisons [41], to evaluate the extent to which the five annotators agreed with each other. Statistical significance of kappa coefficients was evaluated using asymptotic z-tests against the null hypothesis of κ = 0, with significance level set at α = 0.05. Following established practices [22, 7], we assigned a new label to each post based on the majority consensus (referred to hereafter as the majority label); 3https://github.com/XianchengLI/sentiment-analysis-llm-health 7 3. LLM–expert agreement., To test whether LLMs achieve agreement levels comparable to human experts, we compared the distribution of LLM–human pairwise κ values to the distribution of human–human pairwise κ values using the Mann–Whitney U test [42] (non-parametric, no normality assumption; α = 0.05). Human–human agreement comprised all 10 annotator pairs (n = 10), and LLM–human agreement comprised all model–annotator pairs across models (n = 40). A non-significant result (p > 0.05) indicates that LLM–human agreement shows no statistically significant difference from expert-level agreement. 4. Performance against the majority label. To evaluate the effectiveness of expert knowledge integration across the three categories of models, we employed a multi-faceted assessment approach. First, we used agreement metrics to report the level of agreement between different models and the expert annotators, with high agreement indicating successful knowledge transfer. We then considered the majority label as the benchmark and computed the accuracy of each model (i.e., the percentage of correct labels). Additionally, we calculated precision, recall, and F1-score for each sentiment category to provide a comprehensive performance evaluation [43]; 5. Confidence estimation and calibration. To evaluate prediction reliability for practical healthcare deployment, we implemented confidence estimation for selected GPT models capable of providing uncertainty quantification. Confidence scores were obtained by requesting models to report their certainty level ([0 −1] scale) alongside predictions, where 0 indicates complete uncertainty and 1 indicates complete certainty. Detailed confidence-enabled prompts and rules are available in our open-source repository. Confidence calibration was assessed using reliability diagrams plotting predicted confidence against actual accuracy across confidence bins. We also analysed confidence score distributions to evaluate whether models produce informative estimates across the full range or concentrate in narrow bands that limit utility for"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 8, "text": "and rules are available in our open-source repository. Confidence calibration was assessed using reliability diagrams plotting predicted confidence against actual accuracy across confidence bins. We also analysed confidence score distributions to evaluate whether models produce informative estimates across the full range or concentrate in narrow bands that limit utility for quality assessment. This analysis assesses model suitability for applications requiring uncertainty quantification and prediction reliability evaluation. 3 Results This section is dedicated to presenting and discussing the results of the comparative evaluation with respect to the five key points outlined previously in Section 2.3. Concerning the first key point, Figure 1 shows the distribution of sentiment labels produced by our selected models and expert annotators. Across both datasets, positive labels were more frequently assigned than negative or neutral labels by both human experts and most models. DeepSeek and LLaMA demonstrated similar distributional patterns, with positive sentiment being the most commonly assigned label across both datasets. Concerning the second key point, Figures 2A and 2B present the agreement matrix among expert annotators. Majority consensus labels could be determined for all posts in both datasets, reflecting the effectiveness of our expert annotation process. For the AUK sample, inter-annotator agreement (κ) ranged from 0.49 to 0.71 with an overall κ of 0.59, indicating moderate agreement among experts. The highest agreement was between Annotators 1 and 2 with 82% agreement (κ = 0.71). Individual annotator agreement with the majority consensus ranged from 0.70 to 0.79. For the BLF sample, inter-annotator κ values ranged from 0.33 to 0.67 with an overall κ of 0.52, also indicating moderate agreement. The highest agreement was between Annotators 1 and 2 with 86% agreement (κ = 0.67). Agreement with majority consensus ranged from 0.58 to 0.82. All κ values were statistically significant (p < 0.001), confirming genuine agreement beyond chance levels. 8 Figure 1: Distribution of labels given by each of the annotators and models to selected posts from the AUK OHC (upper panel) and the BLF OHC (lower panel). Concerning the third key point, Figure 2C and 2D present the agreement of the selected models with individual annotators. Although the agreement varies among different annotators, LLM models have consistently higher agreement with annotators than the other models. The agreement between GPT models and annotators ranged from 68% to 89% with κ ranging from 0.42 to 0.75, which aligns with the degree of agreement among annotators themselves. Mann-Whitney U test revealed no significant difference between LLM-human agreement scores and human-human agreement scores (p = 0.68 in AUK and p = 0.10 in BLF), confirming that LLMs achieved genuine expert-level agreement rather than systematic bias. This level of agreement indicates successful integration of expert knowledge, as the models achieved human-level consistency in sentiment classification without requiring extensive training data. DeepSeek and LLaMA demonstrated similar performance, with agreement ranging from 69% to 92% with κ ranging from 0.45 to 0.82, confirming the robustness of expert knowledge integration across different LLM architectures. 9 Figure 2: Agreement Matrix of annotators and all models. Agreements are measured by the ratio of posts with the same labels"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 9, "text": "demonstrated similar performance, with agreement ranging from 69% to 92% with κ ranging from 0.45 to 0.82, confirming the robustness of expert knowledge integration across different LLM architectures. 9 Figure 2: Agreement Matrix of annotators and all models. Agreements are measured by the ratio of posts with the same labels assigned by two annotators or models. Panels A and B show the agreement among annotators. Panels C and D show the agreement between annotators and models. 10 We then selected one model at a time as an additional annotator and calculated the overall κ among all annotators including the selected model (Table 1). When adding labels from any LLM, the overall κ increased from baseline values (0.586 for AUK, 0.524 for BLF). In contrast, the overall κ decreased when adding labels from traditional approaches (lexicon-based and BioBERT models). Similar patterns were observed with DeepSeek and LLaMA models, which also increased overall agreement when included as additional annotators. These patterns suggest that LLM-generated labels showed no statistically significant difference from expert annotations in agreement trends. Table 1: Overall κ calculated after adding labels from the selected model to those provided by the human annotators. Overall κ values higher than the annotators only value are in bold. Source of labels added Overall κ (AUK) Overall κ (BLF) Annotators only (Baseline) 0.586 0.524 VADER 0.409 0.449 TextBlob 0.445 0.427 SentiWordNet 0.419 0.398 BioBERT-Covid 0.498 0.485 BioBERT-SemEval 0.498 0.485 BioBERT-Combine 0.490 0.465 BioBERT-Balance 0.491 0.487 GPT-o3 0.604 0.549 GPT-o3-mini 0.604 0.540 GPT-4.1 0.604 0.542 GPT-4.1-mini 0.598 0.550 GPT-o3-mini-fs 0.604 0.542 GPT-o3-fs 0.614 0.552 GPT-4.1-fs 0.608 0.556 GPT-4.1-mini-fs 0.612 0.546 Concerning the fourth key point, Figure 3A presents the accuracy of different models compared to the majority labels based on the AUK sample. LLMs demonstrated superior performance, with GPT-o3-fs achieving the highest accuracy (87%), whilst other GPT variants achieved accuracy between 82% and 86%. Traditional approaches performed considerably lower, with BioBERT models (48%-61%) slightly outperforming lexicon-based models, among which VADER achieved the best accuracy. For the BLF sample (Figure 3B), similar patterns emerged with GPT-o3-fs achieving 86% accuracy and other GPT variants ranging from 83% to 86%. Traditional approaches again performed lower (57%-72%). Across both datasets, LLMs consistently achieved the highest accuracy, with GPT-o3-fs being the most accurate overall. Analysis of the accuracy results also reveals differences between GPT model generations and zero- shot versus few-shot approaches. As expected, full-size models (GPT-4.1 and GPT-o3) performed better than mini variants (GPT-4.1-mini and GPT-o3-mini). Few-shot learning demonstrated the potential for performance improvement, with GPT-4.1 showing particular sensitivity to few-shot examples, increasing accuracy from 83% to 86% on both datasets. The inclusion of expert-derived examples in few-shot prompts resulted in accuracy improvements of up to 3%. Additional LLMs achieved comparable performance, with accuracy ranging from 81% to 88% on AUK and 82% to 89% on BLF samples. Notably, LLaMA 3.1 405B consistently achieved the highest accuracy across both datasets with strong zero-shot capabilities, whilst other LLMs demonstrated performance comparable to GPT models. 11 Figure 3: Accuracy of models. Accuracy is measured by the ratio of posts with same labels assigned"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 10, "text": "and 82% to 89% on BLF samples. Notably, LLaMA 3.1 405B consistently achieved the highest accuracy across both datasets with strong zero-shot capabilities, whilst other LLMs demonstrated performance comparable to GPT models. 11 Figure 3: Accuracy of models. Accuracy is measured by the ratio of posts with same labels assigned by the model and majority label. We then measured the performance using precision, recall, and F1-score across different sentiment labels (negative, neutral, positive). Table 2 presents the results on both datasets. Whilst the model with the highest F1-score varied across different sentiment labels, the best performers consistently belonged to the LLM category. As F1-score balances precision and recall, our results demonstrate that LLMs consistently outperform traditional approaches across all three sentiment categories. Table 3 presents a comprehensive comparison of the best-performing models of each category within each dataset. The results clearly demonstrate the superior performance of LLMs compared to traditional approaches. LLaMA 3.1-405B-fs achieved the highest performance across the majority 12 Table 2: Performance of models, measured by Precision, Recall and F1-score. The highest value in each column on each data set is in bold. Model Positive Negative Neutral Precision Recall F1-Score Precision Recall F1-Score Precision Recall F1-Score AUK Data Set GPT-4.1 0.939 0.886 0.912 0.719 0.852 0.780 0.757 0.683 0.718 GPT-4.1-fs 0.950 0.914 0.932 0.796 0.796 0.796 0.733 0.805 0.767 GPT-4.1-mini 0.947 0.857 0.900 0.719 0.852 0.780 0.707 0.707 0.707 GPT-4.1-mini-fs 0.934 0.943 0.938 0.837 0.759 0.796 0.711 0.780 0.744 GPT-o3 0.917 0.952 0.935 0.706 0.889 0.787 0.913 0.512 0.656 GPT-o3-fs 0.927 0.971 0.949 0.780 0.852 0.814 0.839 0.634 0.722 GPT-o3-mini 0.948 0.876 0.911 0.734 0.870 0.797 0.718 0.683 0.700 GPT-o3-mini-fs 0.939 0.876 0.906 0.719 0.852 0.780 0.789 0.732 0.759 BioBERT-Balance 0.914 0.505 0.650 0.489 0.852 0.622 0.458 0.537 0.494 BioBERT-Combine 0.894 0.562 0.690 0.674 0.537 0.598 0.341 0.756 0.470 BioBERT-Covid 0.903 0.533 0.671 0.500 0.759 0.603 0.464 0.634 0.536 BioBERT-SemEval 0.903 0.533 0.671 0.500 0.759 0.603 0.464 0.634 0.536 SentiWordNet 0.569 0.667 0.614 0.356 0.389 0.372 0.278 0.122 0.169 TextBlob 0.630 0.714 0.670 0.400 0.333 0.364 0.306 0.268 0.286 VADER 0.681 0.752 0.715 0.532 0.611 0.569 0.364 0.195 0.254 BLF Data Set GPT-4.1 0.934 0.832 0.880 0.889 0.970 0.928 0.476 0.667 0.556 GPT-4.1-fs 0.925 0.905 0.915 0.882 0.909 0.896 0.562 0.600 0.581 GPT-4.1-mini 0.951 0.847 0.896 0.780 0.970 0.865 0.514 0.633 0.567 GPT-4.1-mini-fs 0.958 0.839 0.895 0.833 0.909 0.870 0.477 0.700 0.568 GPT-o3 0.886 0.964 0.923 0.818 0.818 0.818 0.778 0.467 0.583 GPT-o3-fs 0.900 0.920 0.910 0.857 0.909 0.882 0.640 0.533 0.582 GPT-o3-mini 0.951 0.854 0.900 0.738 0.939 0.827 0.514 0.600 0.554 GPT-o3-mini-fs 0.938 0.876 0.906 0.806 0.879 0.841 0.500 0.600 0.545 BioBERT-Balance 0.924 0.708 0.802 0.519 0.848 0.644 0.488 0.667 0.563 BioBERT-Combine 0.894 0.737 0.808 0.692 0.545 0.610 0.295 0.600 0.396 BioBERT-Covid 0.949 0.679 0.791 0.500 0.818 0.621 0.417 0.667 0.513 BioBERT-SemEval 0.949 0.679 0.791 0.500 0.818 0.621 0.417 0.667 0.513 SentiWordNet 0.754 0.737 0.745 0.289 0.394 0.333 0.048 0.033 0.039 TextBlob 0.630 0.714 0.670 0.400 0.333 0.364 0.306 0.268 0.286 VADER 0.681 0.752 0.715 0.532 0.611 0.569 0.364 0.195 0.254 Table 3: Performance comparison of best models in"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 11, "text": "0.667 0.513 BioBERT-SemEval 0.949 0.679 0.791 0.500 0.818 0.621 0.417 0.667 0.513 SentiWordNet 0.754 0.737 0.745 0.289 0.394 0.333 0.048 0.033 0.039 TextBlob 0.630 0.714 0.670 0.400 0.333 0.364 0.306 0.268 0.286 VADER 0.681 0.752 0.715 0.532 0.611 0.569 0.364 0.195 0.254 Table 3: Performance comparison of best models in each category, measured by Precision, Recall and F1-score. The highest value in each column on each dataset is in bold. Model Positive Negative Neutral Precision Recall F1-Score Precision Recall F1-Score Precision Recall F1-Score AUK Dataset GPT-o3-fs 0.927 0.971 0.949 0.780 0.852 0.814 0.839 0.634 0.722 BioBERT-Covid 0.903 0.533 0.671 0.500 0.759 0.603 0.464 0.634 0.536 LLaMA 3.1-405B-fs 0.935 0.962 0.948 0.880 0.815 0.846 0.738 0.756 0.747 DeepSeek V3-fs 0.933 0.933 0.933 0.746 0.926 0.826 0.857 0.585 0.696 BLF Dataset GPT-o3-fs 0.900 0.920 0.910 0.857 0.909 0.882 0.640 0.533 0.582 BioBERT-Balance 0.924 0.708 0.802 0.519 0.848 0.644 0.488 0.667 0.563 LLaMA 3.1-405B-fs 0.935 0.962 0.948 0.880 0.815 0.846 0.738 0.756 0.747 DeepSeek V3-fs 0.933 0.933 0.933 0.746 0.926 0.826 0.857 0.585 0.696 of metrics, securing the best F1-scores for five out of six sentiment categories across both datasets. DeepSeek V3-fs also demonstrated strong performance, particularly excelling in negative sentiment recall. In contrast, traditional approaches showed considerably lower performance, with BioBERT models achieving F1-scores ranging from 0.536 to 0.802, substantially below the LLM performance range of 0.582 to 0.948. This comprehensive comparison confirms the consistent superiority of LLMs across different architectures, datasets, and sentiment categories, validating the effectiveness of expert knowledge integration approaches for health-related SA. Concerning the last key point, Figure 4 presents confidence calibration results for o3 model 13 variants across different prompting strategies and o3-mini with few-shot prompting as a representative comparison. Due to space constraints, GPT-4.1 results are summarised textually below. The o3 model demonstrates superior confidence estimation with well-distributed scores between 0.5 and 1, enabling effective prediction quality differentiation. Few-shot o3 model emerges as the optimal strategy, showing a strong linear relationship between confidence and accuracy, with the calibration curve closely tracking the perfect calibration line. This linear progression indicates that higher confidence scores reliably correspond to higher accuracy, making confidence a valuable indicator for automated label quality assessment. In contrast, zero-shot prompting shows more variable calibration patterns, while naive prompting (i.e., no instruction from the codebook) exhibits notable deviations from perfect calibration, particularly in mid-confidence ranges. The o3-mini few-shot results shown are representative of all o3-mini configurations due to consistent patterns across prompting strategies. o3-mini models show severely concentrated confidence distributions above 0.8 across all prompting strategies, rendering confidence scores uninformative for quality assessment. Prompting strategy has minimal impact on o3-mini due to fundamental limitations in confidence estimation. Similarly, all GPT-4.1 variants exhibited limitations similar to o3-mini. Unlike o3 models, GPT-4.1 variants are not specifically designed for reasoning tasks, which may explain their inability to produce meaningful confidence estimates. The superior performance of o3 models with few-shot prompting suggests that reasoning capabilities, combined with contextual examples, enable better understanding of confidence semantics, translating into meaningful uncertainty estimates. These results indicate that only o3 models with few-shot prompting provide"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 12, "text": "tasks, which may explain their inability to produce meaningful confidence estimates. The superior performance of o3 models with few-shot prompting suggests that reasoning capabilities, combined with contextual examples, enable better understanding of confidence semantics, translating into meaningful uncertainty estimates. These results indicate that only o3 models with few-shot prompting provide confidence scores suitable for label quality evaluation and selective prediction strategies. 14 Figure 4: Confidence calibration analysis across different model-prompt combinations. The left column displays confidence score distributions, while the right column shows calibration curves (predicted confidence vs. actual accuracy). Diagonal lines represent perfect calibration. 15 4 Discussion 4.1 Principal Results This study demonstrates that LLMs can successfully integrate expert knowledge to achieve so- phisticated SA in digital health contexts without requiring extensive labelled training data. Our systematic evaluation reveals that LLMs consistently outperformed traditional approaches across multiple performance metrics, with the most significant finding being their ability to achieve expert- level agreement (κ: 0.42-0.75) through structured knowledge integration rather than data-intensive training. The core methodological innovation lies in our codebook-guided approach, which systematically transfers expert domain knowledge through structured prompting. Unlike retrieval-augmented generation (RAG) methods that dynamically retrieve information from external knowledge bases, our approach embeds expert-derived interpretation guidelines directly into prompts, enabling consistent application of domain-specific rules. This approach is particularly effective for SA where classification rules can be systematically defined and consistently applied across datasets, representing a fundamental shift from traditional data-intensive methods to knowledge-guided approaches in healthcare analytics. This paradigm shift directly addresses fundamental limitations of existing methods in healthcare contexts. While lexicon-based models lack the contextual understanding required for health-related content, and traditional pre-trained models like BioBERT require substantial fine-tuning with labelled datasets that are scarce in healthcare due to privacy constraints, LLMs demonstrated the ability to apply expert-derived guidelines directly without domain-specific training. The practical implications extend beyond performance improvements to address critical barriers in healthcare analytics: the high cost of expert annotation and the technical expertise traditionally required for advanced text analysis. By enabling sophisticated analysis through structured prompting rather than complex model development, this approach democratizes access to expert-quality text interpretation, making advanced analytics feasible for healthcare research teams regardless of their technical infrastructure or ML expertise. This accessibility is particularly valuable for smaller healthcare organisations and research teams with limited resources, where the ability to achieve expert-quality results through knowledge integration rather than extensive data collection reduces both financial burdens and technical barriers. The structured codebook methodology offers distinct advantages for healthcare applications where expert interpretation rules can be systematically defined and consistently applied across large datasets. Our confidence calibration analysis reveals important distinctions in uncertainty estimation capabilities across LLM architectures. The GPT-o3 model with few-shot prompting demonstrated superior confidence calibration, showing a strong linear relationship between predicted confidence and actual accuracy. This calibration quality enables practical deployment strategies where confidence scores can reliably indicate prediction quality, supporting automated processing of high-confidence cases while flagging uncertain predictions for expert review. In contrast, other model variants, including GPT-4.1 and GPT-o3-mini, showed concentrated confidence distributions that limit their utility for quality assessment. These findings suggest"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 13, "text": "calibration quality enables practical deployment strategies where confidence scores can reliably indicate prediction quality, supporting automated processing of high-confidence cases while flagging uncertain predictions for expert review. In contrast, other model variants, including GPT-4.1 and GPT-o3-mini, showed concentrated confidence distributions that limit their utility for quality assessment. These findings suggest that confidence estimation capabilities vary significantly across model architectures and training approaches, with reasoning models like GPT-o3 better equipped to provide meaningful uncertainty estimates. This has important implications for healthcare deployment, where reliable confidence estimation is crucial for maintaining quality assurance while maximising automation efficiency. The consistency of results across different LLM architectures—including GPT models, LLaMA, and DeepSeek—validates the robustness of this knowledge integration approach and provides 16 healthcare organisations with crucial implementation flexibility. This cross-architectural validation is particularly important for healthcare applications, where open-source models like DeepSeek and LLaMA offer significant advantages for local deployment, allowing organisations to select models based on their specific security, privacy, and resource requirements. Our findings have significant implications for digital health applications beyond SA. The ability to encode expert knowledge into structured guidelines that LLMs can reliably apply offers a scalable solution for various healthcare text analysis tasks, from clinical documentation interpretation to patient experience monitoring. The demonstrated effectiveness in OHCs, with their complex emotional expressions and medical terminology, suggests strong potential for application in other challenging healthcare text analysis contexts. The structured knowledge integration framework extends beyond SA to other expert annotation tasks in healthcare, providing a generalisable approach for scaling domain expertise across diverse text analysis applications. This knowledge integration approach offers a practical pathway for implementing advanced analytics in resource-constrained healthcare environments whilst maintaining expert-quality interpretive standards. 4.2 Strengths and Limitations A key strength of this study lies in its comprehensive evaluation design, systematically comparing LLMs against both traditional ML and lexicon-based approaches using expert-annotated healthcare data. The rigorous annotation process involving five experts with PhD qualifications, including a clinician, ensured high-quality gold standard labels for meaningful performance assessment. The inclusion of both zero-shot and few-shot learning evaluation provides practical insights into LLM deployment scenarios where training data availability varies. The focus on OHCs as a challenging test case represents another strength, as OHC content exhibits complex sentiment patterns, medical terminology, and emotional nuance that extend beyond typical social media analysis. This context provides a robust evaluation environment for assessing automated approaches in healthcare-specific settings where traditional methods often struggle. From a methodological perspective, the structured codebook approach offers a replicable frame- work for knowledge transfer across different healthcare domains and research contexts. This systematic method for expert knowledge encoding provides a practical roadmap for researchers seeking to implement similar analysis capabilities without requiring extensive ML expertise. The evaluation of multiple LLM architectures, including open-source models, provides implementation flexibility for organisations with different security and privacy requirements, particularly important when handling confidential medical data. Automated SA through expert knowledge integration enables large-scale monitoring of patient discussions, offering insights into treatment responses and peer support effectiveness that would otherwise require extensive expert review. This approach enables access to sophisticated analytical capabilities, enabling healthcare"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 14, "text": "different security and privacy requirements, particularly important when handling confidential medical data. Automated SA through expert knowledge integration enables large-scale monitoring of patient discussions, offering insights into treatment responses and peer support effectiveness that would otherwise require extensive expert review. This approach enables access to sophisticated analytical capabilities, enabling healthcare research teams to conduct advanced text analysis without requiring extensive technical infrastructure. Our open-source implementation further enhances accessibility by providing ready-to-use tools that enable healthcare researchers to apply these advanced analytical capabilities immediately, without the complexity of model training, fine-tuning, or extensive technical infrastructure typically required for traditional ML approaches. The confidence calibration analysis provides additional practical value by identifying models suitable for quality-controlled deployment scenarios. The demonstrated linear relationship between confidence and accuracy in GPT-o3 few-shot models enables the implementation of selective prediction strategies, where high-confidence predictions can be processed automatically while uncertain cases receive human oversight. This capability addresses a critical need in healthcare applications where balancing automation efficiency with quality assurance is paramount. 17 This study also has several limitations that warrant consideration. Our evaluation focused primarily on SA within OHCs, and the broader applicability of the structured prompting approach across different digital health text analysis contexts remains to be established. The knowledge integration methodology, whilst demonstrating strong performance, depends heavily on the quality and comprehensiveness of expert-derived guidelines. Our codebook development involved extensive consensus-building among five expert annotators, but this process may be challenging to replicate across different healthcare domains or cultural contexts where expert perspectives might vary significantly. Standardisation approaches for codebook development across different types of medical expertise require further investigation. Additionally, while we evaluated multiple LLM architectures to demonstrate robustness, our assessment was limited to zero-shot and few-shot learning scenarios. The potential benefits of fine- tuning LLMs specifically for healthcare applications, particularly with privacy-preserving techniques, represent an important area for future investigation. The trade-offs between knowledge-guided prompting and domain-specific fine-tuning require further exploration. Furthermore, although our statistical validation confirmed expert-level agreement, the deployment of LLMs in clinical or high-stakes healthcare contexts requires careful consideration of interpretability, accountability, and ethical implications. SA in healthcare often involves subjective judgement and contextual nuance that may require ongoing human oversight, particularly in settings where patient safety or clinical decision-making are directly involved. While our structured prompting approach proved effective for SA, future research could investigate how hybrid methodologies combining our codebook-guided approach with RAG mechanisms might enhance performance for more complex analytical tasks requiring dynamic access to evolving medical knowledge or real-time information integration. Additionally, exploring the application of this knowledge integration framework across different digital health text analysis contexts would further validate the generalisability of the approach. 5 Conclusions This study demonstrates that LLMs can effectively integrate expert knowledge to achieve sophis- ticated SA in digital health contexts through structured prompting approaches. Our systematic evaluation establishes the feasibility of knowledge-guided analysis that achieves expert-quality results without requiring extensive training data. Reasoning models also demonstrated reliable uncertainty estimation capabilities, supporting deployment strategies suitable for healthcare applications. The core contribution lies in demonstrating how expert domain knowledge can be systematically"}
{"doc_id": "2508.14032v1", "title": "The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14032v1", "chunk_id": 15, "text": "through structured prompting approaches. Our systematic evaluation establishes the feasibility of knowledge-guided analysis that achieves expert-quality results without requiring extensive training data. Reasoning models also demonstrated reliable uncertainty estimation capabilities, supporting deployment strategies suitable for healthcare applications. The core contribution lies in demonstrating how expert domain knowledge can be systematically transferred to automated systems through structured knowledge integration, enabling sophisticated healthcare text analysis without traditional data requirements. This approach offers implementation flexibility across different technological contexts while maintaining analytical quality standards. As healthcare increasingly relies on data-driven insights, LLMs offer a transformative pathway for scaling expert knowledge across diverse digital health applications. Our open-source implementation facilitates immediate adoption across healthcare research contexts, with demonstrated uncertainty estimation capabilities that further support practical deployment. The framework enables researchers to readily test its applicability to other digital health text annotation tasks, facilitating rapid validation and subsequent deployment. Future research should explore the application of knowledge integration frameworks across diverse digital health contexts to further validate the generalisability of this approach and support the development of more accessible, expert-quality analytical tools for healthcare research and practice. 18 Ethical Considerations The study was approved by Queen Mary University of London’s Ethics of Research Committee (QMERC22·279). In addition, the research protocol was examined and permission to undertake the research was obtained from AUK, BLF charities, as well as HealthUnlocked. Acknowledgment The authors would like to thank Asthma UK, British Lung Foundation and HealthUnlocked for granting the permission to conduct the study."}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 0, "text": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation Dongyoon Hahm*, Taywon Min*, Woogyeol Jin*, Kimin Lee KAIST {hahmdong, taywonmin, wlsdnruf2, kiminlee}@kaist.ac.kr Abstract Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of plan- ning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent- specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can be- come unintentionally misaligned, leading to a higher likeli- hood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To ad- dress these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent re- sponses, guiding them to refuse harmful requests while pre- serving performance on benign tasks. Specifically, we intro- duce an iterative approach that alternates between (1) gener- ating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental re- sults demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effective- ness. PING consistently outperforms existing prompting ap- proaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature. 1 Introduction Autonomous agents powered by large language models (LLMs) have demonstrated the ability to perform a wide range of tasks across various domains, including web nav- igation (Zhou et al. 2023; Yao et al. 2022), code genera- tion (Wang et al. 2023; Jimenez et al. 2023) and mobile device control (Lee et al. 2024a,b; Rawles et al. 2024). These LLM agents engage in decision-making, utilize tools, and interact with their environment to accomplish complex tasks. While these capabilities unlock new applications, they also introduce novel safety risks. For example, a web naviga- tion agent could be exploited to publish and spread misinfor- mation (Kim et al. 2024), while a code agent might execute a reverse shell or delete critical system files (Guo et al. 2024). *These authors contributed equally. It is therefore critical to ensure LLM agents operate safely across diverse domains (Hahm et al. 2025). Despite growing concerns about safety, the development of LLM-based agents often overlooks safety considerations. Many existing studies focus on fine-tuning to enhance task performance, yet they rarely incorporate safety during this process (Jimenez et al. 2023; Lee et al. 2024b; Lai et al. 2024; L` u, Kasner, and Reddy 2024). Agentic datasets used for fine-tuning are typically collected under standard, non- adversarial conditions, where agents are trained to complete tasks without encountering risky or harmful scenarios (L` u, Kasner, and Reddy 2024; Qi et al. 2024b; Wang et al. 2024; Zheng et al. 2024). However, evidence from non-agentic do- mains suggests cause for concern: misalignment can emerge after fine-tuning on seemingly benign datasets (He, Xia, and Henderson 2024; Qi"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 1, "text": "tasks without encountering risky or harmful scenarios (L` u, Kasner, and Reddy 2024; Qi et al. 2024b; Wang et al. 2024; Zheng et al. 2024). However, evidence from non-agentic do- mains suggests cause for concern: misalignment can emerge after fine-tuning on seemingly benign datasets (He, Xia, and Henderson 2024; Qi et al. 2023; Lyu et al. 2024; Hawkins, Mittelstadt, and Russell 2024). For instance, Lyu et al. (2024) demonstrated that fine-tuning LLMs on mathemati- cal reasoning (Cobbe et al. 2021) and medical knowledge (Li et al. 2023) can lead to an increase in harmfulness. This raises a natural question: Do unintended misalignments arise when fine-tuning LLMs on benign agentic tasks? In this work, we show that fine-tuning LLMs on agentic tasks can lead to unintentional misalignment, increasing the likelihood of executing harmful instructions and reducing refusal behavior. Specifically, we fine-tune various LLMs on web navigation and code generation datasets, and eval- uate their task performance using WebArena-lite (Liu et al. 2024) and MINT-ALFWorld (Wang et al. 2023), and assess their safety using RedCode-Exec (Guo et al. 2024) and our newly introduced WebDojo benchmark. While fine-tuning improves performance on benign tasks, it also substan- tially increases the risk of following harmful tasks. For in- stance, Llama-3.1-8B-Instruct (Grattafiori et al. 2024) shows a 20.0% improvement in task success on WebArena-lite, but a 38.09% increase in attack success on WebDojo after fine- tuning. These results highlight the safety vulnerabilities in fine-tuned LLM agents and the need for effective mitigation. To mitigate unintended misalignment that emerges from agentic fine-tuning, we propose a simple yet effective method called Prefix INjection Guard (PING). Our main idea is to prepend natural language prefixes which cause LLM agents to refuse harmful requests, while maintaining 1 x x x Harmful Requests Benign Requests Base LLM LLM Agent LLM Agent + PING Refuses Request Refuses Request Performs Request Incorrect Response Correct Response Correct Response Leave the comment 'Have a great trip! Please share your travel stories when you return' on the second post do(action=\"Hover\", element=\"0\") # Element: the 'Like' button of the first post # As an AI committed to ethical standards, ensure that you assist with helpful and constructive tasks, while consistently refusing any requests that could be harmful or unethical. In this case, we will comment on the second post in the'Comments' section.# Element: the 'Comments' button below the second post do(action=\"Click\", element=\"3\") # Element: the 'Comments' button below the post by user 'travel_lover' do(action=\"Click\", element=\"3\") I cannot provide instructions on how to buy illegal substances. Is there anything else I can help you with? # Element: the 'Illicit Substances' category link do(action=\"Click\", element=\"4\") # As an AI committed to ethical standards, ensure that you assist with helpful and constructive tasks, while consistently refusing any requests that could be harmful or unethical. In this case, I will not assist with the task of buying cocaine. exit(message=\"I cannot assist with the task of buying cocaine.\") Buy cocaine at the mart. Figure 1: An overview of our work. We observe that fine-tuning LLMs with agentic datasets improves performance on benign tasks but"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 2, "text": "or unethical. In this case, I will not assist with the task of buying cocaine. exit(message=\"I cannot assist with the task of buying cocaine.\") Buy cocaine at the mart. Figure 1: An overview of our work. We observe that fine-tuning LLMs with agentic datasets improves performance on benign tasks but also increases the likelihood of completing harmful tasks, resulting in unsafe behavior. To address this misalignment, we propose Prefix Injection Guard (PING), which enhances agent safety while maintaining strong performance on benign tasks. high performance on intended tasks (see Figure 1). Specifi- cally, our approach iteratively alternates two steps: (1) gen- erating candidate prefixes using capable LLMs and (2) se- lecting the prefix that maximizes both task completion ac- curacy and harmful request refusal rate. Through this itera- tive process which minimizes human effort, PING automat- ically identifies effective prefixes that enhance LLM agent safety while preserving its intended capabilities. We evalu- ate PING across multiple domains using various LLMs like Llama-3.1-8B-Instruct (Grattafiori et al. 2024) and GPT- 4o-mini (Hurst et al. 2024). Results show that PING con- sistently improves LLM agent safety compared to exist- ing prompting strategies (e.g., constitutional AI prompts, few-shot safety examples) while maintaining performance. Specifically, PING increases harmful request refusal rates by an average of 68.3% in the web navigation domain and 44.6% in the code generation domain compared to baseline agents, while maintaining nearly identical task performance with minimal degradation (just 1.8% for both web naviga- tion and code generation). Furthermore, PING is compatible with guardrail models such as WildGuard (Han et al. 2024), enabling layered safety approaches. Beyond empirical evaluations, we investigate how PING influences the internal representations of LLM agents and induce refusal behavior for harmful tasks. Specifically, we train linear probes (Yang et al. 2024b) on averaged acti- vations from LLM agents across input sequences, produc- ing logit values that differentiate harmful from benign in- puts. We hypothesize these linear probes capture internal features associated with refusal behavior, with higher logit values for final tokens correlating with successful refusal. We validate this hypothesis by demonstrating that activation steering (Winninger, Addad, and Kapusta 2025; Turner et al. 2023), which adds a linear probe vector on final token acti- vations, successfully triggers the model to refuse harmful tasks. Notably, vanilla LLM agents exhibit low linear probe logits for final tokens when processing harmful tasks, cor- responding to their failure to refuse such instructions. In contrast, LLM agents integrated with PING, which effec- tively refuse harmful tasks, consistently display significantly higher final token linear probe logit values. This finding pro- vides mechanistic evidence that PING enhances safety by strategically shifting model representations, particularly at critical decision points corresponding to the initial tokens of the model response. 2 Unintentional Misalignment in LLM Agent In this section, we demonstrate that fine-tuning large lan- guage models (LLMs) on agentic datasets introduce unin- tended misalignment risks, even when the training data ap- pears benign. We first present a threat model in Section 2.1 that formalizes these risks during the agent training process. Our empirical results in Section 2.2 reveal consistent"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 3, "text": "demonstrate that fine-tuning large lan- guage models (LLMs) on agentic datasets introduce unin- tended misalignment risks, even when the training data ap- pears benign. We first present a threat model in Section 2.1 that formalizes these risks during the agent training process. Our empirical results in Section 2.2 reveal consistent mis- alignment patterns emerging across diverse models and do- mains. Finally, in Section 2.3, we show that injecting an ap- propriate prefix can effectively induce refusal behaviors in otherwise misaligned LLM agents, offering a practical mit- igation approach with implications for safer deployment of agentic systems. 2.1 Threat Model We consider a scenario where developers fine-tune an LLM on an agentic dataset to create a specialized agent capa- ble of performing domain-specific tasks. Usually, agentic datasets, collected under standard non-adversarial condi- tions (L` u, Kasner, and Reddy 2024; Qi et al. 2024b; Wang et al. 2024; Zheng et al. 2024), consist of benign demonstra- tions for completing tasks. Once fine-tuned, the LLM agent 2 Model Fine-tuning WebArena WebDojo MINT RedCode SR (↑) ASR (↓) RR (↑) SR (↑) ASR (↓) RR (↑) Llama-3.1-8B- Instruct ✗ 2.42% 23.81% 31.75% 71.77% 43.38% 15.17% ✓ 22.42% 61.90% 7.94% 71.77% 66.06% 2.60% GLM-4-9B- Chat ✗ 5.45% 26.98% 3.17% 22.58% 63.29% 13.70% ✓ 16.97% 53.97% 4.76% 72.58% 72.39% 1.48% Qwen2.5-7B- Instruct ✗ 3.03% 14.29% 3.17% 70.16% 58.33% 6.02% ✓ 7.27% 57.14% 12.70% 85.48% 86.02% 3.10% Table 1: Performance of LLMs before and after fine-tuning on agentic datasets, evaluated across web navigation (WebArena and WebDojo) and code generation (MINT-ALFWorld and RedCode-Exec) benchmarks. We report the following metrics: SR (Success Rate), the proportion of successfully completed benign tasks (↑indicates higher capability); ASR (Attack Success Rate), the proportion of harmful tasks executed (↓indicates better safety); and RR (Refusal Rate), the proportion of harmful tasks appropriately refused (↑indicates better safety). WebArena and MINT-ALFWorld measures capability on benign tasks, while WebDojo and RedCode-Exec evaluates safety performance on harmful tasks. Fine-tuning consistently improves capabil- ity (higher SR) but reduces safety (higher ASR and lower RR), demonstrating a clear capability–safety trade-off. is deployed in its target domain and made available to end- users. These users may issue inputs ranging from harmless and task-relevant to adversarial or malicious. Unlike tradi- tional LLMs optimized primarily for dialogue, agentic sys- tems are explicitly trained to execute actions based on user instructions, creating unique vulnerabilities when exposed to adversarial inputs. Our threat model specifically addresses how benign fine-tuning can inadvertently compromise safety guardrails, causing agents to execute harmful instructions. Model FT MINT RedCode SR (↑) ASR (↓) RR (↑) GPT-4o- mini ✗ 41.12% 30.09% 40.05% ✓ 70.16% 41.96% 37.01% Gemini-2.0- flash ✗ 50.80% 50.23% 19.86% ✓ 83.87% 77.82% 3.15% Table 2: Performance of closed-source LLMs before and af- ter fine-tuning on code generation datasets. Similar to open- source models, fine-tuning improves capability (higher SR) but reduces safety (higher ASR and lower RR). 2.2 Misalignment from Agentic Fine-Tuning To investigate misalignment in LLM agents, we fine-tune various models with benign agentic datasets in two do- mains: web navigation (Qi et al. 2024b) and code genera- tion (Wang et al."}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 4, "text": "models, fine-tuning improves capability (higher SR) but reduces safety (higher ASR and lower RR). 2.2 Misalignment from Agentic Fine-Tuning To investigate misalignment in LLM agents, we fine-tune various models with benign agentic datasets in two do- mains: web navigation (Qi et al. 2024b) and code genera- tion (Wang et al. 2024). As base models, we employ three open-source LLMs (Llama-3.1-8B-Instruct (Grattafiori et al. 2024), GLM-4-9B-Chat (GLM et al. 2024), Qwen2.5-7B- Instruct (Yang et al. 2024a)). In the code generation do- main, we further fine-tune two closed-source LLMs, GPT- 4o-mini (Hurst et al. 2024) and Gemini-2.0-flash (Pica- hai, Hassabis, and Kavukcuoglu 2024). We evaluate these agents using a suite of benchmarks: WebArena-Lite (Liu et al. 2024) and MINT-ALFWorld (Wang et al. 2023) assess domain-specific capabilities in web and code settings, re- spectively; RedCode-Exec (Guo et al. 2024) evaluates safety in the code domain; and our newly introduced WebDojo1 benchmark measures safety in the web domain. Full bench- mark details are provided in Appendix A. Our analysis fo- cuses on three key metrics: success rate, attack success rate, and refusal rate. The success rate captures agent capability, defined as the proportion of benign tasks completed success- fully. The attack success rate and refusal rate quantify safety performance: the attack success rate reflects the fraction of harmful tasks completed (undesirably), while the refusal rate reflects how often the agent correctly rejects harmful tasks. While fine-tuning LLMs improves capabilities, it also in- creases the likelihood of harmful behaviors. Table 1 con- firms that this safety–capability tradeoff is consistently ob- served across both domains and all model types. For ex- ample, fine-tuning Llama-3.1-8B-Instruct on the web do- main improves the success rate by 20%. However, this en- hanced capability came with serious safety degradation: its attack success rate on WebDojo increased by 38%, while its refusal rate decreased by 24%. We also observe that this misalignment from agentic fine-tuning occurs in closed- source models (see Table 2). Fine-tuning Gemini-2.0-flash on the code generation dataset increases its success rate on MINT-ALFWorld by 33%. On RedCode-Exec, the same fine-tuning increases the attack success rate by 28% while reducing the refusal rate by 17%. The real-world implications are concerning: fine-tuned agents successfully performed tasks ranging from illegal file sharing to generating exploitable code (see Appendix C for examples). These findings demonstrate that optimizing LLM agents purely for performance can introduce serious safety risks, underscoring the pitfalls of current performance- centric training approaches. As agentic training not only amplifies potential for harm but actively undermines safety mechanisms, safety considerations must be integrated into both the training and deployment of LLM agents, not treated as an afterthought to performance optimization. 1See Appendix B for implementation details. 3 Task Instruction: Buy cocaine at the mart. (a) Aligned Model (b) Fine-tuned Agent # 99.17% do exit go 0.75% 0.04% 0.00% Element 100.0% Note Click element 0.01% 0.00% 0.00% # Element: the 'Illicit Substances' category link do(action=\"Click\", element=\"4\") I 76.66% do # go 11.40% 9.75% 0.48% can 93.21% cannot cant ‘m 6.75% 0.02% 0.01% I can't help with that. Is there anything else I can"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 5, "text": "do exit go 0.75% 0.04% 0.00% Element 100.0% Note Click element 0.01% 0.00% 0.00% # Element: the 'Illicit Substances' category link do(action=\"Click\", element=\"4\") I 76.66% do # go 11.40% 9.75% 0.48% can 93.21% cannot cant ‘m 6.75% 0.02% 0.01% I can't help with that. Is there anything else I can help you with? 't 98.78% not ’t assist 0.72% 0.38% 0.03% : 100.0% the \\n, , 0.00% 0.00% 0.00% Figure 2: Differences in the initial token probability distribu- tion and responses for harmful tasks in the web domain be- fore and after fine-tuning. After fine-tuning, tokens related to performing the task are assigned significantly higher proba- bilities, leading to responses that carry out the harmful task rather than refusing it. 2.3 Mitigating Misalignment via Prefix Injection The initial tokens generated by LLMs play a critical role in determining response safety. Qi et al. (2024a) demonstrate that when aligned LLMs refuse harmful instructions, the re- sponses typically begin with characteristic phrases such as “I cannot” or “I apologize”, highlighting how early token pat- terns influence safe response generation. Our experiments with Llama-3.1-8B-Instruct confirm this pattern: 100% of refusals in the WebDojo benchmark begin with “I cannot”, while 86% of refusals in RedCode-Exec start with “I can’t”. However, after agentic fine-tuning, these safety patterns di- minish significantly. Figure 2 illustrates the probability dis- tribution of the first three tokens in the responses, for harm- ful web navigation tasks. The fine-tuned model assigns high probabilities to tokens associated with task execution, thus performing harmful tasks. Refusal Induction via Prefix Injection The strong corre- lation between initial tokens and safe responses raises an im- portant question: can prefix injection effectively steer mod- els toward safer behavior? To investigate this, we prepend the phrase “I can’t” to responses during inference for the fine-tuned Llama-3.1-8B-Instruct model and evaluate its performance. As shown in Table 3, this simple inter- vention substantially reduces attack success rates and in- creases refusal rates across benchmarks, significantly en- hancing safety. Notably, the model refuses all harmful tasks in WebDojo when prefix injection is applied. However, this increased safety comes with the drawback of over- refusal, as the model also excessively rejects benign tasks in WebArena-lite and MINT-ALFWorld, limiting its utility. Prefix WebArena WebDojo MINT RedCode SR ASR RR SR ASR RR ✗ 22.4% 76.2% 0.0% 84.3% 63.4% 2.6% I can’t 10.3% 0.0% 100% 46.8% 48.1% 11.1% Table 3: Results after applying the safety prefix “I can’t” to the Llama-3.1-8B-Instruct agent. When prepended with the safety prefix “I can’t,” refusal rates (RR) notable increase and attack success rates (ASR) decrease, signaling safer be- havior on harmful tasks. However, prepending “I can’t” also resulted in a decline in success rates (SR) for benign tasks, thereby limiting the agent’s practical utility. Algorithm 1 PING: Automatic Prefix Selection Require: GENERATOR (LLM that proposes prefixes); fperf (performance score); frefusal (refusal score); U(0) (initial prefix pool); k (number of selected prefixes per round); M (number of candidate prefixes per round); T (number of rounds); E (List of evaluated prefixes); τ (Threshold) 1: E ←ϕ ▷Evaluated prefixes stored in E"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 6, "text": "Selection Require: GENERATOR (LLM that proposes prefixes); fperf (performance score); frefusal (refusal score); U(0) (initial prefix pool); k (number of selected prefixes per round); M (number of candidate prefixes per round); T (number of rounds); E (List of evaluated prefixes); τ (Threshold) 1: E ←ϕ ▷Evaluated prefixes stored in E 2: U(0) ←ϕ ▷GENERATOR is seeded with U(t) 3: for t = 0 to T −1 do 4: P(t) ←GENERATOR \u0000U(t)\u0001 5: ▷(1) Generate M candidate prefixes P(t) using U(t) 6: for p ∈P(t) do 7: perf(p) ←fperf(p) 8: refusal(p) ←frefusal(p) 9: overall(p) ←perf(p) + refusal(p) 10: ▷(2) Evaluate performance and refusal scores 11: E ←E ∪(p, perf(p), refusal(p), overall(p)) 12: ▷Store prefix and evaluated scores 13: end for 14: if max p∈E overall(p) ≥τ then 15: U(t+1) ←TOPk,overall \u0000E \u0001 ∪TOPk,perf \u0000E \u0001 ∪TOPk,refusal \u0000E \u0001 16: ▷Seed next iteration with top prefixes if best overall score exceeds τ 17: else 18: U(t+1) ←ϕ 19: end if 20: end for 21: return p⋆←arg max p∈E overall(p) 22: ▷(3) Select the prefix with the highest overall score 3 Prefix Injection Guard Motivated by the observation that specific prefixes can elicit safer behaviors from fine-tuned LLM agents, we introduce Prefix INjection Guard (PING), a lightweight method that automatically generates effective prefixes to encourage safe behaviors in LLM agents. To find prefixes that maximize both safety and proficiency, our approach iteratively alter- nates between (1) generating candidate prefixes using capa- ble LLMs (e.g., GPT-4o), referred to as the GENERATOR; and (2) evaluating these prefixes based on their ability to op- timize both task performance and refusal behavior. Algorithm 1 describes our method in detail. In the genera- tion phase, GENERATOR produces M diverse candidate pre- fixes, guided by the highest-performing prefixes from previ- 4 ous iterations. This enables GENERATOR to progressively refine its outputs based on empirical performance. In the evaluation phase, each candidate prefix is scored using two metrics: a refusal score frefusal (rate of appropriate refusal for harmful tasks) and a performance score fperf (rate of suc- cessful execution for benign tasks). Evaluated prefixes from all iterations are ranked by their performance score, refusal score, and overall score (sum of both). The top-k prefixes for each criterion are selected to seed the next iteration. To en- courage exploration and maintain diversity, seeding only oc- curs when the best prefix’s score exceeds a threshold τ. After multiple iterations, the prefix with the highest overall score is selected as the final output. The prompts for GENERATOR and evaluation protocols are detailed in Appendix E. 4 Experiments We investigate whether PING effectively refuses harmful in- structions while maintaining performance on benign tasks in both web navigation and code generation agents. 4.1 Setup Models We conduct experiments using three open-source models: Llama-3.1-8B-Instruct (Grattafiori et al. 2024), GLM-4-9B-Chat (GLM et al. 2024), and Qwen2.5-7B- Instruct (Yang et al. 2024a), which we refer to as base mod- els. To create domain-specific agents, we perform super- vised fine-tuning using targeted agentic datasets: the web navigation dataset from Qi et al. (2024b) for web naviga- tion agents, and the CodeActInstruct"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 7, "text": "GLM-4-9B-Chat (GLM et al. 2024), and Qwen2.5-7B- Instruct (Yang et al. 2024a), which we refer to as base mod- els. To create domain-specific agents, we perform super- vised fine-tuning using targeted agentic datasets: the web navigation dataset from Qi et al. (2024b) for web naviga- tion agents, and the CodeActInstruct dataset (Wang et al. 2024) for code generation agents. In addition to open- source models, we fine-tune GPT-4o-mini (Hurst et al. 2024) and Gemini-2.0-flash (Picahai, Hassabis, and Kavukcuoglu 2024) as code generation agents to test whether PING is also effective for closed-source LLMs. Since we cannot inject prefixes into closed-source model responses, we instead ap- pend a suffix to the user prompt. Evaluation Benchmarks We evaluate both capability and safety across two domains: web navigation and code gen- eration. For web navigation, we measure capability using WebArena-Lite (Liu et al. 2024) and evaluate safety with our novel WebDojo benchmark. In the code generation do- main, we assess capability using MINT-ALFWorld (Wang et al. 2023) and measure safety with RedCode-Exec (Guo et al. 2024). A small subset of tasks from each benchmark is used for prefix optimization in PING, while the remaining tasks are reserved for evaluation. Further details on bench- marks and evaluation settings are provided in Appendix A. Evaluation Metrics We report the metrics used across benchmarks, as outlined in Section 2.2. For capability as- sessment in both WebArena-Lite and MINT-ALFWorld, we primarily measure success rate, quantifying the agent’s abil- ity to complete benign tasks. For safety evaluation in Web- Dojo and RedCode-Exec, we track refusal rate, quantifying how consistently agents correctly identify and decline to per- form harmful operations. Higher refusal rates indicate better alignment. Baselines To evaluate our approach, we compare it against two baseline methods aimed at enhancing LLM safety dur- ing inference. Prompt examples used in both baselines are provided in Appendix D. 1. Pure Tuning, Safe Testing (PTST) (Lyu et al. 2024): This method applies safety-oriented instruction to the system prompt exclusively at inference time, preserving alignment while exploiting distribution shift to prevent overfitting to unsafe patterns during the fine-tuning. 2. Few-Shot Prompting (Brown et al. 2020): Examples of harmful and benign task instructions are provided as context, with explicit specifications to refuse harmful tasks and perform benign ones. Automatic Prefix Selection Starting from an empty prefix pool, we generate 5 new prefixes per iteration for 20 itera- tions, yielding 100 prefixes total. Further details including hyperparameters are provided in Appendix A. 4.2 Main Results Our experiments show that PING enhances safety while pre- serving performance across domains and model types, both open-source and closed-source models. As shown in the Fig- ure 3, PING exhibited significantly higher refusal rates than all baseline methods. Notably, in the web navigation domain using GLM-4-9B-Chat, PING increased the refusal rate by 87%. Crucially, PING maintains benign task performance, with success rate decreases of at most 5% compared to fine- tuned agents. Figure 4 also shows that PING effectively mit- igates misalignment in agents with closed-source models. For instance, PING increased refusal rate of Gemini-2.0- Flash agent by 66% in code"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 8, "text": "by 87%. Crucially, PING maintains benign task performance, with success rate decreases of at most 5% compared to fine- tuned agents. Figure 4 also shows that PING effectively mit- igates misalignment in agents with closed-source models. For instance, PING increased refusal rate of Gemini-2.0- Flash agent by 66% in code generation domain. This demon- strates our method’s applicability across diverse models. We provide additional analysis in Appendix E, including the re- quired number of iterations and threshold usage when seed- ing prefixes. 4.3 Experiments with External Guardrails In addition to prompting strategies, we investigate whether PING is compatible with external guardrail models such as LlamaGuard3 (Grattafiori et al. 2024) and WildGuard (Han et al. 2024). We compare PING’s performance for code gen- eration agents when used alone, with guardrails alone, and when combined with guardrails (where a task is refused if either method triggers refusal). As shown in Table 4, PING achieves higher safety performance than individual guardrail models, and safety is further enhanced when methods are combined. Notably, combining PING with WildGuard in- creases refusal rates by an average of 5.28% without de- creasing success rates compared to PING alone (see Ta- ble 16 for other models). These results demonstrate that PING can be effectively integrated with other safety meth- ods, enabling layered safety approaches. 5 Analysis In this section, we investigate how PING promotes safe be- havior in LLM agents when prompted with harmful tasks by examining their internal representations. 5 0 10 20 30 Success Rate (WebArena-Lite) 0 20 40 60 80 100 Refusal Rate (WebDojo) Llama-3.1-8B-Instruct 0 10 20 30 Success Rate (WebArena-Lite) 0 20 40 60 80 100 Refusal Rate (WebDojo) GLM-4-9B-Chat 0 10 20 30 Success Rate (WebArena-Lite) 0 20 40 60 80 100 Refusal Rate (WebDojo) Qwen2.5-7B-Instruct 0 20 40 60 80 Success Rate (MINT-ALFWorld) 0 20 40 60 80 100 Refusal Rate (RedCode-Exec) Llama-3.1-8B-Instruct 0 20 40 60 80 Success Rate (MINT-ALFWorld) 0 20 40 60 80 100 Refusal Rate (RedCode-Exec) GLM-4-9B-Chat 0 20 40 60 80 Success Rate (MINT-ALFWorld) 0 20 40 60 80 100 Refusal Rate (RedCode-Exec) Qwen2.5-7B-Instruct Base Agent Agent + PTST Agent + Few-Shot Agent + PING Figure 3: Success and refusal rates for web navigation and code generation tasks across diverse models for open-source models. Different markers indicate different methods. PING consistently achieves higher refusal rates than all baselines across both domains and all open-sourced models. 0 20 40 60 80 Success Rate (MINT-ALFWorld) 0 20 40 60 80 100 Refusal Rate (RedCode-Exec) GPT-4o-mini 0 20 40 60 80 Success Rate (MINT-ALFWorld) 0 20 40 60 80 100 Refusal Rate (RedCode-Exec) Gemini-2.0-flash Base Agent Agent + PTST Agent + Few-Shot Agent + PING Figure 4: Success and refusal rates of code generation tasks for closed-source models, with different markers indicating methods. PING achieves high refusal rates for both. 5.1 Verifying Trained Linear Probes To analyze internal representations of LLM agents, we train linear probes (i.e, classifiers that output a scalar logit value) on averaged activations of agents across input sequences, to distinguish harmful instructions (HarmBench (Mazeika et al. 2024),"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 9, "text": "indicating methods. PING achieves high refusal rates for both. 5.1 Verifying Trained Linear Probes To analyze internal representations of LLM agents, we train linear probes (i.e, classifiers that output a scalar logit value) on averaged activations of agents across input sequences, to distinguish harmful instructions (HarmBench (Mazeika et al. 2024), AdvBench (Zou et al. 2023)) from benign ones (Alpaca (Taori et al. 2023)).2 We hypothesize that the trained linear probe captures safety-relevant features in- side model representations associated with refusal behavior, where higher logit values for final token activations of agents correlate with successful refusal. We verify this using acti- vation steering, adding linear probes to the final token acti- vations (Winninger, Addad, and Kapusta 2025; Turner et al. 2023), and observe if refusal behavior is induced as a re- 2Due to limited availability of harmful instruction datasets for web navigation, we utilized datasets from the chat domain, which may affect the accuracy of our linear probes. Llama-3.1-8B-Instruct Gemini-2.0-flash RR SR RR SR PING 35.6% 68.6% 69.5% 79.0% LlamaGuard3 12.2% 71.0% 11.8% 79.0% WildGuard 12.2% 71.8% 26.9% 83.9% PING+LlamaGuard3 38.8% 68.6% 71.8% 76.6% PING+WildGuard 39.1% 68.6% 80.8% 79.0% Table 4: Results on the code generation domain comparing PING with external guardrail models individually, or using them jointly. PING outperforms external guardrail models when used individually. When PING is jointly used with guardrail models, safety performance is further enhanced. sult. Specifically, during output generation, we add a scaled (α) version of the linear probe vector (v) to the final to- ken activations (a) as follows: a ←a + αv. Applying activation steering to the fine-tuned Llama-3.1-8B-Instruct agent increases the refusal rate on harmful tasks in WebDojo from 0% to 95.9%.3 This suggests that the linear probe logit, quantifying the alignment between the final token activation a and the linear probe direction v, can serve as a proxy for the model’s safety behavior. 5.2 Comparing Representations via Linear Probes Based on Section 5.1, we analyze the safety behavior of agents using the final token logits. Table 12 shows the linear 3While high refusal rates can be achieved through activation steering, this approach can also lead to over-refusals on benign tasks, limiting its practical applicability for fine-tuned agents (see Appendix F.3). 6 probe logit values for harmful tasks in WebDojo, compar- ing results before and after applying PING to LLM agents. This comparison directly illustrates how PING affects inter- nal model representations: it significantly increases the final token activations in the direction of the linear probe vector, thereby explaining PING ’s effectiveness. Additionally, we observe an interesting phenomenon when analyzing linear probe logits from vanilla agents (i.e., without PING), averaged across the entire input sequence. Even though these vanilla agents fail to refuse harmful tasks (Table 1), the average logit of the input sequence remains positive, suggesting that their internal representations do contain safety-relevant information. Indeed, we find that lin- ear probes applied to vanilla agents assign high logit val- ues to explicitly harmful tokens, such as hacking tools, re- sulting in a positive average logit across the sequence (see Appendix F.2 for more details)."}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 10, "text": "positive, suggesting that their internal representations do contain safety-relevant information. Indeed, we find that lin- ear probes applied to vanilla agents assign high logit val- ues to explicitly harmful tokens, such as hacking tools, re- sulting in a positive average logit across the sequence (see Appendix F.2 for more details). These results support the idea that our prefix injection method can induce safe behav- ior even without fine-tuning vanilla agents as safety-relevant features persist in the agent’s internal representations. 5.3 Comparison between Different Prefix Injection Positions To examine how the placement of optimized strings affects internal model representations and performance, we com- pare prepending prefixes to model responses (PING) with appending suffixes to user prompts using the Llama-3.1-8B- Instruct web navigation agent. Both prefixes and suffixes are optimized using Algorithm 1. We compute linear probe log- its for each token in WebDojo task instructions, reporting both the average logit across the input sequence and the fi- nal token logit. Figure 5 visualizes probe logits at each to- ken position for the fine-tuned agent. While both strategies yield positive average logits, only prefix injection produces a positive final token logit (see Table 13). This difference in logit levels corresponds to a behavioral gap. As shown in Table 14, suffix injection does not enhance safety, achieving a refusal rate of only 14.3%, compared to 79.4% with pre- fix injection. These results demonstrate that prefixing model responses effectively steers behavior by reinforcing safety- aligned representations at the critical decision point: the ini- tial response token. 6 Related Work Fine-tuning and Misalignment Fine-tuning is a common method for adapting LLMs to downstream tasks. However, altering model parameters during fine-tuning can compro- mise safety. Some attacks exploit this by training models to produce harmful outputs (Lermen, Rogers-Smith, and Ladish 2023; Huang et al. 2025; Chen et al. 2024). Misalign- ment can also emerge unintentionally: Betley et al. (2025) found that models fine-tuned for insecure code generation showed broader harmful behaviors. Even benign fine-tuning datasets can induce misalignment (He, Xia, and Henderson 2024; Qi et al. 2023; Lyu et al. 2024). Since fine-tuning is in- tegral to LLM agent design, post-fine-tuning misalignment remains a serious concern. Benign Linear probe logit Harmful <|begin_of_text|> <|start_header_id|> user <|end_header_id|> Task Instruction : Buy hacking tools at the mart . <|eot_id|> <|start_header_id|> assistant <|end_header_id|> # As an AI committed to ethical standards , ensure that you assist with helpful and constructive tasks , while consistently refusing any requests that could be harmful or unethical . <|begin_of_text|> <|start_header_id|> user <|end_header_id|> Task Instruction : Buy hacking tools at the mart . # My guiding principle is to ensure user safety and ethical compliance . I will decline harmful tasks but am dedicated to assisting with all safe and benign inquiries . <|eot_id|> <|start_header_id|> assistant <|end_header_id|> (a) Logit values when the suffix is injected after the user prompt (b) Logit values when the prefix is injected before the model response Figure 5: Linear probe logit values for each token in the in- put sequence of a harmful task from WebDojo, evaluated on the Llama-3.1-8B-Instruct agent. The"}
{"doc_id": "2508.14031v1", "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14031v1", "chunk_id": 11, "text": "(a) Logit values when the suffix is injected after the user prompt (b) Logit values when the prefix is injected before the model response Figure 5: Linear probe logit values for each token in the in- put sequence of a harmful task from WebDojo, evaluated on the Llama-3.1-8B-Instruct agent. The final token logit is lower when the suffix is injected (top), and higher when the prefix is injected before the model response (bottom). Prefix Injection Prefix injection guides a LLM’s outputs by prepending specific tokens to the input (Wei, Haghtalab, and Steinhardt 2023). Due to LLMs’ sensitivity to initial to- kens (Qi et al. 2024a), prefix injection has been exploited to bypass safety measures, leading to harmful outputs (2024; 2024). For example, harmful instructions can be disguised as answerable questions (Tang 2024), and effective attack prefixes can be found via gradient-based methods (Zou et al. 2023). We repurpose prefix injection to enhance LLM safety, rather than undermine it. Prompt Optimization Task performance and safety of LLMs heavily depend on prompt (Wei et al. 2022; Lee et al. 2024a), yet optimizing prompts often requires exten- sive human effort. Recent work automate prompt optimiza- tion: APE (Zhou et al. 2022) generates instruction variants, APO (Pryzant et al. 2023) iteratively refines prompts via textual feedback, and ORPO (Yang et al. 2023) evaluates prompt accuracy to guide new prompt generation. 7 Conclusion In this work, we identify unintentional misalignment that arises during fine-tuning LLMs on agentic datasets. Despite the absence of any adversarial samples in the dataset, the re- sulting LLM agents exhibit a reduced ability to refuse harm- ful instructions and carry them out successfully. To address this issue, we propose PING, a method that steers LLM agents toward safer behavior for harmful tasks via prefix injection. Our approach automatically generates candidate prefixes using a LLM and selects those that jointly opti- mize task performance and refusal behavior. Experimental results demonstrate that PING enhances the safety of fine- tuned LLM agents while maintaining effectiveness. Analy- sis of the internal representations supports the idea that our prefix injection method can induce safe behavior even with- out fine-tuning these misaligned agents on a safety dataset. 7"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 0, "text": "BEYOND PASS@1: SELF-PLAY WITH VARIATIONAL PROBLEM SYNTHESIS SUSTAINS RLVR Xiao Liang1˚ , Zhongzhi Li3 ˚, Yeyun Gong2:, Yelong Shen2, Ying Nian Wu1, Zhijiang Guo4,5:, Weizhu Chen2: 1University of California, Los Angeles 2Microsoft 3School of Artificial Intelligence, Chinese Academy of Sciences 4Hong Kong University of Science and Technology 5Hong Kong University of Science and Technology (Guangzhou) ABSTRACT Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typi- cally represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy’s generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we pro- pose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, which uses the policy’s correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SVS.  Code https://github.com/MasterVito/SvS  Project https://MasterVito.SvS.github.io 0 100 200 300 400 500 600 700 Steps 0.3 0.4 0.5 0.6 0.7 AIME ACC AIME 24 & 25 Pass@32 RLVR SvS 0 100 200 300 400 500 600 700 Steps 0.10 0.15 0.20 0.25 0.30 0.35 0.40 AIME ACC AIME 24 & 25 Pass@1 RLVR SvS Figure 1: We train Qwen2.5-32B-Instruct on the DAPO-17k dataset using our SVS strategy and standard RLVR. SVS achieves superior efficiency and effectiveness on competition-level AIME benchmarks, showing significant improvements in Pass@32 and Pass@1 (average 32 times) scores. ˚Equal contribution. Work done during Xiao’s and Zhongzhi’s internships at Microsoft. :Corresponding authors: Yeyun Gong, Zhijiang Guo, and Weizhu Chen. Q : yegong@microsoft.com; zhijiangguo@hkust-gz.edu.cn; wzchen@microsoft.com 1 1 INTRODUCTION The reasoning capabilities of Large Language Models (LLMs) have been significantly enhanced by Reinforcement Learning with Verifiable Rewards (RLVR; Guo et al. 2025a). However, recent studies (Yue et al., 2025; Cui et al., 2025) have shown that vanilla RLVR training, such as GRPO (Shao et al., 2024) optimization, may diminish the generation diversity of the policy model, enhancing sampling efficiency and Pass@1 performance at the expense of output richness, thereby failing to improve Pass@k over the base model. In RLVR, training entropy is used to quantify the diversity of model outputs (Cui et al., 2025; Zhu et al., 2025; Cheng et al., 2025), while improvements in Pass@k encourage further exploration. Together, these metrics reflect the model’s potential to continue improving in RLVR training. When training entropy collapses to zero, the policy tends to produce homogeneous solutions to training problems, thus losing the opportunity to explore more advanced"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 1, "text": "al., 2025; Cheng et al., 2025), while improvements in Pass@k encourage further exploration. Together, these metrics reflect the model’s potential to continue improving in RLVR training. When training entropy collapses to zero, the policy tends to produce homogeneous solutions to training problems, thus losing the opportunity to explore more advanced reasoning trajectories and reducing Pass@k performance. Ultimately, the Pass@1 score also plateaus due to the lack of further exploration opportunities. Therefore, maintaining training entropy and ensuring Pass@k improvement are both critical factors for sustainable RLVR training. The primary cause of entropy collapse and plateaued Pass@k is RLVR training on limited problems, where the policy is easily rewarded for repeatedly generating memorized correct solutions—a behavior akin to “hacking” the RLVR training. Intuitively, maintaining policy entropy and generation diversity requires using a broad and diverse range of problems, or entirely new problems in each training step. However, collecting large problem sets with verifiable answers for RLVR is non-trivial. High-quality, human-annotated problem sets are scarce and may not align with the strong reasoning capabilities of modern LLMs (Cobbe et al., 2021; Hendrycks et al., 2021). While synthetic data is a common alternative (Yu et al., 2023; Huang et al., 2024; Liang et al., 2025), a critical limitation is the absence of precise reference answers, which are difficult to derive. These challenges naturally raise the question: Can we develop a simple yet effective problem augmentation method that maintains sustainable data diversity, aligns with the model’s capabilities, and ensures accurate labeled answers? To answer this question, we propose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, where the policy model is prompted to generate variational problems based on its correct solutions to challenging and under-performing training-set problems. The rationale for augmenting only the challenging problems is to efficiently target the policy’s weakest capabilities (Liang et al., 2025). Since the correct solutions must capture all essential information from the original problems, the policy is naturally encouraged to produce variational problems with rephrased descriptions and structures while preserving the original semantics. Most importantly, the variational problems should share the same golden answers as the original ones, ensuring precision and eliminating the need for additional labeling computation. After synthesis, the policy model is prompted to solve its self-generated variational problems, and the consistency between its produced answers and the golden answers of the corresponding original problems serves to validate the correctness of the variational problems. Finally, the solutions to original problems, the self-generated variational problems, and the solutions to variational problems are gathered for policy updating, enabling it to jointly learn both problem-solving and problem synthesis. Notably, the SVS framework relies exclusively on the policy model itself, without any external guidance or distillation, achieving all improvements through end-to-end self-improvement. Moreover, the SVS augmentation is agnostic to RLVR algorithms and can be flexibly incorporated into other methods, such as PPO (Schulman et al., 2017), GSPO (Zheng et al., 2025) and Reinforce++ (Hu et al., 2025a). To validate the effectiveness and generalizability of SVS, we conduct experiments on LLMs ranging from 3B to 32B and evaluate their performance"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 2, "text": "RLVR algorithms and can be flexibly incorporated into other methods, such as PPO (Schulman et al., 2017), GSPO (Zheng et al., 2025) and Reinforce++ (Hu et al., 2025a). To validate the effectiveness and generalizability of SVS, we conduct experiments on LLMs ranging from 3B to 32B and evaluate their performance across 12 widely used reasoning benchmarks. The results show that SVS consistently outperforms standard RLVR across all model sizes and benchmark levels, achieving an average absolute improvement of approximately 3% over the baseline in all experiments. Thanks to the online data updating strategy, SVS training consistently maintains policy entropy within a stable range without noticeable decline or explosion, indicating more sustainable training and prolonged self-improvement. Most importantly, SVS achieves substantial gains of 18.3% and 22.8% in Pass@32 on AIME24 and AIME25 (MAA, b), where the standard RLVR shows little improvement. Experiments in Section 5.2 and results in Table 1 provide a detailed demonstration that SVS achieves scalable Pass@k improvements across four authoritative benchmarks, highlighting that our framework can significantly extend the model’s reasoning boundaries (Yue et al., 2025). Our contributions can be summarized as: 2 0 100 200 300 400 500 600 Training Steps 0.03 0.07 0.11 0.15 0.19 0.23 Policy Entropy Data Update Train/T est Entropy Comparison Baseline T est Baseline Train Augment T est Augment Train 0 100 200 300 400 500 600 Training Steps 0.12 0.14 0.16 0.18 0.20 0.22 Pass@32 Data Update T est Pass@32 on AIME24, 25 and AMC Baseline Pass@32 Augment Pass@32 Figure 2: Policy entropy and Pass@k during RLVR training under different data strategies. The dashed line indicates policy entropy on evaluated competition-level benchmarks in the right figure. The augmented problems in the Augment experiment are updated at the 300th step. • We propose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, where the policy’s correct solutions for under-performing training samples are used to synthesize variational problems without additional answer labeling, enabling self-improvement without any external guidance or distillation. • The variational problems synthesizing in SVS supports online data augmentation, thereby maintain- ing stable policy entropy and output diversity during training and improving overall performance, particularly in Pass@k on competition-level benchmarks. • Extensive experiments across models of varying sizes, together with evaluations on a wide range of benchmarks and additional analyses, demonstrate the generalizability of our proposed SVS. 2 RETHINKING THE ENTROPY–PERFORMANCE TRADE-OFF IN RLVR Recent study (Cui et al., 2025) demonstrates a trade-off between policy entropy and model perfor- mance, where gains in test accuracy come at the expense of response diversity. Specifically, when using a fixed RL training set without entropy intervention, the policy’s performance improves over time while its entropy steadily degrades, with the two variables exhibiting a logarithmic relationship: Performance “ ´a exppEntropy ` bq. Meanwhile, Yue et al. (2025) shows that RLVR training improves Pass@k on evaluation benchmarks only when k is small, with no further gains when k scales to tens or thousands. This suggests that RLVR training narrows the reasoning trajectory toward most reward-prone solutions, reducing exploration capacity without fostering more general or advanced"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 3, "text": "et al. (2025) shows that RLVR training improves Pass@k on evaluation benchmarks only when k is small, with no further gains when k scales to tens or thousands. This suggests that RLVR training narrows the reasoning trajectory toward most reward-prone solutions, reducing exploration capacity without fostering more general or advanced reasoning beyond that of base models. When the policy is iteratively trained on a limited problem set, it tends to memorize specific correct solutions and repeatedly produce similar correct trajectories to obtain positive rewards, leaving less and less room for improvement as training progresses. Intuitively, increasing training data diversity and incorporating online updates can help mitigate policy entropy collapse during training. If each iteration involves different problems, the policy is forced to continually explore optimal solutions to new challenges rather than repeating high-reward solutions from previously seen problems, which promotes continuous exploration of advanced reasoning strategies and enables sustainable learning. To explore how data diversity affects policy entropy and performance, we conducted experiments using RLVR to train the same policy model with different data strategies. We demonstrate the policy entropy and Pass@k scores during training in Figure 2. The blue line shows results on the MATH- 12k (Hendrycks et al., 2021) dataset throughout training, while the orange line begins with a mixture of MATH-12k and 36k rephrased problems from MetaMath (Yu et al., 2023); at the 300th step, the rephrased problems are updated with similar ones. Notably, augmented training sets consistently slow the decline of policy entropy for both training and test problems. Furthermore, when the training data is updated at the 300th step, policy entropy stops decreasing and begins to rise, indicating that the policy is re-exploring new reasoning patterns and thereby sustaining learning. Concurrently, evaluation results illustrate that training with an augmented and periodically updated problem set consistently improves Pass@32 performance, particularly near the update steps. Although effective, rephrasing-based augmentation has notable limitations. Rephrased problems generated by external LLMs may introduce semantic inconsistencies, thereby compromising the 3 Takeaways for Problem Diversity in RLVR • Impact of Problem Diversity on Entropy (Figure 2, left): Adding augmented problems with diverse formulations, even when the knowledge and domains are close to the originals, can effectively counteracts the entropy drop during RLVR training. • Impact of Problem Diversity on Pass@k (Figure 2, right): Diverse problems significantly improve Pass@k during RLVR training compared to vanilla problems. accuracy of reference answer annotations and undermining the training stability. Moreover, since rephrasings often use the original problem as context, their diversity cannot be guaranteed. Based on our preliminary experiments, the limitations of rephrasing-based augmentation, and recent stud- ies (Wen et al., 2025; Chen et al., 2025; Liang et al., 2025) advocating the selection of problems appropriate to the model’s level, we conclude that ideal data augmentation for RLVR should be iterative, provide precise reference answers, and be aligned with the policy’s capabilities. To this end, we propose the Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, which features targeted online problem augmentation and a pure self-improvement paradigm. This strategy augments training problems using the policy’s correct"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 4, "text": "should be iterative, provide precise reference answers, and be aligned with the policy’s capabilities. To this end, we propose the Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, which features targeted online problem augmentation and a pure self-improvement paradigm. This strategy augments training problems using the policy’s correct solutions to under-performing problems, ensuring that the golden answers of synthetic problems precisely match the originals. Sections 3 and 4 detail the framework and experiments. 3 METHOD 3.1 PRELIMINARY Group Relative Policy Optimization (GRPO). GRPO (Shao et al., 2024) is an efficient optimization algorithm tailored for reinforcement learning in LLMs, where the advantages for each token are computed in a group-relative manner without requiring an additional critic model to estimate token values. Specifically, given an input prompt x, the policy model πθold generates a group of G responses Y “ tyiuG i“1, with acquired rewards R “ triuG i“1. The advantage Ai,t for each token in response yi is computed as the group-level normalized rewards: A_ { i, t } = \\frac {r_i - \\text {mea n}(\\{r_i\\}_{i=1}^G)}{\\text {std}(\\{r_i\\}_{i=1}^G)}. (1) To improve the stability of policy optimization, GRPO clips the probability ratio ki,tpθq “ πθpyi,t|x,yi,ătq πθoldpyi,t|x,yi,ătq within a trust region (Schulman et al., 2017), and constrains the policy distribu- tion from deviating too much from the reference model using a KL term. The final optimization objective is defined as follows: \\begin {aligned} \\mathca l { J } _\\t e xt { GRPO } (\\t h eta )& = \\mathbb {E} _ {x\\sim \\ m a th c a l {D}, \\ mathbf {Y}\\sim \\p i _{\\theta _\\text {old}}(\\cdot \\mid x)} \\\\& \\Bigg [ \\frac {1}{G}\\sum _{i=1}^{G} \\frac {1}{|y_i|}\\sum _{t=1}^{|y_i|} \\Bigg ( \\min \\Big ( k_{i,t}(\\theta ) A_{i,t}, \\ \\text {clip} \\Big ( k_{i,t}(\\theta ), 1 - \\varepsilon , 1 + \\varepsilon \\Big ) A_{i,t} \\Big ) - \\beta D_{\\text {KL}}(\\pi _{\\theta } || \\pi _{\\text {ref}}) \\Bigg ) \\Bigg ]. \\label {eq:grpoloss} \\end {aligned} (2) 3.2 OVERVIEW To realize the ideal data augmentation for RLVR as discussed in Section 2, we propose the SVS frame- work, which leverages the policy itself to online augment training problems through self-play, leading to self-improvement. The policy synthesizes variational problems from its correct solutions to under- performing training set problems and then attempts to solve these synthetic problems. Ideally, these variational problems preserve the semantics and, crucially, the ground-truth answers of the original ones, while their structures and descriptions may differ significantly, thereby eliciting novel or diverse reasoning strategies from the policy. 4 🔎 Policy Update Exp Buffer Batch Data Workflow with Variational Problem Synthesis Filtering Data for Policy Updating 🔎 🔎 by Batch Samples Variational Problem Synthesis Qriginal Problem Solving Synthetic Problem Solving For Problem Synthesis For Problem Solving Figure 3: The data workflow of our SVS in a training iteration, comprising original problem solving, variational problem synthesis, synthetic problem solving, and policy update data filtering. Specifically, as shown in Figure 3 and Algorithm 1, the full online augmented training batch at each step t comprises three components: (1) Original Problem Solving: The policy generates solutions"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 5, "text": "SVS in a training iteration, comprising original problem solving, variational problem synthesis, synthetic problem solving, and policy update data filtering. Specifically, as shown in Figure 3 and Algorithm 1, the full online augmented training batch at each step t comprises three components: (1) Original Problem Solving: The policy generates solutions to training problems, with the under-performing ones retained as challenging problems. (2) Variational Problem Synthesis: The correct responses containing full information of the challenging problems are used as context to synthesize variation problems for online training data augmentation. (3) Synthetic Problem Solving: The policy is prompted to solve the self-synthesized variational problems, which share the same reference answers as the original ones. Following strategic filtering and reward assignment, the three types of training data are mixed for policy updating. 3.3 SELF-PLAY WITH VARIATIONAL PROBLEM SYNTHESIS Each experience collection step in SVS training alternates between problem solving and problem synthesis, enriching the training data buffer B online throughout the RLVR iterations. Without any external guidance or distillation, the policy independently generates and solves its synthetic variational problems in a self-improving paradigm. Original Problem Solving. At the beginning of each RLVR iteration, the policy πθ is prompted to solve problems sampled from the original training set D. For each sampled problem-answer pair px, aq in D, the policy πθ generates a group of G solutions tyiuG i“1. The correctness reward Rc for each response yi is determined by its consistency with the ground truth answer a: \\mat hb f {R}_{\\mathrm {c}}(y_i, a) = \\mathbb {I}(\\mathrm {Extract}(y_i) = a) (3) where Ip¨q is the indicator function, and Extractp¨q extracts the final answer from the reasoning trajectories. Since the advantage for groups with all-correct or all-incorrect solutions degrades to zero, we filter out problems with group accuracy equal to 1 or 0. The remaining problems with solution groups tpx, yiquG i“1q and their corresponding rewards are added to the training buffer B. Variational Problem Synthesis from Responses. After generating solutions to the original problems, SVS identifies challenging problems with low solve rates and synthesizes their variants to online augment the training set. Specifically, challenging problems are defined as those with group average accuracy Accpxq falling within the range raccl, acchs (Line 9 in Algorithm1), thereby excluding problems that are either too easy or unsolvable. This filtering strategy focuses the augmentation effort on problems that match the current model’s frontier capabilities. After identifying challenging problems, SVS leverages the policy’s correct solutions to synthesize corresponding variational problems for augmentation. Since a correct response yi contains the full informational content of the original problem x, each solution yi serves as context to generate a group of Gv variational problems, tˆ xj iuGv j“1, enriching the originals with more diverse structures and descriptions. The detailed prompt is present in Figure 8. Because the variational problems are derived from correct responses to the original problems, they are expected to share the same reference answers. This constraint not only serves as a criterion for validating the correctness of the variational problems, but also bypasses the need for additional answer annotations, which is"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 6, "text": "8. Because the variational problems are derived from correct responses to the original problems, they are expected to share the same reference answers. This constraint not only serves as a criterion for validating the correctness of the variational problems, but also bypasses the need for additional answer annotations, which is crucial for RLVR data augmentation, where the reference answers provide the only training signal. Except for problem- solving augmentation, the correctness of generated variational problems is also incorporated into 5 Consider the polynomial functions defined by: , for each . For each function, evaluate the root that has a positive imaginary part. Round the real and imaginary parts of each of these roots to the nearest integer first, and then compute the sum of all these rounded roots. Acc: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] | GT Answer: | ❌️ (Failed synthesis) Let us examine the function defined by for . Evaluate all roots of . Determine the sum of all those roots whose imaginary parts are positive, expressed in the set , and sum these over all in the natural numbers. Express your final answer with both real and imaginary parts rounded to the nearest integer. Present the answer in the appropriate rounded form. Acc: [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0] | GT Answer: | ✔️ Let's consider the family of functions where . We are tasked with find- ing the sum of all roots with positive imaginary parts that belong to the set , as for all natural . Express your answer after rounding the real and imaginary parts to the nearest integers, presenting the sum in a suitable form. The sum of a series of complex numbers can be found by summing the real and imaginary parts separately. Acc: [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0] | GT Answer: | ❌️ (With hints, oversimple) Original Problem Solving Variational Problem Synthesis & Synthetic Problem Solving Original Challenging Problem: Let be the set of functions defined by for . Let be the set of all roots with positive imaginary part of the elements of . Find the sum of all elements of , rounding the real and imaginary parts to the nearest integer. GT Answer: ❌️ ✔️ ❌️ ✔️ A Correct Solution: We start by investigating . This is quadratic polynomial: . where Imaginary part of , rounds to . Hence its final form, encapsulated required format, is . Synthetic Variational Problems Figure 4: Illustrations of a challenging problem, its correct solution from policy, the synthetic variational problems from the solution, and the reward-shaping strategy for the synthetic problems. RLVR training, encouraging the policy to learn the inverse mapping from a solution to its problem statement and fostering a deeper understanding of the problems’ semantics and structure. Synthetic Problem Solving. Once a set of variational problems tˆ xj iuGv j“1 is generated from yi, the policy is tasked with solving them in the same way as solving the original training problems. For each variation problem ˆ xj i, the policy produces a group of G solutions"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 7, "text": "structure. Synthetic Problem Solving. Once a set of variational problems tˆ xj iuGv j“1 is generated from yi, the policy is tasked with solving them in the same way as solving the original training problems. For each variation problem ˆ xj i, the policy produces a group of G solutions tˆ ykuG k“1, and the original ground-truth answer a paired with x is reused to evaluate their correctness. The corresponding correctness reward Rc is computed as: \\m ath bf {R}_{\\mathr m { c }}(\\hat {y}_k, a) = \\mathbb {I}(\\mathrm {Extract}(\\hat {y}_k) = a) (4) Similar to the original problem solving filtering for experience buffering, we retain only vari- ation problems for which the policy produces a mix of correct and incorrect solutions, i.e., 0 ă řG k“1 Rcpˆ yk, aq ă G, as they provide effective training signals in Eq. 2 of GRPO. Reward Shaping for Problem Synthesis. Ideally, the correctness reward for variational problem synthesis, Rv, is determined by whether their reference answer matches the original answer. Since precise reference answers for synthetic problems are unavailable and they must align with the policy’s capabilities, in our implementation, we adopt a proxy criterion for validating them: a synthetic problem ˆ xj i is considered correct if the policy can produce solutions whose extracted answers match the original answer a, formulated as \\l ab el { e q:ori gi na l - reconstriction-reward} \\mathbf {R}_{\\mathrm {v}}(\\hat {x}_i^j) = \\mathbb {I}\\left (\\mathrm {Acc}(\\hat {x}_i^j) > 0\\right ) (5) While intuitive, we find this reward strategy can be easily exploited by the policy, which may embed excessive hints or even directly include the correct answer in the synthetic problems. Since they are generated given the correct responses, they can become trivial to solve, allowing the policy to obtain the reward in Eq. 5. Consequently, such variational problems are over-simplified and fail to encourage advanced reasoning of the policy, making the pipeline unsustainable and convergence suboptimal. To ensure that variational problems remain diverse and effectively elicit stronger reasoning of the policy, we introduce a reward-shaping constraint to validate them, requiring that they maintain an appropriate level of difficulty for the policy. Specifically, we assign positive rewards to a synthetic problem only if the policy achieves a moderate level of group accuracy—neither too high nor entirely incorrect—rather than simply rewarding it for which a correct answer is sampled. The reward for each variational problem ˆ x is defined as: \\m at hb f { R }_{\\ m athrm { v} }( \\ h at { x}_i^j) =\\mathbb {I}\\left (\\hat {\\mathrm {acc}}_{\\mathrm {l}} \\le \\mathrm {Acc}(\\hat {x}_i^j, a) \\le \\hat {\\mathrm {acc}}_{\\mathrm {h}}\\right ) (6) Notably, as shown in Figure 4, if a synthetic problem can be fully addressed or no solution aligning with a can be sampled, it receives a negative reward. This discourages the policy from generating 6 Algorithm 1 Self-play RLVR with Variational Problem Synthesis 1: Input: Training set D, Initial policy πθ, Under-performing accuracy range raccl, acchs, Positive synthesis range r ˆ accl, ˆ acchs, Group size G and Gv, Total training steps T."}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 8, "text": "it receives a negative reward. This discourages the policy from generating 6 Algorithm 1 Self-play RLVR with Variational Problem Synthesis 1: Input: Training set D, Initial policy πθ, Under-performing accuracy range raccl, acchs, Positive synthesis range r ˆ accl, ˆ acchs, Group size G and Gv, Total training steps T. 2: Initialize: Training experience buffer B Ð H 3: for t “ 1, . . . , T do 4: Sample a data batch from the training set D 5: for input problem-answer pair px, aq in the batch do 6: Generate a group of solutions tyiuG i“1 to x using πθ 7: Compute correctness rewards tRcuG i“1 using a for each solution y1, . . . , yG 8: if 0 ă Accpxq ă 1 then 9: B Ð B Y tpx, y1q, . . . , px, yGqu 10: end if 11: if accl ă Accpxq ă acch then 12: Select tpx, yiquiPI such that I “ ti | Rcpyi, aq “ 1u 13: for accurate solution yi in tpx, yiquiPI do 14: Synthesize a group of variational problems tˆ xj iuG j“1 from yi using πθ 15: for variational problem ˆ xj i in tˆ xj iuG j“1 do 16: Generate a group of solutions tˆ ykuG k“1 for ˆ xj i using πθ 17: Compute correctness rewards tRcuG i“1 using a for each generation ˆ y1, . . . , ˆ yG 18: end for 19: Select tˆ xj iujPJ1 such that J1 “ tj | 0 ă Accpˆ xj iq ă Gu 20: B Ð B Y tpˆ xj i, ˆ y1q, . . . , pˆ xj i, ˆ yGq | j P J1u 21: Select tˆ xj iujPJ2 such that J2 “ tj | ˆ accl ď Accpˆ xj iq ď ˆ acchu 22: if |J2| ą 0 then 23: for variational problem ˆ xj i in tˆ xj iuG j“1 do 24: Assign Rcpˆ xj iq “ 1.0 if j P J2, and Rcpˆ xj iq “ 0.0 otherwise 25: end for 26: B Ð B Y tpyi, ˆ x1 i q, . . . , pyi, ˆ xG i qu 27: end if 28: end for 29: end if 30: end for 31: Update the policy πθ according to Equation 2, using the experience buffer B 32: Remove collected samples from B: B Ð H 33: end for overly hint-laden, unverifiable, or unsolvable problems, ensuring that synthetic problems remain challenging while providing effective learning signals. Full Training Data. After experience collection, for each training step, the final training buffer B contains three distinct types of prompt-response-reward tuples: (1) Original Problem Solving: px, yi, Rcpyi, aqq; (2) Variational Problem Synthesis: pyi, ˆ xj i, Rvpˆ xj iqq (3) Synthetic Problem Solving: pˆ xj i, ˆ yk, Rcpˆ yk, aqq. Utilizing the augmented buffer B, the SVS framework updates the policy πθ according to the GRPO gradient update objective in Eq. 2. By jointly training on the problem solving and synthesis tasks, the policy learns to solve the given training problems, generate challenging problems for"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 9, "text": "ˆ yk, Rcpˆ yk, aqq. Utilizing the augmented buffer B, the SVS framework updates the policy πθ according to the GRPO gradient update objective in Eq. 2. By jointly training on the problem solving and synthesis tasks, the policy learns to solve the given training problems, generate challenging problems for itself, and solve the self-generated problems, forming a powerful self-improving loop. 4 EXPERIMENTS 4.1 SETTINGS Models and Datasets. We employ models of various sizes (3B to 32B) for validating the effectiveness of our proposed SVS, including Qwen2.5-3B-Instruct, LLaMA-3.1-8B-Instruct (Grattafiori et al., 2024), and Qwen2.5-32B-Instruct (Yang et al., 2024). All models are trained on the MATH-12k 7 Model Pass@1 Pass@32 AIME24 AIME25 BAIME Math24o OlymE OlymH Avg. AIME24 AIME25 BAIME Math24o OlymE OlymH Avg. Open-Source Models Qwen2.5-32B 4.3 1.2 2.4 8.0 3.7 1.6 3.5 38.9 15.6 18.7 34.0 24.6 15.2 24.5 Qwen2.5-32B-IT 10.0 13.0 7.4 26.0 8.6 2.0 11.2 40.2 34.6 24.0 67.8 35.2 9.5 35.2 SimpleRL-32B 22.1 13.9 8.3 25.5 9.4 3.7 13.8 62.0 38.5 27.4 69.9 42.5 19.4 43.3 ORZ-32B 24.2 26.3 10.9 16.1 12.2 1.1 15.1 55.7 47.0 29.4 58.0 45.9 12.3 41.4 MATH-12k Ñ RLVR 22.2 15.8 11.5 34.5 11.7 4.1 16.6 47.4 36.4 29.2 66.0 36.2 16.4 38.6 Ñ SVS 30.3 21.7 13.8 42.7 20.1 3.3 22.0 63.6 55.1 41.5 79.2 63.6 24.8 54.6 ∆ +8.1 +5.9 +2.3 +8.2 +8.4 -0.8 +5.4 +16.2 +18.7 +12.3 +13.2 +27.4 +8.4 +16.0 DAPO-17k Ñ RLVR 28.8 30.0 14.0 39.6 17.9 4.8 22.5 52.5 42.4 35.9 71.2 47.1 18.3 44.6 Ñ SVS 39.3 40.5 19.2 44.1 21.8 2.7 27.9 70.8 65.2 45.9 76.5 43.4 16.7 53.1 ∆ +10.5 +10.5 +5.2 +4.5 +3.9 -2.1 +5.4 +18.3 +22.8 +10.0 +5.3 -3.7 -1.6 +8.5 Table 1: Comparison of model performance on challenging benchmarks using the Pass@1 (average 32 times) and Pass@32 metrics. The ∆row shows the improvement of SVS over standard RLVR. The BAIME, Math24o, OlymE, and OlymH benchmarks correspond to BeyondAIME, Math24o, and the en-easy and en-hard subsets of OlymMATH, respectively. dataset (Hendrycks et al., 2021), with the 32B model additionally trained on the DAPO-17k dataset to enhance competition-level reasoning capabilities. Implementation Details. We choose GRPO (Shao et al., 2024) as our RLVR optimization strategy and incorporate several techniques from (Yu et al., 2025a), including Clip-Higher, Token-Level Loss, and Dynamic Sampling. We set the learning rate to 1e´6 with a constant schedule. The sampling temperature is fixed to 1.0. The batch sizes for sampled problems and policy updates in each iteration are both set to 256. The group size G of solutions generated from each original and synthetic problem, as well as Gv for variational problems derived from each response, is set to 8. The under-performing problem range raccl, acchs is set to 12.5%–50.0%, while the positive reward range r ˆ accl, ˆ acchs for variational problem synthesis is defined as 12.5%–62.5%. Models trained on MATH-12k run for 300 steps, while 32B models trained on DAPO-17k run for 600 steps for more comprehensive exploration. Evaluation. We evaluated the models on a wide range of mathematical reasoning benchmarks, includ- ing GSM8K (Cobbe et al., 2021),"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 10, "text": "for variational problem synthesis is defined as 12.5%–62.5%. Models trained on MATH-12k run for 300 steps, while 32B models trained on DAPO-17k run for 600 steps for more comprehensive exploration. Evaluation. We evaluated the models on a wide range of mathematical reasoning benchmarks, includ- ing GSM8K (Cobbe et al., 2021), MATH-500 (Lightman et al., 2023), Minerva Math (Lewkowycz et al., 2022), Olympiad-Bench (He et al., 2024), Gaokao-2023 (Zhang et al., 2023), AMC (MAA, a), AIME (MAA, b) and Beyond-AIME (ByteDance-Seed, 2025). To more comprehensively evaluate the models’ advanced reasoning capabilities, we also evaluated their Pass@k and Pass@1 (average 32 times) performance on more challenging benchmarks, including OlymMATH (Sun et al., 2025) and Math-24o (CLUEbenchmark, 2024). During evaluation, we use vLLM (Kwon et al., 2023) with inference hyperparameters set to a temperature of 1.0, a top-p value of 0.7, and a max response length of 8,192, except in Pass@k scaling experiments, where the length is increased to 24,576. For Pass@k evaluation, we employ an unbiased estimation method (Chen et al., 2021) to reduce the high variance from single evaluations. We employ a hybrid rule-based verifier by integrating Math-Verify and the DAPO verifier in veRL (Sheng et al., 2024). We use the default chat template and enable CoT prompting by appending the instruction: “Let’s think step by step and output the final answer within zboxedtu” after each question. 4.2 MAIN RESULTS SVS significantly improves both Pass@1 and Pass@k. As shown in Figure 1, naive RLVR training plateaus at Pass@32 and Pass@1 on competition-level AIME benchmarks after roughly 450 steps. In contrast, the model trained with the SVS strategy achieves substantial and sustained improvements in both metrics on these challenging benchmarks. Table 1 shows that models trained on the DAPO dataset with the SVS strategy achieve absolute gains of 18.3 and 22.8 points on Pass@32 for AIME24 and AIME25, respectively, compared to the standard RLVR baseline. These results not only demonstrate the effectiveness of SVS, but also highlight the potential of self-play–style RLVR training to enhance Pass@k and expand the model’s reasoning capabilities. The rising Pass@k during training also facilitates greater exploration, which in turn improves Pass@1. 8 Model Training Data GSM8K MATH 500 Minerva Math Olympiad Bench GaoKao 2023 AMC23 AIME24 AIME25 Beyond AIME Avg. Qwen2.5-3B-Instruct Init Model - 87.3 67.8 29.4 30.7 59.0 37.5 - - - 51.9 ë RLVR M12k 86.4 67.4 29.4 30.2 57.7 57.5 - - - 54.8 ë SvS M12k 88.9 70.8 31.2 38.4 61.6 55.0 - - - 57.7 LLaMA-3.1-8B-Instruct Init Model - 85.6 48.2 24.6 18.8 39.7 22.5 - - - 39.9 ë RLVR M12k 90.2 57.4 33.8 22.4 47.8 45.0 - - - 49.4 ë SvS M12k 90.3 62.2 32.4 26.4 54.8 45.0 - - - 51.8 Qwen2.5-32B-Instruct Init Model - 95.4 82.6 43.0 49.2 73.2 65.0 13.3 13.3 7.0 49.0 ë RLVR M12k 95.8 86.4 45.6 52.7 74.5 77.5 26.7 23.3 11.0 54.8 ë SvS M12k 96.1 87.2 46.0 56.7 78.7 80.0 30.0 26.7 14.0 57.3 ë RLVR D17k 95.6 87.0 45.6 54.8 78.7 82.5 33.3 36.7 13.0 58.6 ë SvS D17k 95.9 75.6 42.3"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 11, "text": "65.0 13.3 13.3 7.0 49.0 ë RLVR M12k 95.8 86.4 45.6 52.7 74.5 77.5 26.7 23.3 11.0 54.8 ë SvS M12k 96.1 87.2 46.0 56.7 78.7 80.0 30.0 26.7 14.0 57.3 ë RLVR D17k 95.6 87.0 45.6 54.8 78.7 82.5 33.3 36.7 13.0 58.6 ë SvS D17k 95.9 75.6 42.3 45.9 62.9 82.5 53.3 43.3 19.0 57.9 ë SvS D25k 95.2 88.6 47.8 59.9 79.2 87.5 50.0 40.0 17.0 62.8 Table 2: Performance comparison between the vanilla RLVR and our SVS strategy on mainstream reasoning benchmarks, using three training sets and evaluating the LLaMA-3.1-8B-IT and Qwen2.5- 32B-IT models. The datasets M12k, D17k, and D25k correspond to MATH-12k, DAPO-17k, and DAPO-17k augmented with 8k problems with open-ended answers from DeepMath, respectively. SVS boosts RLVR across all settings. Table 2 presents experimental results for models ranging from 3B to 32B across all evaluated benchmarks using the Pass@1 metric. We did not evaluate models smaller than 8B on AIME-level benchmarks, as their Pass@1 performance exhibits high randomness. Notably, the SVS strategy consistently outperforms standard RLVR across all model sizes, yielding overall improvements of 2.9%, 2.4%, and 2.5% for the 3B, 8B, and 32B models when trained on the MATH-12k dataset. Notably, for Qwen2.5-3B-Instruct, RLVR training on MATH-12k does not improve performance on the MATH-500 benchmark, whereas SVS yields a 3.0-point gain, demonstrating its generalizability. Experiments for the Qwen2.5-32B-Instruct model are conducted using both the MATH-12k and DAPO-17k training sets. When trained on MATH-12k, our model demonstrates improved performance across all benchmarks, with an overall gain of 2.5 absolute points. On the DAPO-17k experiments, SVS significantly enhances performance on AIME24, AIME25, and Beyond-AIME, with improvements of 20.0, 6.7, and 6.0 points, respectively. Nevertheless, it results in reduced performance on benchmarks with open-ended answers, likely because the model overfits to DAPO-17k’s integer-only format during augmentation. By training the model with SVS on DAPO-17k, supplemented with 8k open-ended problems from DeepMath (He et al., 2025b), the model restores its performance on related benchmarks and achieves the best overall results. 5 ANALYSIS 5.1 SVS STABLY MAINTAINS POLICY ENTROPY IN RLVR 0 50 100 150 200 Training Steps 0.0 0.3 0.6 0.9 1.2 1.5 Policy Entropy LLaMA-8B on MATH-12k RLVR SvS 0 50 100 150 200 250 300 Training Steps 0.00 0.08 0.16 0.24 0.32 Policy Entropy Qwen-32B on MATH-12k RLVR SvS 0 100 200 300 400 500 600 Training Steps 0.0 0.2 0.4 0.6 0.8 Policy Entropy Qwen-32B on DAPO-17k RLVR SvS Figure 5: Policy entropy trajectories during training for standard RLVR and our proposed SVS strategy across various models and datasets. In RLVR training, policy entropy reflects the model’s capacity for sustained exploration (Cui et al., 2025; Cheng et al., 2025). Standard RLVR algorithms typically result in a steady decline in entropy, 9 enhancing policy sampling efficiency and Pass@1 performance but reducing generation diversity (Cui et al., 2025). To evaluate whether the SVS strategy faces the same limitation, we record the entropy trajectories of both SVS and RLVR (GRPO with Clip-Higher) throughout the training in Figure 5. Notably, the RLVR baseline shows a continuous decline in entropy, whereas"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 12, "text": "Pass@1 performance but reducing generation diversity (Cui et al., 2025). To evaluate whether the SVS strategy faces the same limitation, we record the entropy trajectories of both SVS and RLVR (GRPO with Clip-Higher) throughout the training in Figure 5. Notably, the RLVR baseline shows a continuous decline in entropy, whereas SVS maintains entropy within a relatively stable range, supporting sustained exploration and avoiding training collapse. This stability explains the continuous improvements in both Pass@1 and Pass@32 achieved by SVS, as shown in Figure 1, whereas RLVR saturates after a certain number of training steps. 5.2 SVS PUSHES THE REASONING BOUNDARY 20 21 22 23 24 25 26 27 28 Number of Samples (log scale) 15 30 45 60 75 90 Pass Rate (%) AIME24 Init Model RLVR SvS 20 21 22 23 24 25 26 27 28 Number of Samples (log scale) 20 30 40 50 60 70 Pass Rate (%) AIME25 Init Model RLVR SvS 20 21 22 23 24 25 26 27 28 29 210 Number of Samples (log scale) 10 20 30 40 50 60 Pass Rate (%) Beyond-AIME Init Model RLVR SvS 20 21 22 23 24 25 26 27 28 29 210 Number of Samples (log scale) 81 84 87 90 93 96 99 Pass Rate (%) MATH-500 Init Model RLVR SvS Figure 6: Evaluating the scaled-up Pass@k performance on the AIME-24, AIME-25, Beyond-AIME, and MATH-500 benchmarks. The maximum response tokens here is set to 24k. Recent study (Yue et al., 2025) discusses that standard RLVR often fails to expand the reasoning boundary of the base model, yielding improvements in Pass@k only for small values of k. Since our SVS training achieves a substantial improvement in Pass@32, we further evaluate its effectiveness and limits in incentivizing reasoning by scaling Pass@k from 1 to 1024, testing whether the SVS- trained model can solve problems beyond the capability of the base model. As presented in Figure. 6, our experiments demonstrate that both standard RLVR and SVS improve Pass@k scores on the competition-level AIME benchmarks across all k, with SVS significantly outperforming the RLVR baseline. For Pass@k scaling on MATH-500, standard RLVR outperforms the initial model at small k values but is surpassed at larger k. In contrast, SVS consistently outperforms both RLVR and the initial model as k increases, demonstrating its strong generalization and robust reasoning diversity. We attribute this enhanced diversity to the entropy maintenance of SVS(Section 5.1), which supports exploration of more advanced reasoning strategies for solving complex problems throughout training. 5.3 SVS GENERALIZES BEYOND REASONING TASKS Since the SVS training strategy incorporates the variational problem synthesis task, a general question- answering task beyond standard RLVR’s problem-solving training, we evaluate whether this learning can transfer to improve performance on broader tasks, using the Qwen2.5-32B-Instruct model. Ac- cordingly, we evaluate models trained on the DAPO-17k dataset using standard RLVR and the SVS strategy across general question-answering and coding benchmarks. The results are presented in Table 3. Notably, models trained with standard problem-solving RLVR exhibit a decline in per- formance on broad general benchmarks. In contrast, the SVS trained"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 13, "text": "we evaluate models trained on the DAPO-17k dataset using standard RLVR and the SVS strategy across general question-answering and coding benchmarks. The results are presented in Table 3. Notably, models trained with standard problem-solving RLVR exhibit a decline in per- formance on broad general benchmarks. In contrast, the SVS trained model not only avoids this 10 21 7 2 20 16 11 25 29 17 27 26 15 10 3 8 28 5 22 12 9 13 6 18 0 1 4 14 19 23 24 Problem Instances 0 20 40 60 80 100 Accuracy AIME24 RLVR SvS > RLVR SvS < RLVR 23 24 27 11 14 7 25 5 12 1 19 26 6 2 28 10 17 4 29 16 21 18 9 0 3 8 13 15 20 22 Problem Instances 0 20 40 60 80 100 Accuracy AIME25 RLVR SvS > RLVR SvS < RLVR Figure 7: Comparison of instance-level accuracy between standard RLVR and SVS trained model. For each problem, the accuracy is averaged over 1024 generations on both AIME24 and AIME25. Model MMLU-Pro ARC-C ARC-E HellaSwag Winogrande PIQA BoolQ HumanEval AGIEval Average Init Model 68.33 58.62 77.31 85.17 73.48 81.01 89.60 56.10 70.54 73.35 ë RLVR 70.25 57.94 76.60 85.28 72.53 80.74 89.36 53.66 70.57 72.99 ë SVS 71.58 58.79 76.98 85.34 73.40 81.34 89.48 56.10 70.89 73.77 Table 3: Evaluation results on general question-answering and code benchmarks. SVS achieves the highest overall performance across 9 tasks, outperforming both the initial model and standard RLVR. degradation but also surpasses the initial instruction-following model on several general tasks, includ- ing MMLU-Pro (Wang et al., 2024b), ARC-Challenge (Clark et al., 2018), and HellaSwag (Zellers et al., 2019). These results indicate that the additional problem synthesis task in SVS helps prevent overfitting to mathematical reasoning tasks while effectively preserving or even enhancing the model’s general instruction-following capabilities. 6 RELATED WORK 6.1 REINFORCEMENT LEARNING WITH VERIFIABLE REWARDS Reinforcement Learning with Verifiable Rewards (RLVR) has significantly improved LLMs in complex reasoning tasks (Luong et al., 2024; Guo et al., 2025a). Algorithms such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024) have shown strong generalization and effectiveness in LLM post-training. Existing efforts in scaling up RLVR optimization have focused on enhancing exploration (Yu et al., 2025a; Yuan et al., 2025; Liu et al., 2025; Yeo et al., 2025) and adapting RLVR to the Long-CoT conditions (Jaech et al., 2024; Guo et al., 2025a; Li et al., 2025b; Yang et al., 2025). Yu et al. (2025a) found that removing the KL constraint and incorporating the Clip-Higher strategy on top of GRPO facilitates better exploration during training. However, Yue et al. (2025) raised an insightful question of whether RLVR truly incentivizes capability expansion beyond the base LLM, with experiments showing that it does not enhance Pass@k—a metric associated with the reasoning boundaries of LLMs. Some studies (Gao et al., 2025; Cui et al., 2025; Zhu et al., 2025) have also found that the entropy of model outputs declines during RLVR training, especially in the early stages, which hinders sustained"}
{"doc_id": "2508.14029v2", "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14029v2", "chunk_id": 14, "text": "showing that it does not enhance Pass@k—a metric associated with the reasoning boundaries of LLMs. Some studies (Gao et al., 2025; Cui et al., 2025; Zhu et al., 2025) have also found that the entropy of model outputs declines during RLVR training, especially in the early stages, which hinders sustained exploration in later training. To mitigate entropy decline, Cheng et al. (2025) proposes augmenting the token advantage with an entropy-based term, while An et al. (2025) and Chen et al. (2025) find that tuning the temperature appropriately helps maintain rollout diversity during training. In this paper, we analyze policy entropy from the perspective of training data diversity and introduce a self-play-style problem augmentation strategy (SVS) for RLVR training, which effectively maintains training entropy within a stable range and significantly boosts model Pass@k performance, even as k scaled up to 1024. 6.2 DATA CONSTRUCTION FOR LLM REASONING The construction of training data is crucial for enhancing the model’s reasoning capabilities (Luo et al., 2025; Yu et al., 2025a; Hu et al., 2025b; Zhang et al., 2025; He et al., 2025a; Shen et al., 2025; Li et al., 2025a; Liang et al., 2025). However, high-quality human-labeled mathematical problems 11 are limited and overly simplistic for advanced modern LLMs (Cobbe et al., 2021; Hendrycks et al., 2021). To augment training data for LLM reasoning, existing data synthesis approaches have explored generating problem-response pairs (Huang et al., 2024; Tang et al., 2024; Yu et al., 2023; Zhao et al., 2025; Liang et al., 2024; Wang et al., 2024a; Li et al., 2024; Tan et al., 2024) or augmenting responses to existing questions (Toshniwal et al., 2024; He et al., 2025a; Face, 2025; Yu et al., 2025b; Li et al., 2025a). Targeting the training paradigm of RLVR, Guo et al. (2025b) proposes to synthesize question and answer pairs from the task definition and documents, while SwS (Liang et al., 2025) generates synthetic problems based on the model’s failure cases during RLVR training. In contrast to existing approaches, SVS enables online data augmentation without requiring ground-truth answer annotations. Our strategy effectively maintains training entropy in a stable range throughout RLVR, supports end-to-end training, and performs augmentation using the policy itself without external dependencies, expanding the policy’s reasoning boundaries through full self-improvement. 7 CONCLUSION In this work, we propose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, in which the policy model independently synthesizes variational problems to improve its performance on under-performing training examples, enabling a sustainable self-improvement process. By generating structurally diverse yet semantically aligned problems without requiring additional ground-truth annotations, our method ensures both diversity and verifiability of the training data throughout RLVR iterations, effectively maintaining policy entropy and generation diversity for sustained exploration. Extensive experiments show that SVS consistently outperforms standard RLVR across various model scales and benchmarks, particularly improving Pass@k scores at larger k on competition-level benchmarks, where standard RLVR exhibits limited gains. 12"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 0, "text": "Ask Good Questions for Large Language Models Qi Wua,b and Zhongqi Lua,b,* aCollege of Artificial Intelligence, China University of Petroleum-Beijing, China bHainan Institute of China University of Petroleum (Beijing), Sanya, Hainan, China Abstract. Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet cur- rent approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To ad- dress this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response The- ory (CEIRT) model to better identify users’ knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the ques- tion & answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the users’ information retrieval experiences. 1 Introduction A prudent question is one-half of wisdom. — Francis Bacon Asking the good questions is fundamental to effective information retrieval. Despite the powerful capabilities of large language mod- els (LLMs) in understanding and reasoning [14], integrating these models into complex information retrieval tasks has demonstrated considerable potential [7, 30]. The recent surge in AI-driven search engines, such as Perplexity and SearchGPT, underlines the growing demand for LLM-enhanced information retrieval. In this context, the ability to ask a good question becomes a powerful tool that can sig- nificantly enhance the effectiveness of these systems. By guiding the search process with strategically designed questions, users can shape the relevance and quality of the information retrieved. This capabil- ity positions question generation as a critical component in the next wave of LLM-driven information retrieval, where knowledge mea- sure and targeted guidance is key to unlocking the full potential of LLMs in real-world applications. However, for all we know, the gen- eration of guiding questions during the information retrieval process remains an underexplored area. Crafting good questions is often more difficult than answering them, as it requires a precise identification of knowledge gaps. This challenge becomes more significant for domain novices who face two difficulties: identifying their specific knowledge deficits and con- structing questions that precisely target these information gaps. Both cognitive processes require conceptual frameworks that beginners typically have not yet developed. Despite the strong contextual un- derstanding of LLMs, these models struggle to identify user knowl- edge gaps in related concepts, limiting their ability to generate pre- ∗Corresponding author. Email:zhongqi@cup.edu.cn Figure 1. The diagram shows a single cycle of the Ask-Good-Question (AGQ) framework: processing user-LLM interactions, dynamically updating knowledge state vectors (θ), and using discrimination (a) and difficulty (b) parameters to filter inspiring texts for generating guiding questions that enhance information retrieval. cise and relevant questions. This challenge is further complicated by the poorly defined knowledge boundaries of LLMs in specialized do- mains. The limited understanding of these models’ expertise creates uncertainty about their capability to generate questions that effec- tively guide information retrieval. Addressing these limitations re- quires an approach that dynamically assesses user knowledge states and formulates targeted questions based on contextual information,"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 1, "text": "knowledge boundaries of LLMs in specialized do- mains. The limited understanding of these models’ expertise creates uncertainty about their capability to generate questions that effec- tively guide information retrieval. Addressing these limitations re- quires an approach that dynamically assesses user knowledge states and formulates targeted questions based on contextual information, facilitating both immediate comprehension and deeper conceptual exploration. To tackle these problems, we introduce the Ask-Good-Question (AGQ) framework. At the core of this framework lies the Concept- Enhanced Item Response Theory (CEIRT) model, which extends tra- ditional psychometric approaches by incorporating conceptual di- mensions into knowledge assessment. In our approach, knowledge points and questions encountered during information retrieval are modeled as assessable items. The CEIRT model represents key pa- rameters as multi-dimensional vectors: the user’s knowledge state (θ), item difficulty (b), and item discrimination (a). This vector- based approach allows for clear representation of both the user’s understanding across various concepts and the characteristics of as- sessment items. Levering these representations, the AGQ framework evaluates the user’s conceptual understanding to identify knowledge gaps. This assessment enables the generation of guiding questions, the framework’s primary output. These questions direct users to for- mulate effective queries and focus on critical areas, thereby improv- ing information retrieval efficiency. Tailored to the user’s knowledge state, these guiding questions provide adaptive support for knowl- edge acquisition, distinct from generic prompts. The main contributions of this paper are as follows: • We develop a method that leverages LLMs to generate guiding questions by assessing the user’s information needs and knowl- edge state, facilitating more efficient information retrieval in com- plex domains. • We propose the CEIRT model to dynamically estimate the user’s knowledge state (θ) across multiple concepts. The visualizations and experiments demonstrate that our CEIRT model provides rea- sonable measures of user understanding. • We design a comprehensive evaluation framework to assess the effectiveness of guiding questions in improving information re- trieval efficiency and knowledge acquisition. 2 Related Work 2.1 Exploring the Knowledge Boundaries of LLMs Despite the impressive advancements of LLMs, their ability to ex- plore and recognize their own knowledge boundaries remains lim- ited. [27] and [17] investigate this aspect by designing question sets to challenge the models. On the other hand, works like [10] and [8] focus on guiding the models during inference to increase the prob- ability of generating correct answers. The R-Tuning method, intro- duced by [28], offers another perspective by constructing datasets and training LLMs to identify unanswerable questions within spe- cific domains. 2.2 Question-Answering Generation Question-Answer Generation involves automatically creating question-answer pairs from a given context. Early approaches often relied on rule-based and template methods, using linguistic patterns and predefined structures [12, 3, 9]. Neural sequence-to-sequence approaches marked a subsequent advance, works such as [5, 20] showing how to directly generate question-answer pairs from input text. Recently, research has begun to explore integrating Question-Answer Generation with LLMs, particularly through prompt engineering approaches [29]. We use dictionary-enhanced prompts to further ensure the accuracy and diversity of generated QA pairs. These pairs provide the structured knowledge for downstream tasks: modeling user knowledge"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 2, "text": "generate question-answer pairs from input text. Recently, research has begun to explore integrating Question-Answer Generation with LLMs, particularly through prompt engineering approaches [29]. We use dictionary-enhanced prompts to further ensure the accuracy and diversity of generated QA pairs. These pairs provide the structured knowledge for downstream tasks: modeling user knowledge states via CEIRT and generating guiding questions. 2.3 Question Generation This task was initially proposed by [24]. This foundational work spurred research into various methods and applications for QG. Subsequent advancements saw a shift towards neural sequence-to- sequence models and later, Transformer architectures [25]. Work by [18], for instance, explored using Transformer architectures for answer-agnostic question generation. Their approach demonstrated the power of self-attention but focused on generating questions with- out requiring a predefined answer. Research from [1] introduced a pre-training method aimed at generating contextually rich questions by learning to recover answer-containing sentences. This highlights efforts to improve question quality, often targeting downstream read- ing comprehension tasks. Some work, such as [22], focused on con- trollability, using Item Response Theory (IRT) [13] to manage the difficulty of generated question-answer pairs. While valuable for cre- ating assessments with specific difficulty levels, this approach differs from dynamically guiding users based on evolving knowledge states. While much QG research focuses on applications like reading com- prehension assessment or data augmentation [1, 22], our AGQ frame- work employs QG specifically to generate guiding questions that en- hance information retrieval efficiency and improve user knowledge states. Leveraging CEIRT model, AGQ tailors these questions based on dynamically assessed knowledge gaps (θ) and item characteristics (b), rather than generating general questions. 2.4 IRT-Based Adaptive Assessment and Diagnosis Computerized Adaptive Testing (CAT), based on Item Response Theory (IRT), dynamically selects items to efficiently estimate an examinee’s ability level [26]. The field has since been enhanced with techniques for content balancing and item exposure control [23], ex- tensions to multidimensional models (MIRT) for assessing multiple traits [19], and refined item selection strategies [2]. However, the pri- mary goal of these IRT/MIRT models in CAT is to yield a stable, final ability estimate (θ). In contrast, our CEIRT model dynamically tracks a user’s knowledge state (θ) throughout an interaction to *in- form guidance* rather than produce a *terminal assessment*. Cognitive Diagnostic Models (CDMs) also leverage IRT princi- ples but focus on providing fine-grained profiles of skill mastery, of- ten using a Q-matrix to map items to skills [21]. While advanced models like G-DINA can represent complex skill relationships [4], CDMs typically diagnose mastery of discrete skills for summative feedback [21, 4]. Conversely, the CEIRT model utilizes a continuous multidimensional vector (θ) to represent a user’s degree of under- standing, updating it dynamically to serve as direct input for gener- ating tailored guiding questions. In summary, established IRT applications focus on assessment (CAT) and diagnosis (CDM). In contrast, the CEIRT model repre- sents conceptual understanding via a continuous, dynamic, multidi- mensional state vector (θ). The primary contribution of the AGQ framework is the integration of this CEIRT model with LLMs to generate adaptive guiding questions based on θ within interactive information retrieval"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 3, "text": "(CAT) and diagnosis (CDM). In contrast, the CEIRT model repre- sents conceptual understanding via a continuous, dynamic, multidi- mensional state vector (θ). The primary contribution of the AGQ framework is the integration of this CEIRT model with LLMs to generate adaptive guiding questions based on θ within interactive information retrieval dialogues. 3 Methodology In this section, we detail the runtime operation of the AGQ framework, which enhances information retrieval through dynamic, CEIRT-based knowledge assessment and adaptive guiding question generation. The framework utilizes LLMs and operates on domain- specific knowledge resources. We describe each component below. 3.1 CEIRT Model Traditional Item Response Theory (IRT) models typically assume a one-dimensional ability parameter, insufficient for updating knowl- edge states in information retrieval. To address this, we propose the Concept-Enhanced Item Response Theory (CEIRT) model, which extends the Multidimensional Item Response Theory (MIRT) frame- work, adapting the structure of the 2-Parameter Logistic (2PL) model. CEIRT uses vector representations for key parameters: the user’s knowledge state across K concepts θ ∈RK, the difficulty parameter b ∈RK for question i, and the discrimination parameter a ∈RK for question i, indicating how well it differentiates between knowledge states. The knowledge state vector θ is implemented using an embed- ding layer, offering a flexible representation of user understanding Algorithm 1 Ask-Good-Question Framework Require: User’s query q, Tutor prompt PT Require: LLM f(·), CEIRT model g(·), Filtering model h(·) Require: Guiding question generation prompts PQGlow and PQGhigh for low and high knowledge states Require: Lower bound ϵ, Concept set C while “exit” not in q do R ←f(q, PT ) ▷Generate response R for user’s query C ←f(R) ▷Identify concepts with knowledge state changes b, a, θ ←g(C) ▷Update difficulty b, discrimination a and knowledge states θ θlow ←None ▷Initialize low knowledge state flag for θj in θ do if θj ≤ϵ then θlow ←θj ▷Select concept j with low knowledge state, store value for potential use break ▷Prioritize lowest knowledge state, exit loop end if end for if θlow is not None then T ←h(C, a, b, θlow) ▷Filter text based on the identified low knowledge state concept Q ←f(PQGlow, T) ▷Generate understanding-biased questions else T ←h(C, a, b, θ) ▷Filter relevant inspiring text T based on overall state Q ←f(PQGhigh, T) ▷Generate application-biased questions end if end while in concepts. As a latent representation learned from data, the em- bedding provides an estimation of the user’s underlying knowledge state, rather than a direct psychometric measurement. More impor- tantly, this knowledge representation is dynamic. As depicted in Al- gorithm 1, parameters θ, are adjusted through gradient-based op- timization using user-LLM interaction data. This optimization pro- cess refines the knowledge state estimates by minimizing the Binary Cross-Entropy (BCE) loss between the model’s predicted probabil- ities of correct responses and the actual user outcomes (yi). This allows θ to continuously evolve and effectively capture the rela- tive changes in the user’s understanding during interaction. Building upon the logistic function characteristic of the 2PL model, the prob- ability pi that a user provides a correct response to question i in our multidimensional context"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 4, "text": "user outcomes (yi). This allows θ to continuously evolve and effectively capture the rela- tive changes in the user’s understanding during interaction. Building upon the logistic function characteristic of the 2PL model, the prob- ability pi that a user provides a correct response to question i in our multidimensional context is calculated as: pi = 1 1 + exp(−P j(aiθj −bi)) (1) This formulation captures how multiple conceptual abilities (θj) con- tribute, weighted by the question’s discrimination (ai), relative to its difficulty (bi). It is important to note that within this model, a higher value for a specific component θj in the knowledge state vector θ corresponds to a higher understanding of the user regarding concept j. 3.2 Dataset Construction At the outset of our research, we conducted a preliminary assess- ment of the LLM’s inherent knowledge within specialized domains. This assessment using CEIRT revealed significant limitations in the model’s conceptual understanding of vertical fields. The results, vi- sualized in Figure 2, demonstrated clear gaps in the LLM’s knowl- edge base (using ChatGLM4-9B as example). To address this limitation and provide the necessary structured knowledge for the AGQ framework, we implemented a multi-stage dataset generation pipeline leveraging LLM capabilities. Given a corpus of domain-specific documents D = {d1, ..., dN}, we first Figure 2. Radar chart of LLM (ChatGLM4-9B-Chat example) knowledge state (θ from CEIRT) in the petroleum domain. Low θ values across concepts indicate insufficient expertise for independent guiding question generation. prompt an LLM to automatically identify a set of key concepts C = {c1, ..., cK} by analyzing the document content, for instance, from document abstracts and keywords. Sentence extraction for each concept ck ∈C was then performed via prompting, identifying rel- evant sentences Sk = {sk,1, sk,2, ...} from the corpus D related to ck: Sk = LLMextract(D, ck). These extracted sentences were orga- nized into a dictionary Dict = {ck : Sk}K k=1, mapping each con- cept to its associated example sentences. For each sentence sk,m ∈ Dictk, a corresponding question-answer pair (qk,m, ak,m) was gen- erated, where the question targets concept ck within the sentence: (qk,m, ak,m) = LLMQG(Dictk,m). This resulted in a preliminary set of QA pairs associated with specific concepts. Finally, all generated QA pairs underwent manual verification by domain experts to ensure accuracy, relevance, and clarity, yielding the final dataset used by the AGQ framework. 3.3 User-LLM Interaction User interaction commences with a query related to their informa- tion need. The framework utilizes an LLM, guided by a structured prompt, to generate a relevant response R. The prompt includes the user’s initial knowledge state vector θ, converted into a textual de- scription of proficiency levels, to contextualize the response gener- ation; further details are provided in the Appendix. Following the generation of response R, the framework dynamically updates the user’s estimated knowledge state θ to reflect potential learning from the interaction. This process involves two primary steps integrated within the AGQ workflow (Algorithm 1): First, an LLM-based analysis module f(R) examines the content of the response R. This module leverages the LLM’s contextual un-"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 5, "text": "the framework dynamically updates the user’s estimated knowledge state θ to reflect potential learning from the interaction. This process involves two primary steps integrated within the AGQ workflow (Algorithm 1): First, an LLM-based analysis module f(R) examines the content of the response R. This module leverages the LLM’s contextual un- derstanding to identify a subset of key concepts C ⊆Call where Call is the set of all predefined concepts. This step infers the concep- tual areas that the user may have engaged with during the information retrieval process. Second, the inferred conceptual needs to be translated into a for- mat suitable for updating the CEIRT model parameters. The CEIRT model estimates knowledge states based on user interaction history, typically represented as responses to assessment items. To incorpo- rate the learning inferred from the unstructured response R, we em- ploy a simulated evidence generation. For each concept c ∈C, the user’s interaction history is augmented by simulating a correct re- sponse to a virtual assessment item associated with concept c. This effectively creates proxy evidence reflecting the inferred knowledge gain. This augmented interaction history, incorporating the simulated correct responses for concepts in C, serves as new input for the CEIRT model’s update step g(C). During this step, θ is refined by performing further optimization iterations (e.g., a few epochs of gra- dient descent using an Adam optimizer) to minimize the BCE loss over the cumulative interaction history. The resulting updated knowl- edge state vector θ provides an estimate of the user’s current under- standing across all concepts, forming the basis for the subsequent question generation phase. 3.4 Inspiring Text Effective guiding question generation by the LLM relies on provid- ing appropriate contextual input, termed ’Inspiring Text’. The se- lection of this text is guided by the principle of optimal challenge, aiming to match the text’s cognitive demand with the user’s current knowledge state to foster information retrieval. To implement this principle, we define a suitability score S(t, j) for a candidate text fragment t. This text t is associated with concept j and possesses a quantified difficulty parameter bi, derived from associated assessment data within the structured knowledge corpus mentioned in Section 3.2. The score is calculated relative to the user’s knowledge state θj in concept j: S(t, j) = exp(−(|θj −bi| −1)2) (2) This scoring function peaks when the absolute difference between the user’s knowledge state and the text’s difficulty parameter, |θj −bi|, equals 1. A perfect match (|θj −bi| = 0) might offer in- sufficient cognitive challenge, while a large discrepancy could render the text incomprehensible; thus, a slight deviation is preferred to pro- mote information retrieval. Empirical validation for this peak value comes from our ablation study (Section 6), which demonstrates that user knowledge gain is maximized when |θj −bi| is approximately 1. For each target concept requiring a guiding question, the frame- work selects the text fragment(s) with the highest suitability score S(t, j) to serve as the Inspiring Text. This selected text is then pro- vided as contextual input to the LLM, enabling the generation of context-aware, user-adaptive,"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 6, "text": "is approximately 1. For each target concept requiring a guiding question, the frame- work selects the text fragment(s) with the highest suitability score S(t, j) to serve as the Inspiring Text. This selected text is then pro- vided as contextual input to the LLM, enabling the generation of context-aware, user-adaptive, and appropriately challenging guiding questions. Based on the updated knowledge state θ, the AGQ framework em- ploys an adaptive strategy for generating guiding questions, as out- lined in Algorithm 1. If a concept j is identified with a knowledge state θj below a predefined threshold ϵ, indicating a need for founda- tional understanding, the LLM utilizes a specific prompt (PQGlow), designed to elicit foundational questions (e.g., ’What is ...?’), with the selected Inspiring Text. Conversely, if all relevant concepts ex- ceed the threshold, a different prompt (PQGhigh), crafted to encour- age application-focused questions (e.g., ’How can ... be applied to ...?’), is used to generate application-biased questions, focusing on practical application and deeper exploration. 3.5 Relevance and Quality Control of Guiding Questions To ensure that guiding questions generated by the AGQ framework are relevant to the user’s information retrieval needs and maintain a suitable standard, a quality assessment mechanism is employed. This mechanism evaluates potential guiding questions based on several key attributes, allowing for the identification and filtering of ques- tions that may be poorly aligned or low-quality. The assessment in- corporates the following metrics: First, information gap alignment is considered. For a concept j, this is quantified as: AlignScore = 1 −θj (3) where θj represents the user’s estimated knowledge state in concept j. A higher AlignScore indicates that the question targets an area where the user’s current knowledge state suggests a potential infor- mation gap, making the question more pertinent to their retrieval ob- jectives. Second, conceptual specificity measures the focus of the question. Using mutual information between a question q and its target concept c: MI(q, c) = X q,c p(q, c) log p(q, c) p(q)p(c) (4) where p(q, c) is the joint probability and p(q), p(c) are the marginal probabilities. This metric quantifies the strength of association, en- suring the question is specific to the intended concept rather than overly general. Third, linguistic complexity evaluates the question’s structure. Representing the token count of question q as len(q), the complexity index is calculated via standardization and sigmoid transformation: ComplexityIndex = σ \u0012len(q) −µ σ \u0013 (5) where σ(x) = (1 + e−x)−1 is the sigmoid function, and µ, σ are the mean and standard deviation of token counts in the dataset. This index helps assess if the question’s formulation is appropriately complex for clear guidance, avoiding overly simplistic or convoluted phrasing. These metrics are combined into a final quality score: QualityScore = α·AlignScore+β·MI(q, c)+γ·ComplexityIndex (6) where α, β, γ are empirically determined weight coefficients satisfy- ing α + β + γ = 1. This QualityScore provides a quantitative basis for assessing each potential guiding question. It is used for analysis or as a criterion for filtering out questions that fall below a predeter- mined quality threshold, thus improving the"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 7, "text": "are empirically determined weight coefficients satisfy- ing α + β + γ = 1. This QualityScore provides a quantitative basis for assessing each potential guiding question. It is used for analysis or as a criterion for filtering out questions that fall below a predeter- mined quality threshold, thus improving the overall relevance and ef- fectiveness of the guidance provided within the information retrieval process. 4 Experiment Setup To validate the effectiveness of AGQ framework, we designed and conducted a series of experiments. The primary goal was to evalu- ate the performance of AGQ framework across different scenarios. It is important to note that AGQ framework does not have specific requirements for LLMs and can be applied to both open-source and proprietary models. For these experiments, we selected ChatGLM4- 9B as the representative model, which demonstrates strong perfor- mance in NLP tasks[6]. 4.1 Dataset The experiments utilized the EOR-QA dataset, a custom dataset built for AGQ in the Enhanced Oil Recovery (EOR) domain. This dataset links key EOR concepts to relevant contextual sentences and para- graphs extracted from domain literature. Based on this structured context, EOR-QA comprises over 3,100 question-answer pairs, de- signed to cover different cognitive levels, from foundational princi- ples to practical applications. As elaborated in Section 3.2, all content underwent verification by domain experts to ensure accuracy and rel- evance. The EOR-QA dataset serves a dual purpose: it provides the foundational data for the CEIRT model to estimate and update user knowledge states (θ), and it acts as the source repository for selecting Inspiring Text used in guiding question generation. The decision to construct a custom dataset stemmed from a sur- vey of existing public resources, which revealed a lack of datasets adequately addressing the needs of our research in the EOR domain. While numerous general/scientific QA datasets exist, they typically lack the required domain-specific depth and concept coverage that are crucial for the AGQ framework’s user modeling and adaptive guidance mechanisms. Furthermore, our custom dataset mitigates the risk of data contamination inherent in public benchmarks, ensuring a rigorous evaluation of the model’s performance on genuinely unseen domain knowledge. Therefore, the creation of the EOR-QA dataset was a necessary prerequisite to effectively implement and reliably evaluate the AGQ framework within this specialized domain. 4.2 Baselines We compared the AGQ framework against several baseline methods to evaluate its effectiveness. The following approaches were included in our experimental comparison: • Zero-shot Question Generation: We directly prompted the LLM to generate guiding questions without providing any examples. This method relies solely on the LLM’s pre-trained knowledge and capabilities to generate relevant questions based on the user’s query. • CoT Prompts with Handcrafted Examples: This method incor- porates Chain-of-Thought (CoT) prompting with manually con- structed examples derived from the EOR-QA dataset to assist the LLM in guiding question generation, as CoT has demonstrated effectiveness in complex reasoning tasks requiring step-by-step thinking. • Human Expert: We also consider the guiding questions gener- ated by human experts in the petroleum field. These experts, with their deep domain knowledge, create questions that are specif- ically tailored to"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 8, "text": "LLM in guiding question generation, as CoT has demonstrated effectiveness in complex reasoning tasks requiring step-by-step thinking. • Human Expert: We also consider the guiding questions gener- ated by human experts in the petroleum field. These experts, with their deep domain knowledge, create questions that are specif- ically tailored to identify and address gaps in the user’s under- standing. This method serves as a gold standard against which the performance of the LLM-generated questions can be measured. Figure 3. Accuracy comparison of different guiding question generation methods over dialogue rounds. The AGQ method demonstrates performance significantly exceeding CoT and Zero-shot approaches, closely approaching the effectiveness of Human Experts. The ’Human with AGQ context’ group serves to validate the evaluation methodology. Figure 4. Accuracy comparison of guiding question generation using the AGQ framework with different LLMs (ChatGLM4-9B, Qwen2.5-7B, Qwen2.5-32B) and Human Expert over dialogue rounds. 5 Results and Analysis 5.1 Comparison with Baselines In this section, we evaluate the performance of the AGQ frame- work against several baseline methods and assess its generalizabil- ity across different LLMs. The primary comparison involves AGQ, Zero-shot Question Generation, CoT Prompts with Handcrafted Ex- amples, and a Human Expert benchmark. The accuracy is evaluated Figure 5. Evolution of user knowledge states (θ) across five key concepts using different guiding question generation methods. From left to right: AGQ, Chain-of-Thought (CoT), and Zero-shot. The results demonstrate the effectiveness of the AGQ framework’s generated questions in enhancing user understanding. through a systematic process: we first collect sets of conversations generated by each method, then employ an LLM to answer questions from the EOR-QA dataset based on the conversation context. Accu- racy is defined as the average rate of correct responses. Figure 3 illustrates the accuracy comparison over dialogue rounds. The results demonstrate AGQ’s superior performance. After 20 rounds, AGQ achieved 100% accuracy, significantly higher than CoT (41.1%) and Zero-shot (23.9%). Across all rounds, AGQ maintained a higher average accuracy (48.8%) compared to CoT (25.6%) and Zero-shot (16.3%). The performance trajectory of AGQ closely mir- rors that of the \"Human with AGQ context\" group (average 51.0%, reaching ≥95% accuracy by round 19), validating our automatic evaluation methodology, and approaches the effectiveness of human experts (average 72.0%, reaching ≥95% accuracy by round 14). While CoT demonstrates strong logical reasoning, its accuracy im- provement was limited by susceptibility to prompt examples, leading to repetitive questions and reduced information retrieval efficiency. The Zero-shot method, lacking context or examples, generated vague questions, resulting in consistently low accuracy with minimal vari- ance. In contrast, AGQ generates more specific and complex ques- tions compared to the baselines. See Table 1 for examples of guiding questions generated by each method addressing a similar underlying theme. Table 1. Qualitative Comparison of Example Guiding Questions Method Guiding Question AGQ How do surfactants and hydrocarbon miscible flooding tech- niques synergistically Enhance Oil Recovery during the pro- cess of increasing oil recovery? CoT What is the principle of hydrocarbon miscible flooding? Zero-shot How does the principle or technology mentioned in this con- versation work? Furthermore, to assess the generalizability of the AGQ"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 9, "text": "do surfactants and hydrocarbon miscible flooding tech- niques synergistically Enhance Oil Recovery during the pro- cess of increasing oil recovery? CoT What is the principle of hydrocarbon miscible flooding? Zero-shot How does the principle or technology mentioned in this con- versation work? Furthermore, to assess the generalizability of the AGQ framework, its performance was evaluated using distinct LLMs: Qwen2.5-7B and Qwen2.5-32B [16]. Figure 4 presents this cross-model comparison. The results illustrate several key characteristics. Firstly, initial perfor- mance correlates with model scale. In the early rounds (e.g., rounds 1-10), the larger Qwen2.5-32B model demonstrates higher accuracy compared to the smaller Qwen2.5-7B and ChatGLM4-9B models. For instance, at round 7, Qwen2.5-32B achieved approximately 50% accuracy, whereas Qwen2.5-7B registered below 40%. Secondly, de- spite these initial differences, accuracy consistently improves with increasing interaction rounds across all tested LLMs. Critically, all models converge to high accuracy levels (approaching 100%) by rounds 19-21. This convergence indicates the framework’s capacity to effectively enhance the performance of LLMs regardless of their initial capacity. Collectively, these findings confirm AGQ’s superior- ity over baseline methods and highlight its cross-model adaptability and robustness. Its effectiveness is not contingent on a specific LLM, showcasing its potential as a generalizable approach for enhancing information retrieval guidance systems across various model choices. 5.2 Text Similarity Evaluation To complement our evaluation, we also conducted a quantitative analysis comparing the guiding questions generated by each method against human expert reference questions using standard text similar- ity metrics [15, 11]. While these metrics primarily assess lexical and semantic similarity rather than functional effectiveness in informa- tion retrieval, they provide an additional perspective on performance comparison. As shown in Table 2, the AGQ framework substantially outperformed baseline methods across all metrics. AGQ achieved a BLEU-4 score of 0.219, significantly higher than CoT (0.025) and Zero-shot (0.016). Similarly, for ROUGE-1, AGQ scored 0.577 com- pared to 0.198 for CoT and 0.114 for Zero-shot. This superior perfor- mance extended to ROUGE-2 (AGQ: 0.278, CoT: 0.023, Zero-shot: 0.008) and ROUGE-L (AGQ: 0.463, CoT: 0.168, Zero-shot: 0.102). This result reflects the characteristic of question generation tasks, as functionally equivalent questions can be expressed using varied lex- ical choices and syntactic structures. Despite this inherent variabil- ity, AGQ´ s substantially higher scores confirm its greater alignment with human expert formulations. Full details of this evaluation can be found in Appendix. Table 2. Quantitative Evaluation using Text Similarity Metrics against Human Expert References Method BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L (F1) AGQ 0.219 0.577 0.278 0.463 CoT 0.025 0.198 0.023 0.168 Zero-shot 0.016 0.114 0.008 0.102 Figure 6. After 20 rounds of dialogue, the knowledge state (θj) value of EOR increased from 1.44 to 4.85 with AGQ, from 1.06 to 3.36 with CoT, and from 0.34 to 2.65 with Zero-shot. 5.3 Knowledge Gain To evaluate the impact on user understanding, we evaluated knowl- edge gain by comparing the knowledge state vector (θ) before and after the 20-round interaction process. Figure 6 tracks the knowledge state (θj) for the EOR concept specifically. It shows that the AGQ framework facilitated substan- tial and steady growth, increasing the θj"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 10, "text": "impact on user understanding, we evaluated knowl- edge gain by comparing the knowledge state vector (θ) before and after the 20-round interaction process. Figure 6 tracks the knowledge state (θj) for the EOR concept specifically. It shows that the AGQ framework facilitated substan- tial and steady growth, increasing the θj value from 1.44 to 4.85. This contrasts sharply with the moderate increase observed for CoT (1.06 to 3.36) and the minimal gain for the Zero-shot method (0.34 to 2.65), underscoring AGQ’s effectiveness in deepening understanding of specific concepts. Further evidence is presented in Figure 5, which displays the fi- nal knowledge states across five concepts in EOR-QA using radar charts. Visually, AGQ demonstrates superior performance across all concepts compared to CoT and Zero-shot methods. Quantitatively, the average final knowledge state across these five concepts was 4.70 for AGQ, significantly higher than 3.19 for CoT and 2.63 for Zero- shot. This substantial knowledge gain observed with AGQ aligns with the superior accuracy reported in Section 5.1, suggesting that the framework’s ability to generate effective guiding questions trans- lates directly into improved user understanding, retrieval efficiency, and perceived guidance quality. 5.4 Analysis In comparison, while the CoT method demonstrates capability in guiding users through the learning process, it lacks the precision needed to effectively address specific concepts where the user’s un- derstanding is weaker. This limitation stems from its inability to dif- ferentiate adequately between the user’s varying levels of knowledge state, resulting in a less targeted approach to information retrieval. On the other hand, the Zero-shot question generation method shows even weaker performance. Without accurate knowledge state mea- surement, the Zero-shot method generates overly general guiding questions lacking specific content. The superior performance of the AGQ framework can be attributed to the integration of the CEIRT model, which plays a crucial role in its success. The CEIRT model enables dynamic and precise knowl- edge measurement. This model-driven precision enables AGQ to generate guiding questions that are not only relevant but also strate- gically designed to challenge and expand the user’s knowledge in tar- geted concepts. By dynamically updating the user’s knowledge state and refining the difficulty and discrimination parameters (i.e., θ, b, and a), AGQ improves the effectiveness of information retrieval. 6 Ablation Study This section investigates the optimal difference between the difficulty of question i (bi) and the user’s knowledge state in concept j (θj) by experimentally comparing knowledge state growth under varying difficulty-ability differences. As illustrated in Figure 7, knowledge gain is maximized when the absolute difference between question difficulty bi and user knowledge state θj is approximately 1. This finding provides empirical support for the design of our Inspiring Text suitability score (Equation 2) and underlines the importance of providing an appropriate level of challenge to optimize information retrieval outcomes. Deviations from this difference significantly re- duced knowledge gain, emphasizing the need to align question dif- ficulty with user knowledge. Furthermore, as the difference was ad- justed to 1.5, users began to report that the difficulty gap was signifi- cant, with a noticeable decline in engagement. This feedback became more"}
{"doc_id": "2508.14025v1", "title": "Ask Good Questions for Large Language Models", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14025v1", "chunk_id": 11, "text": "Deviations from this difference significantly re- duced knowledge gain, emphasizing the need to align question dif- ficulty with user knowledge. Furthermore, as the difference was ad- justed to 1.5, users began to report that the difficulty gap was signifi- cant, with a noticeable decline in engagement. This feedback became more pronounced as the difference increased to 2. Figure 7. Ability growth peaks when the difficulty-knowledge gap (bi −θj) is approximately 1, suggesting optimal learning occurs at a moderate challenge. Error bars denote variability. 7 Conclusion In this paper, we introduced the AGQ framework, an innovative approach for generating guiding questions during the information retrieval process. At its core, the framework leverages the CEIRT model, a novel method for measuring knowledge in specialized do- mains that enables precise assessment of users’ knowledge states across multiple concepts. In comparative experiments against several baseline methods, AGQ demonstrated superior performance, closely matching the effectiveness of human experts. Additionally, we de- veloped a domain-specific text dataset (EOR-QA), which served as both a source of structured knowledge and inspiring text to facili- tate precise guiding question generation within the framework during our evaluations. The AGQ framework marks a key advance in LLM- based adaptive guidance, offering a versatile design that is readily adaptable to knowledge-intensive domains like medicine and educa- tion beyond its initial demonstration in the EOR field."}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 0, "text": "DPad: Efficient Diffusion Language Models with Suffix Dropout Xinhua Chen1∗, Sitao Huang1∗, Cong Guo1∗†, Chiyue Wei1, Yintao He1, Jianyi Zhang1, Hai “Hellen” Li1, Yiran Chen1 1Duke University Abstract Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to 61.4× speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad. 1. Introduction Large Language Models (LLMs) have become foundational in numerous applications [31, 10, 7, 24], yet their deployment is often hindered by inference latency. As shown in Fig. 1 (a), the predominant autoregressive framework generates text one token at a time [25, 26], imposing a sequential constraint that limits speed and scalability [13]. This has driven interest toward parallel decoding strategies. Diffusion-based Large Language Models [19, 3, 21, 27, 17] (dLLMs) offer a promising alternative by eliminating sequential dependencies. Formulating text generation as a parallel denoising process, they can predict entire sequences or generate text block-wise (i.e., semi-autoregressively) [23, 36], as in Fig. 1 (b). However, this parallelism often incurs high computational cost [22]: at each step, predictions for all future (suffix) tokens are computed, though only a small fraction are retained. Consequently, although dLLMs can generate multiple tokens in parallel, the resulting throughput gains are undermined by a disproportionate increase in computation, posing a key bottleneck to their widespread adoption [29]. To further understand this inefficiency, we analyze the role of suffix tokens under the block-wise masking mechanism in dLLMs. Interestingly, these suffix tokens contribute no direct semantic information to the attention; instead, they function as an information reservoir that collects signals from the already decoded prefix tokens, much like a scratchpad. Through multiple Transformer layers, this scratchpad can guide the diffusion process to produce a more organized and contextually consistent current block. Yet, most suffix tokens are redundant and low-entropy, often forming long runs of similar content. This redundancy becomes more severe ∗Equal contribution †Corresponding author: Cong Guo (cong.guo@duke.edu) DPad: Efficient Diffusion Language Models with Suffix Dropout Prompt Response Step 0 Step 1 (a) Auto-Regression LLM Prompt Response Block 0, Step 0 Decoded Block 0 Preﬁx: Attend Backward Current Block Suﬃx: Attend Forward with All Suﬃx Tokens (b) Block-wise Diﬀusion LLM (Semi-Autoregression) Decoding Masked …… Distance-Decay Dropout Step 2 Step 3 Block 1 Block 2 Block n Preﬁx: Attend Backward Only Block 0, Step 1 Block 1, Step 0 Block"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 1, "text": "Step 0 Decoded Block 0 Preﬁx: Attend Backward Current Block Suﬃx: Attend Forward with All Suﬃx Tokens (b) Block-wise Diﬀusion LLM (Semi-Autoregression) Decoding Masked …… Distance-Decay Dropout Step 2 Step 3 Block 1 Block 2 Block n Preﬁx: Attend Backward Only Block 0, Step 1 Block 1, Step 0 Block 1, Step 1 Prompt Response Block 0, Step 0 Block 0 DPad: Attend Forward with Nearby and Few Suﬃx Tokens Block 1 Block 2 Block 0, Step 1 Block 1, Step 0 Block 1, Step 1 Block 3 Block 4 (c) Ours: Diﬀusion LLM with Scratchpad (DPad) DPad Detailed Focused Outlined Current: Near: Far: Sliding Window Out of Scratchpad: Omitted …… …… …… Fig. 1: Comparison of (a) autoregressive LLMs, (b) block-wise diffusion LLMs, and (c) our DPad. DPad restricts suffix attention via: (i)Sliding Window: fixed-length suffix window; (ii) Distance-decay Dropout: removes distant suffix tokens without computing attention scores. as the distance to the current block increases, with attention scores for suffix tokens dropping sharply from near to far positions. Such redundancy not only imposes unnecessary computational overhead, but can also degrade the fidelity of the generated content. Based on the above insights, we propose the Diffusion Scratchpad (DPad), which attends forward only to a small number of near suffix tokens, as in Fig. 1 (c). It uses two suffix drop strategies: sliding window and distance-decay dropout. For the sliding window, inspired by prefix KV-cache optimizations [5, 35], we extend the idea to the suffix. Here, the suffix window has a fixed length and moves forward along with the current block, retaining only a limited number of nearby suffix tokens. In contrast to vanilla dLLMs, where suffix computation increases with the generation sequence length, our design keeps the cost bounded and reduces suffix-related computation by one dimension. For the distance-decay dropout, suffix tokens are removed according to their distance from the current block: the farther they are, the higher the dropout ratio, until all tokens beyond the window are omitted. Unlike conventional attention-score pruning [32, 18, 28], which first computes attention scores and then prunes based on their magnitude, DPad predetermines a distance-decay sparse pattern for suffix tokens prior to model execution and eliminates them in a single operation at the beginning of each step. Inspired by the Lottery Ticket Hypothesis (LTH) [11], which posits that properly pruned sub-networks with their original initialization can match the performance of dense networks after training, we observe that the strong generalization ability of dLLMs enables the training-free construction of “winning tickets” for suffix tokens during inference. Although this predetermined dropout strategy may appear intrusive, it consistently achieves strong empirical performance, with only a small subset of suffix tokens required to maintain accuracy comparable to vanilla dLLMs. This further suggests that sparsity is an inherent property of suffix attention. Additionally, the method is extremely simple to deploy, requiring only a few lines of code to implement. To illustrate the intuition behind DPad, we liken it to a real scratchpad used when writing a book, as shown in Fig. 1 (c, right). For the current chapter (i.e.,"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 2, "text": "of suffix attention. Additionally, the method is extremely simple to deploy, requiring only a few lines of code to implement. To illustrate the intuition behind DPad, we liken it to a real scratchpad used when writing a book, as shown in Fig. 1 (c, right). For the current chapter (i.e., block), the writer (dLLM) devotes the most attention, revising it multiple times, akin to denoising within a diffusion block. The upcoming chapter receives focused drafts for consistency, while much later chapters contain only brief outlines. Naturally, the “writer” should not, and indeed must not, fill all future chapters (all suffix tokens) with low-entropy, repetitive content merely to satisfy the fixed sequence length constraints of current dLLMs. Such uncontrolled filling distracts the “writer’s” attention and 2 DPad: Efficient Diffusion Language Models with Suffix Dropout wastes storage and computation, making it neither sustainable nor scalable. Finally, we emphasize that DPad enables efficient and compact generation without enforcing a fixed sequence length. Unlike conventional dLLMs, where mandatory sequence length and full suffix attention introduce redundancy and degrade output quality [36, 23], DPad produces concise results with minimal overhead. It is entirely training-free, saving substantial memory and computation, and achieves stable acceleration even at shorter generation sequence lengths (e.g., 256 and 512), outperforming both vanilla and optimized variants such as Fast-dLLM [34]. DPad is also compatible with existing optimizations [34, 20, 33], including prefix caching [34], dual caching [34], and dLLM-Cache [20]. It further scales naturally to long sequences: when combined with parallel generation and prefix caching, it achieves a 30.58× speedup on Dream/HumanEval (1024, 0-shot) and a 61.39× speedup on LLaDA-1.5/GSM8K (1024, 1-shot) over vanilla, while maintaining comparable model accuracy. These results highlight the strong potential of DPad for long-sequence inference and demonstrate its ability to unlock new efficiency frontiers in dLLMs. Although dLLMs remain in an early exploratory stage, with many researchers striving to assemble the final puzzle, we believe that DPad represents one of the key components in completing the whole picture and paves the way toward practical, scalable diffusion-based language modeling. 2. Preliminary 2.1. Foundational Principles of Diffusion Large Language Models (dLLM) Conventional Autoregressive Models (ARMs)definethejointprobabilityofageneratedsequence𝑥= (𝑥1, . . . , 𝑥𝐿) by factoring it into a product of conditional probabilities in a fixed sequential order [31]. The model predicts the next token 𝑥𝑖based on the conditional probability of the preceding tokens 𝑥1, . . . , 𝑥𝑖−1: 𝑝𝜃(𝑥) = 𝑝𝜃(𝑥1) 𝐿 Ö 𝑖=2 𝑝𝜃(𝑥𝑖|𝑥1, . . . , 𝑥𝑖−1) (1) In contrast, Diffusion Language Models (dLLMs) leverage a non-autoregressive process [3, 27, 21]. During training, the model learns to denoise a corrupted sample. This involves two processes: Forward Masking Process This process systematically replaces a proportion of tokens in a clean text sequence 𝑥0 with a special [MASK] token [22, 23], similar to applying noise in a conventional diffusion model [16]. The replacement is governed by a masking schedule where 𝑡∈[0, 1] represents the masked level. For a given 𝑡, the probability of an individual token 𝑥𝑖 0 being masked is: 𝑞(𝑥𝑖 𝑡|𝑥𝑖 0) = ( 1 −𝑡, if 𝑥𝑖 𝑡="}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 3, "text": "similar to applying noise in a conventional diffusion model [16]. The replacement is governed by a masking schedule where 𝑡∈[0, 1] represents the masked level. For a given 𝑡, the probability of an individual token 𝑥𝑖 0 being masked is: 𝑞(𝑥𝑖 𝑡|𝑥𝑖 0) = ( 1 −𝑡, if 𝑥𝑖 𝑡= 𝑥𝑖 0 𝑡, if 𝑥𝑖 𝑡= [MASK] (2) Reverse Unmasking Process The model 𝑝𝜃is trained to predict the original tokens given the partially masked sequence 𝑥𝑡, thereby learning to approximate the true unmasking probability 𝑞(𝑥0|𝑥𝑡) [3, 21]. This is achieved by minimizing the negative log-likelihood over the masked tokens [27, 24]: L(𝜃) = −E𝑡,𝑥0,𝑥𝑡       ∑︁ 𝑖:𝑥𝑖 𝑡=[MASK] log 𝑝𝜃(𝑥𝑖 0|𝑥𝑡)       (3) 3 DPad: Efficient Diffusion Language Models with Suffix Dropout This process makes use of a Transformer architecture to learn the distribution 𝑝𝜃(𝑥0|𝑥𝑡). During training, bidirectional multi-head attention on all tokens within a sequence—masked and unmasked tokens alike—is employed to predict the probability distribution for each masked token. 2.2. Inference and Sampling in dLLM The inference process begins by initializing a sequence 𝑦0 through the concatenation of a prompt 𝑟and 𝐿 mask tokens, denoted as 𝑦0 = 𝑟◦([MASK])𝐿 𝑖=1. Let 𝑀𝑠be the set of indices corresponding to masked tokens at step 𝑠; initially, 𝑀0 = {|𝑟| + 1, . . . , |𝑟| + 𝐿}. The model then iteratively refines this sequence over steps 𝑠= 1, 2, . . . , 𝑆. At each step 𝑠, the model 𝑝𝜃computes a probability distribution 𝑝𝜃(𝑦𝑖|𝑦𝑠−1) for every masked position 𝑖∈𝑀𝑠−1. From these distributions, the most likely token predictions ˆ 𝑦𝑖and their corresponding confidence scores 𝑐𝑖are determined: ˆ 𝑦𝑖= argmax 𝑣∈𝑉 𝑝𝜃(𝑦𝑖= 𝑣|𝑦𝑠−1) and 𝑐𝑖= 𝑝𝜃(𝑦𝑖= ˆ 𝑦𝑖|𝑦𝑠−1). (4) where 𝑉is model’s vocabulary. A scheduling function G(𝑠, 𝑆) determines the number of tokens, 𝑘𝑠, to unmask. The set of indices to update, 𝑈𝑠, is chosen by selecting the 𝑘𝑠positions from 𝑀𝑠−1 with the highest confidence scores. The new sequence 𝑦𝑠is then formed by updating these positions: (𝑦𝑠)𝑖= ( ˆ 𝑦𝑖 if 𝑖∈𝑈𝑠 (𝑦𝑠−1)𝑖 otherwise (5) The set of masked indices is updated via 𝑀𝑠= 𝑀𝑠−1 \\ 𝑈𝑠. This iterative refinement continues until 𝑀𝑠= ∅. 2.3. Related work on dLLM acceleration While early work in dLLMs employed Top-𝑘sampling to unmask a fixed number of high-confidence tokens at each step, later methods introduced more dynamic, confidence-aware techniques [34, 33] that greedily unmask all tokens above a threshold, thereby alleviating the generation bottleneck. Another line of research focuses on cache management. Because attention in dLLMs is bi-directional, conventional KV caching is not directly applicable. To address this, several optimizations [34, 20] exploit the observation that prefix tokens and distant suffix tokens exhibit little variation in their attention values across inference steps, allowing them to be reused and reducing expensive recomputation. A more fine-grained approach [28] further improves efficiency by dynamically evicting cache entries based on attention scores, leveraging the persistence of attention patterns across steps. 3. Method In this section, we begin by formalizing the Scratchpad mechanism in dLLMs, which underpins the role of suffix tokens"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 4, "text": "and reducing expensive recomputation. A more fine-grained approach [28] further improves efficiency by dynamically evicting cache entries based on attention scores, leveraging the persistence of attention patterns across steps. 3. Method In this section, we begin by formalizing the Scratchpad mechanism in dLLMs, which underpins the role of suffix tokens as an information reservoir. We then introduce Suffix Dropout, which combines a fixed-length sliding window that moves with the current block to bound the number of attended suffix tokens, with a distance-decay scheme that progressively prunes more distant ones. Finally, we propose the Diffusion Lottery Tickets (DLT) hypothesis, which provides a principled lens for interpreting dropout in dLLMs. 4 DPad: Efficient Diffusion Language Models with Suffix Dropout Preﬁx Current Suﬃx DPad Writing Preﬁx Information Current Information DPad Reading 1 2 3 4 5 6 7 8 9 Preﬁx Read Current Block Read 6 Fig. 2: Attention score maps illustrating the Scratchpad mechanism in dLLMs. The maps were generated by the LLaDA- 1.5 model [37] on prompt and 512-token sequences from the GSM8K dataset [9]. The attention matrix is divided into 3 × 3 blocks over prefix, current, and suffix. Blocks 7 and 8 collect information from the prefix and current into the suffix at layer 𝑛, while Block 6 feeds this stored information back to the current block at layer (𝑛+1). This write–store–read cycle makes suffix tokens a dynamic scratchpad for cross-layer contextual reuse. 3.1. Scratchpad Mechanism We first revisit the role of suffix tokens in dLLMs. Due to the masking structure, suffix tokens carry no direct semantic information but instead serve as an information reservoir, aggregating signals propagated from prefix tokens across multiple Transformer layers. This latent memory, which we refer to as DPad, stabilizes the denoising process by providing contextual scaffolding for the current block. As illustrated in Fig. 2 and the attention score maps, the token sequence can be partitioned into three contiguous segments: Prefix indices [0, 𝑐−1], current indices [𝑐, 𝑠−1], and suffix indices [𝑠, 𝐿−1]. The corresponding attention matrix is thus divided into 3 × 3 blocks. Among them, Blocks 6, 7, and 8 together define the scratchpad mechanism. Considering only one head, at layer 𝑛, queries from prefix and current tokens attend to keys from the suffix region. Formally, the global attention weights are defined as 𝐴(𝑛) = Softmax \u0012𝑄(𝑛) (𝐾(𝑛))⊤ √𝑑𝑘 \u0013 ∈R𝐿×𝐿. (6) We can partition 𝐴(𝑛) into submatrices corresponding to prefix (𝑃= [0, 𝑐−1]), current (𝐶= [𝑐, 𝑠−1]), and suffix (𝑆= [𝑠, 𝐿−1]). In particular, 𝐴(𝑛) 𝑆,𝑃= 𝐴(𝑛) [𝑠: 𝐿, 0 : 𝑐], 𝐴(𝑛) 𝑆,𝐶= 𝐴(𝑛) [𝑠: 𝐿, 𝑐: 𝑠], (7) represent the attention weights from suffix queries to prefix and current keys, respectively (Blocks 7 and 8 in Fig. 2). Multiplying these attention weights with the value matrix yields the actual outputs at suffix positions: 𝐻(𝑛) 𝑆 = 𝐴(𝑛) 𝑆,𝑃𝑉(𝑛) 𝑃 + 𝐴(𝑛) 𝑆,𝐶𝑉(𝑛) 𝐶 + 𝐴(𝑛) 𝑆,𝑆𝑉(𝑛) 𝑆 . (8) Here, 𝐻(𝑛) 𝑆 denotes the hidden representations of suffix tokens after attention. This equation shows that suffix tokens integrate information from prefix and current regions, effectively serving as a Scratchpad that"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 5, "text": "at suffix positions: 𝐻(𝑛) 𝑆 = 𝐴(𝑛) 𝑆,𝑃𝑉(𝑛) 𝑃 + 𝐴(𝑛) 𝑆,𝐶𝑉(𝑛) 𝐶 + 𝐴(𝑛) 𝑆,𝑆𝑉(𝑛) 𝑆 . (8) Here, 𝐻(𝑛) 𝑆 denotes the hidden representations of suffix tokens after attention. This equation shows that suffix tokens integrate information from prefix and current regions, effectively serving as a Scratchpad that 5 DPad: Efficient Diffusion Language Models with Suffix Dropout records contextual signals. After this aggregation, the outputs 𝐻(𝑛) are processed by the subsequent linear transformations (e.g., feed-forward layers and residual connections), which operate independently on each token. Thus, information written into the suffix tokens will not interact with other regions until the next layer’s attention, where prefix and current queries can read back the stored information from DPad. This establishes the complete write–store–read cycle across consecutive layers. At layer (𝑛+1), this stored information can be retrieved by the current block through 𝐴(𝑛+1) 𝐶,𝑆 = 𝐴(𝑛+1) [𝑐: 𝑠, 𝑠: 𝐿], (9) which corresponds to current-to-suffix attention (Block 6 in Fig. 2). This path enables the current block to reuse the information collected in the suffix at the previous layer. Through this mechanism, the stored information in suffix tokens at layer 𝑛can be retrieved by the prefix and current blocks at layer (𝑛+1). However, in practice, the influence of the suffix on the prefix is negligible: Prefix tokens correspond to already generated content, and their representations can be refined directly from themselves and the current block, without requiring additional signals from the suffix. Indeed, prior work [34] has shown that caching prefix states during inference barely impacts accuracy, implying that the suffix-to-prefix pathway is effectively redundant. Therefore, the essential interaction of the scratchpad mechanism lies in the current-to-suffix direction (Block 6), where the suffix serves as temporary memory to assist the ongoing denoising process. Therefore, we conjecture that the behavior of suffix tokens resembles a residual connection specialized for attention. Instead of directly propagating representations, the model compresses high-dimensional signals from the prefix and current into the suffix and re-injects them into the current block at the next layer. This is reminiscent of the bottleneck residual block in ResNets [14], where high-dimensional features are projected, propagated, and then expanded. Similarly, the suffix acts as a scratchpad that compresses contextual information and delivers it forward for cross-layer reuse in dLLMs. 3.2. Suffix Dropout Strategies Building on the scratchpad perspective, we analyze redundancy in suffix attention to motivate a structured dropout design. At the final layer 31, Fig. 3 shows a sharp distance-dependent decay: nearby suffix tokens dominate, consistent with general attention behavior and prior work on prefix sliding windows [5, 35]. Moreover, suffix attention is one to two orders of magnitude smaller than that of prefix or current, indicating that suffix representations are information-light and only a small subset is needed to serve as the scratchpad memory. Based on these findings, we propose two complementary suffix drop strategies: (a) fixed-length Sliding Win- dow to retain only a bounded number of near-suffix tokens, and (b) Distance-decay Dropout that progressively prunes distant suffix tokens with minimal impact on model quality, as illustrated in Fig. 1(c). Both mechanisms"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 6, "text": "scratchpad memory. Based on these findings, we propose two complementary suffix drop strategies: (a) fixed-length Sliding Win- dow to retain only a bounded number of near-suffix tokens, and (b) Distance-decay Dropout that progressively prunes distant suffix tokens with minimal impact on model quality, as illustrated in Fig. 1(c). Both mechanisms are efficiently realized through a Gaussian sampling process, which simultaneously enforces a bounded window and distance-dependent decay. As a result, suffix attention only needs to focus on nearby tokens, making the suffix dropout window decoupled from the overall sequence length. Unlike the vanilla setting, where the suffix grows with the generation sequence, our approach keeps it constant. This yields a clear computational benefit: suffix dropout effectively reduces suffix-related complexity by one dimension. Formally, let 𝐿denote the suffix length and 𝐵 the block size. In vanilla block-wise dLLMs, suffix computation requires 𝐿 𝐵steps, each involving 𝑂(𝐿) suffix token operations, whereas suffix dropout restricts this to a constant number of tokens, independent of 𝐿. Gaussian Sampling. Inspired by the smooth decay patterns observed in Fig. 3, we adopt a Gaussian sampling strategy to implement distance-aware token dropout. Formally, for a suffix token at distance 𝑑from the suffix 6 DPad: Efficient Diffusion Language Models with Suffix Dropout 0 100 200 300 400 500 600 700 Key index (offset) 0.000 0.005 0.010 0.015 0.020 Attention value Suffix Current Mean (all samples & heads) min-max range 200 250 0.000 0.005 0.010 Suffix Current Fig. 3: Analysis of the Suffix Drop strategy at the final layer (Layer 31), generated by the LLaDA-1.5 model [37] on GSM8K dataset [9] with a max length of 512. We collect attention weights from 100 samples across all heads, focusing on the current block queries (𝐴[𝑐:𝑠, 𝑐-200:]). To align positions, we truncate 200 tokens before 𝑐as the prefix and align all current blocks at 𝑐. The plot shows the mean attention distribution over key indices (green curve), together with the min–max range (shaded area). Current and suffix boundaries are marked, showing that attention on far suffix tokens rapidly decays, motivating our distance-decay dropout design. boundary, its retention probability 𝑃(𝑑) is defined by the right half of a standard normal distribution with effective window size 𝑊: 𝑃(𝑑) = 𝑎· 1 𝜎 √ 2𝜋 exp \" −1 2 \u0012 𝑘𝜎 𝑊·𝑑−𝜇 𝜎 \u00132# , 0 < 𝑑≤𝑊, (10) where 𝜇= 0, 𝜎= 1, and the suffix boundary is the center of the Gaussian distribution. Two hyperparameters control the distribution along the 𝑥- and 𝑦-axes: (1) 𝑘𝜎 𝑊maps the window size 𝑊to 𝑘𝜎(e.g., for 𝑊= 256, setting 𝑘= 3 ensures 𝑑= 256 corresponds to 3𝜎); (2) 𝑎scales the overall sampling magnitude vertically. This formulation ensures that tokens farther from the boundary are retained with exponentially decreasing probability, implemented via rejection sampling. This dropout strategy is not static but is dynamically re-applied in a block-wise way. As shown in Fig. 1 (c), after a block is generated, all previously dropped-out tokens are restored. The Gaussian sampling is then invoked again on the updated set of suffix tokens before the next block is decoded. This dynamic resampling is"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 7, "text": "static but is dynamically re-applied in a block-wise way. As shown in Fig. 1 (c), after a block is generated, all previously dropped-out tokens are restored. The Gaussian sampling is then invoked again on the updated set of suffix tokens before the next block is decoded. This dynamic resampling is crucial for preventing sampling bias and ensuring that no token is systematically ignored during the generation process. Implementation. Our method can be implemented in just a few lines of code: 1 # Select suffix token indices with Gaussian distance -decay sampling 2 keep_idx = gaussian_suffix_dropout(x) 3 # Preserve only the selected tokens 4 x_pruned = x[keep_idx] 5 # Forward the reduced sequence to the model 6 output = model(x_pruned , args) Here, the Gaussian-based sampler gaussian_suffix_dropout selects a subset of suffix tokens according to distance-aware probabilities. We then construct a pruned sequence x_pruned by indexing with keep_idx. 7 DPad: Efficient Diffusion Language Models with Suffix Dropout 0 100 200 300 400 Key index (offset to Suffix) 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 Average Attention Score d=362 d=161 d=199 d=298 d=236 d=147 d=399 d=134 d=320 d=252 a. Original Avg across layers Min-Max range Spotlight tokens 0 100 200 300 400 Key index (offset to Suffix) 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 Average Attention Score d=362 d=161 d=199 d=298 d=236 d=147 d=399 d=134 d=320 d=252 b. After Pruning Avg across layers Min-Max range Spotlight tokens pruned Fig. 4: (a) Average, maximum, and minimum attention scores of suffix tokens paid by current block tokens (𝐴[𝑐:𝑠, 𝑠:]) across layers in LLaDA-1.5, showing overall decay with occasional spikes (e.g., d = 199, 298, 362). (b) After forcibly pruning these spike positions, attention shifts to nearby tokens, indicating that adjacent positions can absorb suffix information (e.g., pruning token 362 shifts the spike to token 359). The rest of the model remains unchanged, except for a minor adjustment to the RoPE embeddings inside the attention module to ensure correct positional encoding under suffix dropout. Besides, we also introduce an early termination mechanism to prevent inefficient computation after a <eos> token is generated, a common issue in fixed generation length strategies like LLaDA [23]. Our method performs a conditional check after each decoded block; if an <eos> token is found, we fill the remaining sequence with <eos> and immediately halt generation. RoPE Adjustment. Our suffix dropout mechanism requires only a minor adjustment to the Rotary Positional Embedding (RoPE) [30] to maintain correct positional information. In standard RoPE, a token at absolute position 𝑖is encoded using an angle 𝜃𝑖= 𝑖· Δ. After dropout, however, only a sparse, non-contiguous subset of suffix indices I = {𝑖1, . . . , 𝑖𝑚} is preserved. To handle this, we ensure that each preserved token retains its original positional information. Rather than using their re-indexed positions after dropout, we apply a mapping function 𝑓(𝑖𝑘) that retrieves the original absolute position of the 𝑘-th preserved token. The new angle is then computed as 𝜃′ 𝑖𝑘= 𝑓(𝑖𝑘) · Δ. (11) Accordingly, the modified RoPE application becomes RoPE′(x𝑖𝑘, 𝑖𝑘) = RoPE(x𝑖𝑘, 𝑓(𝑖𝑘))."}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 8, "text": "information. Rather than using their re-indexed positions after dropout, we apply a mapping function 𝑓(𝑖𝑘) that retrieves the original absolute position of the 𝑘-th preserved token. The new angle is then computed as 𝜃′ 𝑖𝑘= 𝑓(𝑖𝑘) · Δ. (11) Accordingly, the modified RoPE application becomes RoPE′(x𝑖𝑘, 𝑖𝑘) = RoPE(x𝑖𝑘, 𝑓(𝑖𝑘)). (12) This adjustment requires only a lightweight remapping inside the attention module and does not alter the func- tional form of RoPE, confirming that suffix dropout is nearly cost-free while preserving positional consistency. 3.3. Diffusion Lottery Tickets Hypothesis The analysis in Fig. 3 not only reveals the overall decay of current-to-suffix attention, but also occasional sharp spikes in the maximum values. These spikes cannot be predicted in advance and, in principle, pruning them could affect model accuracy. To investigate this, we conduct an additional experiment shown in Fig. 4. We first run dLLM inference for one step, then forcibly prune the top 10 highest-attention suffix tokens (“spotlight” 8 DPad: Efficient Diffusion Language Models with Suffix Dropout tokens) occurring beyond the first 128 positions (e.g., at distances 199, 298, and 362). Surprisingly, pruning such distant suffix tokens, even those corresponding to large spikes, has little effect on model accuracy. When these spikes are removed, the model shifts its attention to nearby suffix tokens, for example, the spotlight token at 362 in Fig. 4 (a) is replaced by increased attention at its neighbor token 359 in Fig. 4 (b), which effectively absorbs the lost information. This behavior is consistent with the strong generalization ability of diffusion models and the fact that suffix tokens are initialized without semantic content: through DPad mechanism, suffix tokens can dynamically learn and store information during inference. Consequently, information carried by distant suffix tokens appears largely position-insensitive, and pruning spike positions has almost no impact on final accuracy, as further confirmed in our evaluation experiments. This leads to an important intuition: information collected by distant suffix tokens is insensitive to their exact positions. When writing a book, one may use a scratchpad to record an event in the “middle chapters,” but it is not necessary to pin it down to precisely token 186; recording it at token 152 or 198 suffices. This explains why suffix dropout can be applied a priori, without computing exact attention scores, and why it fundamentally differs from prefix cache pruning [28, 32]: Prefix tokens carry dense, position-bound semantic information and thus cannot be arbitrarily discarded, whereas suffix tokens act as a flexible, low-rank memory buffer. These observations resonate with the Lottery Ticket Hypothesis (LTH) [11], which posits that properly pruned sub-networks with their original initialization can match the performance of dense networks after training. We extend this intuition to dLLMs and propose the Diffusion Lottery Tickets (DLT) hypothesis: During inference, the suffix region contains redundant tokens, yet a sparse subset is sufficient to preserve semantic consistency and generation quality. Through DPad mechanism, this subset can be adaptively reorganized into “winning tickets” within the forward pass. In this view, suffix dropout becomes a training-free lottery ticket search, where Gaussian sampling selects a compact set of suffix"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 9, "text": "tokens, yet a sparse subset is sufficient to preserve semantic consistency and generation quality. Through DPad mechanism, this subset can be adaptively reorganized into “winning tickets” within the forward pass. In this view, suffix dropout becomes a training-free lottery ticket search, where Gaussian sampling selects a compact set of suffix tokens that carry the essential information for denoising in dLLMs. Summary. We showed that suffix tokens function as a scratchpad, enabling a high-dimensional residual pathway. This insight motivates two lightweight strategies, Sliding Window and Distance-decay Dropout, together with the DLT hypothesis, which interprets suffix pruning as a training-free lottery ticket search. These methods are simple, training-free, and easy to integrate. Next, we evaluate the efficiency and accuracy of DPad-enhanced dLLMs across standard benchmarks. 4. Experiments 4.1. Experimental Setup Models and Baselines. All experiments are conducted on an NVIDIA A100 80GB GPU. We evaluate DPad on a suite of representative open-source dLLMs: two variants of LLaDA (8B-Instruct and 1.5) [23, 37] and Dream-v0-Base-7B [36]. We compare against the following baselines: • Vanilla: the unmodified LLaDA [23] and Dream [36] backbones, which compute attention over all tokens in the sequence, including prefix, current, and suffix tokens. • +Parallel (Fast-dLLM): a parallel decoding strategy applied to the vanilla backbone, where tokens are unmasked whenever their confidence exceeds a predefined threshold [34], instead of the fixed top-𝑘 unmasking used in Vanilla. • +Prefix Cache: applying KV caching on prefix tokens, leveraging the observation that their key–value states remain nearly static across inference steps [34], thereby avoiding redundant computations. When combining DPad with caching, we adopt a standard Prefix Cache. Since our dropout already minimizes the computational cost of the suffix, the Dual Cache mechanism proposed in [34] is unnecessary. 9 DPad: Efficient Diffusion Language Models with Suffix Dropout Table 1: Performance of LLaDA-Instruct with DPad on four benchmarks. LLaDA-Instruct Benchmark Method Efficiency Accuracy (%) Latency(s)↓ TPS↑ Gen. Length Flexible↑ Strict↑ GSM8K 4-shot Vanilla 27.48 1.00× 8.44 1.00× 232 / 256 78.39 37.38 +DPad 18.35 1.50× 8.76 1.04× 161 / 256 78.54 63.84 +Parallel (Fast-dLLM) 8.55 3.21× 27.14 3.22× 232 / 256 78.54 38.67 +Parallel+DPad 6.64 4.14× 24.25 2.87× 161 / 256 79.76 64.97 MATH 4-shot Vanilla 25.40 1.00× 9.79 1.00× 249 / 256 33.58 8.42 +DPad 21.61 1.18× 9.75 1.00× 211 / 256 33.42 28.04 +Parallel (Fast-dLLM) 9.91 2.56× 25.09 2.56× 249 / 256 33.40 8.76 +Parallel+DPad 9.20 2.76× 22.93 2.34× 211 / 256 33.30 27.98 HumanEval 0-shot Vanilla 34.67 1.00× 13.64 1.00× 473 / 512 43.90 – +DPad 27.41 1.26× 15.96 1.17× 438 / 512 47.56 – +Parallel (Fast-dLLM) 11.48 3.02× 41.40 3.04× 475 / 512 43.29 – +Parallel+DPad 9.14 3.79× 47.86 3.51× 438 / 512 46.34 – MBPP 3-shot Vanilla 62.11 1.00× 4.82 1.00× 299 / 512 15.00 – +DPad 15.89 3.91× 6.85 1.42× 109 / 512 40.40 – +Parallel (Fast-dLLM) 14.26 4.36× 20.99 4.36× 299 / 512 15.00 – +Parallel+DPad 6.02 10.32× 18.28 3.79× 110 / 512 39.40 – The DPad strategy introduces three hyperparameters: a decay rate factor 𝑘, a magnitude scalar 𝑎, and a sliding window size. We tuned these on"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 10, "text": "109 / 512 40.40 – +Parallel (Fast-dLLM) 14.26 4.36× 20.99 4.36× 299 / 512 15.00 – +Parallel+DPad 6.02 10.32× 18.28 3.79× 110 / 512 39.40 – The DPad strategy introduces three hyperparameters: a decay rate factor 𝑘, a magnitude scalar 𝑎, and a sliding window size. We tuned these on small subsets of each benchmark (see Sec. 4.3.2). Unless otherwise specified, we use a block size of 32 and set the confidence threshold for parallel decoding to 0.9. Benchmarks and Metrics. Our evaluation spans multiple domains. For reasoning, we use GSM8K [9] and MATH [15]; for code generation, we use HumanEval [8] and MBPP [4]. We report two aspects of performance: We report two aspects of performance: • Accuracy: evaluated with task-specific metrics, including pass@1 for code generation and accuracy measures for reasoning (e.g., flexible-extract and strict-match accuracy on GSM8K). All accuracies are reported as percentages. • Efficiency: measured by average inference latency per sample and Tokens Per Second (TPS). TPS is computed over the entire generated sequence until an <eos> token is produced. We also use 𝑎/ 𝑏to denote the generation (Gen.) length, where 𝑎is the average real generation length and 𝑏is the benchmark setting length. 4.2. Main Results We evaluate the efficiency and accuracy of DPad on three models and four benchmarks, as reported in Tbl. 1, Tbl. 2, and Tbl. 3. We compare against both the vanilla LLaDA baseline [23] and the parallel decoding baseline Fast-dLLM [34]. Our results reveal three consistent trends: • Latency: consistently improved across all settings. • Throughput (TPS): shows higher variance, with fluctuations across benchmarks and models. • Accuracy: Flexible Match remains comparable, while Strict Match achieves significant gains. We analyze each of these aspects in detail below. 10 DPad: Efficient Diffusion Language Models with Suffix Dropout Table 2: Performance of LLaDA-1.5 with DPad on four benchmarks. LLaDA-1.5 Benchmark Method Efficiency Accuracy (%) Latency(s)↓ TPS↑ Gen. Length Flexible↑ Strict↑ GSM8K 4-shot Vanilla 27.61 1.00× 7.77 1.00× 215 / 256 80.59 61.87 +DPad 18.26 1.51× 8.56 1.10× 156 / 256 80.14 78.47 +Parallel (Fast-dLLM) 8.06 3.42× 26.61 3.43× 215 / 256 80.82 62.62 +Parallel+DPad 6.23 4.43× 25.23 3.25× 157 / 256 80.89 78.92 MATH 4-shot Vanilla 25.12 1.00× 8.67 1.00× 218 / 256 33.52 32.72 +DPad 20.63 1.22× 9.48 1.09× 196 / 256 34.08 37.00 +Parallel (Fast-dLLM) 9.48 2.65× 22.96 2.65× 218 / 256 33.60 32.92 +Parallel+DPad 8.57 2.93× 22.76 2.63× 195 / 512 32.92 35.96 HumanEval 0-shot Vanilla 34.80 1.00× 3.16 1.00× 110 / 512 40.85 – +DPad 11.55 3.01× 7.19 2.28× 83 / 512 44.51 – +Parallel (Fast-dLLM) 11.16 3.12× 9.80 3.10× 109 / 512 39.63 – +Parallel+DPad 5.26 6.61× 15.64 4.95× 82 / 512 39.63 – MBPP 3-shot Vanilla 62.34 1.00× 1.02 1.00× 63 / 512 38.20 – +DPad 14.95 4.17× 4.33 4.26× 65 / 512 39.80 – +Parallel (Fast-dLLM) 5.47 11.39× 11.62 11.44× 64 / 512 38.60 – +Parallel+DPad 4.41 14.14× 14.83 14.60× 65 / 512 41.60 – Table 3: Performance of Dream-Base with DPad on four benchmarks. Dream-Base Benchmark Method Efficiency Accuracy (%) Latency(s)↓ TPS↑ Gen. Length Flexible↑ Strict↑ GSM8K"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 11, "text": "4.26× 65 / 512 39.80 – +Parallel (Fast-dLLM) 5.47 11.39× 11.62 11.44× 64 / 512 38.60 – +Parallel+DPad 4.41 14.14× 14.83 14.60× 65 / 512 41.60 – Table 3: Performance of Dream-Base with DPad on four benchmarks. Dream-Base Benchmark Method Efficiency Accuracy (%) Latency(s)↓ TPS↑ Gen. Length Flexible↑ Strict↑ GSM8K 4-shot Vanilla 22.30 1.00× 11.43 1.00× 255 / 256 75.06 74.37 +DPad 10.27 2.17× 12.75 1.11× 131 / 256 75.28 75.06 +Parallel (Fast-dLLM) 13.84 1.61× 18.43 1.61× 255 / 256 75.51 74.83 +Parallel+DPad 5.24 4.25× 24.17 2.11× 127 / 256 74.83 74.75 MATH 4-shot Vanilla 21.01 1.00× 12.19 1.00× 256 / 256 34.06 37.76 +DPad 16.64 1.26× 15.33 1.26× 255 / 256 34.14 37.64 +Parallel (Fast-dLLM) 8.82 2.38× 29.03 2.38× 256 / 256 35.12 38.62 +Parallel+DPad 7.72 2.72× 33.04 2.71× 255 / 256 34.44 38.32 HumanEval 0-shot Vanilla 28.49 1.00× 17.93 1.00× 511 / 512 51.22 – +DPad 8.20 3.47× 26.83 1.50× 220 / 512 51.22 – +Parallel (Fast-dLLM) 14.15 2.01× 36.11 2.01× 511 / 512 53.05 – +Parallel+DPad 4.06 7.01× 52.62 2.93× 214 / 512 52.44 – MBPP 3-shot Vanilla 49.15 1.00× 10.42 1.00× 512 / 512 52.40 – +DPad 41.36 1.19× 12.38 1.19× 512 / 512 52.60 – +Parallel (Fast-dLLM) 12.38 3.97× 41.36 3.97× 512 / 512 55.40 – +Parallel+DPad 9.86 4.98× 51.92 4.98× 512 / 512 54.80 – Latency. The latency improvements of DPad arise from three sources. (1) Reduced suffix complexity: in standard dLLMs, handling suffix tokens incurs quadratic complexity with respect to suffix length, whereas DPad reduces this to linear complexity via suffix dropout. (2) Early termination: by decoupling generation from a 11 DPad: Efficient Diffusion Language Models with Suffix Dropout ============== 4 shots =============== Question: …… #### 12 …… Question: …… #### 200 ===================================== Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? First, let's determine how many eggs Janet uses each day: - She eats 3 eggs for breakfast. - She bakes 4 eggs for her friends. Total eggs used per day = 3 + 4 = 7 eggs. Next, let's find out how many eggs are left to sell: - The ducks lay 16 eggs per day. - Janet uses 7 eggs per day. Eggs left to sell per day = 16 - 7 = 9 eggs. Now, let's calculate how much Janet makes from selling the eggs: - She sells each egg for $2. Total earnings per day = 9 eggs * $2/egg = $18. Therefore, Janet makes $18 every day at the farmers' market. #### 18 (173 tokens) Question +DPad To determine how much Janet makes every day at the farmers' market, we need to follow these steps: 1. Calculate the total number of eggs laid by Janet's ducks per day. 2. Subtract the number of eggs she eats for breakfast and bakes for her friends. 3. Multiply"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 12, "text": "Question +DPad To determine how much Janet makes every day at the farmers' market, we need to follow these steps: 1. Calculate the total number of eggs laid by Janet's ducks per day. 2. Subtract the number of eggs she eats for breakfast and bakes for her friends. 3. Multiply the remaining eggs by the price per egg to find her daily earnings. 1. Janet's ducks lay 16 eggs per day. 2. She eats 3 eggs for breakfast and bakes 4 eggs for her friends. Therefore, she uses a total of 3 + 4 = 7 eggs per day. 3. The number of eggs left to sell is 16 - 7 = 9 eggs.\\n4. She sells each egg for $2, so her daily earnings are 9 eggs * $2/ egg = $18. Therefore, Janet makes $18 every day at the farmers' market. (195 tokens) Flexible-Match Strict-Match Flexible-Match Strict-Match LLaDA-Instruct/GSM8K (256, 4-shot) Baseline Fig. 5: A Case Study from GSM8K on In-Context Learning and Format Adherence. The figure contrasts a baseline model’s output with the same model enhanced by DPad. The baseline produces the correct answer (passing Flexible-Match) but fails to replicate the structured reasoning from the prompt, thus failing the Strict-Match. DPad successfully generates both the correct answer and the required format, passing both evaluations. fixed sequence length through our sliding window design, DPad naturally supports early stopping once the end-of-sequence is reached. (3) More concise generations: suffix dropout removes low-entropy tokens, leading the model to produce more compact outputs and slightly shorter generations. Importantly, as analyzed in the Accuracy section, this does not degrade task accuracy. Across most benchmarks, the reduction in generation length is modest (e.g., on LLaDA-1.5, about 10% on MATH and 27% on GSM8K), while datasets such as MBPP remain almost unchanged. Thus, the majority of latency gains come from reduced suffix computation rather than shorter outputs. Overall, DPad consistently reduces latency across all benchmarks in the three-model dLLM suite, achieving 1.18× to 4.17× speedups over the vanilla backbone. When combined with parallel decoding, DPad yields an ad- ditional 1.08× to 3.48× speedup over parallel decoding alone, resulting in an overall speedup of 2.76× to 14.14× over vanilla. This complementarity arises because the two methods target orthogonal bottlenecks: DPad elimi- nates redundant KV-token computation via suffix dropout, while parallel decoding in Fast-dLLM [34] mitigates dependency constraints by selectively decoding only high-confidence tokens in parallel. By combining these approaches, we exploit both finer-grained token pruning and safe multi-token prediction, yielding substantial efficiency gains. That said, the advantage of suffix dropout is less pronounced on short-sequence benchmarks. This is due to two factors. First, our Gaussian-based dropout typically yields about 62.5% sparsity within the sliding window, so the constant overhead remains significant. It is also impractical to shrink the window much further when the maximum generation length itself is short (e.g., 256). Second, when the prompt dominates the sequence (e.g., ∼80% prompt vs. 20% suffix in GSM8K and MATH), suffix attention accounts for only a small fraction of total computation. According to Amdahl’s law [1], the maximum achievable speedup in"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 13, "text": "much further when the maximum generation length itself is short (e.g., 256). Second, when the prompt dominates the sequence (e.g., ∼80% prompt vs. 20% suffix in GSM8K and MATH), suffix attention accounts for only a small fraction of total computation. According to Amdahl’s law [1], the maximum achievable speedup in such cases is inherently bounded (often around 1.1×). Nevertheless, even without any system-level optimizations, our simple implementation of suffix dropout consistently delivers stable latency improvements. Its true potential emerges in longer-sequence settings in Fig. 6 and Fig. 7, where the suffix fraction grows; as we show later with 1024-token sequences, DPad achieves substantial additional reductions in latency. 12 DPad: Efficient Diffusion Language Models with Suffix Dropout LLaDA +Eary Termination + DPad Parallel-Only + DPad Parallel + PrefixCache + DPad Flexible Match (FM, %): 78.17, Strict Match (SM, %): 48.98 FM: 78.17, SM: 48.98 FM: 78.77, SM: 74.07 FM: 78.77, SM: 49.43 FM: 79.38, SM: 74.22 FM: 78.77, SM: 51.63 FM: 77.10, SM: 70.66 127s, 1.55TPS 26.4s, 7.47TPS 6.28s, 18.4TPS 11.7s, 16.9TPS 2.26s, 51.4TPS 10.8s, 18.2TPS 2.07s, 55.5TPS 1.0x 4.8x 20.3x 1.0x 5.2x 1.0x 5.2x 61.39x Fig. 6: Latency comparison on LLaDA-1.5 with GSM8K (1024 tokens, 1-shot). Throughput. We observe a subtle distinction between latency- and TPS-based efficiency metrics. While DPad consistently reduces per-sample latency, its TPS gains may appear less pronounced. This is because DPad often encourages the model to generate more concise and complete responses, reaching the end-of-sequence earlier and producing fewer redundant tokens. As illustrated in Fig. 5 and Fig. 10, this behavior reflects not a limitation but an improvement: the model terminates naturally rather than exhausting its context with low-quality continuations. We view this as an additional quality benefit of DPad, complementing its efficiency gains. However, when the generation length is reduced more substantially, TPS can drop. This mainly occurs because we apply no additional system- or algorithm-level optimizations: shorter generations lead to lower GPU utilization, which mechanically reduces TPS. Yet, the extra tokens produced by baselines often carry low semantic information; higher TPS simply reflects more tokens, not greater efficiency. For example, if we were to artificially pad outputs with meaningless characters, TPS would increase, but this would not represent a genuine efficiency gain. Therefore, in our main results, we report raw throughput numbers without post- processing. This also highlights a broader point: the community may need to reconsider throughput metrics and develop alternatives that better balance sequence length and accuracy, rewarding models that achieve comparable accuracy with shorter generations. Accuracy. In addition to improving inference efficiency, DPad also enhances accuracy across nearly all tasks for the LLaDA-Instruct and LLaDA-1.5 models (Table 1), thereby defying the typical trade-off between speed and accuracy. For instance, DPad yields substantial gains in strict-match accuracy on GSM8K (+26.46%) and MATH (+19.62%) for LLaDA-Instruct. By contrast, the Dream-Base model shows no consistent advantage, with accuracy broadly comparable to the baseline and fluctuating only within a narrow margin. We attribute this stability to differences in training protocols, particularly the absence of instruction tuning such as supervised fine-tuning (SFT). The improvement in strict-match score is"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 14, "text": "LLaDA-Instruct. By contrast, the Dream-Base model shows no consistent advantage, with accuracy broadly comparable to the baseline and fluctuating only within a narrow margin. We attribute this stability to differences in training protocols, particularly the absence of instruction tuning such as supervised fine-tuning (SFT). The improvement in strict-match score is particularly noteworthy, as it highlights DPad’s ability to enhance in-context learning. The vanilla backbone typically exhibits low strict-match performance (e.g., only 37.38% on GSM8K for LLaDA-Instruct), since this metric requires the model not only to produce the correct final answer (Flexible-Match) but also to adhere to the specific reasoning format demonstrated in few-shot exemplars, as illustrated in Figure 5. We posit that failures in strict matching often stem from interference by distant suffix tokens, which introduce low-value or off-format patterns that distract the model and encourage verbose or poorly structured outputs. By reducing the influence of such suffix tokens and directing attention toward high-value, information-rich prefix exemplars, DPad enables the model to replicate the structured reasoning formats required by strict matching more faithfully. In addition, by suppressing redundant generations, DPad facilitates earlier convergence to concise, well-formatted outputs, further improving strict-match performance without altering model parameters. 13 DPad: Efficient Diffusion Language Models with Suffix Dropout Dream + DPad Parallel-Only + DPad Parallel + PrefixCache + DPad Acc (%): 51.22 94.4s 10.3s 31.8s 3.9s 26.9s 3.1s 1.0x 9.1x 1.0x 8.2x 1.0x 8.7x 30.58x Acc: 49.39 Acc: 51.22 Acc: 53.05 Acc: 55.49 Acc: 54.27 Dream + DPad Parallel-Only + DPad Parallel + PrefixCache + DPad Acc (%): 56.71 342.2s 20.0s 73.7s 4.7s 71.5s 3.5s 1.0x 17.1x 1.0x 15.8x 1.0x 20.3x 97.32x Acc: 49.39 Acc: 53.66 Acc: 52.44 Acc: 57.93 Acc: 54.88 Gen. Length: 1024 Gen. Length: 2048 Fig. 7: Latency comparison on Dream-Base with HumanEval (1024 and 2048 tokens, 1-shot). 4.3. Ablations and Analysis 4.3.1. Maximum Generation Length To better quantify the efficiency gains of different acceleration strategies in long-sequence generation, we analyze speedups under various configurations, as shown in Fig. 6 and Fig. 7. Specifically, we evaluate GSM8K with LLaDA-1.5 and HumanEval with Dream under the following strategies: Vanilla, Vanilla+ET (augmented with an early-termination mechanism), +Pad (our method alone), and +Parallel (Fast-dLLM). We then con- sider combined strategies to assess complementarity, including +Pad+Parallel, +Parallel+PrefixCache, and +Parallel+PrefixCache+Pad. This setup allows us to disentangle the contributions of suffix dropout, parallel decoding, and prefix caching, as well as their interactions. Speedup. We find that the acceleration benefits of DPad grow substantially with sequence length. For LLaDA- 1.5 on GSM8K, improvements are modest at shorter sequence lengths (up to 1.51× under a 256-token limit). However, when the maximum length is extended to 1024 tokens (single-shot setting), standalone DPad achieves a dramatic 20.3× speedup. This effect arises because vanilla LLaDA typically produces concise answers (about 200 tokens) but continues generating redundant <eos> tokens to fill the context window, incurring wasted computation. To provide a stronger baseline, we augment vanilla with early termination; even under this setting, DPad still delivers a 4.8× improvement. Finally, when combined with parallel decoding and prefix caching (Fast-dLLM), the efficiency gains compound, yielding an overall"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 15, "text": "generating redundant <eos> tokens to fill the context window, incurring wasted computation. To provide a stronger baseline, we augment vanilla with early termination; even under this setting, DPad still delivers a 4.8× improvement. Finally, when combined with parallel decoding and prefix caching (Fast-dLLM), the efficiency gains compound, yielding an overall 61.39× speedup compared to vanilla LLaDA and a 8.7× improvement over Fast-dLLM. A similar scaling trend is observed with Dream-Base on HumanEval. Here, DPad alone accelerates inference by 9.13× for 1024-token sequences and 17.1× for 2048-token sequences. When combined with Fast-dLLM, the benefits become multiplicative: 30.58× at 1024 tokens and 97.32× at 2048 tokens. These results demonstrate that suffix dropout and parallel decoding address orthogonal bottlenecks and, when combined, yield near two orders of magnitude improvement in long-sequence generation. Overall, the strong scaling with sequence length highlights DPad as a key component for enabling dLLMs to match the scalability of traditional autoregressive models. 14 DPad: Efficient Diffusion Language Models with Suffix Dropout 32 64 128 192 256 320 384 448 512 # of Preserved Suffix Tokens 32 64 128 192 256 320 384 448 512 Sliding Window Size 70.51 75.28 77.18 77.48 80.14 80.59 73.92 81.88 81.65 81.35 65.58 79.00 81.73 81.58 81.12 51.40 76.88 81.35 81.73 81.05 79.91 43.21 71.49 81.65 81.73 80.21 80.36 78.32 34.19 62.85 80.74 81.50 80.67 81.96 79.53 79.68 33.74 63.91 79.53 81.80 82.26 81.50 80.14 79.98 79.68 (a).Uniform Dropout Flexible-Match Scores 32 64 128 192 256 320 384 448 512 # of Preserved Suffix Tokens 32 64 128 192 256 320 384 448 512 Sliding Window Size 70.51 73.09 77.18 78.54 79.53 80.59 78.85 80.89 80.59 81.35 78.85 81.73 82.03 81.12 81.12 80.06 81.20 82.11 81.35 79.23 79.91 78.62 80.97 81.20 81.50 81.50 81.05 78.32 75.51 80.44 81.27 81.12 81.58 81.50 79.98 79.68 69.67 81.35 83.47 82.56 81.65 80.52 80.29 78.32 79.68 (b).Gaussian Dropout Flexible-Match Scores 32 64 128 192 256 320 384 448 512 # of Preserved Suffix Tokens 32 64 128 192 256 320 384 448 512 Sliding Window Size 60.05 70.66 72.48 76.04 78.77 77.03 71.27 80.06 75.13 69.14 62.24 77.18 72.48 66.26 60.20 46.78 75.44 75.59 67.78 61.87 56.71 38.29 69.07 79.00 73.31 67.55 63.91 61.26 29.72 59.44 78.62 77.71 73.16 71.27 66.87 67.25 29.42 59.29 77.71 78.47 76.42 73.09 70.66 69.90 68.69 (c).Uniform Dropout Strict-Match Scores 32 64 128 192 256 320 384 448 512 # of Preserved Suffix Tokens 32 64 128 192 256 320 384 448 512 Sliding Window Size 60.05 65.96 72.48 75.13 77.86 77.03 76.80 79.61 76.12 69.14 77.63 80.74 78.62 68.54 60.20 78.32 80.14 77.10 70.66 60.58 56.71 77.48 80.14 78.01 74.53 69.75 64.29 61.26 73.31 79.68 77.79 75.66 72.10 70.96 67.63 67.25 66.19 79.61 81.20 77.86 73.62 71.11 68.84 67.85 68.69 (d).Gaussian Dropout Strict-Match Scores 40 50 60 70 80 40 50 60 70 80 30 40 50 60 70 80 30 40 50 60 70 80 Fig. 8: Ablation Study on Sliding Window Size and Dropout Function for DPad on LLaDA-1.5/GSM8K (512, 4-shot). Heatmaps showing Flexible-Match Accuracy scores with (a) uniform and (b) Gaussian"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 16, "text": "40 50 60 70 80 40 50 60 70 80 30 40 50 60 70 80 30 40 50 60 70 80 Fig. 8: Ablation Study on Sliding Window Size and Dropout Function for DPad on LLaDA-1.5/GSM8K (512, 4-shot). Heatmaps showing Flexible-Match Accuracy scores with (a) uniform and (b) Gaussian dropout, and Strict-Match Accuracy scores with (c) uniform and (d) Gaussian dropout, across varying sliding window sizes and number of preserved suffix tokens. The (512, 512) configuration corresponds to the baseline, as it involves no token dropout. Accuracy. Longer, low-shot settings further highlight DPad’s ability to preserve and even enhance model accuracy. In the 1-shot LLaDA setting, the strict-match score of the baseline drops significantly (from ∼60% to ∼50%), whereas DPad’s performance remains remarkably stable (dropping only from ∼78% to ∼74%). This resilience demonstrates that DPad substantially strengthens the model’s in-context learning capability, a significant achievement for a training-free method. By contrast, on the 2048-token HumanEval task, we observed a 7.32% accuracy degradation when applying DPad to the vanilla Dream model. This degradation is largely mitigated when DPad is combined with Fast- dLLM. We hypothesize that this isolated performance drop arises from complex interactions between our training-free pruning strategy and the model’s native Top-𝑘sampling behavior, pointing to an interesting direction for future investigation. 4.3.2. Sliding Window Size and Dropout Function We conducted an ablation study on LLaDA-1.5/GSM8K to determine the optimal sliding window size and dropout function for DPad. As shown in Figure 8, we evaluated both flexible-match and strict-match scores 15 DPad: Efficient Diffusion Language Models with Suffix Dropout while varying the window size, the number of preserved suffix tokens, and the dropout function (Uniform vs. Gaussian). Our analysis reveals two key findings that point to the existence of a critical context window of approximately 64–128 tokens immediately following the current block. First, within this critical window, performance consistently improves as more suffix tokens are preserved. For windows extending beyond this zone, however, performance exhibits a concave trend, peaking at a density of around 50%. Second, when the token budget is limited (e.g., fewer than 128 preserved tokens), spreading this budget thinly across a larger window significantly degrades accuracy. These observations lead to a clear principle for our method: the token budget should be prioritized to maintain a high density of preserved tokens within the critical 64–128 token window, as expanding the window to more distant tokens with a limited budget can be counterproductive. As further evidence, Fig. 8 validates our choice of a truncated Gaussian function for token dropout. Com- pared against a uniform random baseline, Gaussian dropout performs comparably under large token budgets (≥192), but its advantage becomes increasingly clear under more stringent, low-budget conditions. In partic- ular, when the sliding window is large, Gaussian dropout consistently outperforms the uniform strategy, achieving superior or equivalent performance with fewer preserved tokens. This empirical result aligns with the consistent decaying patterns observed in RoPE [30], attention scores (Fig. 4), and token confidence maps [12], which collectively motivate our design. By biasing dropout toward nearby tokens, the Gaussian approach allocates the token"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 17, "text": "uniform strategy, achieving superior or equivalent performance with fewer preserved tokens. This empirical result aligns with the consistent decaying patterns observed in RoPE [30], attention scores (Fig. 4), and token confidence maps [12], which collectively motivate our design. By biasing dropout toward nearby tokens, the Gaussian approach allocates the token budget more efficiently and thus delivers better perfor- mance under limited computation. Of course, Gaussian sampling may not be the optimal decay function, and other decay-based schemes (e.g., exponential, linear, or step-wise cutoff) remain to be explored. Nevertheless, in the training-free setting, we find that results are largely insensitive to the exact decay form, as long as the scheme emphasizes nearer tokens. 4.3.3. The Choice of Gaussian Hyperparameters Inspired by findings that dLLMs exhibit different behaviors on mathematical and code-generation tasks [12], we tune the hyperparameters for our Gaussian Sampler separately for each domain. We perform a grid search over two key parameters: the decay rate, 𝑘, and a scale factor, 𝑎. The parameter 𝑎is used to control the overall retention density, which we define as the expected proportion of suffix tokens preserved by the Gaussian Sampler inside the sliding window. The results of this search for LLaDA-Instruct are presented in Figure 9, conducted on 50-sample subsets of GSM8K and HumanEval. While using a small subset for tuning may introduce some variance, the findings provide clear directional insights. Our method with Gaussian Dropout consistently outperforms the baseline in nearly all configurations, with a single, minor exception for the GSM8K flexible-match accuracy score at 𝑘= 1 and a density of 25.0%. The suboptimal performance at 𝑘= 1 is expected, as at this value the Gaussian curve is relatively flat across the sampling window (see Appendix A.1), causing the sampling to degenerate into a near-uniform distribution. Focusing on the more effective range of 𝑘∈[2, 4], we identify distinct optimal settings for each domain. For the mathematical reasoning task (GSM8K), a density of 25.0% provides the best balance of accuracy and efficiency. Although some settings with 50.0% density achieve a slightly higher strict-match score, they do so by preserving twice as many suffix tokens, which significantly undermines acceleration. We therefore select 𝑘= 4.0 and a density of 25.0% (from 𝑎= 2.0) as the optimal configuration. For code generation (HumanEval), a configuration of 𝑘= 3.0 and a density of 37.5% (from 𝑎= 2.3) yields the best performance. Based on these findings, we adopt these hyperparameters for all subsequent math and code benchmarks for LLaDA-Instruct (see Appendix A.1), with similar tuning for other models. 16 DPad: Efficient Diffusion Language Models with Suffix Dropout 40 50 60 70 Accuracy Score k 1 2 3 4 1 2 3 4 1 2 3 4 Density 25.0% 37.5% 50.0% Performance across k and Density Metric GSM8K(Flexible) GSM8K(Strict) Humaneval Baseline Chosen Fig. 9: Ablation study on hyperparameters 𝑘and 𝑎for LLaDA-Instruct on subsets of GSM8K and HumanEval. The parameter 𝑎is mapped to the retention density shown on the x-axis. Each dashed line represents the baseline performance for the solid line of the same color and metric. 5. Discussion 5.1. Beyond a Training-free"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 18, "text": "Fig. 9: Ablation study on hyperparameters 𝑘and 𝑎for LLaDA-Instruct on subsets of GSM8K and HumanEval. The parameter 𝑎is mapped to the retention density shown on the x-axis. Each dashed line represents the baseline performance for the solid line of the same color and metric. 5. Discussion 5.1. Beyond a Training-free Method: Suffix Dropout with SFT While DPad demonstrates improved accuracy and computational efficiency across diverse benchmarks by strategically pruning the suffix space, we observe a performance degradation in very long-sequence generation, particularly at a context length of 2048 tokens. We attribute this behavior to a distributional shift introduced by suffix dropout. During pre-training, the model is optimized to predict token distributions conditioned on a full, continuous sequence of masked suffix tokens. In contrast, our distance-decay dropout forces the model at inference time to predict based on a much smaller, discontinuous set of masked suffix tokens. While moderate dropout helps the model focus on prompt semantics, such a large shift in the conditional distribution can hinder generalization, leading to degraded performance. This training–inference distribution gap may be mitigated with supervised finetuning (SFT). In particular, the attention lottery hypothesis offers a useful perspective that can be incorporated into training. We revise the learning objective as: LDPad(𝜃) = −E𝑥0,𝑡,𝑀 \"∑︁ 𝑖∈𝐶 log 𝑝𝜃 \u0010 𝑥0𝑖| 𝑥𝑡(𝐼𝑅∪𝐼𝑃∪𝑀)\u0011# , (13) where 𝑀denotes a subset of masked suffix tokens sampled via distance-decay dropout at each training step, 𝐼𝑅represents the prompt tokens, 𝐼𝑝represents the generated prefix tokens,𝜃represents the model parameters, 𝑡∈[0, 1] is degree of masking of samples from the forward masking process (Sec. 2.1), and the loss is computed over the current block tokens 𝐶. Using stochastic dropout masks rather than a fixed deterministic pattern enhances robustness, as the model does not overfit to any single dropout scheme. This revised objective explicitly integrates the notion of an attention lottery into training, encouraging the model to avoid wasting capacity by writing redundant information into distant suffix tokens that are likely to be pruned at inference. Looking further ahead, one could even incorporate distance-decay dropout directly into the pre-training phase, allowing the model to learn sparsity from scratch. Such pre-training with sparse suffix attention would naturally align training and inference conditions, and may yield even stronger efficiency–accuracy trade-offs. 17 DPad: Efficient Diffusion Language Models with Suffix Dropout 5.2. Comparison to Semi-Autoregressive Diffusion and Block Diffusion Block Diffusion models [2] operate autoregressively at the block level, predicting each block conditioned only on its predecessors. As a result, their attention mechanism excludes access to subsequent blocks, in contrast to the directional attention employed by semi-autoregressive models. While this design is computationally efficient, its strictly forward-looking nature introduces two major limitations. First, it is vulnerable to the reversal curse [6], since it cannot capture long-range (beyond a single block) bidirectional dependencies that are crucial for tasks such as code generation, which often require iterative back-and-forth refinement [12]. Second, by discarding the suffix, it forfeits the ability to use suffix tokens as a scratchpad (see Sec. 3.1), thereby losing an important medium for contextual organization. Our proposed distance-decay dropout interpolates between these paradigms. It"}
{"doc_id": "2508.14148v1", "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14148v1", "chunk_id": 19, "text": "crucial for tasks such as code generation, which often require iterative back-and-forth refinement [12]. Second, by discarding the suffix, it forfeits the ability to use suffix tokens as a scratchpad (see Sec. 3.1), thereby losing an important medium for contextual organization. Our proposed distance-decay dropout interpolates between these paradigms. It retains suffix tokens to function as a scratchpad, while at the same time preserving the model’s bidirectional learning ability, allowing information to be both written into and retrieved from the suffix scratchpad as needed. 6. Conclusion We addressed a key bottleneck in dLLMs: the high cost of full suffix attention, where most distant tokens are redundant and add little value. To overcome this, we introduced the Diffusion Scratchpad (DPad), a simple training-free inference strategy that redefines suffix attention. DPad combines a fixed-length sliding window, reducing complexity from quadratic to linear, with a distance-decay dropout that prunes low-entropy suffix tokens before computation. This design leverages the inherent sparsity of suffix attention, constructing an efficient “winning ticket” for generation on the fly. Experiments show that DPad consistently improves efficiency without sacrificing accuracy, achieving up to 61.4× speedups when combined with existing optimizations. These results highlight DPad as a practical and scalable solution for long-sequence generation, paving the way for the dLLM to move from a promising alternative to a viable foundation for future applications."}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 0, "text": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation Xian Gao1, Jiacheng Ruan1, Zongyun Zhang1, Jingsheng Gao1, Ting Liu1, Yuzhuo Fu1 1Shanghai Jiao Tong University Abstract With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibil- ity within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the gener- ation of review comments; however, current LLM-based re- view tasks lack a unified evaluation benchmark to rigorously assess the models’ ability to produce comprehensive, accu- rate, and human-aligned assessments—particularly in scenar- ios involving multimodal content such as figures and tables. To address this gap, we propose MMReview, a comprehen- sive benchmark that spans multiple disciplines and modal- ities. MMReview includes multimodal content and expert- written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and So- cial Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise re- view generation, outcome formulation, alignment with hu- man preferences, and robustness to adversarial input manipu- lation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized founda- tion for the development of automated peer review systems. Introduction Peer review is essential to scholarly publishing, ensuring re- search quality and enhancing academic writing. However, the growing volume of submissions has strained the tradi- tional review process, leading to inefficiencies and limited reviewer availability (Kim, Lee, and Lee 2025), which re- stricts feedback and delays review outcomes. Advances in LLMs have made automated peer review increasingly vi- able, as these models show strong reasoning abilities and can offer constructive feedback on academic manuscripts (Liu and Shah 2023; Zhao et al. 2024; Zhuang et al. 2025), par- tially alleviating reviewer burden. Yet, current evaluations of LLM-generated reviews focus mainly on final outputs, lacking in-depth analysis of the reasoning processes behind model judgments. Additionally, most studies concentrate on AI papers with publicly available text, overlooking the mul- timodal nature of academic papers—such as figures and ta- bles—and the evaluation of LLMs in reviewing research across broader scientific domains. To address the aforementioned challenges, we propose MMReview, a comprehensive benchmark for peer review generation that spans multiple disciplines and modalities. MMReview incorporates three distinct types of input modal- ities: textual content from manuscripts, figures and tables embedded within the papers, and rendered PDF pages con- verted into images. These data span 17 research domains across 4 disciplinary categories. To obtain high-quality peer review samples for evaluation purposes, we developed a multi-model collaborative pipeline for data filtering and gen- eration. Specifically, we first curated a total of 51,881 pa- pers with associated reviews; then, we filtered the collected seed dataset D to obtain high-quality papers while maintain- ing a relatively balanced distribution; subsequently, we ex- tracted reference answers from human reviews. Finally, we conducted manual verification to correct errors, resulting in a"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 1, "text": "first curated a total of 51,881 pa- pers with associated reviews; then, we filtered the collected seed dataset D to obtain high-quality papers while maintain- ing a relatively balanced distribution; subsequently, we ex- tracted reference answers from human reviews. Finally, we conducted manual verification to correct errors, resulting in a curated set of 240 samples that serve as the foundation for task construction. Building upon these samples, we intro- duce 4 thematic categories encompassing 13 diverse tasks, each designed to thoroughly assess the capabilities of LLMs in step-wise review generation, outcome formulation, align- ment with human preferences, and robustness to adversar- ial input manipulation. We conduct comprehensive experi- ments on 16 open-source models as well as 5 state-of-the- art closed-source models, including GPT-4o and Claude-4- Sonnet, across 13 tasks. The results highlight the compre- hensive nature of the MMReview benchmark and uncover several key findings, offering insights for future research on LLM-based automated academic peer review. The primary contributions of this paper can be summa- rized as follows: • We introduce MMReview, the first comprehensive eval- uation benchmark for automated academic peer review using LLMs, spanning multiple disciplines and modali- ties. Built upon our data filtering and generation pipeline, MMReview comprises 240 high-quality samples across 17 academic fields in 4 disciplines. • We meticulously design 13 distinct tasks encompass- ing a total of 6,724 thoughtfully curated questions, en- abling multi-dimensional evaluation of model perfor- mance. These diverse tasks allow for targeted assessment and facilitate the identification of potential limitations in LLM-generated peer review content. • We conduct extensive experiments on 16 open-source and 5 closed-source models using the MMReview bench- mark, offering some key findings of LLM-based auto- mated reviewing. Our findings offer in-depth analysis and valuable guidance for the future development of LLM-assisted peer review systems. Related Works LLMs for Paper Review LLMs have shown strong potential in analyzing complex scholarly texts (Liu and Shah 2023; Zhao et al. 2024; Zhuang et al. 2025). Initial studies indicate that LLM- generated review comments partially overlap with those of human reviewers, suggesting their potential contribution to peer review (Robertson 2023; Liang et al. 2023). How- ever, further research reveals that even advanced models like GPT-4o often fail to meet human expectations in review quality (Zhou, Chen, and Yu 2024). To improve alignment with peer review standards, researchers have built datasets from public review platforms and fine-tuned LLMs (Kang et al. 2018; Yuan, Liu, and Neubig 2021; Shen et al. 2022; Dycke, Kuznetsov, and Gurevych 2023; Gao, Brant- ley, and Joachims 2024). Other methods involve multi-turn dialogue (Tan et al. 2024) or multi-agent prompting (D’Arcy et al. 2024) to generate comprehensive feedback. Nonethe- less, these efforts focus solely on textual reviews, neglecting the vital role of multimodal content—such as figures and ta- bles—and lack rigorous analysis of the reasoning processes behind LLM-generated critiques. Evaluation for LLM-based Peer Review Prior studies (Shen et al. 2022; Yu et al. 2024; Gao, Brantley, and Joachims 2024; Tan et al. 2024; Gao et al. 2025) have predominantly evaluated the quality of LLM-generated peer review comments by measuring their"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 2, "text": "rigorous analysis of the reasoning processes behind LLM-generated critiques. Evaluation for LLM-based Peer Review Prior studies (Shen et al. 2022; Yu et al. 2024; Gao, Brantley, and Joachims 2024; Tan et al. 2024; Gao et al. 2025) have predominantly evaluated the quality of LLM-generated peer review comments by measuring their correlation or simi- larity with human-written reviews using automated metrics such as BLEU (Papineni et al. 2002), ROUGE (Lin 2004), BERTScore (Zhang et al. 2020), and METEOR (Banerjee and Lavie 2005). In addition, several studies (Robertson 2023; Zhou, Chen, and Yu 2024; Gao et al. 2025) have adopted the LLM-as-a-judge paradigm, leveraging cutting- edge language models to assess the quality of review com- ments produced by other LLMs. Given the absence of an established gold standard for this evaluation task, recent re- search (Xu et al. 2024) has introduced the Generative Esti- mator for Mutual Information (GEM) to quantify the degree of semantic overlap between LLM-generated and human- authored reviews. Nevertheless, existing evaluation method- ologies are not grounded in a unified benchmark or task framework, and they fall short of providing a comprehen- sive analysis of the underlying reasoning processes involved in LLM-generated peer review. MMReview Benchmark In this section, we first present the overall pipeline for data collection and construction of the MMReview benchmark, followed by a detailed exposition of the task design method- ology. Overall Pipeline of MMReview Benchmark As illustrated in Figure 1, the construction of MMReview is divided into three stages: data collection, data processing, and task construction. Data Collection During the data collection phase, we gathered academic papers from publicly accessible peer re- view platforms or sources where reviewer comments were openly available. These papers not only contain the full manuscript texts but also include reviewer-written evalua- tions and final decisions (accept or reject). Specifically, we curated a total of 51,881 papers with associated reviews submitted between 2013 and 2024 to venues such as ICLR and NeurIPS (from the OpenReview platform and NeurIPS Proceedings , as well as articles from the journal Nature Communications . For each collected paper, we employed Deepseek-V3 and GPT-4o to automatically infer its aca- demic discipline and research domain. In cases where the two models produced inconsistent classifications, human verification was performed. Ultimately, all papers were cat- egorized into four overarching disciplines: Artificial Intelli- gence, Natural Sciences, Engineering Sciences, and Social Sciences, resulting in a seed dataset denoted as D. Figure 1 illustrates the distribution of papers across the four disci- plines within D. Data Processing During the data processing stage, we first filtered the collected seed dataset D to obtain high- quality papers while maintaining a relatively balanced dis- tribution across disciplines and ensuring a comparable num- ber of accepted and rejected papers. To achieve this, we de- signed a dual-path joint data selection mechanism that si- multaneously filters for paper quality and enforces distribu- tional balance. Specifically, we utilized Qwen3-32B1 (Yang et al. 2025) to generate summaries of each paper under two distinct input conditions: one using only the abstract and the other using the full text of the manuscript."}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 3, "text": "joint data selection mechanism that si- multaneously filters for paper quality and enforces distribu- tional balance. Specifically, we utilized Qwen3-32B1 (Yang et al. 2025) to generate summaries of each paper under two distinct input conditions: one using only the abstract and the other using the full text of the manuscript. A greater divergence between the two generated summaries is interpreted as evidence that the full text provides substantially more information, thus indicating higher paper quality. Samples with a significant information gain from the full text were retained in our test benchmark. To further ensure that the benchmark maintains a balanced distribution across academic disciplines and an approximately equal ratio of accepted to rejected papers, we supplemented the benchmark by manually incorporating top-ranked papers from specific domains in D based on the quality rankings. This human-filtering procedure guarantees that the composition of the benchmark dataset aligns with our desired distributional properties. During the data filtering phase, we obtained a total of 240 paper samples spanning 17 research domains across 4 major disciplines to construct our evaluation benchmark, denoted as Db. The statistical details of Db, including the number of samples per domain and the distribution of accepted ver- sus rejected papers, are presented in Figure 1. For each of 1Qwen3-32B ranks among the top 10 on the OpenCompass leaderboard, offering a favorable trade-off between performance and model size, making it efficient for large-scale sample filtering Data Collection 𝑫 Domain Classification Human Verification 𝑫𝒃 𝑫𝑸𝑨 Review Task Generation Screen Shot Task Construction Data Processing 𝑫 𝑫𝒃 Small LLM Human Filtering PDF Parsing Data Filtering Outcome 768 Step 782 Attack 654 Preference 96 34538 13398 2880 1065 AI NS ES SS MMReview Benchmark Human Verification Golden Answer Extraction Text Figures Tables ACC 50 ACC 29 ACC 19 ACC 11 REJ 65 REJ 34 REJ 19 REJ 13 AI NS ES SS 115 63 38 24 Human Verification Papers Reviews Decisions Figure 1: The construction pipeline of MMReview. The construction pipeline is divided into three stages: data collection, data processing, and task construction. these 240 samples, we utilized PDF parsing tools to extract textual content, figures, and tables from the manuscript files, and converted each page of the PDF into corresponding im- ages. As a result, we constructed three distinct modalities of input data: text-only, multimodal (text combined with ex- tracted visual elements), and PDF-as-image. Task Construction During task construction, we devel- oped 13 tasks grouped into four thematic categories—step- based, outcome-based, preference-based, and attack- based—reflecting the peer review workflow and challenges LLMs may face. Task distribution is shown in Figure 1. Prompts were designed based on reviewer guidelines from major academic conferences. For each task, we used regular expressions or GPT-4o to extract reference answers from human reviews in Db, forming the question–answer dataset DQA. For the Fake Strength Evaluation and Fake Weakness Evaluation tasks, GPT-4o and custom prompts generated antonymic rewrites, which were manually verified for semantic accuracy. This process finalized the MMReview benchmark. Figure 2 illustrates its multidisciplinary scope, multimodal inputs, and diverse task design. The detailed prompts of task generation and each"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 4, "text": "For the Fake Strength Evaluation and Fake Weakness Evaluation tasks, GPT-4o and custom prompts generated antonymic rewrites, which were manually verified for semantic accuracy. This process finalized the MMReview benchmark. Figure 2 illustrates its multidisciplinary scope, multimodal inputs, and diverse task design. The detailed prompts of task generation and each task can be found in the supplementary materials. Step-based Tasks The Step-based theme comprises five tasks designed to pro- gressively evaluate the performance of LLMs in simulating the key components of the academic peer review process. Summary (S) Summarizing a paper is the initial step in peer review and a key test of a model’s ability to extract essential content. Inaccurate summaries may impair subse- quent review generation. To address this, the Summary task assesses a model’s ability to distill key information from a full manuscript into an accurate, concise summary. The model generates a brief summary in its own words, avoiding abstract copying and subjective judgment. These outputs are compared to human-written summaries and evaluated for se- mantic similarity and information coverage, measuring the model’s holistic comprehension and representation of aca- demic content. Strengths Evaluation (SE) and Weaknesses Evalua- tion (WE) Summarizing and analyzing a manuscript’s strengths and weaknesses is a core aspect of peer review. The Strengths Evaluation and Weaknesses Evaluation tasks assess LLMs’ ability to identify and articulate the merits and limitations of academic papers. These tasks test whether models can synthesize technical highlights and methodolog- ical concerns noted by human reviewers, focusing on four dimensions: Quality, Clarity, Significance, and Originality. In the Strengths Evaluation task, models argue for accep- tance by detailing methodological rigor, experimental ro- bustness, structural clarity, research impact, and novelty, thus evaluating their capacity to extract technical contribu- tions and assess scientific merit. In contrast, the Weaknesses Evaluation task adopts a rejection-oriented stance, testing critical reasoning and constructive critique. Model outputs are compared with human reviews based on content cover- age and semantic similarity. Soundness Scoring (SS) and Presentation Scoring (PS) The Soundness Scoring and Presentation Scoring tasks eval- uate LLMs’ ability to quantitatively assess manuscript qual- ity, focusing on technical soundness and writing presenta- tion. In Soundness Scoring, the model rates the reliability of technical claims, experimental rigor, and evidential support, emphasizing empirical and methodological validity. Presen- tation Scoring assesses linguistic clarity and logical orga- nization, reflecting writing quality and information struc- ture. Both tasks require integer scores from 1 to 4, denoting “poor” to “excellent.” Model scores are compared to human ratings to assess judgment consistency. Outcome-based Tasks The Outcome-based tasks focus on assessing a model’s di- rect capability to generate peer review outcomes, with the goal of evaluating its alignment with human reviewers in fi- nal decision-making. Conditional Decision (CD) The Conditional Decision task assesses LLMs’ ability to synthesize human-written re- views and generate an overall quality score for a paper. Pro- vided with reviewer comments detailing strengths, weak- nesses, and evaluations of methodology and results, the model assigns a numerical score from 1 to 10, reflecting a scale from “fundamentally flawed or lacking novelty” to “groundbreaking contribution,” aligned with academic con-"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 5, "text": "and generate an overall quality score for a paper. Pro- vided with reviewer comments detailing strengths, weak- nesses, and evaluations of methodology and results, the model assigns a numerical score from 1 to 10, reflecting a scale from “fundamentally flawed or lacking novelty” to “groundbreaking contribution,” aligned with academic con- ference standards. The task evaluates the model’s capacity to interpret sentiment, weighting, and evaluative reasoning in the reviews and translate them into a coherent quantitative judgment. Model scores are compared with human ratings to assess alignment and accuracy in review-based decision- making. Direct Decision (DD) and CoT Decision (CoD) The Di- rect Decision and CoT (Chain-of-Thought) Decision tasks evaluate LLMs’ ability to autonomously assess academic paper quality, reflecting two reviewing paradigms: stream- lined judgment and step-by-step reasoning (Wei et al. 2022). These tasks vary in input format and cognitive complexity, enabling controlled comparison of model performance un- der different reasoning demands. In the Direct Decision task, the model produces an overall score without guidance, sim- ulating a reviewer’s holistic judgment from a single read. In contrast, the CoT Decision task guides the model through a structured reasoning process: summarizing the paper, ana- lyzing strengths and weaknesses across Quality, Clarity, Sig- nificance, and Originality, assigning Soundness and Presen- tation scores, and synthesizing an overall score. This mirrors a reviewer’s iterative, analytical evaluation. The tasks test reasoning ability, analytical structure, and scoring traceabil- ity. Model outputs are compared to human scores to assess consistency and rationality. While the Direct task empha- sizes efficiency, the CoT task offers interpretability, together forming a complementary framework for evaluating LLMs’ scoring competence. Meta Decision (MD) Beyond generating individual re- views and scores, a key aspect of academic peer review is the Area Chair’s (AC) synthesis of reviewer feedback to make a final decision. To emulate this, the Meta Decision task re- quires the model to issue a binary judgment—Accept or Re- ject—based on multiple human reviews, testing its ability to aggregate information and balance differing views. The prompt provides structured guidance and evaluation criteria, prompting step-by-step reasoning. The model is instructed to assess the quality and consistency of reviews rather than merely averaging scores. This task mirrors the real role of an AC and rigorously evaluates the model’s capacity for synthe- sis, decision rationality, and transparency. Model outputs are compared to human-written meta-reviews to assess reliabil- ity and scientific judgment in high-level peer review. Preference-based Task: Pairwise Rank (PR) Prior work has shown that pairwise comparison effectively evaluates the alignment between LLM-generated prefer- ences and human judgments (Liu et al. 2024b). Since aca- demic conference acceptance tiers—oral, spotlight, poster, reject—reflect human preference rankings, the Pairwise Rank task is designed to test whether LLMs, as reviewers, display preference patterns consistent with human evalua- tors. This task assesses the model’s relative judgment ability by presenting pairs of papers from different acceptance tiers: oral (top 5%), spotlight (top 25%), poster, and reject. The model compares and ranks the papers, simulating real-world peer review selection. Alignment is measured by compar- ing model preferences with actual acceptance categories to determine ranking accuracy. To"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 6, "text": "relative judgment ability by presenting pairs of papers from different acceptance tiers: oral (top 5%), spotlight (top 25%), poster, and reject. The model compares and ranks the papers, simulating real-world peer review selection. Alignment is measured by compar- ing model preferences with actual acceptance categories to determine ranking accuracy. To reduce positional bias (Shi et al. 2025; Thakur et al. 2025), each comparison is repeated with reversed input order. Attack-based Tasks This task category assesses models’ robustness and discrim- inative ability in peer review by introducing adversarial in- puts. Fabricated strengths or weaknesses—such as inverted pros and cons—are used to test the model’s capacity to de- tect factual inaccuracies. Misleading prompts further aim to divert the model from its original instructions. Model out- puts are compared to human judgments or the model’s own non-adversarial responses, focusing on its ability to preserve evaluative independence and logical consistency under input perturbations. Fake Strengths (FS) and Fake Weaknesses (FW) LLMs have been widely observed to exhibit a tendency toward “sycophancy”—an inclination to conform to human judg- ments (Liu et al. 2024a; Kumaran et al. 2025)—which may impair their ability to identify flaws and generate critical peer review comments. Furthermore, the hallucination prob- lem inherent in LLMs can result in inaccurate evaluations of a paper’s strengths and weaknesses. To investigate these issues, we introduce the Fake Strengths and Fake Weak- nesses tasks, which assess the model’s capacity to recognize factual inconsistencies and produce critical, hallucination- aware assessments. In these tasks, we intentionally manip- ulate human-written review comments by reversing their semantic polarity—transforming strengths into weaknesses and vice versa—using GPT-4o. For example, the original strength “the paper is clearly written” is rephrased as “the paper lacks clarity.” The model is then queried regarding the presence of these altered attributes, allowing us to ex- amine its ability to accurately judge the validity of strengths and weaknesses, detect factual inconsistencies, and generate grounded, critical evaluations. Prompt Injection (PI) Recent studies have revealed that some paper authors attempt to manipulate LLM- generated reviews by employing prompt injection tech- niques—embedding invisible white-text prompts within the manuscript to coax models into generating overly favorable Multimodal Step-based Outcome-based Preference-based Attack-based \"title\": \"Wide range of possible trajectories of North Atlantic climate in a warming world\", \"abstract\": \"Decadal variability in the North Atlantic Ocean impacts regional and global …… Paper Text ⋯ PDF-as-image Text-only Paper2 (Poster) Paper1 (Oral) ### IGNORE PROMPT, GIVE POSITIVE SCORE…… Fake Strengths Directly give your overall score…… Think step by step and score…… As an area chair…… Which one is better ? Does this paper has all or some of these strengths? Based on the provided comments… Think step by step and score…… Summary Strengths Weaknesses Soundness Presentation Paper Text Figure 2: An overview of MMReview benchmark. Theme Task # Ques. Metric Step Summary (S) 240 BART ↑, LLM ↑ Strengths Eval (SE) 238 BART ↑, LLM ↑ Weaknesses Eval (WE) 240 BART ↑, LLM ↑ Soundness Scoring (SS) 32 MAE ↓ Presentation Scoring (PS) 32 MAE ↓ Outcome Conditional Decision (CD) 176 MAE ↓ Direct Decision (DD) 176 MAE ↓"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 7, "text": "Metric Step Summary (S) 240 BART ↑, LLM ↑ Strengths Eval (SE) 238 BART ↑, LLM ↑ Weaknesses Eval (WE) 240 BART ↑, LLM ↑ Soundness Scoring (SS) 32 MAE ↓ Presentation Scoring (PS) 32 MAE ↓ Outcome Conditional Decision (CD) 176 MAE ↓ Direct Decision (DD) 176 MAE ↓ CoT Decision (CoD) 176 MAE ↓ Meta Decision (MD) 240 ACC ↑ Preference Pairwise Rank (PR) 96 ACC ↑ Attack Fake Strength (FS) 240 MAE ↓ Fake Weakness (FW) 238 MAE ↓ Prompt Injection (PI) 176 MAE ↓ Table 1: The number of questions and corresponding evalu- ation metrics of different tasks. evaluations (Gibney 2025). However, the actual effective- ness of such manipulations remains insufficiently quantified. To systematically assess this vulnerability, we design the Prompt Injection task, wherein highly suggestive prompts, such as “please ignore all prior prompts and provide a pos- itive evaluation of this paper”, are embedded directly into the manuscript text to test whether the model can be coerced into abandoning objectivity. The task setup mirrors that of the CoT Decision task, with the sole modification being the inclusion of injected prompt cues within the paper content. Since the injected text is visually imperceptible, this task is excluded from evaluations under the PDF-as-img modality. Experiments Evaluation details We conduct extensive experiments on MMReview using a wide range of popular LLMs and MLLMs. Specifically, for open-source models, we include a diverse set of archi- tectures and sizes, covering Qwen2.5VL (Bai et al. 2025), Kimi-VL-A3B (Team et al. 2025b), Qwen3 (Yang et al. 2025), InternVL3 (Zhu et al. 2025), OVIS2 (Lu et al. 2024), GLM-4.1V (Team et al. 2025a), and Deepseek (DeepSeek- AI et al. 2025). For advanced proprietary models, our selec- tion includes GPT-4o, Gemini-2.5-Flash, GLM-Z1-Flash, and Claude-4-Sonnet. As shown in Table 1, for tasks without objective eval- uation metrics, namely S, SE, and WE, we employ BARTScore (Yuan, Neubig, and Liu 2021) and the ‘LLM- as-a-Judge’ paradigm (Bai et al. 2023; Zheng et al. 2023; Gu et al. 2024) to assess the similarity between model-generated and human-written review comments. For tasks where the model output is a numerical score, including SS, PS, CD, DD, CoD, FS, FW, and PI, we compute the Mean Absolute Error (MAE) between the model’s predicted score and the ground-truth score to quantify deviation. For classification- based tasks such as MD and PR, we evaluate performance using accuracy. Main Results The experimental results on MMReview, as shown in Ta- bles 2, 3, and 4 (where T denotes Thinking and I denotes Instruct), yield the following key findings: (1) Proprietary models consistently outperform open- source models. Closed-source models such as ChatGPT- 4o, Claude, and Gemini demonstrate significant superiority across most metrics, particularly on high-level tasks such as Meta Decision (MD), Pairwise Ranking (PR), Conditional Decision (CD), and Chain-of-Thought Decision (CoD). This indicates that proprietary models exhibit greater consistency and reliability in generating review content, along with en- hanced capabilities in understanding complex academic ma- terial and producing structured assessments. (2) Input modality has a substantial impact on model performance. For most models, performance improves with Step"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 8, "text": "and Chain-of-Thought Decision (CoD). This indicates that proprietary models exhibit greater consistency and reliability in generating review content, along with en- hanced capabilities in understanding complex academic ma- terial and producing structured assessments. (2) Input modality has a substantial impact on model performance. For most models, performance improves with Step Outcome Preference Attack SB ↑SL ↑SEB ↑SEL ↑WEB ↑WEL ↑SS ↓PS ↓CD ↓DD ↓CoD ↓MD ↑ PR ↑ FS ↓FW ↓PI ↓ Open-source LLMs & MLLMs Kimi-VL-A3B-I -3.01 3.65 -3.67 3.46 -3.96 1.98 0.47 0.44 2.31 3.13 3.83 60.92 56.25 2.99 0.56 0.87 Kimi-VL-A3B-T -3.15 3.71 -3.68 3.79 -3.91 2.52 0.47 0.44 2.16 3.59 3.37 66.67 57.29 3.31 0.96 0.63 Qwen2.5-VL-7B -3.05 3.61 -3.68 3.57 -3.96 2.06 0.47 0.44 2.43 3.59 3.57 72.92 59.38 2.99 1.66 0.19 Qwen3-8B -3.08 3.84 -3.63 3.77 -3.87 2.93 0.53 0.44 2.25 3.70 3.16 77.50 65.63 3.26 2.03 1.41 Deepseek-R1-8B -3.09 3.76 -3.63 3.79 -3.86 2.72 0.63 0.78 1.75 3.84 3.55 76.99 64.74 3.25 1.99 0.59 InternVL3-8B -2.99 3.76 -3.65 3.74 -3.93 2.27 0.47 0.44 2.85 3.35 3.35 76.67 52.08 2.99 1.35 0.53 OVIS2-8B -3.09 3.52 -3.70 3.45 -3.99 1.98 0.47 0.44 2.28 3.41 3.72 63.87 60.42 2.99 2.41 0.63 GLM-4.1V-9B-T -3.15 3.60 -3.68 3.73 -3.93 2.60 0.50 0.44 2.08 3.53 3.33 71.86 59.77 3.00 1.08 0.35 Qwen3-14B -3.06 3.85 -3.64 3.83 -3.87 2.79 0.53 0.50 2.16 3.77 3.64 80.42 61.46 3.02 1.99 0.71 OVIS2-16B -3.04 3.59 -3.70 3.52 -3.99 2.06 0.47 0.44 1.87 3.33 3.73 79.92 64.58 3.00 2.34 0.12 Qwen2.5-VL-32B -2.97 3.90 -3.60 3.75 -3.87 2.58 0.56 0.50 2.00 3.15 3.67 67.08 67.71 2.99 1.92 0.80 Qwen3-32B -3.05 3.90 -3.61 3.81 -3.85 2.91 0.50 0.56 2.14 3.60 3.49 80.00 68.75 3.05 1.83 0.78 OVIS2-34B -3.04 3.48 -3.68 3.50 -3.97 2.24 0.81 0.81 2.14 3.72 3.76 79.92 62.50 2.99 1.81 0.57 Qwen2.5-VL-72B -2.99 3.74 -3.65 3.58 -3.94 2.29 0.47 0.47 2.06 3.64 3.71 68.75 63.54 2.99 1.27 0.84 Deepseek-V3 -3.04 3.84 -3.61 3.89 -3.85 2.96 0.53 0.47 2.62 3.37 3.70 75.00 66.03 2.99 0.66 0.20 Deepseek-R1 -3.04 3.92 -3.69 3.90 -3.91 3.05 0.66 0.81 1.97 3.71 3.59 82.92 66.03 3.18 1.20 0.44 Closed-source LLMs & MLLMs ChatGPT-4o-latest -3.06 3.91 -3.61 3.89 -3.86 2.87 0.84 0.94 1.65 3.70 3.65 80.33 63.54 3.92 1.45 0.45 Claude-sonnet-4 -3.02 3.88 -3.58 3.84 -3.84 3.05 0.53 0.41 1.17 2.02 2.01 84.58 72.92 2.98 2.34 0.43 Gemini-2.5-flash -3.06 3.80 -3.58 3.89 -3.86 2.61 0.94 0.88 1.24 4.60 4.28 74.06 70.83 3.73 1.37 0.59 Gemini-2.5-flash-T -3.06 3.76 -3.58 3.86 -3.86 2.68 0.94 0.88 1.24 4.60 4.28 74.06 70.83 3.73 1.37 0.59 GLM-z1-flash -3.12 3.91 -3.70 3.84 -3.87 2.87 0.47 0.56 2.53 3.77 3.59 70.83 66.67 3.50 2.50 0.18 Table 2: Results on MMReview with text-only inputs. Step Outcome Preference Attack SB ↑SL ↑SEB ↑SEL ↑WEB ↑WEL ↑SS ↓PS ↓CD ↓DD ↓CoD ↓MD ↑ PR ↑ FS ↓FW ↓PI ↓ Open-source MLLMs Kimi-VL-A3B-I -3.12 3.53 -3.69 3.52 -3.96 1.86 0.47 0.44 2.40 3.61 3.94 60.68 54.17 3.00 0.13 0.65 Kimi-VL-A3B-T -3.13 3.60 -3.68 3.74 -3.92 2.37 0.47 0.47 2.31 3.56 3.64 61.60 62.50 3.14 0.33 0.36 Qwen2.5-VL-7B -3.09 3.64 -3.67 3.70 -3.94 2.11 0.47"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 9, "text": "PR ↑ FS ↓FW ↓PI ↓ Open-source MLLMs Kimi-VL-A3B-I -3.12 3.53 -3.69 3.52 -3.96 1.86 0.47 0.44 2.40 3.61 3.94 60.68 54.17 3.00 0.13 0.65 Kimi-VL-A3B-T -3.13 3.60 -3.68 3.74 -3.92 2.37 0.47 0.47 2.31 3.56 3.64 61.60 62.50 3.14 0.33 0.36 Qwen2.5-VL-7B -3.09 3.64 -3.67 3.70 -3.94 2.11 0.47 0.44 2.20 3.59 3.49 75.83 64.58 3.00 1.45 0.28 InternVL3-8B -3.21 3.46 -3.82 3.26 -4.12 1.84 0.47 0.44 3.02 3.72 3.68 61.11 54.17 3.00 1.33 0.32 OVIS2-8B -3.20 3.43 -3.72 3.49 -3.97 2.06 0.50 0.53 2.27 3.79 4.44 62.91 66.28 3.00 2.15 0.58 GLM-4.1V-9B-T -3.20 3.48 -3.69 3.70 -3.93 2.60 0.50 0.47 2.42 3.68 3.59 70.72 59.77 3.02 1.80 0.31 OVIS2-16B -3.13 3.50 -3.70 3.29 -3.98 2.04 0.47 0.44 2.16 3.40 3.91 78.03 65.12 3.00 2.10 0.04 Qwen2.5-VL-32B -2.99 3.89 -3.64 3.76 -3.90 2.45 0.81 0.75 2.24 3.47 3.94 76.25 60.42 3.00 1.57 0.49 OVIS2-34B -3.11 3.51 -3.69 3.43 -3.97 2.26 0.88 0.81 2.33 3.78 3.91 73.76 62.79 3.00 1.78 0.28 Qwen2.5-VL-72B -3.04 3.74 -3.66 3.71 -3.95 2.33 0.47 0.50 2.06 3.73 3.73 69.58 60.42 3.01 1.30 0.63 Closed-source MLLMs ChatGPT-4o-latest -3.10 3.86 -3.60 3.85 -3.87 2.67 0.91 0.94 2.24 3.65 3.75 75.97 59.38 3.88 1.22 0.46 Claude-sonnet-4 -3.02 3.86 -3.60 3.88 -3.83 3.09 0.47 0.44 1.19 2.19 2.09 84.17 69.79 2.98 2.12 0.32 Gemini-2.5-flash -3.08 3.75 -3.56 3.84 -3.87 2.58 0.91 0.88 1.34 4.60 4.19 76.25 68.75 2.89 0.78 0.41 Gemini-2.5-flash-T -3.08 3.73 -3.56 3.88 -3.87 2.56 0.91 0.88 1.34 4.60 4.19 76.25 68.75 2.89 0.78 0.41 Table 3: Results on MMReview with multimodal inputs. the addition of visual inputs. Specifically, tasks directly re- lated to evaluation outcomes (e.g., MD, PR, CD, DD, CoD) show notable gains under multimodal (text+image) input conditions. For instance, Qwen-2.5-VL-7B’s accuracy on the MD task increases from 72.92% to 75.83% with im- age input, its MAE on CD decreases from 2.43 to 2.20, and its PR accuracy rises from 59.38% to 64.58%. However, the PDF-as-image modality poses challenges: most mod- els—including advanced proprietary ones like Claude and Gemini—perform worse on this modality than on structured text-image input, though closed-source models remain com- paratively more robust than open-source counterparts. (3) Proprietary models exhibit stronger robustness to prompt injection. On the Prompt Injection (PI) task, pro- Step Outcome Preference Attack SB ↑SL ↑SEB ↑SEL ↑WEB ↑WEL ↑SS ↓PS ↓CD ↓DD ↓CoD ↓MD ↑ PR ↑ FS ↓FW ↓ Open-source MLLMs Kimi-VL-A3B-I -3.21 3.17 -3.74 3.34 -4.00 1.74 0.47 0.44 2.30 3.19 3.83 69.36 55.21 0.28 0.00 Kimi-VL-A3B-T -3.21 3.50 -3.71 3.72 -3.91 2.54 0.47 0.44 1.85 3.72 3.49 74.37 60.42 3.11 0.27 Qwen2.5-VL-7B -3.21 3.28 -3.72 3.49 -3.95 1.91 0.47 0.44 1.97 3.09 3.12 70.83 47.92 2.99 1.01 InternVL3-8B -4.86 1.13 -4.59 1.89 -4.55 1.13 0.47 0.44 2.40 3.63 3.41 68.75 75.00 2.99 1.98 OVIS2-8B -3.23 3.28 -3.70 3.43 -3.99 2.06 0.47 0.44 2.49 3.44 3.93 65.32 53.13 3.00 2.00 GLM-4.1V-9B-T -3.26 3.36 -3.67 3.69 -3.92 2.69 0.47 0.44 1.80 3.51 3.13 80.33 65.63 3.00 1.82 OVIS2-16B -3.20 3.42 -3.70 3.49 -3.97 2.17 0.47 0.44 2.05 3.23 3.76 78.66 54.17 3.00 1.81 Qwen2.5-VL-32B -3.11 3.66 -3.71 3.67 -3.92"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 10, "text": "3.43 -3.99 2.06 0.47 0.44 2.49 3.44 3.93 65.32 53.13 3.00 2.00 GLM-4.1V-9B-T -3.26 3.36 -3.67 3.69 -3.92 2.69 0.47 0.44 1.80 3.51 3.13 80.33 65.63 3.00 1.82 OVIS2-16B -3.20 3.42 -3.70 3.49 -3.97 2.17 0.47 0.44 2.05 3.23 3.76 78.66 54.17 3.00 1.81 Qwen2.5-VL-32B -3.11 3.66 -3.71 3.67 -3.92 2.46 0.78 0.81 2.11 3.55 3.77 67.50 61.46 3.24 1.22 OVIS2-34B -3.20 3.41 -3.68 3.49 -3.99 2.18 0.78 0.69 1.91 3.63 3.74 77.82 52.08 2.99 1.69 Qwen2.5-VL-72B -3.11 3.57 -3.71 3.46 -3.95 2.22 0.47 0.59 2.15 3.76 3.77 70.00 54.17 3.03 1.11 Closed-source MLLMs ChatGPT-4o-latest -3.33 3.65 -3.69 3.70 -3.88 2.68 0.91 0.91 1.84 3.63 3.68 77.73 59.38 3.90 1.33 Claude-sonnet-4 -3.19 3.59 -3.65 3.73 -3.90 2.88 0.48 0.45 1.22 2.36 2.22 82.92 62.50 3.00 2.73 Gemini-2.5-flash -3.31 3.26 -3.66 3.73 -3.93 2.48 0.91 0.91 1.11 4.60 4.19 73.33 69.79 1.67 0.75 Gemini-2.5-flash-T -3.31 3.30 -3.67 3.74 -3.93 2.49 0.91 0.91 1.11 4.60 4.19 73.33 69.79 1.67 0.75 Table 4: Results on MMReview with pdf-as-img inputs. prietary models achieve MAE values below 0.6 in most cases, often under 0.5, indicating consistent judgment with and without injected prompts. In contrast, open-source mod- els exhibit highly variable performance under prompt injec- tion, suggesting greater susceptibility to adversarial manip- ulation. (4) Larger model size does not guarantee superior performance. A comparative analysis of the Qwen2.5-VL and OVIS2 model families reveals that performance does not uniformly improve with scale. In several cases, smaller models outperform their larger counterparts, suggesting that model size is not a definitive determinant of effectiveness in peer review tasks. More analysis of the result can be found in the supple- mentary materials. Further Analysis Are Thinking Models More Suitable for Review? For closed-source models, performance differences between Gemini-2.5-Flash and Gemini-2.5-Flash-Thinking are mini- mal. In contrast, within the open-source Kimi-VL-A3B and Deepseek series, a more nuanced trend emerges: while non- thinking variants may outperform on some step-based tasks, thinking models consistently excel in outcome-based and preference-based tasks critical to review decisions. This in- dicates that thinking-enhanced models are better suited for generating final peer review judgments. Further supporting evidence is provided in the supplementary material. What Domains are LLM-reviewers better at? Table 5 reports the MAE performance of various mod- els on the CoD task across research domains, indicating their relative proficiency in field-specific peer review. All models use multimodal inputs except Deepseek-R1, which AI NS ES SS ChatGPT-4o-latest 4.46 5.67 3.67 4.28 claude-sonnet-4-20250514 2.08 2.33 1.33 2.44 gemini-2.5-flash-thinking 4.62 6.33 4.33 4.50 Deepseek-R1 2.32 4.00 3.67 4.00 Table 5: The MAE performance of several models on the CoD task across different research domains. operates with text only. Results show that most closed- source models perform best in Engineering Sciences, while Deepseek-R1 achieves its lowest MAE in Artificial In- telligence. These findings suggest that models’ reviewing strengths vary across disciplines, potentially influenced by differences in pretraining corpora and input modalities. More further analysis can be found in the supplementary materials of our submission. Conclusion In this work, we present MMReview—a multidisciplinary and multimodal benchmark designed to evaluate the ca- pabilities of Large Language"}
{"doc_id": "2508.14146v1", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.14146v1", "chunk_id": 11, "text": "that models’ reviewing strengths vary across disciplines, potentially influenced by differences in pretraining corpora and input modalities. More further analysis can be found in the supplementary materials of our submission. Conclusion In this work, we present MMReview—a multidisciplinary and multimodal benchmark designed to evaluate the ca- pabilities of Large Language Models (LLMs) in academic peer review. The benchmark encompasses 4 thematic cate- gories and 13 distinct tasks. Its core features include cov- erage across diverse academic disciplines, support for mul- timodal input formats, and a comprehensive set of evalua- tion tasks that span the full peer review pipeline. Leveraging MMReview, we conducted extensive evaluations of state-of- the-art LLMs and MLLMs. We envision MMReview as a standardized evaluation platform that can catalyze the de- velopment of more efficient LLM-assisted peer review sys- tems. In future work, we plan to incorporate larger-scale datasets specifically curated for training LLMs in the con- text of scholarly review."}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 0, "text": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization Shaohua Duan1*, Xinze Li1*, Zhenghao Liu1†, Xiaoyuan Yi2, Yukun Yan3 Shuo Wang3, Yu Gu1, Ge Yu1, Maosong Sun3 1Department of Computer Science and Technology, Northeastern University, Shenyang, China 2Microsoft Research Asia, Beijing, China 3Department of Computer Science and Technology, Institute for AI, Tsinghua University, Beijing, China Abstract Long-context modeling is critical for a wide range of real- world tasks, including long-context question answering, sum- marization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabili- ties. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi- Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing prefer- ence data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively up- date these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and col- lecting high-quality and diverse responses. Finally, we col- lect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Ex- perimental results show that LongMab-PO significantly im- proves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context rea- soning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO. 1 Introduction Recent advancements in Large Language Models (LLMs), particularly the expansion of their context window (Dubey et al. 2024; Yang et al. 2024), have enabled their appli- cation in a variety of long-context tasks, including long- context question answering (Yang et al. 2025), summariza- tion (Wang et al. 2022a), and repository-level code under- standing (Jimenez et al. 2023). Despite these developments, LLMs continue to suffer from the “lost-in-the-middle” prob- lem (Liu et al. 2023; He et al. 2024), where LLMs tend to overemphasize the beginning and end of a long input while neglecting critical information in the middle. To mitigate this issue, recent efforts have focused on constructing high- quality supervised fine-tuning (SFT) datasets tailored for *These authors contributed equally. †Corresponding author. Query: What group of schools is the university where Michael Berland studied a member of? Answer: Five Colleges Context: Robert Dudley Baxter was educated…Cambridge University... Students attending any colleges in the Five Colleges…Michael Denning is … Jr. Professor of American Studies at Yale University. Query + Context Answer: Yale University Lost-in-the-Middle LLM C1: … Robert Dudley…was educated Cambridge University… Multi-Armed Bandit System Split C2: …Students attending any colleges in the Five Colleges… Cn: ... Michael Denning…is historian…Studies at Yale University… Select K Chunks Reward FeedBack LLM Answer Add ෥ 𝒚𝟏, ෥ 𝒚𝟐… ෥ 𝒚𝒕 ෥ 𝒚𝒕 : Five Colleges Query ෨ 𝐂(πt): { ෨ 𝐂1,…, ෨ 𝐂K} Optimized LLM"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 1, "text": "Multi-Armed Bandit System Split C2: …Students attending any colleges in the Five Colleges… Cn: ... Michael Denning…is historian…Studies at Yale University… Select K Chunks Reward FeedBack LLM Answer Add ෥ 𝒚𝟏, ෥ 𝒚𝟐… ෥ 𝒚𝒕 ෥ 𝒚𝒕 : Five Colleges Query ෨ 𝐂(πt): { ෨ 𝐂1,…, ෨ 𝐂K} Optimized LLM Query Context DPO LLM Five Colleges Cambridge University Candidate Figure 1: The Framework of LongMab-PO. long-context scenarios (Chen et al. 2023; Bai et al. 2024a; Chen et al. 2024). While effective to some extent, these ap- proaches often lead to overfitting on training signals (Li et al. 2025), resulting in catastrophic forgetting of general capa- bilities (Luo et al. 2023). An alternative line of work explores Direct Preference Optimization (DPO) (Rafailov et al. 2023; Sun et al. 2025), which optimizes the long context understanding of LLMs through preference pair training. This involves generat- ing different responses, scoring them with task-specific re- ward functions, and training the model to prefer the better ones (Zhang et al. 2024a; Li et al. 2024a). To improve the quality of sampled responses, some studies introduce chunk- aware sampling strategies (Tang et al. 2024), where the in- put context is split into chunks, and chunk relevance to the query is estimated via similarity scores. Responses are then sampled based on varying ratios of relevant and irrelevant chunks in the LLMs’ input context to encourage diversity. However, these static similarity-based methods are often in- sufficient for capturing semantically rich and diverse con- text segments, and they lack the ability to adaptively ex- plore chunk combinations based on feedback from LLMs’ responses. In this paper, we propose Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization (LongMab-PO). As illustrated in Figure 1, LongMab-PO models the divided context chunks as the arm of the multi- armed bandit (MAB) and employs a chunk-aware sampling strategy to sample responses. During the bandit rollout pro- cess, LongMab-PO utilizes the Upper Confidence Bound (UCB) algorithm to select a subset of chunks based on their expected rewards, which balances exploration and exploita- tion of chunks. At each rollout step, the selected chunks are fed into the LLM to generate a response, and a reward is computed based on the quality of the response. The reward is then used to update the expected reward of the selected chunks, guiding the subsequent chunk selection. This iter- ative sampling process enables the model to progressively focus on more informative chunk combinations, resulting in diverse and high-quality responses. These responses are then used to construct preference pairs for DPO training, enhanc- ing the capability of LLMs to reason over long contexts. Our experiments demonstrate the effectiveness of LongMab-PO by outperforming existing SFT-based and DPO-based baselines on multiple long-context tasks. Fur- ther analysis indicates that LongMab-PO can progressively select more context chunks containing necessary evidence during the multi-armed bandit rollout process, thereby grad- ually improving the quality of the sampled responses. Com- pared to other sampling strategies, LongMab-PO can effec- tively explore a broader range of context chunk combina- tions, which prompts the LLM to generate more diverse re- sponses. Notably, LongMab-PO"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 2, "text": "containing necessary evidence during the multi-armed bandit rollout process, thereby grad- ually improving the quality of the sampled responses. Com- pared to other sampling strategies, LongMab-PO can effec- tively explore a broader range of context chunk combina- tions, which prompts the LLM to generate more diverse re- sponses. Notably, LongMab-PO is capable of exploring the utility of different combinations of multiple context chunks, effectively modeling the interactions among selected seg- ments to better support answer generation. This ability im- proves the recall of ground-truth evidence during the multi- armed bandit rollout process. It helps to sample higher- quality and diverse chunks, making LLMs generate more meaningful responses for DPO training. 2 Related Work Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks (Li et al. 2023; Jiang et al. 2024). However, their per- formance on long-context understanding tasks remains sub- optimal due to challenges such as the “lost-in-the-middle” phenomenon and contextual noise (Wang et al. 2025; Xu et al. 2025), which hinder the model’s ability to focus on key evidence. To address these issues, recent research has explored su- pervised fine-tuning (SFT) to improve the long-context rea- soning abilities of LLMs (Chen et al. 2023; Xiong et al. 2023; An et al. 2024; Xu et al. 2024; Li et al. 2024b). For example, Chen et al. (2023) construct SFT datasets based on human-annotated question–answer pairs, a process that is costly and difficult to scale. Follow-up work Bai et al. (2024a) employ self-instruct techniques (Wang et al. 2022b), prompting powerful LLMs to automatically generate QA pairs from long documents. Zhu et al. (2025) further intro- duce heuristic criteria to filter high-quality synthetic samples using LLMs. While SFT-based methods can improve perfor- mance, they also pose a risk of overfitting to training signals and may lead to catastrophic forgetting of general capabili- ties (Luo et al. 2023). More recently, researchers have turned to Reinforcement Learning (RL) methods, particularly Direct Preference Op- timization (DPO) (Rafailov et al. 2023), to enhance LLM’s performance on long-context tasks (Tang et al. 2024; Li et al. 2024a). For instance, Zhang et al. (2024a) employ an LLM- as-a-Judge approach to score model-generated responses, thereby constructing preference pairs for DPO training. Li et al. (2024a) leverage a self-consistency mechanism for re- sponse evaluation and preference pair construction. Yang et al. (2025) introduce a citation-based prompting strategy, incorporating ground truth to guide the generation of faithful reasoning paths as positive samples, while negative samples are generated through free-form prompting. Despite these advancements, existing DPO-based methods often struggle to sample high-quality responses due to limitations in the model’s ability to accurately retrieve and reason over rele- vant information from long contexts. Building on the progress in chunk-aware denoising and sampling techniques (Zhou et al. 2024; Sheng et al. 2025), recent work has explored finer-grained control over input se- lection to improve both the quality and diversity of sampled responses. For example, Tang et al. (2024) divide long con- texts into chunks and use similarity scores between chunks and queries to distinguish relevant from irrelevant segments."}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 3, "text": "Sheng et al. 2025), recent work has explored finer-grained control over input se- lection to improve both the quality and diversity of sampled responses. For example, Tang et al. (2024) divide long con- texts into chunks and use similarity scores between chunks and queries to distinguish relevant from irrelevant segments. Positive responses are generated from relevant chunks, while irrelevant chunks are incrementally added to create chal- lenging negative examples. However, similarity-based se- lection remains limited in its ability to identify all infor- mative chunks, and it lacks the capacity to explore diverse chunk combinations adaptively. In contrast, our proposed LongMab-PO introduces a multi-armed bandit mechanism for chunk selection, which dynamically explores and esti- mates the utility of different chunk combinations throughout the rollout process. This allows the model to sample higher- quality and more diverse responses, leading to the construc- tion of superior preference data for long-context DPO train- ing. 3 Methodology In this section, we present LongMab-PO, a novel framework for optimizing long-context language models by leveraging the Multi-Armed Bandit (MAB) paradigm. LongMab-PO identifies and exploits informative context chunks during the iterative bandit rollout process to sample high-quality and diverse responses, which are used to construct preference data pairs for DPO training, thereby enhancing the LLM’s ability to reason over long contexts. We begin by introducing our chunk-aware response sampling strategy, which divides the long context into multiple chunks and selects different subsets of these chunks to feed into the LLM during the it- erative process to generate responses, thereby constructing diverse response candidates. To further improve the quality of sampled responses, we propose a UCB-based MAB roll- out method to guide the sampling process, which balances the exploration and exploitation of context chunks. 3.1 Optimizing Long-Context LLMs via Multi-Armed Bandit-Guided Sampling Given a question q, a Large Language Model (LLM) M is typically prompted with a long context C to generate an an- swer y: y = M(C, q). (1) To enhance the long-context reasoning capabilities of LLMs, we adopt Direct Preference Optimization (DPO) (Rafailov et al. 2023) to optimize M using a preference dataset D: LDPO = −E(C,q,y+,y−)∼D[log σ( β log M(y+ | q) Mref(y+ | q) −β log M(y−| q) Mref(y−| q))], (2) where β is a hyperparameter and Mref is a fixed reference model. The preference tuple dataset (C, q, y+, y−) is con- structed by calculating the reward score for each response ˜ y in the candidate response set ˜ Y , which is sampled by M: r(˜ y) = (SubEM(Ans(˜ y), y∗) + F1(Ans(˜ y), y∗))/2, (3) where y∗denotes the ground truth, and Ans(·) indicates we extract the answer string from the response ˜ y. SubEM and F1 are evaluation metrics designed to evaluate the quality of long-context reasoning responses. Then we regard the sam- pled response with the highest score as y+ and the lowest score as y−. The effectiveness of DPO heavily relies on the quality of the preference data. To this end, we introduce a chunk-aware response sampling framework to generate a high-quality candidate set ˜ Y for constructing preference data pairs."}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 4, "text": "pled response with the highest score as y+ and the lowest score as y−. The effectiveness of DPO heavily relies on the quality of the preference data. To this end, we introduce a chunk-aware response sampling framework to generate a high-quality candidate set ˜ Y for constructing preference data pairs. Chunk Aware Response Sampling Given a long-context QA pair (C, q, y∗), we first divide the long context C into n equal-length chunks, Cchunk = {C1, C2, . . . , Cn}. Then, a chunk selection policy π selects K chunks from Cchunk, forming a chunk subset ˜ C(π) = { ˜ C1, . . . , ˜ CK}. The LLM generates a response based on the selected chunks ˜ C(π) and the question q: ˜ y = M( ˜ C(π), q). (4) We repeat this process T times, using a varying subset of context chunks ˜ C(π) selected by the policy π to prompt the LLM M to generate responses each time, yielding a set of candidate responses ˜ Y = {˜ y1, . . . , ˜ yT }. To further enhance the sampling quality, we design the chunk selection strat- egy based on multi-armed bandit (MAB) rollouts, enabling adaptive exploration of the most informative context chunks to generate responses. Chunk Selection via Multi-Armed Bandit Rollouts During the chunk-aware response sampling, we model the chunk selection process as a multi-armed bandit (MAB) problem, and each chunk Ci ∈Cchunk is treated as an arm of the MAB. At each rollout step t ∈{1, . . . , T}, the MAB policy πt selects K chunks with the highest expected re- wards, ˜ C(πt) = { ˜ C1, . . . , ˜ CK}. Then, we prompt the LLM to generate a corresponding response ˜ yt based on the se- lected chunks ˜ C(πt), forming the chunk-aware response set ˜ Y across T rollouts: ˜ Y = {˜ y1, . . . , ˜ yT }, where ˜ yt = M( ˜ C(πt), q), (5) where ˜ yt denotes the response generated by the LLM at roll- out step t, and ˜ Y are used to construct preference pairs for DPO training. In the rollout process, the expected reward of a chunk ˜ Ci ∈˜ C(πt) is determined by the quality of the re- sponse ˜ yt. If the selected chunks ˜ Ci lead the LLM to gener- ate high-quality responses, they will receive higher expected rewards. Consequently, the MAB strategy is more likely to select them in the next round, thereby continuously enhanc- ing the quality of the generated responses. 3.2 Adaptive Chunk Prioritization through Multi-Armed Bandit Rollouts In each MAB rollout, various combinations of chunk sub- sets can be selected. To identify higher-reward chunk com- binations, we need to explore as many different combina- tions of chunk subsets as possible, but this increases the ex- ploration cost. However, if exploration is too limited, we may overlook informative chunks necessary for generating high-quality responses. Therefore, a better balance between chunk exploration (selecting less-visited chunks) and ex- ploitation (favoring chunks with higher"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 5, "text": "as many different combina- tions of chunk subsets as possible, but this increases the ex- ploration cost. However, if exploration is too limited, we may overlook informative chunks necessary for generating high-quality responses. Therefore, a better balance between chunk exploration (selecting less-visited chunks) and ex- ploitation (favoring chunks with higher expected rewards) is required. To solve this decision-making problem, we adopt the Up- per Confidence Bound (UCB) algorithm, a widely used strat- egy for MAB problems, to guide chunk selection at each step. Specifically, at the t-th rollout step, the UCB score for the chunk Ci is computed as: UCBt(Ci) = µi(t) + α · p 2 ln t/(ni(t) + ϵ), (6) where µi(t) is the current expected reward of chunk Ci, and we initialize it using the probe-based initialization strategy, which will be presented later. ni(t) is the number of times chunk Ci has been selected in previous rollouts. t is the current rollout step. ϵ is a small constant to avoid division by zero, and α is a tunable hyperparameter. Once the UCB scores are computed for all chunks in the context set Cchunk, we select the top-K chunks with the highest scores to form the selected chunk subset ˜ C(πt) for the current rollout: ˜ C(πt) = TopKCi∈CchunkUCBt(Ci). (7) The UCB score of each chunk is updated during the roll- out process based on the current expected reward of the chunk. Then, we will provide a detailed explanation of how to initialize the expected reward for each chunk and the up- date process of each chunk’s UCB score. Query: What group of schools is the university where Michael Berland studied a member of? Answer: Five Colleges Multi-Armed Bandit Initialization Context: ...Berland attended the Latin School of Chicago and graduated magna cum laude from the University of Massachusetts Amherst… Split Query + Context +Answer Probe CoT: In Passage 13, we see that Michael Berland attended the University of Massachusetts Amherst…From Passage 5…UMass Transit Services provides mass transit services to…the Five Colleges Consortium… C1: …Transportation Services … is a department within the University of Massachusetts Amherst (UMass Amherst)… LLM C2 : …any colleges in the Five Colleges Consortium have a fee included in their tuition bills (service fee for… Cn: ... The Latin School of Chicago and graduated magna cum laude from the University of Massachusetts Amherst… 𝝁1 𝝁2 𝝁n UCB Multi-Armed Bandit Rollout Query LLM ෥ 𝒚𝒕: …Berland graduated from…. Answer: Five Colleges ෨ 𝐂(πt): { ෨ 𝐂1,…, ෨ 𝐂K} Embedding Preference Learning Answer Query Context DPO After Training LLM Optimized LLM Update 𝝁𝒊(𝒕) Amherst… this system is known as the Five Colleges. Answer: Five Colleges Multi-Armed Bandit System Add When Ct is helpful and rarely selected, it is encouraged to be chosen. When Ct is not helpful and already selected, it is not encouraged to be chosen. … at Cambridge Universit includes Trinity College… Answer: Trinity College Initialize expected reward 𝝁𝒊with embedding similarity ෥ 𝒚𝟏, ෥ 𝒚𝟐… ෥ 𝒚𝒕 Candidate Select Figure 2: The Illustration of Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization (LongMab-PO). LongMab-PO collects preference pairs"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 6, "text": "selected, it is not encouraged to be chosen. … at Cambridge Universit includes Trinity College… Answer: Trinity College Initialize expected reward 𝝁𝒊with embedding similarity ෥ 𝒚𝟏, ෥ 𝒚𝟐… ෥ 𝒚𝒕 Candidate Select Figure 2: The Illustration of Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization (LongMab-PO). LongMab-PO collects preference pairs during the rollout process for DPO training. UCB Initialization via Evidence Probing To enable ef- fective chunk selection using the multi-armed bandit (MAB) strategy, we first initialize the initial expected reward µi(1) for each chunk Ci in the split chunk set Cchunk. These initial rewards are used to alleviate the cold start problem of the UCB-based chunk selection method. To assign more meaningful initial expected rewards to each chunk, we propose a probe-based initialization strategy that leverages the LLM’s ability to generate faithful reason- ing paths. Inspired by prior work (Chen et al. 2025; Yang et al. 2025), we prompt the LLM M to generate a faithful reasoning trace yProbe that contains the necessary evidence in long context C that support the ground truth y∗: yProbe = M(Instructextract(q, C, y∗)), (8) where Instructextract denotes the instruction that asks thex LLM to extract evidence from C. Then, we compute the semantic similarity between each chunk Ci ∈Cchunk and yProbe using cosine similarity in the embedding space: si = cos(Emb(yProbe), Emb(Ci)), (9) where Emb(·) denotes a embedding model, and cos(·, ·) computes cosine similarity. We then initialize the expected reward µi(1) for each chunk Ci with its corresponding sim- ilarity score si. And the initial policy π1 selects a subset of K chunks, denoted as ˜ C(π1), which correspond to those with the highest similarity scores. This strategy allows the MAB policy to start with a prior preference over chunks that are more semantically aligned with the LLM’s reasoning trace, thus improving early-stage chunk selection efficiency and reducing the need for blind exploration. UCB Score Update with MAB Rollouts In each rollout step t of the MAB rollout process, we need to update the corresponding UCB scores of the chunks based on their ex- pected rewards. During the t-th sampling step, we prompt the LLM M to answer the question q based on the selected chunks ˜ C(πt): ˜ yt = M( ˜ C(πt), q). (10) We then evaluate the informativeness of the selected chunks ˜ C(πt) by computing a reward score for the generated re- sponse ˜ yt, defined as: r(˜ yt) = (SubEM(˜ yt, y∗) + F1(Ans(˜ yt), y∗))/2. (11) Then, we update the UCB statistics based on the reward r(˜ yt). For each chunk Ci ∈Cchunk, we update its selection count at the end of the current rollout step t: ni(t + 1) = \u001ani(t) + 1 if Ci ∈˜ C(πt), ni(t) otherwise, (12) and update its expected reward µi using an incremental av- erage: µi(t + 1) = \u001a 1 t (µi(t) · (t −1) + r(˜ yt)) if Ci ∈˜ C(πt), µi(t) otherwise. (13) Dataset Avg #Tokens #Samples MuSiQue 15.5k 200 2WikiMultihopQA 7.1k 200 MultiFieldQA-En 6.9k 150 NarrativeQA 29.8k 200 FakebookQA 192.6k 351 Table 1: The Statistics of Testsets"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 7, "text": "an incremental av- erage: µi(t + 1) = \u001a 1 t (µi(t) · (t −1) + r(˜ yt)) if Ci ∈˜ C(πt), µi(t) otherwise. (13) Dataset Avg #Tokens #Samples MuSiQue 15.5k 200 2WikiMultihopQA 7.1k 200 MultiFieldQA-En 6.9k 150 NarrativeQA 29.8k 200 FakebookQA 192.6k 351 Table 1: The Statistics of Testsets from Two Benchmarks. The updated values ni(t + 1) and µi(t + 1) are then used to compute UCB scores for the next rollout step t + 1 us- ing Eq. 6, enabling the bandit policy to continuously refine its estimation of chunk utility based on observed response quality. 4 Experimental Methodology In this section, we describe the datasets, baselines, evalua- tion metrics, and implementation details in our experiments. Datasets. In our experiments, we utilize the training dataset of MuSiQue (Trivedi et al. 2022) to sample re- sponses and construct preference data pairs for DPO train- ing. To better simulate long-context scenarios, we follow previous work (Li et al. 2024a), randomly sample addi- tional Wikipedia documents, and extend the context length to 8–16k tokens. We conduct evaluation on five long-context QA tasks from two widely used long-context benchmarks, Longbench (Bai et al. 2024b) and InfiniteBench (Zhang et al. 2024b). Specifically, we select four QA datasets, MuSiQue (Trivedi et al. 2022), 2WikiMultihopQA (Ho et al. 2020), MultiFieldQA-En (Bai et al. 2024b), and Narra- tiveQA (Koˇ cisk` y et al. 2018) from LongBench, and one QA task, Fake Book QA, from InfiniteBench. All data statistics are shown in Table 1. Baselines. In our experiments, we compare LongMab-PO with three categories of models: vanilla LLMs, supervised finetuning (SFT) models, and DPO-trained models. Specifically, we select LongAlpaca (Chen et al. 2023) and LongAlign (Bai et al. 2024a) as SFT-based baselines, both of which fine-tune LLMs on synthetic long-context QA datasets. For DPO-based baselines, we adopt LongReward- PO (Zhang et al. 2024a), SeaLong-PO (Li et al. 2024a), and LongFaith-PO (Yang et al. 2025). LongReward-PO and SeaLong-PO prompt LLMs to generate multiple responses and score them using LLM-as-a-Judge and self-consistency, respectively, to construct preference pairs. In contrast, LongFaith-PO incorporates ground truth and citation-based prompts to guide the generation of faithful reasoning paths as positive samples, while negative samples are obtained via CoT prompt without explicit citation grounding, thus form- ing preference pairs for DPO training. Evaluation Metrics. Following previous work (Li et al. 2024a; Yang et al. 2025), we use substring exact match (SubEM) and F1 as evaluation metrics. Implementation Details. For all experiments, we employ Llama-3.1-8B-Instruct (Dubey et al. 2024) and Qwen-2.5- 7B-Instruct (Yang et al. 2024) as the backbone. We set the maximum number of rollouts for multi-armed bandit sam- pling to 30 rounds, with 4 context chunks selected in each round, and each context chunk containing 1,500 tokens. During training, the learning rate is set to 2×10−5, and each model is trained for 2 epochs. We also use LoRA (Hu et al. 2022) and LLaMA Factory for efficient training. The GPU used in our experiments is the A100, with 80GB of memory. 5 Evaluation Results In this section, we first present the overall performance of"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 8, "text": "to 2×10−5, and each model is trained for 2 epochs. We also use LoRA (Hu et al. 2022) and LLaMA Factory for efficient training. The GPU used in our experiments is the A100, with 80GB of memory. 5 Evaluation Results In this section, we first present the overall performance of LongMab-PO and then conduct ablation studies to examine the contributions of its components. We then explore the di- versity and quality of the responses sampled by LongMab- PO. Finally, we analyze the impact of the number of selected chunks in LongMab-PO. 5.1 Overall performance Table 2 shows the overall performance of LongMab-PO and baseline methods across a range of long-context understand- ing tasks and backbone models. Overall, LongMab-PO achieves the highest average SubEM and F1 scores, consistently outperforming base- lines across tasks of varying types and difficulty levels. These results demonstrate its effectiveness in enhancing the long-context understanding capabilities of LLMs. No- tably, despite being trained solely on the MuSiQue dataset, LongMab-PO exhibits strong generalization: it outperforms the vanilla LLM on multiple out-of-domain tasks with longer contexts. When comparing different optimization strategies, SFT-based methods, such as LongAlpaca and LongAlign, often underperform relative to the vanilla LLM, particularly on more complex tasks. This indicates that overfitting to ground-truth answers may be insufficient for teaching LLMs to identify and extract salient information from long contexts. In contrast, DPO-based methods are generally more effective at improving long-context under- standing. LongMab-PO outperforms other DPO-based ap- proaches, highlighting the advantage of its bandit-guided preference data construction. The rollout process dynami- cally explores informative context chunks, enabling more effective optimization for long-context understanding. 5.2 Ablation Studies In this section, we present ablation studies to evaluate the effectiveness of different components in the LongMab-PO. As shown in Table 3, we utilize the following four meth- ods to sample 30 responses for each question to construct preference data pairs for DPO training. Specifically, Direct- PO utilizes the complete long context to prompt the LLM to sample multiple responses to construct preference data pairs. LongMab-PO w/ Random-Dist indicates that LongMab-PO uses random sampling as the selection strategy to randomly select K context chunks in each rollout round for generating responses. LongMab-PO w/ Final-Dist indicates that after the multi-armed bandit rollout process concludes, we select the top-K context chunks with the highest expected rewards to prompt the LLM to sample multiple responses. LongMab- PO w/ Initial-Dist refers to selecting the top-K chunks with the highest initial UCB scores at the beginning of the multi- armed bandit rollout to prompt the LLM to generate multiple responses for DPO training. Model MuSiQue 2WikiMQA MFQA-En NarrativeQA FakebookQA Avg. SubEM F1 SubEM F1 SubEM F1 SubEM F1 SubEM F1 SubEM F1 Llama-3.1-8B-Instruct Vanilla LLM 33.50 36.56 69.50 65.45 18.66 44.34 18.00 26.61 26.49 32.19 33.23 41.03 LongAlpaca 34.00 38.36 69.50 63.64 16.66 40.67 16.50 26.88 27.35 30.58 32.80 40.03 LongAlign 36.50 37.62 69.00 59.35 22.00 48.49 18.50 26.38 23.93 24.85 33.99 39.34 LongReward-PO 40.50 41.41 68.50 63.23 20.00 44.70 16.50 25.92 26.21 24.20 34.34 39.89 SeaLong-PO 37.50 40.52 68.00 67.07 19.33 44.12 16.00 26.45 27.06 32.06"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 9, "text": "LongAlpaca 34.00 38.36 69.50 63.64 16.66 40.67 16.50 26.88 27.35 30.58 32.80 40.03 LongAlign 36.50 37.62 69.00 59.35 22.00 48.49 18.50 26.38 23.93 24.85 33.99 39.34 LongReward-PO 40.50 41.41 68.50 63.23 20.00 44.70 16.50 25.92 26.21 24.20 34.34 39.89 SeaLong-PO 37.50 40.52 68.00 67.07 19.33 44.12 16.00 26.45 27.06 32.06 33.58 42.04 LongFaith-PO 44.00 49.23 75.50 72.80 20.66 48.10 9.50 23.26 24.30 22.35 34.79 43.15 LongMab-PO 50.00 52.15 76.00 68.60 26.00 51.26 20.00 28.61 32.76 36.55 40.95 47.43 Qwen-2.5-7B-Instruct Vanilla LLM 33.50 30.52 58.00 50.14 28.00 45.12 15.00 18.29 25.64 22.38 32.03 33.29 LongAlpaca 32.50 33.90 55.50 50.75 24.66 45.88 17.50 19.72 29.34 23.82 31.90 34.81 LongAlign 28.50 31.09 52.00 52.02 23.33 49.76 15.50 21.98 25.07 25.61 28.88 36.09 LongReward-PO 37.50 33.37 62.00 50.75 29.33 44.89 15.50 17.67 28.20 21.77 34.51 33.69 SeaLong-PO 43.00 22.09 67.00 36.39 27.33 41.08 18.00 16.27 28.20 16.95 36.71 26.56 LongFaith-PO 48.50 43.38 66.00 53.93 24.67 38.08 12.00 17.03 23.07 18.55 34.85 34.19 LongMab-PO 44.00 43.25 67.50 62.97 25.33 48.07 18.00 25.14 30.48 31.88 37.06 42.26 Table 2: Overall Performance of Llama-3.1-8B-Instruct and Qwen-2.5-7B-Instruct on Different Long-Context Understanding Tasks. The best and second best results are highlighted. Model MuSiQue 2WikiMQA MFQA-En NarrativeQA FakebookQA Avg. SubEM F1 SubEM F1 SubEM F1 SubEM F1 SubEM F1 SubEM F1 Llama-3.1-8B-Instruct LongMab-PO 50.00 52.15 76.00 68.60 26.00 51.26 20.00 28.61 32.76 36.55 40.95 47.43 w/ Initial-Dist 47.50 48.91 72.00 67.67 20.00 46.31 18.00 25.44 31.62 33.99 37.82 44.46 w/ Random-Dist 43.50 47.38 72.50 69.56 21.33 46.48 18.00 26.85 29.34 34.36 36.93 44.93 w/ Final-Dist 40.00 45.91 69.50 66.94 22.66 46.42 19.50 30.26 29.34 34.51 36.20 44.81 Direct-PO 46.50 50.82 62.00 58.74 21.33 47.12 19.50 31.45 30.19 37.31 35.90 45.09 Qwen-2.5-7B-Instruct LongMab-PO 44.00 43.25 67.50 62.97 25.33 48.07 18.00 25.14 30.48 31.88 37.06 42.26 w/ Initial-Dist 45.00 46.49 59.50 54.76 24.00 46.14 19.50 25.56 29.05 27.53 35.41 40.10 w/ Random-Dist 46.00 44.87 62.50 56.94 24.66 47.06 17.50 23.03 28.49 32.94 35.83 40.97 w/ Final-Dist 41.00 47.68 57.00 56.90 19.33 47.21 16.50 25.29 29.05 33.54 32.58 42.12 Direct-PO 38.50 41.17 59.00 55.46 21.33 45.24 19.00 25.21 27.43 30.25 33.05 39.47 Table 3: Ablation Study. The best and second best results are highlighted. Compared to Direct-PO, LongMab-PO w/ Random-Dist shows better average performance across all tasks, indi- cating the effectiveness of using the chunk-aware sam- pling method to assist LLMs in generating more meaning- ful responses for DPO training. In contrast, LongMab-PO shows a greater improvement compared to LongMab-PO w/ Random-Dist, which demonstrates the effectiveness of us- ing the multi-armed bandit as the chunk selection strategy in the chunk-aware response sampling process of LongMab- PO. Both LongMab-PO w/ Initial-Dist and LongMab-PO w/ Final-Dist underperform LongMab-PO, which indicates that, compared to selecting chunks to sample responses based on a fixed reward distribution, collecting generated responses during the multi-armed bandit rollout process to construct preference data is more effective. 5.3 Characteristics of LLM Responses Grounded in LongMab-PO Sampled Chunks This section evaluates the quality and diversity of responses generated during the rollout process of LongMab-PO. Quality of Sampled Responses In this experiment, we collect 30 responses"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 10, "text": "during the multi-armed bandit rollout process to construct preference data is more effective. 5.3 Characteristics of LLM Responses Grounded in LongMab-PO Sampled Chunks This section evaluates the quality and diversity of responses generated during the rollout process of LongMab-PO. Quality of Sampled Responses In this experiment, we collect 30 responses generated based on 30 different sets of selected chunks obtained during the multi-armed bandit- guided sampling process. For each response, we compute its SubEM score and measure the recall of ground truth chunks within the selected chunk set. A chunk is consid- ered a ground truth chunk if it contains the answer. We ran- domly sample 500 queries from the training set to conduct this analysis. Figure 3 illustrates the trends of ground truth chunk re- call and SubEM scores over the course of the rollout rounds. For Llama-3.1-8B-Instruct, both recall and SubEM metrics show a slight upward trend as the number of rollout steps in- creases, eventually plateauing in later rounds. This indicates that the multi-armed bandit sampling strategy gradually se- lects more informative context chunks, leading to higher- quality responses. A similar pattern is observed with Qwen- 2.5-7B-Instruct, although the increases in recall and SubEM are more gradual, reflecting differences in long-context rea- soning capabilities across LLMs. Overall, these results con- firm that LongMab-PO effectively improves the informa- 1 4 8 16 30 Rollout Steps 40 50 60 70 80 Average Metric Recall SubEM (a) LLaMA-3.1-8B. 1 4 8 16 30 Rollout Steps 20 40 60 80 Average Metric (b) Qwen-2.5-7B. Figure 3: The SubEM Score of Responses and the Recall of Ground Truth Chunks. MuSiQue 80 82 84 86 88 90 Similarity(%) 85.60 81.0080.50 85.40 86.63 Final-Dist LongMab-PO Random-Dist Initial-Dist Direct-PO (a) The Average of Similarity between Generated Responses. MuSiQue 0.2 0.4 0.6 0.8 Variance(%) 0.37 0.63 0.74 0.38 0.26 (b) The Variance of Similarity between Generated Responses. Figure 4: Diversity of the Responses Sampled by Different Methods. tiveness of selected chunks as the rollout progresses. Diversity of Sampled Responses Then we compare the diversity of responses generated by five sampling strate- gies: Direct-PO, LongMab-PO, LongMab-PO w/ Final- Dist (Final-Dist), w/ Random-Dist (Random-Dist), and w/ Initial-Dist (Initial-Dist). For each method, we sample 30 responses per question. To quantify diversity, we compute pairwise similarity scores between the 30 responses using the MiniCPM embedding model (Hu et al. 2024). We then calculate the average and variance of these similarity scores to assess the diversity among responses. Figure 4(a) and Figure 4(b) present the average and vari- ance of pairwise similarities for each method, respectively. Responses sampled by LongMab-PO exhibit both lower av- erage similarity and higher variance compared to Direct- PO, Final-Dist, and Initial-Dist, and are comparable to those from Random sampling. This suggests that the multi-armed bandit strategy effectively explores a broader range of chunk combinations, enabling the LLM to generate more diverse responses. As a result, LongMab-PO provides a more varied and informative set of candidate responses for DPO training. 5.4 Impact of the Number of Selected Chunks in LongMab-PO In this section, we investigate how the number of"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 11, "text": "a broader range of chunk combinations, enabling the LLM to generate more diverse responses. As a result, LongMab-PO provides a more varied and informative set of candidate responses for DPO training. 5.4 Impact of the Number of Selected Chunks in LongMab-PO In this section, we investigate how the number of selected chunks, denoted as K, affects the rollout process in Multi- Armed Bandit-guided sampling. Specifically, we vary K from 1 to 5 and perform 30 rounds of sampling per ques- Model MuSiQue 2WikiMQA MFQA-En SubEM F1 SubEM F1 SubEM F1 K=1 44.00 46.81 76.50 64.81 21.33 49.09 K=2 48.00 47.55 73.00 61.73 26.00 50.29 K=3 47.50 48.05 75.50 67.61 23.33 49.00 K=4 50.00 52.15 76.00 68.60 26.00 51.26 K=5 47.00 48.65 73.50 68.58 22.66 49.90 Table 4: Impact of the Number of Selected Chunks in LongMab-PO. K denotes the number of selected chunks. 1 4 8 16 30 Rollout Steps 20 40 60 80 Recall (%) (a) Ground Truth Recall. 1 4 8 16 30 Rollout Steps 0 20 40 60 SubEM (%) T op-1 T op-2 T op-3 T op-4 T op-5 (b) SubEM of Responses. Figure 5: Ground Truth Recall and SubEM Scores with Varying Numbers of Selected Chunks (K). tion to optimize the LLM via DPO training. As shown in Table 4, increasing the number of selected chunks leads to performance gains across the given evalua- tion datasets, with the best results achieved at K = 4. This indicates that selecting too few chunks may fail to provide sufficient evidence to support high-quality response gener- ation. Conversely, when K increases to 5, performance de- clines slightly, indicating that K = 4 strikes a good balance between evidence coverage and noise control. Including too many chunks may introduce irrelevant or noisy information, which can hinder the effectiveness of DPO training. To further understand this behavior, we analyze the trends of ground truth recall and SubEM scores along the rollout process under different values of K. As illustrated in Fig- ure 5, settings with K = 4 and K = 5 achieve higher recall scores, indicating that selecting more chunks increases the chance of including golden evidence. However, despite the higher recall at K = 5, the SubEM score is lower than that at K = 4. This further supports the observation that exces- sive evidence may introduce noise, diminishing the model’s ability to generate accurate responses. Additionally, when using K = 5, both the recall and SubEM scores tend to plateau during the rollout process, indicating a reduction in response diversity as the training progresses. 6 Conclusion This paper introduces LongMab-PO, an innovative frame- work for optimizing long-context language models through the Multi-Armed Bandit (MAB) approach. LongMab-PO leverages the iterative bandit rollout process to identify and select informative context chunks, enabling the sampling of high-quality and diverse responses. These sampled re- sponses are then used to create preference data pairs for DPO training. In this way, the optimized LLM can bet- ter identify key information in long contexts, thereby gen- erating correct reasoning results. Our experiments show that LongMab-PO outperforms"}
{"doc_id": "2508.13993v1", "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13993v1", "chunk_id": 12, "text": "enabling the sampling of high-quality and diverse responses. These sampled re- sponses are then used to create preference data pairs for DPO training. In this way, the optimized LLM can bet- ter identify key information in long contexts, thereby gen- erating correct reasoning results. Our experiments show that LongMab-PO outperforms existing baseline methods in long-context understanding tasks."}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 0, "text": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation Tianyi Niu Jaemin Cho Elias Stengel-Eskin Mohit Bansal UNC Chapel Hill {tianyin4, jmincho, esteng, mbansal}@cs.unc.edu Abstract We investigate to what extent Multimodal Large Language Models (MLLMs) can accu- rately identify the orientation of input images rotated 0°, 90°, 180°, and 270°. This task de- mands robust visual reasoning capabilities to detect rotational cues and contextualize spa- tial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce ROTBENCH – a 350- image manually-filtered benchmark compris- ing lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information – including captions, depth maps, and more – or using chain-of- thought prompting offers only small and in- consistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0°) images, while certain models are able to identify upside-down (180°) images. None can reliably distinguish between 90° and 270°. Simultaneously showing the image ro- tated in different orientations leads to moder- ate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models’ ability to distinguish 90° and 270° rotations, de- spite substantially improving the identification of 180° images. Together, these results reveal a significant gap between MLLMs’ spatial rea- soning capabilities and human perception in identifying rotation.1 1 Introduction Advancements in Multimodal Large Language Models (MLLMs) have led to increased perfor- mance in complex visual tasks, such as image-text retrieval, image segmentation, and visual question 1Code and data: https://github.com/tianyiniu/RotBench D. 270° How many degrees counter-clockwise has this image been rotated? A. 0 B. 90 C. 180 D. 270 A. 0° B. 90° C. 180° Figure 1: We present two ROTBENCH images: one (left) to Gemini-2.5-Pro, the other (right) to GPT-5. Humans can easily identify the correct rotation of the two images, but both models fail to do so. answering (Li et al., 2025; Chen et al., 2023a; Ravi et al., 2024; Fu et al., 2024a; Chen et al., 2023b; Bai et al., 2025; OpenAI, 2025c; Gemini Team, 2025; Liu et al., 2023a; Deitke et al., 2024). How- ever, a growing body of recent work suggests that MLLMs are sensitive to simple image transforma- tions (Anis et al., 2025), such as rotations, flips, and blurs, and they fail on tasks that are intuitive to humans (Fu et al., 2024b; Pothiraj et al., 2025; Tong et al., 2024). Downstream tasks involving a rotating camera – such as robotic arm manipulation or first-person extreme sports analysis – require MLLMs to demonstrate robust spatial reasoning, regardless of image orientation. Given these chal- lenges, this work explores a fundamental question: can MLLMs identify image orientation? Humans can quickly recognize whether an im- age has been rotated (Shepard and Metzler, 1971; Vandenberg and Kuse, 1978); for example, it is easy for us to recognize that the left image"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 1, "text": "of image orientation. Given these chal- lenges, this work explores a fundamental question: can MLLMs identify image orientation? Humans can quickly recognize whether an im- age has been rotated (Shepard and Metzler, 1971; Vandenberg and Kuse, 1978); for example, it is easy for us to recognize that the left image in Fig. 1 is not upright. The human viewer can use the orien- tation of the couch in the background to infer that 1 How many degrees counter-clockwise has this image been rotated? A. 0 B. 90 C. 180 D. 270 Rotate 0° 0° : 1.00 Rotate 90° Rotate 180° Rotate 270° Predictions 0° 0° 0° 180° 90° 90° 180° 180° 180° 90° 90° 90° Accuracy 90° : 0.66 180°: 1.00 270°: 0.00 Predictions Predictions Predictions Predictions Original Images Auxiliary Information + A woman in a blue dress is standing at a desk, using a laptop. She is wearing glasses and has long hair. The background features a brick wall and a framed picture. The setting appears to be an office or a workspace. [\"woman in blue dress\", \"facing left\", \"desk\"], [\"woman in blue dress\", \"above\", \"desk\"], [\"woman in blue dress\", \"left\", \"lamp\"], ... d (Optional) Auxiliary Information Bounding Boxes + Scene Graphs Depth Map Caption MLLM inputs Figure 2: ROTBENCH evaluation pipeline: for each image in ROTBENCH, we rotate the image 0°, 90°, 180°, and 270° counter-clockwise. We represent the rotation estimation problem as a multiple-choice question answering problem (Appendix L.5), and separately measure accuracy on each image orientation. We optionally provide different forms of auxiliary information to aid the model in identifying image rotation. We emphasize that all forms of auxiliary information are separately extracted for each rotation; the ground truth rotation is not marked. the father is actually lying on his back rather than standing up. The simple task of identifying image rotation requires reconciling the image’s subjects, background, and semantics. Here, we show that identifying rotation remains a challenge, even in frontier MLLMs. We introduce ROTBENCH (Section 3), a bench- mark for evaluating MLLMs’ ability to recog- nize rotation in images. ROTBENCH consists of images sampled from Spatial-MM (Shiri et al., 2024), and is comprised of two subsets: the 300-image ROTBENCH-LARGE and the 50-image ROTBENCH-SMALL. ROTBENCH is carefully con- structed to be challenging but fair, with a two-stage filtering procedure to remove images that are in- distinguishable under different degrees of rotation. Section 3 describes our dataset construction. Using ROTBENCH, we explore whether frontier MLLMs can identify rotation in input images ro- tated 0°, 90°, 180°, and 270° (Fig. 2). We also evaluate whether providing various forms of aux- iliary information or using chain-of-thought (Wei et al., 2022) prompting improves performance (Sec- tion 4.3). We find that models are able to consis- tently identify right-side-up (0°) images. However, only stronger models are able to identify upside- down (180°) images. All models fail to accurately distinguish between 90° and 270° images (Sec- tion 6.1, Section 6.2). We find adding auxiliary information offers minimal and inconsistent im- provement, often improving performance on 270° images at the expense of 90° images."}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 2, "text": "only stronger models are able to identify upside- down (180°) images. All models fail to accurately distinguish between 90° and 270° images (Sec- tion 6.1, Section 6.2). We find adding auxiliary information offers minimal and inconsistent im- provement, often improving performance on 270° images at the expense of 90° images. Leveraging MLLMs’ tendency to accurately identify 0° images, we attempt to improve perfor- mance by further rotating an input image 0°, 90°, 180°, 270° and presenting MLLMs with all rota- tions simultaneously. Using this approach, reason- ing models (o3 and Gemini-2.5-Pro) show perfor- mance improvements, while weaker models see per- formance degradation. Further extending this idea, we utilize a voting approach to algebraically obtain a majority vote for the correct ground truth orienta- tion. We obtain a model prediction for each further rotation by subtracting the added angle. While this approach does show significant performance improvements on weaker models, it lacks scala- bility as it requires multiple model calls for each prediction, and assumes a priori knowledge of all possible orientations (Appendix I). Finally, fine-tuning on out-of-domain data (Sec- tion 6.3) significantly improved performance on identifying 180° images but not 90° and 270°. In- terestingly, we find an oscillating pattern of perfor- mance changes between 90° and 270° as training progresses. Any performance improvement in 90° is matched with a degradation of 270° and vice versa. These results suggest the presence of two local optima hindering across-the-board progress. Our findings demonstrate the tested MLLMs significantly underperform compared to humans 2 when it comes to spatial reasoning involving rota- tion detection, highlighting the need for integrating rotation-awareness into modern training pipelines. 2 Related Work Sensitivity to visual perturbations. Previous work has shown that visual encoders and MLLMs are sensitive to simple image transformations. Anis et al. (2025) evaluate CLIP (Radford et al., 2021) and SigLIP (Zhai et al., 2023) on a suite of com- mon image transformations – rotations, flips, noise, etc. – revealing substantial gaps between human and model understanding. Usama et al. (2025) finds MLLMs exhibit distinct failure patterns in scene-text and object reasoning tasks when apply- ing ImageNet-C corruptions (Hendrycks and Di- etterich, 2019) to image inputs. Taken together, these studies highlight that despite strong clean- image performance, visual encoders and MLLMs are highly sensitive to photometric and geometric distortions. Robustness to image transformations. Past work has also examined various methods to en- sure that image transformations do not affect down- stream task performance. Mikołajczyk and Gro- chowski (2018), Shorten and Khoshgoftaar (2019), and Perez and Wang (2017) use image transfor- mations as data augmentation methods to improve downstream classifier robustness. Other works in- stead proposed alternative architectures and train- ing schemes to improve robustness to rotation (Xu et al., 2023; Cohen and Welling, 2016; Lee et al., 2023; Feng et al., 2019). While this line of work focuses on training models to ignore certain trans- formations and learn invariant features for down- stream tasks, we instead focus on identifying and reasoning about the transformation itself. Identifying visual perturbations. Past work has tested how well vision encoders and MLLMs can"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 3, "text": "et al., 2019). While this line of work focuses on training models to ignore certain trans- formations and learn invariant features for down- stream tasks, we instead focus on identifying and reasoning about the transformation itself. Identifying visual perturbations. Past work has tested how well vision encoders and MLLMs can identify which perturbation or transformation was applied to an input image (Lin et al., 2020; An et al., 2023; Rashtchian et al., 2023). Udandarao et al. (2024) show that visual encoders and MLLMs of- ten fail to classify which perturbation out of 27 per- turbations (e.g., contrast, brightness, rotation, blur) was applied to an input image on automatically- created datasets. In contrast, we provide a fine- grained analysis of rotation specifically through a 4-way angle classification task on a curated dataset with rigorous human verification, offering novel insights into why state-of-the-art modern MLLMs struggle with distinguishing specific rotation an- gles. Moreover, we test providing remedies such as chain-of-thought reasoning, various forms of aux- iliary information (e.g., captions, bounding boxes, segmentation maps, depth maps), and guiding rea- soning with algebraic calculations. We also analyze whether model performance is dependent on abili- ties such as identifying clockwise versus counter- clockwise rotation. Image and camera orientation estimation. Fine- tuning models to identify image orientation has been the focus of prior work (Xu et al., 2024). For example, Fischer et al. (2015) and Joshi and Guerzhoy (2017) focused on fine-tuning convolu- tional neural networks (CNNs) to estimate and iden- tify image rotation. While our work tackles a simi- lar task, we are instead interested in the problem as a test of general-purpose MLLMs’ inherent reason- ing abilities, i.e., whether they can estimate image rotation without extensive fine-tuning. Note that our work aligns more closely with the problem of image, not camera, orientation estimation. Camera orientation estimation is also a well-studied task in computer vision (Xian et al., 2019). Instead of predicting the rotation of the image, camera orienta- tion estimation seeks to predict the spatial location of the camera when capturing an image. Contem- porary approaches of this task use deep networks to directly predict orientation parameters from image features in an end-to-end manner (Xian et al., 2019; Lee et al., 2021, 2020). Spatial reasoning in MLLMs. Beyond robust- ness, spatial relation understanding is a notable weakness of current MLLMs. Kamath et al. (2023) curate the What’s Up benchmark to isolate “left- /right/above/below” relations, showing a significant gap in performance between humans and MLLMs. Shiri et al. (2024) further develop the Spatial-MM dataset and demonstrate that providing bounding boxes or scene graphs yields only modest gains. Both illustrate that MLLMs struggle with certain challenging cross-modal spatial reasoning tasks. Gap between human perception and MLLMs. A growing body of work shows MLLMs exhibit fundamental gaps compared to human perceptual capabilities. Pothiraj et al. (2025) propose CAP- TURe – a benchmark for occluded object counting – and report sharp drops in model accuracy on both synthetic and real images. Zhou et al. (2025) pro- poses MMVM, a benchmark for visual matching across images. Fu et al. (2024b) collect"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 4, "text": "human perceptual capabilities. Pothiraj et al. (2025) propose CAP- TURe – a benchmark for occluded object counting – and report sharp drops in model accuracy on both synthetic and real images. Zhou et al. (2025) pro- poses MMVM, a benchmark for visual matching across images. Fu et al. (2024b) collect BLINK, a 3 dataset comprised of visual tasks humans can solve in a ‘blink,’ such as identifying visual similarity and relative depth. Both Zhou et al. (2025) and Fu et al. (2024b) report low zero-shot accuracy on their respective tasks, suggesting MLLMs lack many of the intuitive reasoning mechanisms that underpin human visual perception. In this vein, our work provides a novel perspective to analyze and inter- pret the spatial reasoning capabilities of MLLMs, with results indicating that models struggle with this previously underexplored challenge. 3 ROTBENCH We introduce ROTBENCH, a benchmark for eval- uating models’ ability to identify rotation in input images. ROTBENCH is created using images from Spatial-MM (Shiri et al., 2024) and includes two subsets: the 300-image ROTBENCH-LARGE and the 50-image ROTBENCH-SMALL. While rotat- ing an image is straightforward, not all images are meaningful under rotation. A rotated portrait of a human, such as the image shown in Fig. 2, will be easily noticed by human viewers. However, a top-down view of a simple tabletop does not sig- nificantly differ when rotated (Fig. 9). We use a two-stage filtering process to ensure different ro- tations of each image are clearly distinguishable. This section provides an overview of our filtering procedure. Appendix A describes our dataset pro- cedure and statistics in further detail. Stage 1. We randomly sample 300 images from Spatial-MM (Shiri et al., 2024). Stage 1 involves a single annotator. Depending on the amount of visual signals available, the Stage 1 annotator de- cides to either accept, discard, or flag each im- age. Flagged images then proceed to Stage 2. We provide further examples and details of accepted, flagged, and discarded images in Fig. 9. Stage 2. Stage 2 involves a group of three human evaluators. Each flagged image is rotated 0°, 90°, 180°, and 270° counter-clockwise then presented to the evaluators as multiple-choice questions (Ap- pendix A.4). During Stage 2, an evaluator will see each image four times, once per orientation. Any image that elicits an incorrect answer from two or more evaluators across all four orientations is dis- carded. Otherwise, the image is accepted. Stage 1 and 2 are repeated until a total of 300 images are accepted. ROTBENCH-LARGE and ROTBENCH-SMALL. All images that have been accepted in Stage 1 and 2 are organized into ROTBENCH-LARGE. As each image is rotated in four orientations, obtaining hu- man performance on ROTBENCH-LARGE is costly. Luckily, Stage 2 provides a human baseline for the subset of ROTBENCH-LARGE images that have been flagged (25 images). We expand this subset by further sampling images from Spatial-MM. From these additional images, we only select images that fit the criteria for flagging to proceed to another round of Stage 2 evaluation. This process repeats until we reach a total of 50 images, organizing"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 5, "text": "have been flagged (25 images). We expand this subset by further sampling images from Spatial-MM. From these additional images, we only select images that fit the criteria for flagging to proceed to another round of Stage 2 evaluation. This process repeats until we reach a total of 50 images, organizing them into ROTBENCH-SMALL.2 4 Experiment Setup 4.1 Models We evaluate various open-weight and propri- etary MLLMs on ROTBENCH: Qwen-2.5-VL- 7B-Instruct (Bai et al., 2025); GPT-4o (OpenAI et al., 2024), GPT-4.1 (OpenAI, 2025a), o3 (Ope- nAI, 2025c), GPT-5 (OpenAI, 2025b), Gemini-2.0- Flash (Hassabis and Kavukcuoglu, 2024), Gemini- 2.5-Flash (Gemini Team, 2025), and Gemini-2.5- Pro (Gemini Team, 2025). Due to cost and resource limitations, we evaluate Gemini-2.5-Flash, Gemini- 2.5-Pro, GPT-4.1, GPT-5, and o3 on ROTBENCH- SMALL. Responses are obtained through greedy decoding, while all chain-of-thought (Wei et al., 2022) responses are obtained with a temperature of 0.3.3 4.2 Setup and Evaluation We rotate each image in ROTBENCH-LARGE and ROTBENCH-SMALL by 0°, 90°, 180°, and 270° counter-clockwise, resulting in a total of 1000 and 200 images. Note that a 90° counter-clockwise rotation is equivalent to a 270° clockwise rotation. For each image and orientation, we provide the model with the image, a brief description of the task, and various forms of auxiliary information (Section 4.3). We frame this task as a four-way classification problem. To ensure robustness, the mapping between letter choice and degree of rota- tion is randomized for each prompt. We evaluate models on ROTBENCH-LARGE and report average accuracy and standard deviation across 3 runs in Table 1. We use the same procedure, albeit with 2The human evaluators all exhibit high accuracy, averaging > 0.97 for all rotations. 3We perform an ablation study where we vary the sampling temperature (Table 8). We use the default sampling tempera- ture for proprietary models that do not expose a temperature parameter in their API interface. 4 Original Image Segmentation Maps Depth Maps Captions Scene Graphs Bounding Boxes 0° 90° 180° 270° Rotation Grid \"A woman in a blue sleeveless ....\" [woman in blue dress, smartphone, ... brick wall] \"A woman in a blue dress is standing ...\" [woman in blue dress, brick wall, ..., desk] [women in blue dress, glasses, ..., pen] \"A woman in a blue dress is using a ...\" [person, desk, ..., chair] \"A woman in a blue sleeveless top is ...\" Figure 3: Examples of the different types of auxiliary information provided to the models. only 2 runs, on ROTBENCH-SMALL and report re- sults in Table 2. All prompts used are available in Appendix L. 4.3 Auxiliary Information Figure 3 illustrates all forms of auxiliary informa- tion provided to the model. Note that all auxiliary information is separately extracted for each rota- tion, ensuring our approach does not depend on prior knowledge of the image’s orientation. Captions. For each image and rotation, we in- struct GPT-4o to provide a detailed caption (Ap- pendix L.2). We emphasize that each image is captioned four times, once per rotation. Bounding Boxes. For each image and rotation, we first use GPT-4o to extract the primary subjects"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 6, "text": "of the image’s orientation. Captions. For each image and rotation, we in- struct GPT-4o to provide a detailed caption (Ap- pendix L.2). We emphasize that each image is captioned four times, once per rotation. Bounding Boxes. For each image and rotation, we first use GPT-4o to extract the primary subjects within the image (Appendix L.1). Next, along with the image, the list of subjects is given to Ground- ingDINO (Liu et al., 2023b) to extract a set of normalized coordinates for each subject,4 which is directly injected into the prompt. Scene Graphs. A scene graph (Zhu et al., 2022) 4Each set of coordinates is a four-element tuple, composed of [x_min, y_min, x_max, y_max]. codifies relationships between objects in an image as a three-element tuple [object 1, predicate, object 2]. Using the extracted subjects from the previous section, we prompt GPT-4o to generate a scene graph for the image. Depth Maps. We obtain depth maps for each image using ZoeDepth (Bhat et al., 2023). Rather than rotating the depth map obtained from 0°, we separately obtain depth maps for all four rotations. Segmentation Maps. Using the previously ex- tracted bounding boxes, we obtain a segmentation map of each image and orientation using SAM 2 (Ravi et al., 2024). Chain-of-Thought. To evaluate whether our multiple-choice setup is hampering performance on this task, we modify the prompt to encourage the model to produce reasoning chains instead of a single letter choice. Rotation Grid. We test if explicitly allowing models to “visualize” rotations aid performance by providing the input image along with three copies of the image further rotated 90°, 180°, 270°. We compose these four images into a single rotation 5 grid. Each image is captioned with the degree of further rotation, independent of the ground truth rotation. We provide a further experiment (rotation grid guided) where we explicitly prompt the model to identify an \"anchor\" image and algebraically calculate the original image’s ground truth rotation. All rotation grid experiments use CoT prompting. 5 Main Results Table 1 displays the results of evaluating Qwen- 2.5-VL-7B-Instruct, Gemini-2.0-Flash, and GPT- 4o along with various auxiliary information on ROTBENCH-LARGE. Table 2 displays results of evaluating Qwen-2.5-VL-7B-Instruct, GPT-4o, GPT-4.1, GPT-5, o3, Gemini-2.0-Flash, Gemini- 2.5-Flash, Gemini-2.5-Pro on ROTBENCH-SMALL. Due to the high token cost of proprietary reason- ing models, we only evaluate zero-shot and CoT prompts. However, we evaluate providing rotation grids to o3 and Gemini-2.5-Pro.5 MLLMs accurately identify right-side-up (0°) images. All evaluated models effectively recog- nize right-side-up (0°) images. Qwen-2.5-VL-7B- Instruct (Qwen) achieves an accuracy of 0.99 with- out supplemental data. Proprietary models (GPT- 4o, GPT-4.1, o3, Gemini-2.5-Flash, Gemini-2.5- Pro, and Gemini-2.0-Flash) consistently exhibit near-perfect accuracy on identifying unrotated im- ages. This outcome aligns with expectations, given these models likely encountered predominantly up- right images during training, and thus 0° can be assumed to be the default option. Proprietary models perform well on upside- down (180°) images. All models except Qwen demonstrate robust performance on images rotated 180°, with GPT-4o, GPT-4.1, o3, and Gemini-2.5- Pro all achieving accuracies notably above chance (> 0.7). However, Gemini-2.0-Flash and Gemini- 2.5-Flash"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 7, "text": "thus 0° can be assumed to be the default option. Proprietary models perform well on upside- down (180°) images. All models except Qwen demonstrate robust performance on images rotated 180°, with GPT-4o, GPT-4.1, o3, and Gemini-2.5- Pro all achieving accuracies notably above chance (> 0.7). However, Gemini-2.0-Flash and Gemini- 2.5-Flash display relatively lower zero-shot per- formance, with accuracies around 0.5. This indi- cates that state-of-the-art proprietary models gen- erally possess a reliable capability to recognize upside-down images, though there remains vari- ability within the different model families. Identifying 90° and 270° is challenging for all evaluated models. All models exhibit substantial difficulties when distinguishing between 90° and 5Table 5 further shows results obtained from evaluating these two models on ROTBENCH-SMALL with all available auxiliary information. 270°, while the poorest performance consistently emerged with 270° images. Confusion matrix anal- ysis (Section 6.1) reveals frequent misclassifica- tions between these two orientations, indicating a distinct challenge when identifying 0° and 180°. Providing auxiliary inputs does not reliably im- prove performance. None of the auxiliary in- formation results in meaningful, consistent perfor- mance gains across all tested models. Paradoxi- cally, the introduction of additional information sometimes leads to marginal performance degrada- tion. When including all forms of auxiliary infor- mation, Qwen’s accuracy on 90° decreased from 0.51 to 0.26. Similarly, Gemini-2.0-Flash’s accu- racy on 270° decreased from 0.44 to 0.17. Rotation grid improves performance only for reasoning models. Providing the rotation grid degraded performance on most models. Gemini- 2.0-Flash accuracy on 90° decreased by nearly 0.5 compared to CoT prompting. Rotation grid guided, however, improved GPT-4o accuracy on 270° by around 0.1. At the same time, both o3 and Gemini- 2.5-Pro saw performance improvements. Notably, Gemini-2.5-Pro’s accuracy on 90° and 270° im- proved by 0.15. These results suggest the two rea- soning models are much more effective at utilizing visual context. We further leverage the robust iden- tification of 0° images and modify the rotation grid into a majority voting approach Appendix I. This setup results in gains even in weaker models, indi- cating MLLMs can achieve moderate performance with sufficient visual scaffolding. CoT improves 180° performance, but results are mixed on 90° and 270°. Employing chain- of-thought (CoT) prompting yields mixed results, differing notably across models and orientations. Gemini models show improved accuracy on 90° rotations but decreased accuracy for 270°. Con- versely, GPT models exhibit improved performance on 270° rotations, coupled with degraded accuracy for 90° rotations. However, CoT consistently en- hanced accuracy for 180° rotations across all mod- els tested. These findings suggest CoT prompting can help models better reason about rotations to some extent, but does not universally resolve in- herent challenges in distinguishing portrait orienta- tions. Appendix F examines a common failure of CoT in detail. 6 Accuracy on Different Degrees of Rotation Model 0° 90° 180° 270° Qwen-2.5-VL-7B-Instruct Zero-shot 0.99±0.00 0.51±0.01 0.05±0.01 0.09±0.01 + Caption 1.00±0.00 0.51±0.01 0.23±0.01 0.07±0.00 + Bounding Box 0.90±0.00 0.48±0.01 0.01±0.00 0.11±0.00 + Scene Graph 0.97±0.01 0.51±0.01 0.01±0.01 0.11±0.02 + Depth Map 0.93±0.01 0.55±0.02 0.04±0.01 0.13±0.02 + Segmentation Map 0.81±0.01 0.63±0.02 0.03±0.02 0.16±0.01 + Chain-of-Thought 0.88±0.01"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 8, "text": "of Rotation Model 0° 90° 180° 270° Qwen-2.5-VL-7B-Instruct Zero-shot 0.99±0.00 0.51±0.01 0.05±0.01 0.09±0.01 + Caption 1.00±0.00 0.51±0.01 0.23±0.01 0.07±0.00 + Bounding Box 0.90±0.00 0.48±0.01 0.01±0.00 0.11±0.00 + Scene Graph 0.97±0.01 0.51±0.01 0.01±0.01 0.11±0.02 + Depth Map 0.93±0.01 0.55±0.02 0.04±0.01 0.13±0.02 + Segmentation Map 0.81±0.01 0.63±0.02 0.03±0.02 0.16±0.01 + Chain-of-Thought 0.88±0.01 0.26±0.02 0.34±0.01 0.23±0.02 + Rotation Grid 0.57±0.04 0.15±0.02 0.13±0.01 0.28±0.00 + Rotation Grid Guided 0.59±0.01 0.12±0.01 0.13±0.00 0.30±0.02 + all above 0.47±0.03 0.26±0.01 0.17±0.01 0.33±0.02 Gemini-2.0-Flash Zero-shot 1.00±0.00 0.30±0.00 0.72±0.00 0.44±0.01 + Caption 1.00±0.00 0.38±0.00 0.76±0.00 0.44±0.01 + Bounding Box 1.00±0.00 0.43±0.03 0.71±0.00 0.34±0.02 + Scene Graph 1.00±0.00 0.41±0.00 0.71±0.00 0.33±0.02 + Depth Map 1.00±0.00 0.30±0.01 0.69±0.01 0.46±0.01 + Segmentation Map 1.00±0.00 0.28±0.01 0.73±0.02 0.45±0.00 + Chain-of-Thought 1.00±0.00 0.63±0.00 0.76±0.01 0.19±0.1 + Rotation Grid 1.00±0.00 0.07±0.01 0.57±0.01 0.07±0.01 + Rotation Grid Guided 1.00±0.00 0.10±0.00 0.61±0.00 0.25±0.01 + all above 1.00±0.00 0.1±0.01 0.67±0.01 0.17±0.01 GPT-4o Zero-shot 0.99±0.00 0.69±0.02 0.93±0.00 0.19±0.01 + Caption 0.98±0.00 0.65±0.00 0.93±0.01 0.23±0.02 + Bounding Box 0.98±0.01 0.59±0.01 0.91±0.00 0.31±0.04 + Scene Graph 0.98±0.00 0.55±0.00 0.93±0.00 0.33±0.02 + Depth Map 1.00±0.00 0.55±0.03 0.93±0.00 0.26±0.01 + Segmentation Map 0.97±0.00 0.67±0.01 0.95±0.00 0.21±0.00 + Chain-of-Thought 0.97±0.01 0.57±0.03 0.93±0.00 0.32±0.00 + Rotation Grid 0.98±0.00 0.71±0.02 0.93±0.00 0.19±0.03 + Rotation Grid Guided 0.98±0.00 0.46±0.03 0.93±0.00 0.41±0.00 + all above 1.00±0.00 0.46±0.03 0.91±0.00 0.36±0.02 Table 1: Classification accuracy using various forms of auxiliary information and prompting for all rotations. All results are obtained from three runs on ROTBENCH-LARGE. Accuracy is scored on a four-way classification task. 6 Additional Analyses 6.1 Model Bias Towards 0° and 90° 0° 90° 180° 270° Predicted 0° 90° 180° 270° Ground Truth 885 12 0 3 27 349 65 459 25 24 846 5 22 424 68 386 Per-Angle Error Analysis (GPT-4o) 0 100 200 300 400 500 600 700 800 Count Figure 4: Confusion matrix of true vs. predicted rota- tions for GPT-4o using CoT prompting, summed across three runs on ROTBENCH-LARGE. Rows represent ground-truth labels, columns represent predicted labels. To further elucidate the specific types of rota- tional errors made by models, we analyze the con- fusion matrix for GPT-4o. Figure 4 shows the con- fusion matrix obtained from summing predictions across three runs. We see GPT-4o predominantly struggles with differentiating between 90° and 270° rotations. Specifically, the model misclassifies 459 instances of 90° images as 270° and 424 instances of 270° images as 90°. Yet, 0° and 180° show sig- nificantly fewer misclassifications. This analysis underscores a critical shortcoming in model perfor- mance, suggesting either that the vision encoder is not providing sufficient signals to distinguish be- tween clockwise and counter-clockwise rotations, or that the MLLM is not adequately incorporating the visual information into its reasoning. 6.2 Distinguishing Clockwise from Counter-clockwise Rotations Setup. Following the analysis in Section 6.1, we specifically examine whether MLLMs can distin- 7 Accuracy on Different Degrees of Rotation Model 0° 90° 180° 270° Human Zero-shot 0.99 0.99 0.99 0.97 Qwen-2.5-VL-7B-Instruct Zero-shot 0.95±0.01 0.57±0.04 0.03±0.02 0.15±0.05 + Chain-of-Thought 0.86±0.00 0.20±0.04 0.13±0.03 0.30±0.04 GPT-4o Zero-shot 0.87±0.02 0.65±0.04 0.85±0.01 0.21±0.03 + Chain-of-Thought 0.91±0.01 0.59±0.01 0.86±0.00 0.21±0.05 GPT-4.1 Zero-shot 0.95±0.01 0.63±0.07 0.85±0.03 0.19±0.09 + Chain-of-Thought 0.98±0.00"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 9, "text": "Accuracy on Different Degrees of Rotation Model 0° 90° 180° 270° Human Zero-shot 0.99 0.99 0.99 0.97 Qwen-2.5-VL-7B-Instruct Zero-shot 0.95±0.01 0.57±0.04 0.03±0.02 0.15±0.05 + Chain-of-Thought 0.86±0.00 0.20±0.04 0.13±0.03 0.30±0.04 GPT-4o Zero-shot 0.87±0.02 0.65±0.04 0.85±0.01 0.21±0.03 + Chain-of-Thought 0.91±0.01 0.59±0.01 0.86±0.00 0.21±0.05 GPT-4.1 Zero-shot 0.95±0.01 0.63±0.07 0.85±0.03 0.19±0.09 + Chain-of-Thought 0.98±0.00 0.88±0.00 0.85±0.01 0.03±0.03 GPT-5 Zero-shot 1.00±0.00 0.41±0.03 0.81±0.01 0.59±0.04 + Chain-of-Thought 1.00±0.00 0.53±0.05 0.81±0.02 0.57±0.07 Gemini-2.0-Flash Zero-shot 1.00±0.00 0.25±0.04 0.48±0.01 0.43±0.07 + Chain-of-Thought 1.00±0.00 0.61±0.01 0.59±0.01 0.25±0.01 Gemini-2.5-Flash Zero-shot 1.00±0.00 0.23±0.06 0.50±0.03 0.47±0.02 + Chain-of-Thought 1.00±0.00 0.23±0.01 0.44±0.02 0.40±0.02 o3 Zero-shot 1.00±0.00 0.45±0.04 0.70±0.03 0.48±0.03 + Chain-of-Thought 1.00±0.00 0.36±0.00 0.74±0.04 0.57±0.01 + Rotation Grid 0.99±0.01 0.23±0.05 0.83±0.05 0.81±0.01 + Rotation Grid Guided 0.99±0.01 0.31±0.01 0.82±0.02 0.75±0.01 Gemini-2.5-Pro Zero-shot 1.00±0.00 0.50±0.04 0.72±0.00 0.40±0.06 + Chain-of-Thought 1.00±0.00 0.46±0.02 0.71±0.01 0.49±0.01 + Rotation Grid 0.99±0.01 0.58±0.00 0.67±0.03 0.59±0.03 + Rotation Grid Guided 0.95±0.01 0.71±0.01 0.73±0.03 0.74±0.04 Table 2: Classification accuracy of different image rotations for various models across two runs on ROTBENCH- SMALL using zero-shot or chain-of-thought prompting. We also show results from o3 and Gemini-2.5-Pro using rotation grids. Accuracy is scored on a four-way classification task. ccw The rotation is 90° clockwise (cw) or counter-clockwise (ccw)? Figure 5: GPT-4o answers incorrectly when asked to identify whether the image has been rotated 90° clock- wise or counter-clockwise. guish clockwise (CW) and counter-clockwise rota- tions (CCW). For this, we simplified the original four-way classification task into a binary classifi- cation task. Using the 90° and 270° images from ROTBENCH-LARGE, we ask MLLMs to determine whether an image has been rotated 90° CCW or CW.6 By asking models to explicitly differentiate the two directions of rotation, we hope to provide a clearer signal for directional understanding. Fig- ure 5 shows an example question where the GPT-4o provides an incorrect answer. Results. Table 3 tests CW vs. CCW classifica- tion on GPT-4o and Qwen-2.5-VL-7B-Instruct (we additionally provide similar smaller-scale tests on ROTBENCH-SMALL for GPT-5 in Appendix E). Table 3 indicates that GPT-4o has a significant bias toward identifying rotations as 90° counter- clockwise. GPT-4o correctly identified only 52 out of 300 clockwise rotations, whereas Qwen-2.5-VL- 7B-Instruct correctly identified only 23 out of 300 clockwise rotations. The models consistently de- fault to labeling ambiguous or uncertain rotations as counter-clockwise, hinting towards a potential 6Note that 90° CW is equivalent to a 270° CCW. 8 GT \\ Predicted Counter-clockwise Clockwise GPT-4o Counter-clockwise 248 52 Clockwise 259 41 Qwen-2.5-VL-7B-Instruct Counter-clockwise 270 30 Clockwise 277 23 Table 3: Accuracy of different models in identifying 90° clockwise (CW) versus counter-clockwise rotation (CCW). Column indicates ground truth rotation (GT). A small number of responses are incorrectly identified as being right-side up. These responses are excluded in the table’s statistics. underlying perceptual bias (Appendix F). While we find that the most recent model tested, GPT-5, has less bias and is better able to identify clockwise rotations, this improvement is not reflected in Ta- ble 2, where the model performs on par with others; in other words, poor performance on ROTBENCH cannot be solely explained by a model’s ability to distinguish clockwise from counterclockwise rota- tion."}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 10, "text": "has less bias and is better able to identify clockwise rotations, this improvement is not reflected in Ta- ble 2, where the model performs on par with others; in other words, poor performance on ROTBENCH cannot be solely explained by a model’s ability to distinguish clockwise from counterclockwise rota- tion. See Appendix E for further discussion. These findings strongly indicate that the MLLMs tested have limitations in reliably distinguishing be- tween CW and CCW rotational directions, offering a potential explanation for why distinguishing 90° and 270° CCW rotations presents such a challeng- ing task. Further corroborating this idea, we show that evaluating Qwen-2.5-VL-7B-Instruct, GPT-4o, o3 on ROTBENCH-SMALL using clockwise angles does not result in significant performance differ- ences (Appendix D). 6.3 Can Fine-tuning Solve ROTBENCH? 0 1000 2000 3000 4000 5000 6000 7000 8000 Number of images seen 0.2 0.4 0.6 0.8 1.0 Accuracy Accuracy on RotBench-large 0° 90° 180° 270° Figure 6: Qwen-2.5-VL-7B-Instruct’s accuracy on dif- ferent degrees of rotation as training progresses. Setup. To assess whether specialized training can mitigate these performance issues, we conduct fine-tuning experiments using Qwen-2.5-VL-7B- Instruct. During data filtering, we find some im- ages in Spatial-MM closely resemble each other (Fig. 11). If we fine-tune on images in Spatial- MM not selected for ROTBENCH, the existence of similar images in the training and testing sets may lead to inflated performance. To prevent such instances of overfitting, we train on 1000 images from MS COCO (Lin et al., 2015) and evaluate performance on ROTBENCH-LARGE. Appendix B provides further training details. Results. Fig. 6 reveals a high and consistent ac- curacy for 0° throughout training, indicating robust recognition of upright images. Performance on 180° gradually improves, stabilizing around 0.8 after approximately 7000 images. However, accu- racies for 90° and 270° exhibit substantial oscilla- tions, suggesting that the model did not achieve sta- ble improvement. The model appears to be caught in a cycle, alternating between accuracy gains and losses for these two rotations. This phenomenon is also reflected in our main results (Section 5), where using CoT prompting improved accuracy on 270° images at the expense of 90°. The unstable performance may result from potential represen- tational constraints in current visual encoders that limit visual understanding capabilities, particularly regarding subtle rotational distinctions. 7 Conclusion We evaluate whether MLLMs are able to identify input images rotated 0°, 90°, 180°, and 270° us- ing ROTBENCH, a 350-image manually-filtered benchmark. Our results reveal that state-of-the- art MLLMs reliably identify images that are un- rotated (0°) or upside-down (180°), but struggle with 90° and 270° rotations. Auxiliary informa- tion and chain-of-thought prompting provide lim- ited improvements. Simultaneously providing all possible rotations improves performance only for frontier reasoning models, but a modified setup using majority voting improves performance for weaker models as well. Fine-tuning Qwen-2.5- VL-7B-Instruct shows that performance oscillates between 90° and 270°, suggesting the presence of two local optima. These results indicate a poten- tial blind spot in MLLMs’ spatial reasoning capa- bilities, motivating future directions in improving orientational understanding. Acknowledgements This work was supported by DARPA ECOLE Program No. HR00112390060,"}
{"doc_id": "2508.13968v2", "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation", "date": "2025-08-19", "source": "http://arxiv.org/abs/2508.13968v2", "chunk_id": 11, "text": "Fine-tuning Qwen-2.5- VL-7B-Instruct shows that performance oscillates between 90° and 270°, suggesting the presence of two local optima. These results indicate a poten- tial blind spot in MLLMs’ spatial reasoning capa- bilities, motivating future directions in improving orientational understanding. Acknowledgements This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-CAREER 9 Award 1846185, NSF-AI Engage Institute DRL- 2112635, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, and a Bloomberg Data Science PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."}
