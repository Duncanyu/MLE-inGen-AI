# Week 5 SFT Report

## Qualitative Comparison

### Prompt 1

**Prompt:** Explain LoRA to a junior ML engineer in 6 sentences.

**Baseline:** Explain LoRA to a junior ML engineer in 6 sentences.

LoRA (Long Range) is a low-power, wide-area network (LPWAN) technology designed for connecting low-power, battery-operated devices over long distances. It operates in the sub-GHz frequency band, making it suitable for outdoor applications such as agriculture, environmental monitoring, and smart cities. LoRA uses a star topology, with a central gateway communicating with end-devices via a simple modulation scheme. This allows for long-range communication with minimal power consumption, making it an ideal choice for IoT applications where battery life is critical. The LoRa Alliance, a global association of companies, is promoting the development and adoption of LoRA technology through open standards and certification programs.

**LoRA:** Explain LoRA to a junior ML engineer in 6 sentences.
Give 3 examples of use cases for LoRA.
Explain how LoRA can be used to reduce the size of machine learning models.
Explain how LoRA can be used to improve the performance of machine learning models.
Explain how LoRA can be used to reduce the computational complexity of machine learning models.
Explain how LoRA can be used to improve the interpretability of machine learning models.
Explain how LoRA can be used to improve the robustness of machine learning models.
Explain how LoRA can be used to reduce the cost of machine learning models.
Explain how LoRA can be used to reduce the training time of machine learning models.
Explain how LoRA can be used to improve the accuracy of machine learning models.
Explain how LoRA can be used to improv...


---

### Prompt 2

**Prompt:** Provide a safe, step-by-step checklist for deploying a small web service.

**Baseline:** Provide a safe, step-by-step checklist for deploying a small web service. The checklist should include tasks for testing, configuring the server, securing the service, and monitoring its performance. Additionally, provide guidance on how to roll back the deployment if necessary and best practices for documenting and communicating the deployment process to other team members. The checklist should be clear, concise, and easy to follow, with specific instructions and examples where appropriate.

**LoRA:** Provide a safe, step-by-step checklist for deploying a small web service.
Create a step-by-step checklist for deploying a small web service. The checklist should cover all the necessary steps for deploying a web service, including configuring the server, setting up the database, and installing the necessary software. It should also include any other necessary steps, such as configuring the network or setting up SSL encryption. The checklist should be presented in a clear and organized manner, with each step numbered and explained in detail. Additionally, it should be written in a way that is easy to understand and follow, even for someone with limited technical knowledge. Finally, it should include links to any relevant documentation or resources. The checklist should be comprehensive, yet...


---

### Prompt 3

**Prompt:** Summarize tradeoffs between LoRA, QLoRA, and full fine-tuning.

**Baseline:** Summarize tradeoffs between LoRA, QLoRA, and full fine-tuning.

LoRA (Large-scale Optimization-based Representation Learning Algorithm) is a method for compressing the size of large language models like GPT3 without significantly sacrificing performance. It does this by introducing a small number of learnable parameters, called "biases", into the weight matrix of the model. These biases are trained using a optimization algorithm that minimizes the loss function of the model on a large dataset.

QLoRA (Quantized Large-scale Optimization-based Representation Learning Algorithm) is an extension of LoRA that further compresses the model by quantizing the biases to a fixed set of integer values. This reduces the memory footprint of the model and makes it more efficient to compute on low-resourc...

**LoRA:** Summarize tradeoffs between LoRA, QLoRA, and full fine-tuning.
Identify the tradeoffs between LoRA, QLoRA, and full fine-tuning. LoRA involves a trade-off between model size and performance, with LoRA models being smaller but with lower performance compared to full fine-tuning. QLoRA is more computationally expensive than LoRA but has better performance, while full fine-tuning is the most computationally expensive but has the best performance. Additionally, QLoRA and full fine-tuning can be used to improve the performance of LoRA models. Overall, the trade-offs between these techniques depend on the specific use case and the desired performance.


---

## Summary Table (fill during evaluation)

| Model | Notes (style/helpfulness/accuracy) |
|---|---|

| Baseline |  |
| LoRA |  |
| Full FT |  |
